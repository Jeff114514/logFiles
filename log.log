UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1901
    Start 1901: test_nanmedian

1901: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_nanmedian"
1901: Environment variables: 
1901:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1901: Test timeout computed to be: 10000000
1901: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1901: WARNING: Logging before InitGoogleLogging() is written to STDERR
1901: I0814 08:43:08.664366 11697 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1901: I0814 08:43:09.778450 11697 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=enable_record_memory,gpugraph_enable_hbm_table_collision_stat,gpugraph_load_node_list_into_hbm,static_executor_perfstat_filepath,auto_free_cudagraph_allocations_on_launch,pinned_memory_as_cpu_backend,gpu_allocator_retry_time,enable_fuse_parallel_matmul_pass,free_idle_chunk,cublaslt_exhaustive_search_times,use_system_allocator,sort_sum_gradient,nccl_blocking_wait,fraction_of_gpu_memory_to_use,cuda_malloc_async_pool_memory_throttle_ratio,cudnn_exhaustive_search,enable_graph_multi_node_sampling,save_static_runtime_data,gpugraph_force_device_batch_num_equal,cudnn_exhaustive_search_times,cublaslt_device_best_config,initial_gpu_memory_in_mb,alloc_fill_value,enable_cublas_tensor_op_math,use_pinned_memory,sync_after_alloc,gemm_use_half_precision_compute_type,call_stack_level,local_exe_sub_scope_limit,graph_metapath_split_opt,enable_api_kernel_fallback,op_dir,logging_trunc_pir_py_code,gpugraph_sparse_table_storage_mode,use_xqa_optim,memory_fraction_of_eager_deletion,fraction_of_cpu_memory_to_use,lapack_dir,enable_gpu_memory_usage_log_mb,new_executor_use_local_scope,multi_node_sample_use_gpu_table,search_cache_max_number,cusparselt_dir,new_executor_serial_run,cinn_compile_thread_num,apply_pass_to_program,enable_dump_main_program,enable_collect_shape,pir_apply_shape_optimization_pass,enable_auto_detect_gpu_topo,conv2d_disable_cudnn,use_fast_math,graph_load_in_parallel,max_inplace_grad_add,query_dest_rank_by_multi_node,print_allocator_trace_info,benchmark_nccl,enable_async_trace,curand_dir,embedding_deterministic,use_cuda_managed_memory,pir_broadcast_tree_limit,enable_pir_in_executor_trace_run,get_host_by_name_time,enable_all2all_use_fp16,prim_all,conv_workspace_size_limit,enable_adjust_op_order,disable_dyshape_in_train,cuda_memory_async_pool_realease_threshold,enable_cse_in_dy2st,jit_engine_type,mklml_dir,initial_cpu_memory_in_mb,use_auto_growth_pinned_allocator,inner_op_parallelism,enable_neighbor_list_use_uva,allow_cinn_ops,accuracy_check_atol_bf16,use_cinn,use_virtual_memory_auto_growth,logging_pir_py_code_dump_symbolic_dims,gpugraph_dedup_pull_push_mode,enable_tracker_all2all,pir_apply_inplace_pass,cusolver_dir,new_executor_sequential_run,tensor_operants_mode,run_kp_kernel,benchmark,paddle_num_threads,use_autotune,use_cuda_malloc_async_allocator,log_memory_stats,tensorrt_dir,accuracy_check_rtol_fp32,use_auto_growth_v2,gpu_memory_limit_mb,win_cuda_bin_dir,async_trace_count,enable_cinn_auto_tune,cse_max_count,prim_check_ops,init_allocated_mem,eager_delete_scope,print_ir,enable_interpretercore_launch_cinn,print_sub_graph_dir,fraction_of_cuda_pinned_memory_to_use,check_nan_inf,use_mkldnn,enable_cinn_compile_cache,nvidia_package_dir,static_runtime_data_save_path,cudnn_dir,use_stride_kernel,allocator_strategy,enable_blaslt_global_search,cublas_dir,ir_inplace_kernel_blacklist,prim_skip_dynamic,add_dependency_for_communication_op,pir_debug,free_when_no_cache_hit,enable_opt_get_features,fast_eager_deletion_mode,auto_growth_chunk_size_in_mb,cinn_subgraph_graphviz_dir,tracer_onednn_ops_off,gpugraph_merge_grads_segment_size,gpugraph_parallel_stream_num,gpugraph_offload_param_extends,cupti_dir,cudnn_deterministic,cusparse_dir,gpugraph_enable_gpu_direct_access,gpugraph_storage_mode,enable_fusion_fallback,low_precision_op_list,graph_embedding_split_infer_mode,prim_forward_blacklist,set_to_1d,new_executor_use_inplace,dygraph_debug,enable_pir_with_pt_in_dy2st,host_trace_level,prim_enabled,gpugraph_debug_gpu_memory,selected_gpus,gpugraph_enable_print_op_debug,enable_gpu_memory_usage_log,accuracy_check_rtol_bf16,npu_storage_format,enable_auto_rdma_trans,reallocate_gpu_memory_in_mb,dynamic_static_unified_comm,all_blocks_convert_trt,enable_unused_var_check,tracer_profile_fname,tracer_onednn_ops_on,deny_cinn_ops,graph_get_neighbor_id,accuracy_check_atol_fp32,accuracy_check_atol_fp16,manually_trans_conv_filter,fleet_executor_with_standalone,enable_dependency_builder_debug_info,prim_backward,new_executor_static_build,sync_nccl_allreduce,gpugraph_enable_segment_merge_grads,enable_pir_in_executor,cuda_dir,mkl_dir,eager_delete_tensor_gb,gpugraph_hbm_table_load_factor,convert_all_blocks,fuse_parameter_groups_size,dump_chunk_info,trt_ibuilder_cache,executor_log_deps_every_microseconds,custom_device_mem_record,enable_exit_when_partial_worker,gpugraph_slot_feasign_max_num,prim_forward,allreduce_record_one_event,dataloader_use_file_descriptor,enable_sparse_inner_gather,einsum_opt,reader_queue_speed_test_mode,dist_threadpool_size,check_nan_inf_level,check_kernel_launch,use_shm_cache,logging_pir_py_code_dir,multiple_of_cupti_buffer_size,use_stream_safe_cuda_allocator,gpugraph_parallel_copyer_split_maxsize,cudnn_batchnorm_spatial_persistent,pir_subgraph_saving_dir,cache_inference_while_scope,enable_cinn_accuracy_check,logging_pir_py_code_int_tensor_element_limit,nccl_dir,prim_enable_dynamic,enable_pir_api,fuse_parameter_memory_size,gpugraph_offload_param_stat,accuracy_check_rtol_fp16,new_executor_use_cuda_graph,graph_neighbor_size_percent,gpugraph_offload_gather_copy_maxsize,check_infer_symbolic 
1901: I0814 08:43:09.778566 11697 init.cc:108] After Parse: argc is 2
1901: I0814 08:43:14.888327 11697 scope.cc:202] Create variable X
1901: I0814 08:43:14.888420 11697 scope.cc:202] Create variable Out
1901: I0814 08:43:14.888438 11697 scope.cc:202] Create variable MedianIndex
1901: I0814 08:43:14.888620 11697 op_registry.cc:112] CreateOp directly from OpDesc is deprecated. It should only beused in unit tests. Use CreateOp(const OpDesc& op_desc) instead.
1901: I0814 08:43:14.889360 11697 allocator_facade.cc:212] selected allocator strategy:1
1901: I0814 08:43:14.889685 11697 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1901: I0814 08:43:17.946861 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:17.946941 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.947106 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:17.947114 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.948534 11697 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0814 08:43:17.948570 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.948623 11697 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0814 08:43:17.948632 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.949194 11697 pybind.cc:1827] need skip: 0
1901: I0814 08:43:17.949296 11697 pybind.cc:1827] need skip: 0
1901: I0814 08:43:17.949796 11697 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0814 08:43:17.950237 11697 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0814 08:43:17.950259 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0814 08:43:17.950367 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0814 08:43:17.950384 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0814 08:43:17.954114 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:17.954978 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:17.954999 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:17.955015 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:17.958333 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:17.958355 11697 scope.cc:202] Create variable feed
1901: I0814 08:43:17.958467 11697 program_interpreter.cc:243] New Executor is Running.
1901: I0814 08:43:17.958477 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:17.958487 11697 scope.cc:202] Create variable MedianIndex
1901: I0814 08:43:17.958500 11697 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x43a2ae90 type is 7
1901: I0814 08:43:17.958513 11697 scope.cc:202] Create variable Out
1901: I0814 08:43:17.958519 11697 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x43a2b400 type is 7
1901: I0814 08:43:17.958523 11697 scope.cc:202] Create variable Out@GRAD
1901: I0814 08:43:17.958526 11697 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x43a2b8b0 type is 7
1901: I0814 08:43:17.958531 11697 scope.cc:202] Create variable X
1901: I0814 08:43:17.958534 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x43a2c490 type is 7
1901: I0814 08:43:17.958539 11697 scope.cc:202] Create variable X@GRAD
1901: I0814 08:43:17.958540 11697 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x43a2c700 type is 7
1901: I0814 08:43:17.958545 11697 scope.cc:202] Create variable _generated_var_0
1901: I0814 08:43:17.958547 11697 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x43a2c940 type is 7
1901: I0814 08:43:17.958552 11697 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0814 08:43:17.958554 11697 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x43a2cba0 type is 7
1901: I0814 08:43:17.958559 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x43a2b800 type is 9
1901: I0814 08:43:17.958563 11697 scope.cc:202] Create variable fetch
1901: I0814 08:43:17.958566 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43a2c920 type is 10
1901: I0814 08:43:17.958711 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:17.958720 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:17.958725 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:17.958730 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: W0814 08:43:17.959453 11697 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1901: I0814 08:43:17.959826 11697 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1901: W0814 08:43:17.961019 11697 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1901: I0814 08:43:17.961290 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.961333 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.961524 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.961535 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.961562 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.966522 11697 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966552 11697 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966575 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.966675 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.966801 11697 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966812 11697 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966874 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.966902 11697 interpreter_util.cc:647] Standalone Executor is Used.
1901: I0814 08:43:17.966940 11697 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966949 11697 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.966969 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0814 08:43:17.967097 11697 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.967109 11697 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.967129 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0814 08:43:17.967262 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:17.967296 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:17.967332 11697 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:17.967343 11697 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x43aa9420Variable Type 7
1901: I0814 08:43:17.967386 11697 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:17.967414 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:17.967468 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.967492 11697 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:17.967650 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:17.967693 11697 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:17.968425 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.968482 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:17.970160 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:17.970189 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.970253 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:17.970263 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.971087 11697 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0814 08:43:17.971109 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.971145 11697 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0814 08:43:17.971153 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.971518 11697 pybind.cc:1827] need skip: 0
1901: I0814 08:43:17.971585 11697 pybind.cc:1827] need skip: 0
1901: I0814 08:43:17.971967 11697 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0814 08:43:17.972064 11697 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0814 08:43:17.972074 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0814 08:43:17.972146 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0814 08:43:17.972158 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0814 08:43:17.974661 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:17.975281 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:17.975307 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:17.975314 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:17.979737 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:17.979765 11697 scope.cc:202] Create variable feed
1901: I0814 08:43:17.979823 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:17.979835 11697 scope.cc:202] Create variable MedianIndex
1901: I0814 08:43:17.979840 11697 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x43604750 type is 7
1901: I0814 08:43:17.979849 11697 scope.cc:202] Create variable Out
1901: I0814 08:43:17.979856 11697 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x439e76f0 type is 7
1901: I0814 08:43:17.979861 11697 scope.cc:202] Create variable Out@GRAD
1901: I0814 08:43:17.979864 11697 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x435e71e0 type is 7
1901: I0814 08:43:17.979868 11697 scope.cc:202] Create variable X
1901: I0814 08:43:17.979871 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4a2c220 type is 7
1901: I0814 08:43:17.979876 11697 scope.cc:202] Create variable X@GRAD
1901: I0814 08:43:17.979878 11697 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x4a2c320 type is 7
1901: I0814 08:43:17.979883 11697 scope.cc:202] Create variable _generated_var_0
1901: I0814 08:43:17.979887 11697 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x43a09ba0 type is 7
1901: I0814 08:43:17.979890 11697 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0814 08:43:17.979893 11697 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x43a09c50 type is 7
1901: I0814 08:43:17.979897 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x439e75b0 type is 9
1901: I0814 08:43:17.979902 11697 scope.cc:202] Create variable fetch
1901: I0814 08:43:17.979905 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43a09b80 type is 10
1901: I0814 08:43:17.980038 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:17.980049 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:17.980055 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:17.980064 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:17.980144 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980170 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980254 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980263 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980283 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:17.980937 11697 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980957 11697 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.980973 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.981045 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.981120 11697 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981130 11697 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981176 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.981217 11697 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981227 11697 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981245 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0814 08:43:17.981364 11697 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981379 11697 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981397 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0814 08:43:17.981519 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:17.981534 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:17.981551 11697 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:17.981559 11697 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x43aee910Variable Type 7
1901: I0814 08:43:17.981582 11697 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:17.981604 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:17.981628 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:17.981644 11697 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:17.981714 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:17.981742 11697 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:17.982295 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0814 08:43:17.982348 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:17.984380 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:17.984644 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: I0814 08:43:17.984700 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:17.985790 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:17.985864 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:17.986497 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x43a2b8d0)  to GradNodeAccumulation (addr: 0x43a01b20)
1901: I0814 08:43:17.986663 11697 dygraph_functions.cc:51757] Running AD API: mean
1901: I0814 08:43:17.986696 11697 dygraph_functions.cc:51814] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:17.986797 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.986822 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from MeanGradNode (addr: 0x42ccb850)  to NanmedianGradNode (addr: 0x43a2b8d0)
1901: I0814 08:43:17.986958 11697 backward.cc:442] Run in Backward
1901: I0814 08:43:17.986970 11697 backward.cc:113] Start Backward
1901: I0814 08:43:17.986996 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:17.987062 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.987098 11697 backward.cc:255] Preparing GradNode:MeanGradNode addr:0x42ccb850
1901: I0814 08:43:17.987115 11697 nodes.cc:25338] Running AD API GRAD: mean_grad
1901: I0814 08:43:17.987161 11697 nodes.cc:25394] { Input: [ 
1901: ( grad_out , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:17.987246 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:17.987279 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:17.987291 11697 backward.cc:335] Node: MeanGradNode addr:0x42ccb850, Found pending node: NanmedianGradNode addr: 0x43a2b8d0
1901: I0814 08:43:17.987311 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:17.987354 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x43a2b8d0
1901: I0814 08:43:17.987370 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:17.987401 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:17.987464 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:17.987493 11697 backward.cc:335] Node: NanmedianGradNode addr:0x43a2b8d0, Found pending node: GradNodeAccumulation addr: 0x43a01b20
1901: I0814 08:43:17.987502 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:17.987519 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43a01b20
1901: I0814 08:43:17.987531 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:17.987541 11697 accumulation_node.cc:40] Move Tensor ptr: 0x42385d10
1901: I0814 08:43:17.987545 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:17.987550 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: /home/code/Paddle/build/python/paddle/base/framework.py:667: VisibleDeprecationWarning: [93m
1901: Warning:
1901: API "paddle.base.dygraph.tensor_patch_methods.gradient" is deprecated since 2.1.0, and will be removed in future versions.
1901:     Reason: Please use tensor.grad, which returns the tensor value of the gradient. [0m
1901:   return func(*args, **kwargs)
1901: I0814 08:43:17.998417 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:17.998734 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:17.998796 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.mean" (%1) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1901:     (%5) = "pd_op.full_like" (%3, %4) {dtype:(pd_op.DataType)float16,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<1xf32>) -> builtin.tensor<f16>
1901:     (%6) = "pd_op.mean_grad" (%1, %5) {axis:(pd_op.IntArray)[],keepdim:false,reduce_all:false,stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%7) = "pd_op.nanmedian_grad" (%0, %2, %6) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false]} : (builtin.tensor<100x100xf16>, builtin.tensor<2xi64>, builtin.tensor<f16>) -> builtin.tensor<100x100xf16>
1901:     (%8) = "pd_op.fetch" (%7) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<100x100xf16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: I0814 08:43:18.082995 11697 pir_interpreter.cc:161] PirInterpreter(): 0x45cd5c80 on Place(gpu:0)
1901: I0814 08:43:18.083084 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.083118 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_1
1901: I0814 08:43:18.083129 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_2
1901: I0814 08:43:18.083140 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_3
1901: I0814 08:43:18.083151 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_4
1901: I0814 08:43:18.083159 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_5
1901: I0814 08:43:18.083168 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_6
1901: I0814 08:43:18.083174 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_7
1901: I0814 08:43:18.083184 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_8
1901: I0814 08:43:18.083191 11697 scope.cc:202] Create variable 0x45cd5c801723624998083065438_inner_var_9
1901: I0814 08:43:18.083200 11697 scope.cc:202] Create variable fetch0@fetch
1901: I0814 08:43:18.083756 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0814 08:43:18.083776 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.083781 11697 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: I0814 08:43:18.083837 11697 pir_interpreter.cc:1455] New Executor is Running ...
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.mean ( 2 ) 
1901: 3: ( 5 )  = pd_op.full
1901: 4: ( 6 )  = pd_op.full_like ( 5 )  ( 4 ) 
1901: 5: ( 7 )  = pd_op.mean_grad ( 6 )  ( 2 ) 
1901: 6: ( 8 )  = pd_op.nanmedian_grad ( 7 )  ( 3 )  ( 1 ) 
1901: 7: ( 9 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 8: ( 10 )  = pd_op.fetch ( 9 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x45cd4380
1901: 1 -> 0x45cd5c801723624998083065438_inner_var_1 -> 0x45cd5ab0
1901: 2 -> 0x45cd5c801723624998083065438_inner_var_2 -> 0x45cd1cf0
1901: 3 -> 0x45cd5c801723624998083065438_inner_var_3 -> 0x45cd5370
1901: 4 -> 0x45cd5c801723624998083065438_inner_var_4 -> 0x45cd4210
1901: 5 -> 0x45cd5c801723624998083065438_inner_var_5 -> 0x45cd6510
1901: 6 -> 0x45cd5c801723624998083065438_inner_var_6 -> 0x45cd6930
1901: 7 -> 0x45cd5c801723624998083065438_inner_var_7 -> 0x45cd6d50
1901: 8 -> 0x45cd5c801723624998083065438_inner_var_8 -> 0x45cd5c60
1901: 9 -> 0x45cd5c801723624998083065438_inner_var_9 -> 0x45cd7170
1901: 10 -> fetch0@fetch -> 0x45cd7980
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 4 
1901: 3 -> 4 
1901: 4 -> 5 
1901: 5 -> 6 
1901: 6 -> 7 
1901: 7 -> 8 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->3[pd_op.full]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.mean]->
1901: 2 downstreams: 
1901: 3 downstreams: 4[pd_op.full_like]->
1901: 4 downstreams: 5[pd_op.mean_grad]->
1901: 5 downstreams: 6[pd_op.nanmedian_grad]->
1901: 6 downstreams: 7[pd_op.memcpy_d2h]->
1901: 7 downstreams: 8[pd_op.fetch]->
1901: 8 downstreams: 
1901: I0814 08:43:18.084889 11697 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1901: I0814 08:43:18.105445 11738 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.105547 11738 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x45cd5c801723624998083065438_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.105685 11738 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x45cd5c801723624998083065438_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1901: I0814 08:43:18.105763 11736 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.106361 11737 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.107371 11739 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.107457 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.107539 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.107607 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cd5c801723624998083065438_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_3:[dtype=;place=;dim=;lod={};, 0x45cd5c801723624998083065438_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108183 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cd5c801723624998083065438_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x45cd5c801723624998083065438_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.108212 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x45cd5c801723624998083065438_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108268 11739 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.108281 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x45cd5c801723624998083065438_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.108311 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x45cd5c801723624998083065438_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x45cd5c801723624998083065438_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108363 11739 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.108386 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x45cd5c801723624998083065438_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x45cd5c801723624998083065438_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.108410 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x45cd5c801723624998083065438_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x45cd5c801723624998083065438_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108458 11739 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.108474 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x45cd5c801723624998083065438_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x45cd5c801723624998083065438_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.108492 11739 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x45cd5c801723624998083065438_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x45cd5c801723624998083065438_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x45cd5c801723624998083065438_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108551 11739 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x45cd5c801723624998083065438_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x45cd5c801723624998083065438_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x45cd5c801723624998083065438_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.108656 11734 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0814 08:43:18.108855 11735 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.108913 11735 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cd5c801723624998083065438_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.108935 11735 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.109041 11735 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cd5c801723624998083065438_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cd5c801723624998083065438_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.109068 11735 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x45cd5c801723624998083065438_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.109094 11735 tensor_utils.cc:57] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.109130 11735 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x45cd5c801723624998083065438_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.109158 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x45cd5df0) got event_name: TaskCompletion
1901: I0814 08:43:18.109184 11697 tensor_util.cc:48] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.113248 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.113308 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.113395 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.113404 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.115072 11734 thread_data_registry.h:135] Add data {current : -20004, peak : 0} from thread 6852239066212050991 to 13619009410064744616 , after update, data is {current : 19996, peak : 40000}.
1901: I0814 08:43:18.115105 11734 thread_data_registry.h:135] Add data {current : -20024, peak : 0} from thread 6852239066212050991 to 5783759323292387178 , after update, data is {current : 0, peak : 330659}.
1901: I0814 08:43:18.115267 11735 thread_data_registry.h:135] Add data {current : 19996, peak : 40000} from thread 13619009410064744616 to 5783759323292387178 , after update, data is {current : 19996, peak : 40000}.
1901: I0814 08:43:18.115509 11738 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5564527677899807048 to 5783759323292387178 , after update, data is {current : 20000, peak : 40000}.
1901: I0814 08:43:18.122556 11739 thread_data_registry.h:135] Add data {current : 20000, peak : 40000} from thread 5783759323292387178 to 6465755474443411432 , after update, data is {current : 20000, peak : 40000}.
1901: I0814 08:43:18.122599 11739 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 5783759323292387178 to 6465755474443411432 , after update, data is {current : 120000, peak : 440631}.
1901: I0814 08:43:18.124675 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.125293 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.125947 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.125967 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.125974 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.128672 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:18.128695 11697 scope.cc:202] Create variable feed
1901: I0814 08:43:18.128757 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:18.128765 11697 scope.cc:202] Create variable MedianIndex
1901: I0814 08:43:18.128772 11697 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x45cf1780 type is 7
1901: I0814 08:43:18.128782 11697 scope.cc:202] Create variable Out
1901: I0814 08:43:18.128787 11697 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x45cf0d10 type is 7
1901: I0814 08:43:18.128793 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.128796 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x45cf1700 type is 7
1901: I0814 08:43:18.128801 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x45cf0d30 type is 9
1901: I0814 08:43:18.128808 11697 scope.cc:202] Create variable fetch
1901: I0814 08:43:18.128811 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf1b30 type is 10
1901: I0814 08:43:18.128908 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:18.128913 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.128918 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.128924 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.129001 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.129022 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.129114 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.129123 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.129148 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.129797 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.129820 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.129842 11697 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.129849 11697 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45cf5b10Variable Type 7
1901: I0814 08:43:18.129874 11697 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.129897 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.129927 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.129945 11697 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.130002 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.130030 11697 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:18.130087 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.130096 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.130115 11697 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.130120 11697 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45cf97f0Variable Type 7
1901: I0814 08:43:18.130134 11697 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.130147 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.130165 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.130177 11697 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.130213 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.130245 11697 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0814 08:43:18.130712 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.130743 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.132628 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.132658 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.132728 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.132736 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.135216 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.135839 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.136377 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.136390 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.136395 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.143199 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:18.143385 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:18.143401 11697 scope.cc:202] Create variable MedianIndex
1901: I0814 08:43:18.143410 11697 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x45d28350 type is 7
1901: I0814 08:43:18.143420 11697 scope.cc:202] Create variable Out
1901: I0814 08:43:18.143424 11697 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x45d27680 type is 7
1901: I0814 08:43:18.143429 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.143433 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x45d28080 type is 7
1901: I0814 08:43:18.143438 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x45cf0d30 type is 9
1901: I0814 08:43:18.143445 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf1b30 type is 10
1901: I0814 08:43:18.143556 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:18.143563 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.143569 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.143574 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.143651 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.143671 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.143761 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.143769 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.143791 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.144444 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.144459 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.144480 11697 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.144486 11697 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45d2c5f0Variable Type 7
1901: I0814 08:43:18.144512 11697 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.144534 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.144562 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.144578 11697 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.144626 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.144654 11697 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:18.144706 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.144713 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.144731 11697 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.144737 11697 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45d2c610Variable Type 7
1901: I0814 08:43:18.144752 11697 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.144764 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.144781 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.144794 11697 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.144829 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.144851 11697 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0814 08:43:18.145203 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.145234 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.307181 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: I0814 08:43:18.307705 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.307780 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.308475 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.308539 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.309934 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: I0814 08:43:18.310109 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.310165 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.311527 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.311576 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.314286 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: I0814 08:43:18.314530 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.314590 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0814 08:43:18.318490 11697 pir_interpreter.cc:161] PirInterpreter(): 0x45cdcbd0 on Place(gpu:0)
1901: I0814 08:43:18.318534 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.318559 11697 scope.cc:202] Create variable 0x45cdcbd01723624998318521134_inner_var_1
1901: I0814 08:43:18.318567 11697 scope.cc:202] Create variable 0x45cdcbd01723624998318521134_inner_var_2
1901: I0814 08:43:18.318575 11697 scope.cc:202] Create variable 0x45cdcbd01723624998318521134_inner_var_3
1901: I0814 08:43:18.318583 11697 scope.cc:202] Create variable 0x45cdcbd01723624998318521134_inner_var_4
1901: I0814 08:43:18.318593 11697 scope.cc:202] Create variable fetch0@fetch
1901: I0814 08:43:18.319056 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0814 08:43:18.319067 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.319072 11697 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x45d079d0
1901: 1 -> 0x45cdcbd01723624998318521134_inner_var_1 -> 0x45d4f340
1901: 2 -> 0x45cdcbd01723624998318521134_inner_var_2 -> 0x43aa7450
1901: 3 -> 0x45cdcbd01723624998318521134_inner_var_3 -> 0x45cf90d0
1901: 4 -> 0x45cdcbd01723624998318521134_inner_var_4 -> 0x43ade720
1901: 5 -> fetch0@fetch -> 0x45d07a10
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0814 08:43:18.320353 11744 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.320371 11746 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.320478 11746 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.320545 11746 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.320593 11746 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cdcbd01723624998318521134_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_3:[dtype=;place=;dim=;lod={};, 0x45cdcbd01723624998318521134_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.321204 11746 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cdcbd01723624998318521134_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x45cdcbd01723624998318521134_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.320371 11742 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.324347 11745 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.324376 11744 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cdcbd01723624998318521134_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.324442 11744 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.324548 11744 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cdcbd01723624998318521134_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cdcbd01723624998318521134_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.324581 11744 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x45cdcbd01723624998318521134_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.324600 11744 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.324613 11744 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x45cdcbd01723624998318521134_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.320371 11743 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.320372 11741 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0814 08:43:18.328361 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x45cdcd40) got event_name: TaskCompletion
1901: I0814 08:43:18.328408 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.329445 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.329667 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.329730 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0814 08:43:18.333258 11697 pir_interpreter.cc:161] PirInterpreter(): 0x45cf7a40 on Place(gpu:0)
1901: I0814 08:43:18.333312 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.333338 11697 scope.cc:202] Create variable 0x45cf7a401723624998333289452_inner_var_1
1901: I0814 08:43:18.333346 11697 scope.cc:202] Create variable 0x45cf7a401723624998333289452_inner_var_2
1901: I0814 08:43:18.333355 11697 scope.cc:202] Create variable 0x45cf7a401723624998333289452_inner_var_3
1901: I0814 08:43:18.333364 11697 scope.cc:202] Create variable 0x45cf7a401723624998333289452_inner_var_4
1901: I0814 08:43:18.333379 11697 scope.cc:202] Create variable fetch0@fetch
1901: I0814 08:43:18.333845 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0814 08:43:18.333858 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.333863 11697 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x45cfaac0
1901: 1 -> 0x45cf7a401723624998333289452_inner_var_1 -> 0x45cfab00
1901: 2 -> 0x45cf7a401723624998333289452_inner_var_2 -> 0x45ce5550
1901: 3 -> 0x45cf7a401723624998333289452_inner_var_3 -> 0x45cd77b0
1901: 4 -> 0x45cf7a401723624998333289452_inner_var_4 -> 0x45cfab20
1901: 5 -> fetch0@fetch -> 0x45d28980
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0814 08:43:18.340389 11750 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.341379 11752 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.341462 11752 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.341529 11752 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.341575 11752 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cf7a401723624998333289452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_3:[dtype=;place=;dim=;lod={};, 0x45cf7a401723624998333289452_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.342170 11752 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45cf7a401723624998333289452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x45cf7a401723624998333289452_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.343343 11749 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.343345 11748 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.343418 11749 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cf7a401723624998333289452_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.343463 11749 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.343544 11749 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45cf7a401723624998333289452_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45cf7a401723624998333289452_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.343570 11749 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x45cf7a401723624998333289452_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.343588 11749 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.343601 11749 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x45cf7a401723624998333289452_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.346343 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x45cf7bb0) got event_name: TaskCompletion
1901: I0814 08:43:18.346387 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.343348 11751 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.343345 11747 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0814 08:43:18.348421 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192ba0c0 for it.
1901: I0814 08:43:18.348683 11697 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.348739 11697 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0814 08:43:18.352378 11697 pir_interpreter.cc:161] PirInterpreter(): 0x43ae45b0 on Place(gpu:0)
1901: I0814 08:43:18.352424 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.352448 11697 scope.cc:202] Create variable 0x43ae45b01723624998352410374_inner_var_1
1901: I0814 08:43:18.352458 11697 scope.cc:202] Create variable 0x43ae45b01723624998352410374_inner_var_2
1901: I0814 08:43:18.352465 11697 scope.cc:202] Create variable 0x43ae45b01723624998352410374_inner_var_3
1901: I0814 08:43:18.352473 11697 scope.cc:202] Create variable 0x43ae45b01723624998352410374_inner_var_4
1901: I0814 08:43:18.352483 11697 scope.cc:202] Create variable fetch0@fetch
1901: I0814 08:43:18.352931 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0814 08:43:18.352944 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.352948 11697 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x45d4cc30
1901: 1 -> 0x43ae45b01723624998352410374_inner_var_1 -> 0x45d50aa0
1901: 2 -> 0x43ae45b01723624998352410374_inner_var_2 -> 0x45d4ff80
1901: 3 -> 0x43ae45b01723624998352410374_inner_var_3 -> 0x45ce2b70
1901: 4 -> 0x43ae45b01723624998352410374_inner_var_4 -> 0x45d4cbf0
1901: 5 -> fetch0@fetch -> 0x45cf6bb0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0814 08:43:18.354408 11757 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.369393 11756 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.370348 11758 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.370446 11758 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.370522 11758 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0814 08:43:18.370568 11758 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43ae45b01723624998352410374_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_3:[dtype=;place=;dim=;lod={};, 0x43ae45b01723624998352410374_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.371155 11758 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43ae45b01723624998352410374_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x43ae45b01723624998352410374_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.371340 11755 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.371380 11756 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43ae45b01723624998352410374_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.371429 11756 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.371510 11756 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43ae45b01723624998352410374_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43ae45b01723624998352410374_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.371539 11756 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x43ae45b01723624998352410374_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.371557 11756 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.371572 11756 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x43ae45b01723624998352410374_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.372339 11754 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.374368 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x43ae4720) got event_name: TaskCompletion
1901: I0814 08:43:18.374424 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.374809 11697 shape_analysis.cc:506] InferShapeOrDataForValue,  defining_op: pd_op.fetch id:48
1901: I0814 08:43:18.372340 11753 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0814 08:43:18.465404 11741 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 5783759323292387178 to 16530207054716996263 , after update, data is {current : -4, peak : 0}.
1901: I0814 08:43:18.465456 11741 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 5783759323292387178 to 16530207054716996263 , after update, data is {current : -36, peak : 0}.
1901: I0814 08:43:18.473392 11744 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17661820560825697720 to 16530207054716996263 , after update, data is {current : 0, peak : 4}.
1901: I0814 08:43:18.476420 11746 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 1014923434093688843 to 16530207054716996263 , after update, data is {current : 0, peak : 16}.
1901: I0814 08:43:18.476459 11746 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 1014923434093688843 to 16530207054716996263 , after update, data is {current : -18, peak : 330659}.
1901: I0814 08:43:18.480391 11747 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 4330463102636029516 to 16530207054716996263 , after update, data is {current : -2, peak : 16}.
1901: I0814 08:43:18.480432 11747 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 4330463102636029516 to 16530207054716996263 , after update, data is {current : -36, peak : 330659}.
1901: I0814 08:43:18.489385 11749 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 663275528784083137 to 16530207054716996263 , after update, data is {current : 2, peak : 16}.
1901: I0814 08:43:18.495393 11752 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 1119773983933121714 to 16530207054716996263 , after update, data is {current : 2, peak : 16}.
1901: I0814 08:43:18.495436 11752 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 1119773983933121714 to 16530207054716996263 , after update, data is {current : -18, peak : 330659}.
1901: I0814 08:43:18.498687 11753 thread_data_registry.h:135] Add data {current : 2, peak : 16} from thread 16530207054716996263 to 14150987610383390233 , after update, data is {current : 6, peak : 16}.
1901: I0814 08:43:18.498723 11753 thread_data_registry.h:135] Add data {current : -18, peak : 330659} from thread 16530207054716996263 to 8391972051808149853 , after update, data is {current : 0, peak : 330659}.
1901: I0814 08:43:18.501441 11756 thread_data_registry.h:135] Add data {current : 6, peak : 16} from thread 14150987610383390233 to 8391972051808149853 , after update, data is {current : 6, peak : 16}.
1901: I0814 08:43:18.510388 11758 thread_data_registry.h:135] Add data {current : 6, peak : 16} from thread 8391972051808149853 to 6465755474443411432 , after update, data is {current : 20006, peak : 40000}.
1901: I0814 08:43:18.510433 11758 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 8391972051808149853 to 6465755474443411432 , after update, data is {current : 200000, peak : 560633}.
1901: I0814 08:43:18.519668 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.519907 11697 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 512(0x7fe2b3623000), and remaining 0
1901: I0814 08:43:18.520043 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.520088 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.520459 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.521466 11697 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.521560 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.521590 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.521804 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.522590 11697 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.522660 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.522684 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.522845 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.523494 11697 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.523572 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.523595 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.523761 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:464: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data)
1901: I0814 08:43:18.524498 11697 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.524551 11697 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fe2b3623200), and remaining 0
1901: I0814 08:43:18.524603 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.524624 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.525162 11697 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.525225 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.525246 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.529898 11697 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.529991 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.530023 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.530519 11697 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.530583 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.530606 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.531229 11697 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.531293 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.531325 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.531538 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.532233 11697 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.532311 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.532334 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.532495 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.533205 11697 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.533273 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.533295 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.533509 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.534127 11697 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.534194 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.534216 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.534389 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.535058 11697 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.535125 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.535147 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.535384 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.536032 11697 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.536108 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.536130 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.536294 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.537019 11697 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.537086 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.537108 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.537276 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.537971 11697 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.538040 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.538064 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.538215 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.538910 11697 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.538977 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.539000 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.539206 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.539831 11697 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.539908 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.539930 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.540093 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.540627 11697 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.540688 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.540709 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.540859 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.545703 11697 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.545812 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.545841 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.546018 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.546787 11697 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.546866 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.546890 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.547199 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:478: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data, axis)
1901: I0814 08:43:18.549292 11697 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.549379 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.549405 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.549616 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.551041 11697 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.551123 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.551148 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.551398 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.552774 11697 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.552847 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.552872 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.553066 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.554323 11697 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.554395 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.554419 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.554718 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.555987 11697 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.556058 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.556082 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.556277 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.557593 11697 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.557663 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.557686 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.557878 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.563653 11697 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.563778 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.563819 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.564078 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.565481 11697 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.565572 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.565603 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.565894 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.567333 11697 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.567422 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.567451 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.567667 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.568943 11697 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.569017 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.569043 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.569283 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.570536 11697 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.570618 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.570643 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.570837 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.572157 11697 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.572228 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.572252 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.572460 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.573695 11697 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.573765 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.573787 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.573971 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.575264 11697 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.579401 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.579449 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.579667 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.581072 11697 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.581156 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.581183 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.581406 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.582685 11697 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.582770 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.582796 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.582994 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.583778 11697 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.583847 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.583871 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.584055 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.587291 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.587340 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.588277 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.588297 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.589114 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.589133 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.589948 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.589968 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.590744 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.590770 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.597867 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.598515 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.599057 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.599586 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.600106 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.601145 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.601161 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.601167 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.606590 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:18.606727 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:18.606740 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.606747 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x45d1a0f0 type is 7
1901: I0814 08:43:18.606758 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x45cf0d30 type is 9
1901: I0814 08:43:18.606763 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf1b30 type is 10
1901: I0814 08:43:18.606770 11697 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0814 08:43:18.606774 11697 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x45d193b0 type is 7
1901: I0814 08:43:18.606779 11697 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0814 08:43:18.606783 11697 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x45d19fa0 type is 7
1901: I0814 08:43:18.606788 11697 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0814 08:43:18.606792 11697 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x45cfdd40 type is 7
1901: I0814 08:43:18.606797 11697 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0814 08:43:18.606801 11697 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x45d18d90 type is 7
1901: I0814 08:43:18.606806 11697 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0814 08:43:18.606810 11697 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x45d1a1c0 type is 7
1901: I0814 08:43:18.606815 11697 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0814 08:43:18.606819 11697 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x45d1a380 type is 7
1901: I0814 08:43:18.606824 11697 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0814 08:43:18.606827 11697 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x45d1a590 type is 7
1901: I0814 08:43:18.606832 11697 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0814 08:43:18.606835 11697 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x45d1a7f0 type is 7
1901: I0814 08:43:18.606840 11697 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0814 08:43:18.606843 11697 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x45d1aa50 type is 7
1901: I0814 08:43:18.606848 11697 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0814 08:43:18.606853 11697 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x45d1acb0 type is 7
1901: I0814 08:43:18.607021 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:18.607028 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.607033 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.607039 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.607127 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607147 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607237 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607245 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607266 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.607569 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.607784 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607795 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.607813 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.607952 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.608143 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608153 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608170 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.608296 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.608505 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608515 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608530 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.608692 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.608897 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608907 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.608923 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.609071 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.609264 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609274 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609294 11697 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.609311 11697 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4616eee0Variable Type 7
1901: I0814 08:43:18.609339 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.609361 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.609390 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.609406 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.609453 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.609475 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:18.609530 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609539 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609555 11697 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.609560 11697 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46193670Variable Type 7
1901: I0814 08:43:18.609576 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.609588 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.609606 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.609617 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.609650 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.609679 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0814 08:43:18.609725 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609732 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609748 11697 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.609753 11697 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46193000Variable Type 7
1901: I0814 08:43:18.609768 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.609779 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.609794 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.609805 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.609836 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.609848 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0814 08:43:18.609890 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609897 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.609910 11697 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.609915 11697 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4616ee20Variable Type 7
1901: I0814 08:43:18.609936 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.609947 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.609962 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.609973 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.610004 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.610014 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0814 08:43:18.610055 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.610064 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.610078 11697 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.610082 11697 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46196000Variable Type 7
1901: I0814 08:43:18.610095 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.610106 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.610121 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.610131 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.610160 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.610172 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0814 08:43:18.610906 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.610940 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.610956 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.610970 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.610985 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0814 08:43:18.622638 11697 pir_interpreter.cc:161] PirInterpreter(): 0x46170540 on Place(gpu:0)
1901: I0814 08:43:18.622697 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.622725 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_1
1901: I0814 08:43:18.622733 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_2
1901: I0814 08:43:18.622741 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_3
1901: I0814 08:43:18.622749 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_4
1901: I0814 08:43:18.622757 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_5
1901: I0814 08:43:18.622766 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_6
1901: I0814 08:43:18.622773 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_7
1901: I0814 08:43:18.622781 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_8
1901: I0814 08:43:18.622788 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_9
1901: I0814 08:43:18.622797 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_10
1901: I0814 08:43:18.622803 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_11
1901: I0814 08:43:18.622812 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_12
1901: I0814 08:43:18.622820 11697 scope.cc:202] Create variable fetch0@fetch
1901: I0814 08:43:18.622833 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_14
1901: I0814 08:43:18.622841 11697 scope.cc:202] Create variable fetch1@fetch
1901: I0814 08:43:18.622849 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_16
1901: I0814 08:43:18.622857 11697 scope.cc:202] Create variable fetch2@fetch
1901: I0814 08:43:18.622865 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_18
1901: I0814 08:43:18.622874 11697 scope.cc:202] Create variable fetch3@fetch
1901: I0814 08:43:18.622881 11697 scope.cc:202] Create variable 0x461705401723624998622682103_inner_var_20
1901: I0814 08:43:18.622888 11697 scope.cc:202] Create variable fetch4@fetch
1901: I0814 08:43:18.623346 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0814 08:43:18.623360 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.623364 11697 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x462bc990
1901: 1 -> 0x461705401723624998622682103_inner_var_1 -> 0x46196080
1901: 2 -> 0x461705401723624998622682103_inner_var_2 -> 0x4612dfd0
1901: 3 -> 0x461705401723624998622682103_inner_var_3 -> 0x461944f0
1901: 4 -> 0x461705401723624998622682103_inner_var_4 -> 0x4612e120
1901: 5 -> 0x461705401723624998622682103_inner_var_5 -> 0x46198a00
1901: 6 -> 0x461705401723624998622682103_inner_var_6 -> 0x4612d270
1901: 7 -> 0x461705401723624998622682103_inner_var_7 -> 0x46193520
1901: 8 -> 0x461705401723624998622682103_inner_var_8 -> 0x461758c0
1901: 9 -> 0x461705401723624998622682103_inner_var_9 -> 0x461957f0
1901: 10 -> 0x461705401723624998622682103_inner_var_10 -> 0x46174cc0
1901: 11 -> 0x461705401723624998622682103_inner_var_11 -> 0x45d19860
1901: 12 -> 0x461705401723624998622682103_inner_var_12 -> 0x461afac0
1901: 13 -> fetch0@fetch -> 0x461940f0
1901: 14 -> 0x461705401723624998622682103_inner_var_14 -> 0x45d19c80
1901: 15 -> fetch1@fetch -> 0x4612e180
1901: 16 -> 0x461705401723624998622682103_inner_var_16 -> 0x461940d0
1901: 17 -> fetch2@fetch -> 0x462bf2f0
1901: 18 -> 0x461705401723624998622682103_inner_var_18 -> 0x461c0aa0
1901: 19 -> fetch3@fetch -> 0x46188660
1901: 20 -> 0x461705401723624998622682103_inner_var_20 -> 0x462bf2d0
1901: 21 -> fetch4@fetch -> 0x461a0510
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0814 08:43:18.625355 11761 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.625363 11763 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.625375 11762 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.625375 11760 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.625417 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.625476 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0814 08:43:18.625535 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_3:[dtype=;place=;dim=;lod={};, 0x461705401723624998622682103_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.625835 11763 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.625996 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x461705401723624998622682103_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.626055 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_5:[dtype=;place=;dim=;lod={};, 0x461705401723624998622682103_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.626171 11763 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.626320 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x461705401723624998622682103_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.626364 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_7:[dtype=;place=;dim=;lod={};, 0x461705401723624998622682103_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.626427 11762 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.626477 11763 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.626492 11762 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.626634 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.626660 11760 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.626730 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.626785 11762 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.626844 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x461705401723624998622682103_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.626888 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_9:[dtype=;place=;dim=;lod={};, 0x461705401723624998622682103_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627036 11763 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.627188 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x461705401723624998622682103_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.627224 11763 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_11:[dtype=;place=;dim=;lod={};, 0x461705401723624998622682103_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627353 11763 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.627503 11763 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461705401723624998622682103_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x461705401723624998622682103_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.627596 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627619 11760 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.627632 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627651 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627665 11760 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.627697 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627724 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627738 11760 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.627749 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627765 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627777 11760 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.627804 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627827 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627841 11760 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.627851 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627867 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627880 11760 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.627908 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461705401723624998622682103_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461705401723624998622682103_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.627931 11760 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.627944 11760 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.627954 11760 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.628012 11762 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.628027 11762 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.628038 11762 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461705401723624998622682103_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.628327 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x461706b0) got event_name: TaskCompletion
1901: I0814 08:43:18.628345 11759 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.628365 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.628401 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.628412 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.628423 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.628433 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.630646 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.630780 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.630821 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.631059 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.631163 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46133360)  to GradNodeAccumulation (addr: 0x43a01b20)
1901: I0814 08:43:18.631341 11697 backward.cc:459] Run in Grad
1901: I0814 08:43:18.631357 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.631417 11697 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x46133360 to ptr: 0x461adb30
1901: I0814 08:43:18.631429 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.631477 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.631521 11697 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x43a01b20 to ptr: 0x46151090
1901: I0814 08:43:18.631551 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x461adb30
1901: I0814 08:43:18.631556 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.631595 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.631670 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.631677 11697 backward.cc:335] Node: NanmedianGradNode addr:0x461adb30, Found pending node: GradNodeAccumulation addr: 0x46151090
1901: I0814 08:43:18.631685 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.631713 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x46151090
1901: I0814 08:43:18.631718 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.631722 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.631729 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.631732 11697 backward.cc:435] Finish Backward
1901: I0814 08:43:18.636837 11697 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0814 08:43:18.636868 11697 dygraph_functions.cc:77659] { Input: []} 
1901: I0814 08:43:18.637071 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.637105 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.637298 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.637391 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46133360)  to GradNodeAccumulation (addr: 0x43a01b20)
1901: I0814 08:43:18.637523 11697 backward.cc:442] Run in Backward
1901: I0814 08:43:18.637529 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.637542 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.637575 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.637611 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x46133360
1901: I0814 08:43:18.637619 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.637655 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.637717 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.637745 11697 backward.cc:335] Node: NanmedianGradNode addr:0x46133360, Found pending node: GradNodeAccumulation addr: 0x43a01b20
1901: I0814 08:43:18.637753 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.637770 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43a01b20
1901: I0814 08:43:18.637775 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.637779 11697 accumulation_node.cc:40] Move Tensor ptr: 0x461770b0
1901: I0814 08:43:18.637782 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.637786 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.641193 11697 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1901: I0814 08:43:18.641959 11697 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.642125 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.642158 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.642385 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x461830b0)  to GradNodeAccumulation (addr: 0x19352680)
1901: I0814 08:43:18.642534 11697 backward.cc:442] Run in Backward
1901: I0814 08:43:18.642540 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.642549 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.642587 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.642613 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x461830b0
1901: I0814 08:43:18.642621 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.642655 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.642709 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.642731 11697 backward.cc:335] Node: NanmedianGradNode addr:0x461830b0, Found pending node: GradNodeAccumulation addr: 0x19352680
1901: I0814 08:43:18.642738 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.642755 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x19352680
1901: I0814 08:43:18.642760 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.642764 11697 accumulation_node.cc:40] Move Tensor ptr: 0x42d76c10
1901: I0814 08:43:18.642768 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.642772 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.643720 11697 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.643810 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.643836 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.644093 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.644196 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46133360)  to GradNodeAccumulation (addr: 0x19352680)
1901: I0814 08:43:18.644349 11697 backward.cc:459] Run in Grad
1901: I0814 08:43:18.644359 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.644380 11697 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x46133360 to ptr: 0x462790c0
1901: I0814 08:43:18.644387 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.644424 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.644449 11697 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x19352680 to ptr: 0x46151090
1901: I0814 08:43:18.644471 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x462790c0
1901: I0814 08:43:18.644477 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.644510 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.644658 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.644665 11697 backward.cc:335] Node: NanmedianGradNode addr:0x462790c0, Found pending node: GradNodeAccumulation addr: 0x46151090
1901: I0814 08:43:18.644670 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.644688 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x46151090
1901: I0814 08:43:18.644693 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.644696 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.644701 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.644706 11697 backward.cc:435] Finish Backward
1901: I0814 08:43:18.649992 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.650131 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.650169 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.650410 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.652246 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.652323 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.652352 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.695079 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.695134 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.700788 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.700834 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.702088 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.702248 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.702293 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.702605 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.703280 11697 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.707460 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.707513 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.707765 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.708529 11697 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.708622 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.708653 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.708854 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.709467 11697 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.709543 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.709569 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.709749 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.710376 11697 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.710429 11697 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fe2b3618e00), and remaining 0
1901: I0814 08:43:18.710485 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.710508 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.711009 11697 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.711061 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.711077 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.715633 11697 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.715735 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.715766 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.716331 11697 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.716396 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.716426 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.716918 11697 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.716985 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.717008 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.717248 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.717839 11697 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.717912 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.717936 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.718108 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.718730 11697 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.718801 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.718825 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.718991 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.719584 11697 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.719656 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.719681 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.719852 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.720469 11697 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.720541 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.720566 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.720734 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.725338 11697 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.725450 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.725477 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.725649 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.726291 11697 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.726385 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.726420 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.726593 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.727203 11697 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.727277 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.727315 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.727483 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.728117 11697 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.728188 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.728212 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.728397 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.728965 11697 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.729038 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.729061 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.729233 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.729748 11697 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.729799 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.729816 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.729944 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.734738 11697 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.734844 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.734880 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.735069 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.735812 11697 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.735890 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.735918 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.736119 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.736869 11697 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.736936 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.736959 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.737130 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.737815 11697 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.737880 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.737902 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.738080 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.738732 11697 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.738806 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.738829 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.739003 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.741835 11697 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.741919 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.741947 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.742158 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.742820 11697 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.742897 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.742920 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.743098 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.743749 11697 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.743816 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.743839 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.744017 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.744663 11697 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.744728 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.744751 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.744921 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.749886 11697 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.750020 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.750058 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.750322 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.751107 11697 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.751189 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.751215 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.751436 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.752112 11697 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.752179 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.752202 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.752414 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.753062 11697 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.753127 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.753150 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.753350 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.753988 11697 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.754055 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.754078 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.754268 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.759156 11697 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.759255 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.759289 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.759507 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.760174 11697 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.760246 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.760269 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.760463 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.761098 11697 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.761165 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.761188 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.761379 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.761991 11697 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.762055 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.762077 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.762249 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.762984 11697 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.763067 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.763092 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.763283 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.770030 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.770074 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.770849 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.770872 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.771536 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.771553 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.772223 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.772238 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.772884 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.772899 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.779402 11762 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 13619009410064744616 to 2543632203767960070 , after update, data is {current : -20, peak : 0}.
1901: I0814 08:43:18.779441 11762 thread_data_registry.h:135] Add data {current : 4, peak : 8} from thread 13619009410064744616 to 2543632203767960070 , after update, data is {current : 20, peak : 20}.
1901: I0814 08:43:18.779565 11760 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 2543632203767960070 to 1014923434093688843 , after update, data is {current : 0, peak : 1260}.
1901: I0814 08:43:18.779572 11760 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 2543632203767960070 to 1014923434093688843 , after update, data is {current : 20, peak : 20}.
1901: I0814 08:43:18.785384 11763 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 1014923434093688843 to 6465755474443411432 , after update, data is {current : 0, peak : 260}.
1901: I0814 08:43:18.785420 11763 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 1014923434093688843 to 6465755474443411432 , after update, data is {current : 20026, peak : 40000}.
1901: I0814 08:43:18.785427 11763 thread_data_registry.h:135] Add data {current : 0, peak : 1260} from thread 1014923434093688843 to 6465755474443411432 , after update, data is {current : 162560, peak : 560633}.
1901: I0814 08:43:18.790592 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.791056 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.791489 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.791913 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.792369 11697 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0814 08:43:18.793399 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.793412 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.793417 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.802515 11697 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0814 08:43:18.802603 11697 interpreter_util.cc:1169] Creating Variables
1901: I0814 08:43:18.802613 11697 scope.cc:202] Create variable X
1901: I0814 08:43:18.802619 11697 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4614fde0 type is 7
1901: I0814 08:43:18.802628 11697 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x45cf0d30 type is 9
1901: I0814 08:43:18.802634 11697 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf1b30 type is 10
1901: I0814 08:43:18.802637 11697 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0814 08:43:18.802641 11697 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x46150460 type is 7
1901: I0814 08:43:18.802646 11697 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0814 08:43:18.802649 11697 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x4614fc90 type is 7
1901: I0814 08:43:18.802654 11697 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0814 08:43:18.802657 11697 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x46150ff0 type is 7
1901: I0814 08:43:18.802661 11697 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0814 08:43:18.802664 11697 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x4617a700 type is 7
1901: I0814 08:43:18.802668 11697 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0814 08:43:18.802671 11697 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x4617a7e0 type is 7
1901: I0814 08:43:18.802675 11697 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0814 08:43:18.802677 11697 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x4617aa80 type is 7
1901: I0814 08:43:18.802681 11697 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0814 08:43:18.802685 11697 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x4617ac90 type is 7
1901: I0814 08:43:18.802687 11697 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0814 08:43:18.802690 11697 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x4617aef0 type is 7
1901: I0814 08:43:18.802695 11697 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0814 08:43:18.802697 11697 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x4617b150 type is 7
1901: I0814 08:43:18.802701 11697 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0814 08:43:18.802704 11697 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x4617b3b0 type is 7
1901: I0814 08:43:18.802863 11697 interpreter_util.cc:594] Static build: 0
1901: I0814 08:43:18.802870 11697 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0814 08:43:18.802875 11697 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0814 08:43:18.802879 11697 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0814 08:43:18.802960 11697 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.802978 11697 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803058 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803066 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803083 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.803349 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.803540 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803555 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803571 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.803689 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.803864 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803872 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.803886 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.803990 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.804157 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.804164 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.804177 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.808420 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.808746 11697 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.808765 11697 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.808794 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.808988 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.809172 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809181 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809201 11697 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.809207 11697 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x461a7690Variable Type 7
1901: I0814 08:43:18.809237 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.809257 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.809280 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.809293 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.809343 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.809365 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0814 08:43:18.809409 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809415 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809429 11697 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.809433 11697 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46292e60Variable Type 7
1901: I0814 08:43:18.809446 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.809456 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.809470 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.809480 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.809509 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.809535 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0814 08:43:18.809576 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809582 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809597 11697 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.809600 11697 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x461a9840Variable Type 7
1901: I0814 08:43:18.809613 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.809623 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.809635 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.809643 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.809670 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.809679 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0814 08:43:18.809715 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809720 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809731 11697 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.809734 11697 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x461a7970Variable Type 7
1901: I0814 08:43:18.809746 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.809753 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.809765 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.809774 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.809799 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.809808 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0814 08:43:18.809839 11697 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809844 11697 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0814 08:43:18.809854 11697 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0814 08:43:18.809859 11697 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x461aa000Variable Type 7
1901: I0814 08:43:18.809870 11697 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0814 08:43:18.809878 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.809890 11697 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0814 08:43:18.809899 11697 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.809924 11697 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0814 08:43:18.809933 11697 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0814 08:43:18.810643 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.810675 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.810691 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.810706 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0814 08:43:18.810721 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0814 08:43:18.827240 11697 pir_interpreter.cc:161] PirInterpreter(): 0x46173080 on Place(gpu:0)
1901: I0814 08:43:18.827344 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_1
1901: I0814 08:43:18.827359 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_2
1901: I0814 08:43:18.827368 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_3
1901: I0814 08:43:18.827376 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_4
1901: I0814 08:43:18.827383 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_5
1901: I0814 08:43:18.827391 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_6
1901: I0814 08:43:18.827399 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_7
1901: I0814 08:43:18.827406 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_8
1901: I0814 08:43:18.827415 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_9
1901: I0814 08:43:18.827423 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_10
1901: I0814 08:43:18.827431 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_11
1901: I0814 08:43:18.827440 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_12
1901: I0814 08:43:18.827461 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_14
1901: I0814 08:43:18.827474 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_16
1901: I0814 08:43:18.827486 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_18
1901: I0814 08:43:18.827498 11697 scope.cc:202] Create variable 0x461730801723624998827280677_inner_var_20
1901: I0814 08:43:18.828044 11697 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x461a0820
1901: 1 -> 0x461730801723624998827280677_inner_var_1 -> 0x461a75d0
1901: 2 -> 0x461730801723624998827280677_inner_var_2 -> 0x461a9610
1901: 3 -> 0x461730801723624998827280677_inner_var_3 -> 0x461a9fe0
1901: 4 -> 0x461730801723624998827280677_inner_var_4 -> 0x45d16080
1901: 5 -> 0x461730801723624998827280677_inner_var_5 -> 0x46182d30
1901: 6 -> 0x461730801723624998827280677_inner_var_6 -> 0x45d06bf0
1901: 7 -> 0x461730801723624998827280677_inner_var_7 -> 0x461b4a40
1901: 8 -> 0x461730801723624998827280677_inner_var_8 -> 0x45ce6fd0
1901: 9 -> 0x461730801723624998827280677_inner_var_9 -> 0x46187dc0
1901: 10 -> 0x461730801723624998827280677_inner_var_10 -> 0x461be9c0
1901: 11 -> 0x461730801723624998827280677_inner_var_11 -> 0x461a7e60
1901: 12 -> 0x461730801723624998827280677_inner_var_12 -> 0x194580e0
1901: 13 -> fetch0@fetch -> 0x461940f0
1901: 14 -> 0x461730801723624998827280677_inner_var_14 -> 0x462764a0
1901: 15 -> fetch1@fetch -> 0x4612e180
1901: 16 -> 0x461730801723624998827280677_inner_var_16 -> 0x46195c70
1901: 17 -> fetch2@fetch -> 0x462bf2f0
1901: 18 -> 0x461730801723624998827280677_inner_var_18 -> 0x462b3b70
1901: 19 -> fetch3@fetch -> 0x46188660
1901: 20 -> 0x461730801723624998827280677_inner_var_20 -> 0x45cf2a90
1901: 21 -> fetch4@fetch -> 0x461a0510
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0814 08:43:18.830363 11766 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0814 08:43:18.832347 11767 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0814 08:43:18.830370 11768 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0814 08:43:18.834337 11765 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0814 08:43:18.834386 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.834457 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0814 08:43:18.834514 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_3:[dtype=;place=;dim=;lod={};, 0x461730801723624998827280677_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.834852 11768 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.835014 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x461730801723624998827280677_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.835069 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_5:[dtype=;place=;dim=;lod={};, 0x461730801723624998827280677_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.835191 11768 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.834337 11764 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0814 08:43:18.835337 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x461730801723624998827280677_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.835379 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_7:[dtype=;place=;dim=;lod={};, 0x461730801723624998827280677_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.835412 11764 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.835464 11764 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.835536 11764 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.836360 11767 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.836436 11767 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.839408 11764 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.839474 11764 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.839491 11764 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.840389 11767 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.843537 11768 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.843750 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x461730801723624998827280677_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.843858 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_9:[dtype=;place=;dim=;lod={};, 0x461730801723624998827280677_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.844055 11768 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.844203 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x461730801723624998827280677_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.844247 11768 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_11:[dtype=;place=;dim=;lod={};, 0x461730801723624998827280677_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.844399 11768 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.844367 11765 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.844439 11765 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.845355 11764 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.845404 11764 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.845468 11764 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.848388 11765 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.851514 11768 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x461730801723624998827280677_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x461730801723624998827280677_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0814 08:43:18.852376 11766 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0814 08:43:18.852442 11766 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0814 08:43:18.852617 11766 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x461730801723624998827280677_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x461730801723624998827280677_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.852705 11766 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.852738 11766 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.852751 11766 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.853408 11764 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.853459 11764 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.853471 11764 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.853484 11767 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.853515 11767 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.853523 11767 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.854377 11765 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.854423 11765 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.854434 11765 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x461730801723624998827280677_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0814 08:43:18.854503 11697 pir_interpreter.cc:1766] main_thread_blocker_(0x461731f0) got event_name: TaskCompletion
1901: I0814 08:43:18.854537 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.854570 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.854580 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.854588 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.854597 11697 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0814 08:43:18.856721 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19352680 for it.
1901: I0814 08:43:18.856861 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.856904 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.857163 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.857259 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x462bda00)  to GradNodeAccumulation (addr: 0x19352680)
1901: I0814 08:43:18.857431 11697 backward.cc:459] Run in Grad
1901: I0814 08:43:18.857445 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.857465 11697 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x462bda00 to ptr: 0x461ad2a0
1901: I0814 08:43:18.857476 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.857515 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.857544 11697 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x19352680 to ptr: 0x46151090
1901: I0814 08:43:18.857569 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x461ad2a0
1901: I0814 08:43:18.857576 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.857617 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.857702 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.857709 11697 backward.cc:335] Node: NanmedianGradNode addr:0x461ad2a0, Found pending node: GradNodeAccumulation addr: 0x46151090
1901: I0814 08:43:18.857714 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.857741 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x46151090
1901: I0814 08:43:18.857746 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.857750 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.857755 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.857760 11697 backward.cc:435] Finish Backward
1901: I0814 08:43:18.858857 11697 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0814 08:43:18.858904 11697 dygraph_functions.cc:77659] { Input: []} 
1901: I0814 08:43:18.859031 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.859074 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.859261 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.859367 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x462bda00)  to GradNodeAccumulation (addr: 0x19352680)
1901: I0814 08:43:18.859505 11697 backward.cc:442] Run in Backward
1901: I0814 08:43:18.859529 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.859548 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.859599 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.859640 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x462bda00
1901: I0814 08:43:18.859660 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.859707 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.859794 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.859841 11697 backward.cc:335] Node: NanmedianGradNode addr:0x462bda00, Found pending node: GradNodeAccumulation addr: 0x19352680
1901: I0814 08:43:18.859861 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.859892 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x19352680
1901: I0814 08:43:18.859911 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.859930 11697 accumulation_node.cc:40] Move Tensor ptr: 0x461770b0
1901: I0814 08:43:18.859944 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.859959 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.860553 11697 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.860721 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.860769 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.860971 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x461ad2a0)  to GradNodeAccumulation (addr: 0x43a01b20)
1901: I0814 08:43:18.861117 11697 backward.cc:442] Run in Backward
1901: I0814 08:43:18.861145 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.861164 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.861217 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.861259 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x461ad2a0
1901: I0814 08:43:18.861281 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.861335 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.861415 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.861471 11697 backward.cc:335] Node: NanmedianGradNode addr:0x461ad2a0, Found pending node: GradNodeAccumulation addr: 0x43a01b20
1901: I0814 08:43:18.861495 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.861526 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43a01b20
1901: I0814 08:43:18.861546 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.861562 11697 accumulation_node.cc:40] Move Tensor ptr: 0x43a0df20
1901: I0814 08:43:18.861577 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.861593 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.862566 11697 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.862694 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.862740 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.862982 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.863098 11697 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x461ad2a0)  to GradNodeAccumulation (addr: 0x43a01b20)
1901: I0814 08:43:18.863258 11697 backward.cc:459] Run in Grad
1901: I0814 08:43:18.863286 11697 backward.cc:113] Start Backward
1901: I0814 08:43:18.863328 11697 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x461ad2a0 to ptr: 0x461a5050
1901: I0814 08:43:18.863351 11697 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0814 08:43:18.863401 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.863443 11697 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x43a01b20 to ptr: 0x46151090
1901: I0814 08:43:18.863482 11697 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x461a5050
1901: I0814 08:43:18.863502 11697 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0814 08:43:18.863642 11697 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.863821 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.863849 11697 backward.cc:335] Node: NanmedianGradNode addr:0x461a5050, Found pending node: GradNodeAccumulation addr: 0x46151090
1901: I0814 08:43:18.863868 11697 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0814 08:43:18.863898 11697 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x46151090
1901: I0814 08:43:18.863916 11697 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.863930 11697 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0814 08:43:18.863946 11697 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0814 08:43:18.863962 11697 backward.cc:435] Finish Backward
1901: I0814 08:43:18.865092 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.865216 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.865260 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.865463 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.867218 11697 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43a01b20 for it.
1901: I0814 08:43:18.867344 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.867386 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.869982 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.870054 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.871228 11697 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0814 08:43:18.871289 11697 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0814 08:43:18.872206 11697 dygraph_functions.cc:33459] Running AD API: full
1901: I0814 08:43:18.872257 11697 dygraph_functions.cc:33480] { Input: []} 
1901: I0814 08:43:18.872403 11697 dygraph_functions.cc:33459] Running AD API: full
1901: I0814 08:43:18.872431 11697 dygraph_functions.cc:33480] { Input: []} 
1901: I0814 08:43:18.872515 11697 dygraph_functions.cc:33459] Running AD API: full
1901: I0814 08:43:18.872537 11697 dygraph_functions.cc:33480] { Input: []} 
1901: I0814 08:43:18.872607 11697 dygraph_functions.cc:82227] Running AD API: arange
1901: I0814 08:43:18.872675 11697 dygraph_functions.cc:82291] { Input: [ 
1901: ( start , [[ Not specified tensor log level ]]),  
1901: ( end , [[ Not specified tensor log level ]]),  
1901: ( step , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.872915 11697 dygraph_functions.cc:62112] Running AD API: reshape
1901: I0814 08:43:18.873003 11697 dygraph_functions.cc:62169] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.873044 11697 api.cc:40807] Perform View between Output and Input Tensor, share allocation and inplace version.
1901: I0814 08:43:18.873132 11697 dygraph_functions.cc:14224] Running AD API: cast
1901: I0814 08:43:18.873162 11697 dygraph_functions.cc:14268] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.873262 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=200, vec_size=1, block_size=64, grid_size=4, limit_blocks=2147483647, limit_threads=512
1901: I0814 08:43:18.873366 11697 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0814 08:43:18.873404 11697 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0814 08:43:18.873660 11697 gpu_launch_config.h:156] Get 1-D launch config: numel=100, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: test_nanmedian failed
1901:  ss.E.................
1901: ======================================================================
1901: ERROR: test_check_output (test_nanmedian.TestNanmedianFP16Op)
1901: ----------------------------------------------------------------------
1901: Traceback (most recent call last):
1901:   File "/home/code/Paddle/build/test/legacy_test/test_nanmedian.py", line 617, in test_check_output
1901:     self.check_output(check_pir=True)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1901:     res = self.check_output_with_place(
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2786, in check_output_with_place
1901:     symbol_checker.check()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2637, in check
1901:     self.infer_and_compare_symbol()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2645, in infer_and_compare_symbol
1901:     self.op_test._infer_and_compare_symbol(place)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1712, in _infer_and_compare_symbol
1901:     self._compare_symbol(program, outs)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1633, in _compare_symbol
1901:     shape_analysis.get_shape_or_data_for_var(
1901: RuntimeError: (PreconditionNotMet) The op operand index [1] must less than operands size[1].
1901:   [Hint: Expected index < num_operands_, but received index:1 >= num_operands_:1.] (at /home/code/Paddle/paddle/pir/src/core/operation.cc:420)
1901: 
1901: 
1901: ----------------------------------------------------------------------
1901: Ran 21 tests in 3.988s
1901: 
1901: FAILED (errors=1, skipped=2)
1901: 
1901: I0814 08:43:18.875419 11697 mmap_allocator.cc:348] PID: 11697, MemoryMapFdSet: set size - 0
1901: I0814 08:43:18.888746 11697 mmap_allocator.cc:348] PID: 11697, MemoryMapFdSet: set size - 0
1901: I0814 08:43:18.968011 11766 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 14150987610383390233 to 13619009410064744616 , after update, data is {current : -8, peak : 0}.
1901: I0814 08:43:18.968051 11766 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 14150987610383390233 to 13619009410064744616 , after update, data is {current : 0, peak : 4}.
1901: I0814 08:43:18.968068 11767 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 11673694969441073773 to 13619009410064744616 , after update, data is {current : -12, peak : 0}.
1901: I0814 08:43:18.968111 11767 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 11673694969441073773 to 13619009410064744616 , after update, data is {current : 0, peak : 4}.
1901: I0814 08:43:18.968178 11765 thread_data_registry.h:135] Add data {current : -12, peak : 0} from thread 13619009410064744616 to 1014923434093688843 , after update, data is {current : -20, peak : 0}.
1901: I0814 08:43:18.968186 11765 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 13619009410064744616 to 1014923434093688843 , after update, data is {current : 0, peak : 4}.
1901: I0814 08:43:18.968364 11764 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 1014923434093688843 to 5564527677899807048 , after update, data is {current : 0, peak : 1252}.
1901: I0814 08:43:18.968396 11764 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 1014923434093688843 to 5564527677899807048 , after update, data is {current : 0, peak : 16}.
1901: I0814 08:43:18.971374 11768 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 5564527677899807048 to 6465755474443411432 , after update, data is {current : 0, peak : 260}.
1901: I0814 08:43:18.971410 11768 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 5564527677899807048 to 6465755474443411432 , after update, data is {current : 20026, peak : 40000}.
1901: I0814 08:43:18.971416 11768 thread_data_registry.h:135] Add data {current : 0, peak : 1252} from thread 5564527677899807048 to 6465755474443411432 , after update, data is {current : 162560, peak : 560633}.
1901: I0814 08:43:19.246212 11697 mmap_allocator.cc:348] PID: 11697, MemoryMapFdSet: set size - 0
1/1 Test #1901: test_nanmedian ...................***Failed   11.79 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  11.98 sec

The following tests FAILED:
	1901 - test_nanmedian (Failed)
