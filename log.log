UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:34:45.304997 17809 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:34:46.164698 17809 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=cinn_subgraph_graphviz_dir,enable_cublas_tensor_op_math,use_fast_math,conv_workspace_size_limit,cublaslt_exhaustive_search_times,mklml_dir,cupti_dir,use_stride_kernel,prim_skip_dynamic,gpugraph_parallel_copyer_split_maxsize,sync_nccl_allreduce,apply_pass_to_program,fleet_executor_with_standalone,fuse_parameter_memory_size,gpugraph_hbm_table_load_factor,all_blocks_convert_trt,cuda_memory_async_pool_realease_threshold,prim_check_ops,low_precision_op_list,max_inplace_grad_add,enable_pir_with_pt_in_dy2st,sync_after_alloc,einsum_opt,multi_node_sample_use_gpu_table,gemm_use_half_precision_compute_type,enable_exit_when_partial_worker,accuracy_check_rtol_fp16,call_stack_level,gpugraph_load_node_list_into_hbm,async_trace_count,reallocate_gpu_memory_in_mb,add_dependency_for_communication_op,graph_embedding_split_infer_mode,use_auto_growth_v2,use_system_allocator,initial_gpu_memory_in_mb,enable_pir_in_executor,fuse_parameter_groups_size,graph_neighbor_size_percent,cublaslt_device_best_config,custom_device_mem_record,dump_chunk_info,check_nan_inf_level,fraction_of_gpu_memory_to_use,dist_threadpool_size,enable_sparse_inner_gather,tensorrt_dir,prim_enable_dynamic,enable_cse_in_dy2st,set_to_1d,accuracy_check_rtol_bf16,gpugraph_force_device_batch_num_equal,enable_graph_multi_node_sampling,check_nan_inf,enable_fusion_fallback,dataloader_use_file_descriptor,cusparselt_dir,logging_pir_py_code_int_tensor_element_limit,dynamic_static_unified_comm,new_executor_serial_run,use_autotune,use_cuda_malloc_async_allocator,enable_cinn_compile_cache,cache_inference_while_scope,auto_free_cudagraph_allocations_on_launch,gpugraph_storage_mode,prim_forward,nccl_blocking_wait,use_stream_safe_cuda_allocator,sort_sum_gradient,gpugraph_enable_gpu_direct_access,accuracy_check_atol_bf16,paddle_num_threads,static_runtime_data_save_path,enable_api_kernel_fallback,lapack_dir,cusolver_dir,check_infer_symbolic,enable_auto_rdma_trans,selected_gpus,allow_cinn_ops,gpugraph_offload_param_extends,graph_metapath_split_opt,use_cuda_managed_memory,allocator_strategy,new_executor_sequential_run,gpugraph_merge_grads_segment_size,cublas_dir,local_exe_sub_scope_limit,enable_opt_get_features,gpugraph_sparse_table_storage_mode,cse_max_count,conv2d_disable_cudnn,allreduce_record_one_event,free_when_no_cache_hit,host_trace_level,mkl_dir,new_executor_use_inplace,print_allocator_trace_info,check_kernel_launch,inner_op_parallelism,init_allocated_mem,enable_gpu_memory_usage_log,enable_neighbor_list_use_uva,gpugraph_enable_segment_merge_grads,new_executor_static_build,fraction_of_cuda_pinned_memory_to_use,jit_engine_type,use_shm_cache,tracer_profile_fname,gpugraph_enable_print_op_debug,get_host_by_name_time,executor_log_deps_every_microseconds,convert_all_blocks,cudnn_exhaustive_search_times,pir_apply_inplace_pass,fast_eager_deletion_mode,log_memory_stats,enable_pir_in_executor_trace_run,pir_debug,gpu_memory_limit_mb,gpu_allocator_retry_time,print_sub_graph_dir,prim_forward_blacklist,deny_cinn_ops,cudnn_exhaustive_search,logging_pir_py_code_dir,enable_dependency_builder_debug_info,gpugraph_slot_feasign_max_num,auto_growth_chunk_size_in_mb,memory_fraction_of_eager_deletion,free_idle_chunk,pir_apply_shape_optimization_pass,cudnn_batchnorm_spatial_persistent,logging_trunc_pir_py_code,reader_queue_speed_test_mode,op_dir,gpugraph_parallel_stream_num,dygraph_debug,gpugraph_offload_param_stat,cuda_malloc_async_pool_memory_throttle_ratio,query_dest_rank_by_multi_node,new_executor_use_cuda_graph,pir_broadcast_tree_limit,enable_blaslt_global_search,cudnn_deterministic,accuracy_check_rtol_fp32,enable_all2all_use_fp16,curand_dir,prim_enabled,static_executor_perfstat_filepath,enable_fuse_parallel_matmul_pass,enable_pir_api,graph_get_neighbor_id,tracer_onednn_ops_on,disable_dyshape_in_train,new_executor_use_local_scope,fraction_of_cpu_memory_to_use,win_cuda_bin_dir,cinn_compile_thread_num,cuda_dir,enable_gpu_memory_usage_log_mb,accuracy_check_atol_fp16,enable_dump_main_program,tensor_operants_mode,manually_trans_conv_filter,gpugraph_enable_hbm_table_collision_stat,prim_all,enable_async_trace,search_cache_max_number,cudnn_dir,use_cinn,initial_cpu_memory_in_mb,enable_unused_var_check,logging_pir_py_code_dump_symbolic_dims,use_mkldnn,gpugraph_dedup_pull_push_mode,alloc_fill_value,benchmark_nccl,enable_interpretercore_launch_cinn,eager_delete_tensor_gb,pir_subgraph_saving_dir,gpugraph_debug_gpu_memory,nccl_dir,enable_record_memory,run_kp_kernel,eager_delete_scope,use_pinned_memory,print_ir,multiple_of_cupti_buffer_size,prim_backward,enable_cinn_auto_tune,enable_tracker_all2all,trt_ibuilder_cache,use_virtual_memory_auto_growth,embedding_deterministic,npu_storage_format,enable_collect_shape,nvidia_package_dir,use_auto_growth_pinned_allocator,save_static_runtime_data,graph_load_in_parallel,accuracy_check_atol_fp32,pinned_memory_as_cpu_backend,enable_adjust_op_order,enable_cinn_accuracy_check,use_xqa_optim,ir_inplace_kernel_blacklist,benchmark,cusparse_dir,gpugraph_offload_gather_copy_maxsize,enable_auto_detect_gpu_topo,tracer_onednn_ops_off 
1884: I0815 04:34:46.164806 17809 init.cc:108] After Parse: argc is 2
1884: I0815 04:34:54.633874 17809 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:34:54.633932 17809 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:34:54.634624 17809 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:34:54.635154 17809 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:34:54.635974 17809 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:34:54.636066 17809 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:34:54.636158 17809 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:34:54.636831 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f01bc000000), and remaining 0
1884: I0815 04:34:54.637183 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.637250 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.637346 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f01bc000200), and remaining 0
1884: I0815 04:34:54.637377 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f01bc000400), and remaining 0
1884: I0815 04:34:54.641116 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f01bc000600), and remaining 0
1884: I0815 04:34:54.641244 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f01bc000800), and remaining 0
1884: I0815 04:34:54.641319 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f01bc000a00), and remaining 0
1884: I0815 04:34:54.641407 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.641428 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.641502 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.641515 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.642961 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192645c0 for it.
1884: I0815 04:34:54.643128 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.643153 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.643208 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f01bc000e00), and remaining 0
1884: I0815 04:34:54.643280 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f01bc0c4400), and remaining 0
1884: I0815 04:34:54.775326 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192645c0 for it.
1884: I0815 04:34:54.775553 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.775602 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.776273 17809 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f01bc200000), and remaining 0
1884: I0815 04:34:54.786468 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x192645c0 for it.
1884: I0815 04:34:54.786588 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:54.786624 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.786670 17809 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:54.786883 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:54.788123 17809 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:34:54.788142 17809 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:34:54.788204 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:54.788321 17809 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:34:54.788349 17809 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.788424 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:54.788573 17809 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:34:54.788595 17809 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.788640 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:54.789041 17809 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:34:54.789065 17809 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.789278 17809 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:34:54.789316 17809 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:54.789408 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:54.793578 17809 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:34:54.793768 17809 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:34:54.793798 17809 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:34:54.793875 17809 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:34:56.131016 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.131078 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.131369 17809 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:34:56.131388 17809 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.136292 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.136338 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.137375 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.137394 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.137408 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.138150 17809 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:34:56.138164 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.138180 17809 scope.cc:202] Create variable feed
1884: I0815 04:34:56.138190 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.138198 17809 scope.cc:202] Create variable fetch
1884: I0815 04:34:56.138203 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.138216 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.138221 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.138224 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.138228 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.140539 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.140880 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.140893 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.140897 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.142545 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.142591 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.142601 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.142607 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.142616 17809 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:34:56.142622 17809 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x5fbd97e0 type is 7
1884: I0815 04:34:56.142628 17809 scope.cc:202] Create variable x
1884: I0815 04:34:56.142632 17809 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5fbd82b0 type is 7
1884: I0815 04:34:56.142692 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.142699 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.142702 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.142705 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.142827 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.142849 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.142966 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.142978 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.142995 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.143148 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.143177 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.143198 17809 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.143204 17809 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fbe0300Variable Type 7
1884: I0815 04:34:56.143225 17809 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.143246 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.143297 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.143325 17809 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.144558 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.144608 17809 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.144986 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.148689 17809 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:34:56.148710 17809 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:34:56.148794 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.148820 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.149312 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: I0815 04:34:56.149384 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.149406 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.149865 17809 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: I0815 04:34:56.149928 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.149950 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.149973 17809 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.150221 17809 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:34:56.150231 17809 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:34:56.150357 17809 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:34:56.150381 17809 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.150760 17809 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:34:56.150772 17809 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:34:56.150815 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.150833 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.151015 17809 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:34:56.151026 17809 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:34:56.151062 17809 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:34:56.151078 17809 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:34:56.151094 17809 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.153685 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.153708 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.153759 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.153769 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.155628 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.155982 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.155997 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.156001 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.157754 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.157804 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.157815 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.157824 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fc0af30 type is 7
1884: I0815 04:34:56.157831 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.157837 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fc0b2a0 type is 7
1884: I0815 04:34:56.157842 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.157848 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.157903 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.157910 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.157914 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.157919 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.157965 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.157980 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.158041 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.158051 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.158067 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.158313 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.158329 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.158346 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.158354 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fc119f0Variable Type 7
1884: I0815 04:34:56.158372 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.158390 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.158414 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.158430 17809 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.159149 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.159174 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.159346 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.169785 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: I0815 04:34:56.169965 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:34:56.175908 17809 pir_interpreter.cc:161] PirInterpreter(): 0x5fdce230 on Place(gpu:0)
1884: I0815 04:34:56.175951 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.175980 17809 scope.cc:202] Create variable 0x5fdce2301723696496175935460_inner_var_1
1884: I0815 04:34:56.175992 17809 scope.cc:202] Create variable 0x5fdce2301723696496175935460_inner_var_2
1884: I0815 04:34:56.176002 17809 scope.cc:202] Create variable 0x5fdce2301723696496175935460_inner_var_3
1884: I0815 04:34:56.176012 17809 scope.cc:202] Create variable 0x5fdce2301723696496175935460_inner_var_4
1884: I0815 04:34:56.176021 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.176421 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.176438 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.176442 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:34:56.176486 17809 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fdce190
1884: 1 -> 0x5fdce2301723696496175935460_inner_var_1 -> 0x5fdce210
1884: 2 -> 0x5fdce2301723696496175935460_inner_var_2 -> 0x5fdceae0
1884: 3 -> 0x5fdce2301723696496175935460_inner_var_3 -> 0x5fdcd970
1884: 4 -> 0x5fdce2301723696496175935460_inner_var_4 -> 0x5fdcee90
1884: 5 -> fetch0@fetch -> 0x5fdcf6a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.177223 17809 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:34:56.177479 17847 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.177590 17848 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.177603 17849 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.177687 17850 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.177736 17851 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.177763 17850 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fdce2301723696496175935460_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.177803 17852 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.177855 17850 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fdce2301723696496175935460_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.177861 17852 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.177904 17852 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:34:56.177953 17852 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fdce2301723696496175935460_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fdce2301723696496175935460_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.178136 17852 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fdce2301723696496175935460_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fdce2301723696496175935460_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:34:56.178207 17850 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fdce2301723696496175935460_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.178232 17850 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.179450 17850 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fdce2301723696496175935460_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fdce2301723696496175935460_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:34:56.179492 17850 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fdce2301723696496175935460_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.179517 17850 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.180076 17850 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fdce2301723696496175935460_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:34:56.180114 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x5fdce3a0) got event_name: TaskCompletion
1884: I0815 04:34:56.180140 17809 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.252166 17847 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 916022666657388364 to 14506814953673276383 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:34:56.252187 17847 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 916022666657388364 to 2403116344835204979 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:34:56.252194 17847 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 916022666657388364 to 2403116344835204979 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:34:56.252398 17850 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:34:56.252417 17850 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:34:56.252585 17852 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:34:56.252600 17852 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.258337 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.258363 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.258423 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.258431 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.260178 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.260536 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.260550 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.260556 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.262074 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.262171 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.262181 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.262187 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fdd0e20 type is 7
1884: I0815 04:34:56.262195 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.262198 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fbb9c40 type is 7
1884: I0815 04:34:56.262202 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.262207 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.262264 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.262270 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.262275 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.262279 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.262341 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.262356 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.262423 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.262431 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.262445 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.262581 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.262593 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.262607 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.262614 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fbcafb0Variable Type 7
1884: I0815 04:34:56.262630 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.262648 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.262670 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.262684 17809 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.264364 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.264398 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.264608 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.269093 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: I0815 04:34:56.269268 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:34:56.272321 17809 pir_interpreter.cc:161] PirInterpreter(): 0x5fbcaa80 on Place(gpu:0)
1884: I0815 04:34:56.272358 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.272380 17809 scope.cc:202] Create variable 0x5fbcaa801723696496272349378_inner_var_1
1884: I0815 04:34:56.272392 17809 scope.cc:202] Create variable 0x5fbcaa801723696496272349378_inner_var_2
1884: I0815 04:34:56.272404 17809 scope.cc:202] Create variable 0x5fbcaa801723696496272349378_inner_var_3
1884: I0815 04:34:56.272414 17809 scope.cc:202] Create variable 0x5fbcaa801723696496272349378_inner_var_4
1884: I0815 04:34:56.272425 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.272747 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.272763 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.272768 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fbbfb10
1884: 1 -> 0x5fbcaa801723696496272349378_inner_var_1 -> 0x5fdd0d50
1884: 2 -> 0x5fbcaa801723696496272349378_inner_var_2 -> 0x3c01be0
1884: 3 -> 0x5fbcaa801723696496272349378_inner_var_3 -> 0x1ed7ec0
1884: 4 -> 0x5fbcaa801723696496272349378_inner_var_4 -> 0x259c300
1884: 5 -> fetch0@fetch -> 0x5fda7fd0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.273453 17853 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.273545 17854 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.273558 17855 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.273626 17857 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.273634 17856 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.273672 17858 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.273675 17855 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fbcaa801723696496272349378_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.273710 17858 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.273753 17855 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fbcaa801723696496272349378_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.273763 17858 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:34:56.273816 17858 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fbcaa801723696496272349378_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fbcaa801723696496272349378_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.273942 17858 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fbcaa801723696496272349378_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fbcaa801723696496272349378_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.274000 17855 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fbcaa801723696496272349378_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.274026 17855 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.276724 17855 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fbcaa801723696496272349378_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fbcaa801723696496272349378_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.276774 17855 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fbcaa801723696496272349378_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.276798 17855 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.278836 17855 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fbcaa801723696496272349378_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.278887 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x5fbcabf0) got event_name: TaskCompletion
1884: I0815 04:34:56.278910 17809 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.315366 17853 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 14506814953673276383 to 2839402878587457699 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:34:56.315378 17853 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 14506814953673276383 to 2403116344835204979 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:34:56.315384 17853 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 14506814953673276383 to 2403116344835204979 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:34:56.315541 17855 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:34:56.315551 17855 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:34:56.315732 17858 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.319372 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.319394 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.319443 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.319451 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.320996 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.321333 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.321347 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.321352 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.322835 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.322921 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.322932 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.322937 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4397cbe0 type is 7
1884: I0815 04:34:56.322947 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.322952 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5b285aa0 type is 7
1884: I0815 04:34:56.322957 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.322961 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.323016 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.323022 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.323026 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.323030 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.323077 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.323091 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.323148 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.323156 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.323169 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.323206 17809 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.323335 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.323395 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.323405 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.323421 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.323426 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3c242a0Variable Type 7
1884: I0815 04:34:56.323443 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.323460 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.323482 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.323496 17809 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.323758 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.323779 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.323961 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.324676 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: I0815 04:34:56.324846 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:34:56.327739 17809 pir_interpreter.cc:161] PirInterpreter(): 0x5fbdc2b0 on Place(gpu:0)
1884: I0815 04:34:56.327770 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.327792 17809 scope.cc:202] Create variable 0x5fbdc2b01723696496327763650_inner_var_1
1884: I0815 04:34:56.327803 17809 scope.cc:202] Create variable 0x5fbdc2b01723696496327763650_inner_var_2
1884: I0815 04:34:56.327814 17809 scope.cc:202] Create variable 0x5fbdc2b01723696496327763650_inner_var_3
1884: I0815 04:34:56.327826 17809 scope.cc:202] Create variable 0x5fbdc2b01723696496327763650_inner_var_4
1884: I0815 04:34:56.327836 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.328152 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.328168 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.328172 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fbf7180
1884: 1 -> 0x5fbdc2b01723696496327763650_inner_var_1 -> 0x6251f770
1884: 2 -> 0x5fbdc2b01723696496327763650_inner_var_2 -> 0x623eaac0
1884: 3 -> 0x5fbdc2b01723696496327763650_inner_var_3 -> 0x3c02fd0
1884: 4 -> 0x5fbdc2b01723696496327763650_inner_var_4 -> 0x3c1bbd0
1884: 5 -> fetch0@fetch -> 0x3c33ba0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.328835 17859 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.328918 17860 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.328935 17861 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.328979 17862 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.329000 17863 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.329035 17864 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.329032 17862 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fbdc2b01723696496327763650_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.329064 17864 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.329093 17862 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fbdc2b01723696496327763650_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.329099 17864 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:34:56.329135 17864 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fbdc2b01723696496327763650_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fbdc2b01723696496327763650_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.329174 17864 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.329279 17864 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.329313 17864 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fbdc2b01723696496327763650_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fbdc2b01723696496327763650_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:34:56.329376 17862 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fbdc2b01723696496327763650_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.329397 17862 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.329691 17862 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fbdc2b01723696496327763650_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x5fbdc2b01723696496327763650_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:34:56.329717 17862 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fbdc2b01723696496327763650_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.329735 17862 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.329746 17862 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fbdc2b01723696496327763650_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:34:56.329773 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x5fbdc420) got event_name: TaskCompletion
1884: I0815 04:34:56.329795 17809 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.361258 17859 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 2839402878587457699 to 14506814953673276383 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:34:56.361270 17859 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2839402878587457699 to 14506814953673276383 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:34:56.361276 17859 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2839402878587457699 to 14506814953673276383 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:34:56.361483 17862 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 2403116344835204979 to 14506814953673276383 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:34:56.361495 17862 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 2403116344835204979 to 14506814953673276383 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:34:56.361660 17864 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:34:56.361671 17864 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:34:56.361676 17864 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.365487 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.365509 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.365556 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.365564 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.367090 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.367422 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.367435 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.367440 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.368912 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.368986 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.368996 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.369001 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3c445b0 type is 7
1884: I0815 04:34:56.369009 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.369014 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fda1d90 type is 7
1884: I0815 04:34:56.369017 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.369022 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.369074 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.369081 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.369086 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.369089 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.369135 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.369148 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.369202 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.369210 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.369223 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.369441 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.369453 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.369469 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.369477 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x625030e0Variable Type 7
1884: I0815 04:34:56.369493 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.369509 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.369531 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.369545 17809 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.370231 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.370258 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.370447 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.373209 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.373231 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.373278 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.373287 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.374977 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.375352 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.375366 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.375371 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.377122 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.377241 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.377254 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.377260 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fbf2390 type is 7
1884: I0815 04:34:56.377267 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.377274 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fbf1910 type is 7
1884: I0815 04:34:56.377280 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.377285 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.377351 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.377359 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.377364 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.377368 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.377415 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.377429 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.377494 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.377504 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.377521 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.380513 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.380525 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.380542 17809 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:34:56.380810 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.380821 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.380868 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.380873 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.381309 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: I0815 04:34:56.381443 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:34:56.383862 17809 pir_interpreter.cc:161] PirInterpreter(): 0x6119fa00 on Place(gpu:0)
1884: I0815 04:34:56.383888 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.383905 17809 scope.cc:202] Create variable 0x6119fa001723696496383881629_inner_var_1
1884: I0815 04:34:56.383914 17809 scope.cc:202] Create variable 0x6119fa001723696496383881629_inner_var_2
1884: I0815 04:34:56.383922 17809 scope.cc:202] Create variable 0x6119fa001723696496383881629_inner_var_3
1884: I0815 04:34:56.383932 17809 scope.cc:202] Create variable 0x6119fa001723696496383881629_inner_var_4
1884: I0815 04:34:56.383939 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.384200 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.384212 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.384217 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62234de0
1884: 1 -> 0x6119fa001723696496383881629_inner_var_1 -> 0x61092bb0
1884: 2 -> 0x6119fa001723696496383881629_inner_var_2 -> 0x62505e80
1884: 3 -> 0x6119fa001723696496383881629_inner_var_3 -> 0x62235cc0
1884: 4 -> 0x6119fa001723696496383881629_inner_var_4 -> 0x62398a30
1884: 5 -> fetch0@fetch -> 0x611012a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.384765 17865 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.384828 17866 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.384856 17867 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.384883 17868 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.384922 17869 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.384951 17870 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.384945 17869 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6119fa001723696496383881629_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.384974 17870 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.384990 17869 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6119fa001723696496383881629_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.385008 17870 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:34:56.385038 17870 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6119fa001723696496383881629_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6119fa001723696496383881629_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.385206 17870 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6119fa001723696496383881629_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6119fa001723696496383881629_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:34:56.385267 17869 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6119fa001723696496383881629_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.385288 17869 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.386480 17869 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6119fa001723696496383881629_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6119fa001723696496383881629_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:34:56.386521 17869 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6119fa001723696496383881629_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.386543 17869 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.387086 17869 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6119fa001723696496383881629_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:34:56.387128 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x6119fb70) got event_name: TaskCompletion
1884: I0815 04:34:56.387148 17809 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.389964 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.389986 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.390033 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.390043 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.391703 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.392069 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.392083 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.392088 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.393844 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.393885 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.393895 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.393901 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x625221b0 type is 7
1884: I0815 04:34:56.393909 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.393913 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6109ee00 type is 7
1884: I0815 04:34:56.393919 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.393926 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.393986 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.393994 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.393999 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.394006 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.394047 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.394068 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.394124 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.394138 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.394155 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.397117 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.397130 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.397145 17809 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:34:56.397447 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.397460 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.397509 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.397514 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.428515 17865 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 14506814953673276383 to 2839402878587457699 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:34:56.428531 17865 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14506814953673276383 to 12737338817016061393 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:34:56.428536 17865 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14506814953673276383 to 12737338817016061393 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:34:56.428682 17869 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12737338817016061393 to 18011463120533723125 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 04:34:56.428692 17869 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12737338817016061393 to 18011463120533723125 , after update, data is {current : 4000832, peak : 5600832}.
1884: I0815 04:34:56.428856 17870 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 3205136, peak : 3207136}.
1884: I0815 04:34:56.428869 17870 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.433846 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.433867 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.433914 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.433923 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.435488 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.435817 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.435828 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.435833 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.437309 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.437351 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.437361 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.437366 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3c117e0 type is 7
1884: I0815 04:34:56.437373 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.437377 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3c57760 type is 7
1884: I0815 04:34:56.437382 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.437386 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.437438 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.437444 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.437448 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.437453 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.437496 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.437510 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.437561 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.437570 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.437583 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.437705 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.437716 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.437731 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.437737 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3c09ed0Variable Type 7
1884: I0815 04:34:56.437753 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.437770 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.437791 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.437806 17809 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.439481 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.439514 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.439702 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.448163 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.448185 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.448233 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.448242 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.449920 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.450294 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.450316 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.450322 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.452082 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.452170 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.452183 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.452188 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6251e690 type is 7
1884: I0815 04:34:56.452195 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.452203 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fbbcfd0 type is 7
1884: I0815 04:34:56.452208 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.452214 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.452272 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.452280 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.452286 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.452291 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.452342 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.452356 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.452412 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.452422 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.452436 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.461285 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.461303 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.461319 17809 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:34:56.462028 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.462040 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.462090 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.462095 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.462527 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: I0815 04:34:56.462664 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:34:56.465121 17809 pir_interpreter.cc:161] PirInterpreter(): 0x60ef29c0 on Place(gpu:0)
1884: I0815 04:34:56.465147 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.465165 17809 scope.cc:202] Create variable 0x60ef29c01723696496465140133_inner_var_1
1884: I0815 04:34:56.465175 17809 scope.cc:202] Create variable 0x60ef29c01723696496465140133_inner_var_2
1884: I0815 04:34:56.465183 17809 scope.cc:202] Create variable 0x60ef29c01723696496465140133_inner_var_3
1884: I0815 04:34:56.465190 17809 scope.cc:202] Create variable 0x60ef29c01723696496465140133_inner_var_4
1884: I0815 04:34:56.465198 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.469472 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.469488 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.469492 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fbc8a00
1884: 1 -> 0x60ef29c01723696496465140133_inner_var_1 -> 0x5fbc8a80
1884: 2 -> 0x60ef29c01723696496465140133_inner_var_2 -> 0x5fbe28d0
1884: 3 -> 0x60ef29c01723696496465140133_inner_var_3 -> 0x5fbdd380
1884: 4 -> 0x60ef29c01723696496465140133_inner_var_4 -> 0x60ef2f20
1884: 5 -> fetch0@fetch -> 0x623b1e50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.470057 17871 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.470136 17872 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.470161 17873 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.470224 17874 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.470227 17875 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.470257 17874 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60ef29c01723696496465140133_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.470324 17874 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60ef29c01723696496465140133_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.470383 17876 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.470404 17876 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.470428 17876 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:34:56.470463 17876 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60ef29c01723696496465140133_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60ef29c01723696496465140133_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.470638 17876 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60ef29c01723696496465140133_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60ef29c01723696496465140133_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.470719 17874 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60ef29c01723696496465140133_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.470757 17874 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.473351 17874 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60ef29c01723696496465140133_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60ef29c01723696496465140133_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.473397 17874 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60ef29c01723696496465140133_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.473417 17874 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.475389 17874 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60ef29c01723696496465140133_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:34:56.475438 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x60ef2b30) got event_name: TaskCompletion
1884: I0815 04:34:56.475461 17809 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.480741 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.480762 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.480810 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.480819 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.482476 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.482837 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.482851 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.482856 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.484627 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.484670 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.484680 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.484686 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6117ccc0 type is 7
1884: I0815 04:34:56.484694 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.484700 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x611a1060 type is 7
1884: I0815 04:34:56.484706 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.484711 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.484769 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.484776 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.484782 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.484788 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.484830 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.484843 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.484897 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.484906 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.484921 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.493798 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.493813 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.493829 17809 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:34:56.494578 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.494592 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.494642 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.494647 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.525916 17871 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 2839402878587457699 to 14506814953673276383 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:34:56.525928 17871 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2839402878587457699 to 2403116344835204979 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:34:56.525933 17871 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2839402878587457699 to 2403116344835204979 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:34:56.526139 17874 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 04:34:56.526150 17874 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 2403116344835204979 to 18011463120533723125 , after update, data is {current : 6400896, peak : 11200896}.
1884: I0815 04:34:56.526297 17876 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 14506814953673276383 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.530539 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.530561 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.530604 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.530612 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.532109 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.532434 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.532447 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.532451 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.533898 17809 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:34:56.533937 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.533946 17809 scope.cc:202] Create variable Out
1884: I0815 04:34:56.533951 17809 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fbca630 type is 7
1884: I0815 04:34:56.533959 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.533962 17809 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3c3bff0 type is 7
1884: I0815 04:34:56.533968 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.533972 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.534022 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.534029 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.534034 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.534039 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.534075 17809 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.534087 17809 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.534133 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.534142 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.534155 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.534193 17809 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.534329 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.534381 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.534391 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.534406 17809 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.534411 17809 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fbf5650Variable Type 7
1884: I0815 04:34:56.534428 17809 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.534446 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.534466 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.534479 17809 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.534610 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.534631 17809 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.534797 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.535506 17809 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19282b20 for it.
1884: I0815 04:34:56.535692 17809 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1934c5b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:34:56.538596 17809 pir_interpreter.cc:161] PirInterpreter(): 0x60ef29c0 on Place(gpu:0)
1884: I0815 04:34:56.538628 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.538650 17809 scope.cc:202] Create variable 0x60ef29c01723696496538621224_inner_var_1
1884: I0815 04:34:56.538661 17809 scope.cc:202] Create variable 0x60ef29c01723696496538621224_inner_var_2
1884: I0815 04:34:56.538671 17809 scope.cc:202] Create variable 0x60ef29c01723696496538621224_inner_var_3
1884: I0815 04:34:56.538681 17809 scope.cc:202] Create variable 0x60ef29c01723696496538621224_inner_var_4
1884: I0815 04:34:56.538690 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.539001 17809 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:34:56.539016 17809 scope.cc:202] Create variable X
1884: I0815 04:34:56.539019 17809 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x3c26230
1884: 1 -> 0x60ef29c01723696496538621224_inner_var_1 -> 0x611989c0
1884: 2 -> 0x60ef29c01723696496538621224_inner_var_2 -> 0x43981440
1884: 3 -> 0x60ef29c01723696496538621224_inner_var_3 -> 0x6229e7a0
1884: 4 -> 0x60ef29c01723696496538621224_inner_var_4 -> 0x60ef5f60
1884: 5 -> fetch0@fetch -> 0x5fbc5e70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:34:56.539685 17877 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.539764 17878 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.539784 17879 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.539824 17880 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.539852 17881 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.539893 17882 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.539887 17881 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60ef29c01723696496538621224_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.539914 17882 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.539932 17881 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60ef29c01723696496538621224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.539942 17882 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:34:56.539971 17882 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60ef29c01723696496538621224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60ef29c01723696496538621224_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.540010 17882 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.540115 17882 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.540140 17882 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60ef29c01723696496538621224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60ef29c01723696496538621224_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:34:56.540194 17881 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60ef29c01723696496538621224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.540213 17881 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.540369 17881 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60ef29c01723696496538621224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x60ef29c01723696496538621224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:34:56.540392 17881 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60ef29c01723696496538621224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.540409 17881 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.540421 17881 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60ef29c01723696496538621224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:34:56.540447 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x60ef2b30) got event_name: TaskCompletion
1884: I0815 04:34:56.540470 17809 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.573173 17877 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 14506814953673276383 to 2839402878587457699 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:34:56.573191 17877 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 14506814953673276383 to 2839402878587457699 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:34:56.573197 17877 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 14506814953673276383 to 2839402878587457699 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:34:56.573436 17881 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12737338817016061393 to 2839402878587457699 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:34:56.573457 17881 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12737338817016061393 to 2839402878587457699 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:34:56.573592 17882 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 04:34:56.573609 17882 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 6401600, peak : 11200896}.
1884: I0815 04:34:56.573614 17882 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 2839402878587457699 to 18011463120533723125 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:34:56.579823 17809 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:34:56.579872 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:34:56.580948 17809 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:34:56.581776 17809 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:34:56.581805 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:34:56.583081 17809 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:34:56.583106 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:34:56.583791 17809 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:34:56.584748 17809 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:34:56.584774 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:34:56.586066 17809 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:34:56.586086 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:34:56.586671 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.586699 17809 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:34:56.586704 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.586711 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.588595 17809 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:34:56.588621 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:34:56.589548 17809 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:34:56.589574 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:34:56.590519 17809 pybind.cc:1827] need skip: 0
1884: I0815 04:34:56.590814 17809 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:34:56.592522 17809 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:34:56.596125 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.596141 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.596148 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.598131 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.598151 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.598160 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.598172 17809 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:34:56.598179 17809 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61092b90 type is 7
1884: I0815 04:34:56.598184 17809 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:34:56.598187 17809 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61e14310 type is 7
1884: I0815 04:34:56.598192 17809 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:34:56.598196 17809 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61e11950 type is 7
1884: I0815 04:34:56.598255 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.598263 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.598266 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.598270 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.598330 17809 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598345 17809 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598367 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:34:56.598498 17809 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598510 17809 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598573 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.598613 17809 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598623 17809 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.598647 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.599622 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.600929 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.601372 17809 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:34:56.601585 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.601878 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.602090 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.602105 17809 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:34:56.602169 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.602175 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.602180 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.602280 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.602293 17809 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:34:56.603832 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.605145 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.606235 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.606427 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.606441 17809 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:34:56.606446 17809 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x62101690 type is 7
1884: I0815 04:34:56.606452 17809 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:34:56.606456 17809 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x62101410 type is 7
1884: I0815 04:34:56.606460 17809 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:34:56.606464 17809 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x62101500 type is 7
1884: I0815 04:34:56.606469 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.606474 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.606479 17809 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:34:56.606483 17809 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x62102ad0 type is 7
1884: I0815 04:34:56.606487 17809 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61092b90 type is 7
1884: I0815 04:34:56.606492 17809 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61e14310 type is 7
1884: I0815 04:34:56.606496 17809 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:34:56.606500 17809 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x62102ab0 type is 7
1884: I0815 04:34:56.606505 17809 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:34:56.606508 17809 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x62102f80 type is 7
1884: I0815 04:34:56.606514 17809 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61e11950 type is 7
1884: I0815 04:34:56.606520 17809 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:34:56.606524 17809 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x621031f0 type is 7
1884: I0815 04:34:56.606529 17809 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:34:56.606532 17809 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x62103430 type is 7
1884: I0815 04:34:56.606539 17809 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:34:56.606544 17809 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61cff430 type is 7
1884: I0815 04:34:56.606622 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.606637 17809 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:34:56.606695 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.606701 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.606707 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.606711 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.606755 17809 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.606766 17809 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.606781 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:34:56.606881 17809 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.606891 17809 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.606910 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:34:56.606979 17809 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:34:56.607054 17809 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:34:56.608134 17809 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608152 17809 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608214 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.608279 17809 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608289 17809 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608309 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:34:56.608333 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.608372 17809 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608381 17809 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608393 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:34:56.608474 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608484 17809 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608498 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.608597 17809 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.608677 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.608734 17809 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608744 17809 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608757 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:34:56.608793 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.608842 17809 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608851 17809 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608865 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:34:56.608973 17809 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.608983 17809 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.609002 17809 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.609043 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.609051 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.609066 17809 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.609073 17809 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60dd1e80Variable Type 7
1884: I0815 04:34:56.609090 17809 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.609107 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.609125 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.609138 17809 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.609176 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.609200 17809 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:34:56.609226 17809 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.609234 17809 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.609246 17809 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:34:56.609252 17809 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60dd4ee0Variable Type 7
1884: I0815 04:34:56.609267 17809 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:34:56.609278 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.609292 17809 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.609310 17809 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.609344 17809 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:34:56.609357 17809 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:34:56.609760 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:34:56.609793 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:34:56.609812 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:34:56.609841 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:34:56.609874 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.609891 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:34:56.613653 17809 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:34:56.613687 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:34:56.614374 17809 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:34:56.614398 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:34:56.614706 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.616366 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.617169 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.617286 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.617796 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.618676 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.620728 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.621749 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.623466 17809 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:34:56.624166 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.624181 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.624186 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.625372 17809 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:34:56.625389 17809 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c09d10 type is 9
1884: I0815 04:34:56.625396 17809 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x23a3ef0 type is 10
1884: I0815 04:34:56.625401 17809 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61e14310 type is 7
1884: I0815 04:34:56.625404 17809 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61e11950 type is 7
1884: I0815 04:34:56.625411 17809 scope.cc:202] Create variable saved_params
1884: I0815 04:34:56.625414 17809 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x49a62b0 type is 17
1884: I0815 04:34:56.625443 17809 interpreter_util.cc:594] Static build: 0
1884: I0815 04:34:56.625449 17809 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:34:56.625453 17809 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:34:56.625456 17809 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:34:56.625491 17809 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.625502 17809 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:34:56.626173 17809 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:34:56.626214 17809 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:34:56.626263 17809 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:34:56.627400 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.627458 17809 scope.cc:202] Create variable feed
1884: I0815 04:34:56.627466 17809 naive_executor.cc:189] 0x61d34090 Create persistable variable feed, which pointer is 0x61db4320
1884: I0815 04:34:56.627471 17809 scope.cc:202] Create variable fetch
1884: I0815 04:34:56.627475 17809 naive_executor.cc:189] 0x61d34090 Create persistable variable fetch, which pointer is 0x61db41c0
1884: I0815 04:34:56.627480 17809 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:34:56.627483 17809 naive_executor.cc:189] 0x61d34090 Create persistable variable linear_0.b_0, which pointer is 0x61d34230
1884: I0815 04:34:56.627489 17809 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:34:56.627492 17809 naive_executor.cc:189] 0x61d34090 Create persistable variable linear_0.w_0, which pointer is 0x61d334b0
1884: I0815 04:34:56.627508 17809 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:34:56.627846 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.627924 17809 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:34:56.627969 17809 executor.cc:183] Old Executor is Running.
1884: I0815 04:34:56.628010 17809 executor.cc:92] Creating Variables for block 0
1884: I0815 04:34:56.628017 17809 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:34:56.628021 17809 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x61d34230 type is 7
1884: I0815 04:34:56.628026 17809 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:34:56.628028 17809 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x61d334b0 type is 7
1884: I0815 04:34:56.628058 17809 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.628130 17809 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:34:56.628168 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.628175 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.628319 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.628427 17809 graph.cc:149] create OpNode by feed
1884: I0815 04:34:56.628464 17809 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:34:56.628479 17809 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:34:56.628494 17809 graph.cc:149] create OpNode by abs
1884: I0815 04:34:56.628504 17809 graph.cc:149] create OpNode by assign_value
1884: I0815 04:34:56.628520 17809 graph.cc:149] create OpNode by multinomial
1884: I0815 04:34:56.628530 17809 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:34:56.628544 17809 graph.cc:149] create OpNode by scale
1884: I0815 04:34:56.628556 17809 graph.cc:149] create OpNode by scale
1884: I0815 04:34:56.628568 17809 graph.cc:149] create OpNode by fetch
1884: I0815 04:34:56.628583 17809 graph.cc:149] create OpNode by fetch
1884: I0815 04:34:56.628604 17809 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:34:56.629773 17809 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:34:56.629781 17809 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:34:56.629850 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.629858 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:34:56.629966 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630213 17809 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:34:56.630272 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630277 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:34:56.630319 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630326 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:34:56.630368 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630429 17809 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:34:56.630460 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630465 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:34:56.630484 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630497 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.630520 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630525 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:34:56.630565 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630586 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.630609 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630614 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:34:56.630658 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630733 17809 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:34:56.630760 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630765 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:34:56.630797 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.630816 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.630838 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.630843 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:34:56.630874 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631026 17809 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:34:56.631053 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.631058 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:34:56.631093 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631108 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.631130 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.631135 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:34:56.631157 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631172 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.631192 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.631197 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:34:56.631220 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631234 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.631255 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.631259 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:34:56.631283 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631356 17809 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:34:56.631389 17809 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:34:56.631404 17809 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:34:56.631418 17809 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:34:56.631443 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.631448 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:34:56.631472 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.631512 17809 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:34:56.631532 17809 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:34:56.631543 17809 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:34:56.631556 17809 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:34:56.631588 17809 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:34:56.631599 17809 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:34:56.632750 17809 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:34:56.632792 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.632798 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:34:56.632825 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.632846 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.632872 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.632877 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:34:56.632903 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.632954 17809 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:34:56.632983 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.632988 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:34:56.633008 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.633023 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.633045 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.633050 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:34:56.633085 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.633172 17809 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:34:56.633199 17809 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:34:56.633216 17809 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:34:56.633230 17809 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:34:56.633244 17809 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:34:56.633256 17809 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:34:56.633272 17809 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:34:56.633294 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.633390 17809 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:34:56.633414 17809 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:34:56.633427 17809 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:34:56.633441 17809 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:34:56.633455 17809 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:34:56.633468 17809 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:34:56.633484 17809 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:34:56.633529 17809 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:34:56.633791 17809 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:34:56.633821 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.633826 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:34:56.633873 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.633934 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.633967 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634013 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634040 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634083 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634105 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634142 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634162 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634195 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634213 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634245 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634259 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634286 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634305 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634330 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634341 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634359 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634385 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634390 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:34:56.634418 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634456 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634482 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634487 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:34:56.634500 17809 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:34:56.634502 17809 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:34:56.634552 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634573 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634598 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634603 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:34:56.634613 17809 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:34:56.634617 17809 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:34:56.634657 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634678 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634703 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634708 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:34:56.634717 17809 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:34:56.634721 17809 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:34:56.634752 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634771 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634793 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634799 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:34:56.634807 17809 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:34:56.634809 17809 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:34:56.634848 17809 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:34:56.634868 17809 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:34:56.634891 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634896 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:34:56.634909 17809 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:34:56.634948 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.634953 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:34:56.635025 17809 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:34:56.635046 17809 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.635063 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:34:56.635107 17809 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:34:56.635123 17809 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:34:56.635149 17809 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:34:56.635172 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.635177 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:34:56.636049 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.636063 17809 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:34:56.636116 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.636122 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.636726 17809 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:34:56.636936 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.637008 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.637014 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.637426 17809 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:34:56.637627 17809 graph.h:183] deleting __fuse_statis__
1884: I0815 04:34:56.637635 17809 graph.h:183] deleting pass_recorder
1884: I0815 04:34:56.637640 17809 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:34:56.637814 17809 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:34:56.637825 17809 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:34:56.637828 17809 naive_executor.cc:195] 0x61d34090 Create variable abs_0.tmp_0, which pointer is 0x622ffbf0
1884: I0815 04:34:56.637835 17809 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:34:56.637837 17809 naive_executor.cc:195] 0x61d34090 Create variable gaussian_0.tmp_0, which pointer is 0x62297b30
1884: I0815 04:34:56.637851 17809 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:34:56.637857 17809 naive_executor.cc:195] 0x61d34090 Create variable linear_0.tmp_1, which pointer is 0x62258950
1884: I0815 04:34:56.637861 17809 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:34:56.637863 17809 naive_executor.cc:195] 0x61d34090 Create variable multinomial_0.tmp_0, which pointer is 0x622583f0
1884: I0815 04:34:56.637866 17809 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:34:56.637869 17809 naive_executor.cc:195] 0x61d34090 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x622586f0
1884: I0815 04:34:56.637873 17809 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:34:56.637876 17809 naive_executor.cc:195] 0x61d34090 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62256cf0
1884: I0815 04:34:56.637882 17809 scope.cc:202] Create variable feed
1884: I0815 04:34:56.637887 17809 scope.cc:202] Create variable fetch
1884: I0815 04:34:56.637905 17809 naive_executor.cc:46] NaiveExecutor init with scope 0x61d34090
1884: I0815 04:34:56.637912 17809 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:34:56.637991 17809 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:34:56.638005 17809 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:34:56.638031 17809 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:34:56.638037 17809 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:34:56.638044 17809 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.638074 17809 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:34:56.638264 17809 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.638279 17809 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:34:56.638334 17809 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.638360 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:34:56.642047 17809 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:34:56.642143 17809 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.642171 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:34:56.642233 17809 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:34:56.642266 17809 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.642292 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:34:56.642351 17809 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:34:56.642396 17809 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.642414 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:34:56.642465 17809 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:34:56.642495 17809 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:34:56.642511 17809 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:34:56.642546 17809 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:34:56.642565 17809 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.642596 17809 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:34:56.642622 17809 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:34:56.643081 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.643091 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:34:56.683866 17809 pir_interpreter.cc:161] PirInterpreter(): 0x60dd5110 on Place(gpu:0)
1884: I0815 04:34:56.683902 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_0
1884: I0815 04:34:56.683918 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_1
1884: I0815 04:34:56.683928 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_2
1884: I0815 04:34:56.683936 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_3
1884: I0815 04:34:56.683974 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_4
1884: I0815 04:34:56.683988 17809 scope.cc:202] Create variable 0x60dd51101723696496683891631_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x60dd51101723696496683891631_inner_var_0 -> 0x61d21a20
1884: 1 -> 0x60dd51101723696496683891631_inner_var_1 -> 0x61db8b10
1884: 2 -> 0x60dd51101723696496683891631_inner_var_2 -> 0x5fbbac20
1884: 3 -> linear_1.w_0 -> 0x61db8aa0
1884: 4 -> linear_1.b_0 -> 0x6117b3e0
1884: 5 -> learning_rate_1 -> 0x6251b670
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:34:56.684793 17883 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.684811 17884 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.684835 17885 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.684867 17886 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.684906 17887 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.684904 17886 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.684913 17885 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.684921 17883 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.684938 17887 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.684963 17886 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:34:56.684973 17883 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.684985 17887 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.684979 17885 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60dd51101723696496683891631_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.685009 17887 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:34:56.685034 17887 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.685050 17887 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.685060 17887 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:34:56.685075 17887 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x60dd51101723696496683891631_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60dd51101723696496683891631_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60dd51101723696496683891631_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.685132 17887 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x60dd51101723696496683891631_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60dd51101723696496683891631_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60dd51101723696496683891631_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:34:56.685186 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x60dd5280) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:34:56.687193 17809 pir_interpreter.cc:161] PirInterpreter(): 0x61198fa0 on Place(gpu:0)
1884: I0815 04:34:56.687228 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_1
1884: I0815 04:34:56.687244 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_4
1884: I0815 04:34:56.687253 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_5
1884: I0815 04:34:56.687261 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_6
1884: I0815 04:34:56.687283 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_7
1884: I0815 04:34:56.687294 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_8
1884: I0815 04:34:56.687311 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_9
1884: I0815 04:34:56.687340 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_10
1884: I0815 04:34:56.687350 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_11
1884: I0815 04:34:56.687361 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_12
1884: I0815 04:34:56.687368 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_13
1884: I0815 04:34:56.687378 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_14
1884: I0815 04:34:56.687387 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_15
1884: I0815 04:34:56.687394 17809 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:34:56.687405 17809 scope.cc:202] Create variable 0x61198fa01723696496687216006_inner_var_17
1884: I0815 04:34:56.687413 17809 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x6251b670
1884: 1 -> 0x61198fa01723696496687216006_inner_var_1 -> 0x61e16310
1884: 2 -> linear_1.b_0 -> 0x6117b3e0
1884: 3 -> linear_1.w_0 -> 0x61db8aa0
1884: 4 -> 0x61198fa01723696496687216006_inner_var_4 -> 0x6251ba80
1884: 5 -> 0x61198fa01723696496687216006_inner_var_5 -> 0x623f74a0
1884: 6 -> 0x61198fa01723696496687216006_inner_var_6 -> 0x62516580
1884: 7 -> 0x61198fa01723696496687216006_inner_var_7 -> 0x610a5010
1884: 8 -> 0x61198fa01723696496687216006_inner_var_8 -> 0x61e161e0
1884: 9 -> 0x61198fa01723696496687216006_inner_var_9 -> 0x61dfbf60
1884: 10 -> 0x61198fa01723696496687216006_inner_var_10 -> 0x60dd8ee0
1884: 11 -> 0x61198fa01723696496687216006_inner_var_11 -> 0x62209000
1884: 12 -> 0x61198fa01723696496687216006_inner_var_12 -> 0x623482d0
1884: 13 -> 0x61198fa01723696496687216006_inner_var_13 -> 0x61e0cc60
1884: 14 -> 0x61198fa01723696496687216006_inner_var_14 -> 0x62506840
1884: 15 -> 0x61198fa01723696496687216006_inner_var_15 -> 0x62235200
1884: 16 -> fetch0@fetch -> 0x60ddb730
1884: 17 -> 0x61198fa01723696496687216006_inner_var_17 -> 0x5fde2910
1884: 18 -> fetch1@fetch -> 0x624010f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:34:56.689036 17888 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.689159 17889 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.689167 17890 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.689250 17891 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.689280 17890 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689298 17889 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689329 17892 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:34:56.689321 17890 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.689371 17889 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:34:56.689379 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689409 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:34:56.689436 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689477 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.689515 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:34:56.689531 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:34:56.689576 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:34:56.689594 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61198fa01723696496687216006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689633 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61198fa01723696496687216006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:34:56.689661 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61198fa01723696496687216006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689697 17892 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:34:56.689729 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61198fa01723696496687216006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.689756 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61198fa01723696496687216006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61198fa01723696496687216006_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689796 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.689810 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61198fa01723696496687216006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61198fa01723696496687216006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.689843 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61198fa01723696496687216006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689864 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.689867 17889 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61198fa01723696496687216006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689875 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61198fa01723696496687216006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.689893 17889 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.689891 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61198fa01723696496687216006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.689918 17892 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.689958 17889 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61198fa01723696496687216006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.689991 17889 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61198fa01723696496687216006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690002 17892 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.690013 17889 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.690029 17889 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61198fa01723696496687216006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.690043 17892 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.690100 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.690117 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61198fa01723696496687216006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61198fa01723696496687216006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.690152 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690160 17889 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690176 17889 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:34:56.690179 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.690192 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.690210 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61198fa01723696496687216006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690210 17889 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61198fa01723696496687216006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.690239 17889 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61198fa01723696496687216006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690258 17889 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.690269 17889 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61198fa01723696496687216006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.690281 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61198fa01723696496687216006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:34:56.690310 17892 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61198fa01723696496687216006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61198fa01723696496687216006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.690338 17892 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:34:56.690351 17892 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61198fa01723696496687216006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61198fa01723696496687216006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61198fa01723696496687216006_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:34:56.690390 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x61199110) got event_name: TaskCompletion
1884: I0815 04:34:56.690425 17809 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.690464 17809 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:34:56.696278 17809 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:34:56.696336 17809 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:34:56.697075 17809 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:34:56.697127 17809 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:34:56.697610 17809 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966976699080"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966976699080"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:34:56.697816 17809 pir_interpreter.cc:161] PirInterpreter(): 0x62566e30 on Place(cpu)
1884: I0815 04:34:56.697839 17809 scope.cc:202] Create variable 0x62566e301723696496697831468_inner_var_0
1884: I0815 04:34:56.697868 17809 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966976699080"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236964966976699080 -> 0x623aef30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:34:56.698016 17809 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:34:56.698132 17893 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.698264 17894 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.698266 17895 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.698421 17896 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.698427 17897 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.698419 17895 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236964966976699080:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.698467 17895 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236964966976699080:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:34:56.698493 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x62566fa0) got event_name: TaskCompletion
1884: I0815 04:34:56.698757 17895 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11604858123054477971 to 655489240136741322 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:34:56.698766 17895 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11604858123054477971 to 655489240136741322 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:34:56.698822 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.698830 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966989032641"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966989032641"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:34:56.699064 17809 pir_interpreter.cc:161] PirInterpreter(): 0x62566e30 on Place(cpu)
1884: I0815 04:34:56.699085 17809 scope.cc:202] Create variable 0x62566e301723696496699078533_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964966989032641"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236964966989032641 -> 0x61092740
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:34:56.699321 17898 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.699381 17899 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.699410 17900 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.699443 17901 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.699468 17902 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.699462 17900 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236964966989032641:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.699527 17900 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236964966989032641:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.699555 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x62566fa0) got event_name: TaskCompletion
1884: I0815 04:34:56.699734 17900 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11604858123054477971 to 655489240136741322 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:34:56.699743 17900 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11604858123054477971 to 655489240136741322 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:34:56.699843 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.699852 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236964966989032641",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236964966999386392"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236964966989032641",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236964966999386392"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:34:56.700114 17809 pir_interpreter.cc:161] PirInterpreter(): 0x62566e30 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236964966989032641",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236964966999386392"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236964966999386392 -> 0x61092740
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:34:56.700407 17903 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.700477 17904 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.700500 17905 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.700536 17906 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.700556 17907 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.700556 17906 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.700587 17906 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.700613 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x62566fa0) got event_name: TaskCompletion
1884: I0815 04:34:56.700906 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.700914 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964967009987943"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964967009987943"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:34:56.701146 17809 pir_interpreter.cc:161] PirInterpreter(): 0x62566e30 on Place(cpu)
1884: I0815 04:34:56.701169 17809 scope.cc:202] Create variable 0x62566e301723696496701161348_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236964967009987943"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236964967009987943 -> 0x623f6560
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:34:56.701388 17908 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.701462 17909 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:34:56.701480 17910 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:34:56.701519 17911 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:34:56.701542 17912 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:34:56.701539 17911 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236964967009987943:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.701581 17911 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236964967009987943:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:34:56.701606 17809 pir_interpreter.cc:1766] main_thread_blocker_(0x62566fa0) got event_name: TaskCompletion
1884: I0815 04:34:56.701790 17911 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7209293451220972402 to 655489240136741322 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:34:56.701798 17911 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7209293451220972402 to 655489240136741322 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:34:56.701905 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.701915 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.701980 17809 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:34:56.702044 17809 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:34:56.702087 17809 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964967009987943"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964966999386392"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964967009987943"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964966999386392"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:34:56.702857 17809 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:34:56.702875 17809 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:34:56.702911 17809 pir_interpreter.cc:161] PirInterpreter(): 0x62566e30 on Place(cpu)
1884: I0815 04:34:56.702945 17809 scope.cc:202] Create variable feed_name_0
1884: I0815 04:34:56.702962 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_5
1884: I0815 04:34:56.702989 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_6
1884: I0815 04:34:56.703004 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_7
1884: I0815 04:34:56.703017 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_8
1884: I0815 04:34:56.703044 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_9
1884: I0815 04:34:56.703059 17809 scope.cc:202] Create variable 0x62566e301723696496702927771_inner_var_10
1884: I0815 04:34:56.703085 17809 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.703109 17809 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:34:56.703244 17809 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.703261 17809 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964967009987943"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236964966999386392"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236964967009987943 -> 0x623f6560
1884: 1 -> constant_folding@_17236964966999386392 -> 0x61092740
1884: 2 -> linear_1.b_0 -> 0x5fbf07d0
1884: 3 -> linear_1.w_0 -> 0x623f4f10
1884: 4 -> feed_name_0 -> 0x623e3520
1884: 5 -> 0x62566e301723696496702927771_inner_var_5 -> 0x62175a80
1884: 6 -> 0x62566e301723696496702927771_inner_var_6 -> 0x6107a000
1884: 7 -> 0x62566e301723696496702927771_inner_var_7 -> 0x61db9db0
1884: 8 -> 0x62566e301723696496702927771_inner_var_8 -> 0x623e3640
1884: 9 -> fetch_name_0 -> 0x62502fa0
1884: 10 -> fetch_name_1 -> 0x61d0ab30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:34:56.703892 17809 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:34:56.703965 17913 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:34:56.703960 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704030 17809 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:34:56.704056 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.704090 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62566e301723696496702927771_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62566e301723696496702927771_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704135 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62566e301723696496702927771_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.704173 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704198 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.704221 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704258 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236964966999386392:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.704290 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236964967009987943:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704335 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236964967009987943:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62566e301723696496702927771_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:34:56.704370 17809 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236964967009987943:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:34:56.704407 17809 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236964967009987943:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62566e301723696496702927771_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:34:56.704443 17809 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:34:56.704468 17809 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:34:56.704494 17809 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:34:56.704627 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.704635 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.704685 17913 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 14028342524612449942 to 655489240136741322 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:34:56.704694 17913 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 14028342524612449942 to 655489240136741322 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:34:56.704733 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.704742 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [<paddle.base.libpaddle.Tensor object at 0x7f01916d3870>]
1884: [<paddle.base.libpaddle.Tensor object at 0x7f01916936f0>]
1884: [<paddle.base.libpaddle.Tensor object at 0x7f0191693330>]
1884: [<paddle.base.libpaddle.Tensor object at 0x7f0191672c70>]
1884: I0815 04:34:56.707028 17809 mmap_allocator.cc:348] PID: 17809, MemoryMapFdSet: set size - 0
1884: I0815 04:34:56.719722 17809 mmap_allocator.cc:348] PID: 17809, MemoryMapFdSet: set size - 0
1884: I0815 04:34:56.830761 17885 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 10788882360970600871 to 655489240136741322 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:34:56.830773 17885 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 10788882360970600871 to 655489240136741322 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:34:56.830798 17886 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 2403116344835204979 to 655489240136741322 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:34:56.830808 17886 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 2403116344835204979 to 655489240136741322 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:34:56.830830 17883 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2839402878587457699 to 655489240136741322 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:34:56.830842 17883 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2839402878587457699 to 655489240136741322 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:34:56.830987 17887 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 1770056163471629340 to 655489240136741322 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:34:56.830998 17887 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 1770056163471629340 to 655489240136741322 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:34:56.831004 17887 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 1770056163471629340 to 11217058432339162990 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:34:56.831248 17889 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 11217058432339162990 to 655489240136741322 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:34:56.831259 17889 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 11217058432339162990 to 655489240136741322 , after update, data is {current : 24, peak : 280}.
1884: I0815 04:34:56.831264 17889 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 11217058432339162990 to 655489240136741322 , after update, data is {current : 24, peak : 280}.
1884: I0815 04:34:56.831326 17890 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7658566565144259010 to 655489240136741322 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:34:56.831334 17890 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7658566565144259010 to 655489240136741322 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:34:56.831470 17892 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 655489240136741322 to 18011463120533723125 , after update, data is {current : 6401792, peak : 11200896}.
1884: I0815 04:34:56.831478 17892 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 655489240136741322 to 18011463120533723125 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 04:34:56.831483 17892 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 655489240136741322 to 18011463120533723125 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:34:56.988262 17809 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:34:56.988293 17809 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:34:56.988343 17809 mmap_allocator.cc:348] PID: 17809, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   12.78 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.95 sec
