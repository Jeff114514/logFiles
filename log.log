UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0814 08:07:04.914438 30891 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0814 08:07:05.709388 30891 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=enable_fuse_parallel_matmul_pass,gpugraph_offload_param_stat,enable_cublas_tensor_op_math,graph_neighbor_size_percent,nccl_dir,async_trace_count,enable_exit_when_partial_worker,call_stack_level,pir_apply_inplace_pass,prim_forward,sort_sum_gradient,new_executor_use_cuda_graph,enable_cinn_auto_tune,einsum_opt,win_cuda_bin_dir,gpugraph_storage_mode,tensor_operants_mode,prim_skip_dynamic,fraction_of_gpu_memory_to_use,enable_opt_get_features,use_cinn,benchmark_nccl,use_virtual_memory_auto_growth,new_executor_sequential_run,alloc_fill_value,cache_inference_while_scope,enable_pir_api,use_pinned_memory,log_memory_stats,enable_all2all_use_fp16,gpugraph_dedup_pull_push_mode,tracer_onednn_ops_off,gpugraph_merge_grads_segment_size,pir_broadcast_tree_limit,check_infer_symbolic,cinn_subgraph_graphviz_dir,cuda_memory_async_pool_realease_threshold,initial_gpu_memory_in_mb,gpugraph_parallel_stream_num,use_autotune,embedding_deterministic,mklml_dir,cuda_dir,enable_cinn_accuracy_check,new_executor_serial_run,convert_all_blocks,cudnn_batchnorm_spatial_persistent,accuracy_check_atol_bf16,local_exe_sub_scope_limit,enable_pir_in_executor,print_allocator_trace_info,set_to_1d,custom_device_mem_record,enable_cinn_compile_cache,dump_chunk_info,enable_dump_main_program,gpugraph_debug_gpu_memory,print_sub_graph_dir,pir_subgraph_saving_dir,cinn_compile_thread_num,enable_fusion_fallback,trt_ibuilder_cache,logging_pir_py_code_dir,use_cuda_managed_memory,gpu_allocator_retry_time,reader_queue_speed_test_mode,prim_enabled,memory_fraction_of_eager_deletion,enable_auto_detect_gpu_topo,graph_metapath_split_opt,add_dependency_for_communication_op,use_fast_math,enable_unused_var_check,gpugraph_hbm_table_load_factor,accuracy_check_atol_fp32,cupti_dir,dynamic_static_unified_comm,fleet_executor_with_standalone,gpugraph_enable_segment_merge_grads,run_kp_kernel,allocator_strategy,disable_dyshape_in_train,use_shm_cache,gpugraph_sparse_table_storage_mode,pir_apply_shape_optimization_pass,logging_trunc_pir_py_code,new_executor_static_build,enable_dependency_builder_debug_info,gemm_use_half_precision_compute_type,gpugraph_slot_feasign_max_num,check_kernel_launch,tracer_profile_fname,auto_growth_chunk_size_in_mb,all_blocks_convert_trt,save_static_runtime_data,manually_trans_conv_filter,cudnn_deterministic,gpugraph_offload_param_extends,op_dir,gpugraph_load_node_list_into_hbm,mkl_dir,gpugraph_enable_gpu_direct_access,logging_pir_py_code_int_tensor_element_limit,cudnn_dir,executor_log_deps_every_microseconds,conv_workspace_size_limit,enable_cse_in_dy2st,gpugraph_force_device_batch_num_equal,low_precision_op_list,reallocate_gpu_memory_in_mb,use_stream_safe_cuda_allocator,use_system_allocator,enable_api_kernel_fallback,pinned_memory_as_cpu_backend,prim_all,cusolver_dir,cublaslt_exhaustive_search_times,use_mkldnn,cublaslt_device_best_config,check_nan_inf_level,gpugraph_enable_hbm_table_collision_stat,cublas_dir,cusparse_dir,accuracy_check_atol_fp16,accuracy_check_rtol_bf16,dygraph_debug,nccl_blocking_wait,npu_storage_format,max_inplace_grad_add,accuracy_check_rtol_fp16,multiple_of_cupti_buffer_size,nvidia_package_dir,enable_sparse_inner_gather,cudnn_exhaustive_search_times,eager_delete_tensor_gb,fraction_of_cpu_memory_to_use,static_runtime_data_save_path,enable_graph_multi_node_sampling,sync_after_alloc,dataloader_use_file_descriptor,curand_dir,allreduce_record_one_event,use_auto_growth_v2,conv2d_disable_cudnn,enable_adjust_op_order,graph_get_neighbor_id,paddle_num_threads,prim_forward_blacklist,tracer_onednn_ops_on,prim_check_ops,pir_debug,accuracy_check_rtol_fp32,enable_tracker_all2all,lapack_dir,search_cache_max_number,eager_delete_scope,enable_async_trace,gpugraph_enable_print_op_debug,prim_enable_dynamic,print_ir,sync_nccl_allreduce,deny_cinn_ops,enable_pir_with_pt_in_dy2st,selected_gpus,graph_embedding_split_infer_mode,enable_auto_rdma_trans,enable_gpu_memory_usage_log_mb,cudnn_exhaustive_search,enable_interpretercore_launch_cinn,query_dest_rank_by_multi_node,graph_load_in_parallel,free_when_no_cache_hit,check_nan_inf,cse_max_count,static_executor_perfstat_filepath,multi_node_sample_use_gpu_table,dist_threadpool_size,new_executor_use_local_scope,initial_cpu_memory_in_mb,fuse_parameter_groups_size,enable_gpu_memory_usage_log,gpugraph_parallel_copyer_split_maxsize,get_host_by_name_time,gpugraph_offload_gather_copy_maxsize,ir_inplace_kernel_blacklist,gpu_memory_limit_mb,enable_record_memory,use_xqa_optim,enable_neighbor_list_use_uva,free_idle_chunk,fast_eager_deletion_mode,use_stride_kernel,enable_pir_in_executor_trace_run,use_auto_growth_pinned_allocator,enable_collect_shape,enable_blaslt_global_search,apply_pass_to_program,tensorrt_dir,prim_backward,logging_pir_py_code_dump_symbolic_dims,auto_free_cudagraph_allocations_on_launch,benchmark,fraction_of_cuda_pinned_memory_to_use,jit_engine_type,inner_op_parallelism,fuse_parameter_memory_size,cusparselt_dir,allow_cinn_ops,use_cuda_malloc_async_allocator,host_trace_level,cuda_malloc_async_pool_memory_throttle_ratio,new_executor_use_inplace,init_allocated_mem 
1884: I0814 08:07:05.709503 30891 init.cc:108] After Parse: argc is 2
1884: I0814 08:07:13.473131 30891 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:07:13.473166 30891 dygraph_functions.cc:77659] { Input: []} 
1884: W0814 08:07:13.473875 30891 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0814 08:07:13.474210 30891 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0814 08:07:13.475039 30891 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0814 08:07:13.475126 30891 allocator_facade.cc:212] selected allocator strategy:1
1884: I0814 08:07:13.475214 30891 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0814 08:07:13.475896 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5a8fc00000), and remaining 0
1884: I0814 08:07:13.476135 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.476193 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.476274 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5a8fc00200), and remaining 0
1884: I0814 08:07:13.476307 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5a8fc00400), and remaining 0
1884: I0814 08:07:13.480337 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5a8fc00600), and remaining 0
1884: I0814 08:07:13.480460 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5a8fc00800), and remaining 0
1884: I0814 08:07:13.480523 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f5a8fc00a00), and remaining 0
1884: I0814 08:07:13.480604 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.480624 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.480687 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.480700 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.481614 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a920ee0 for it.
1884: I0814 08:07:13.481748 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.481771 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.481827 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f5a8fc00e00), and remaining 0
1884: I0814 08:07:13.481899 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f5a8fcc4400), and remaining 0
1884: I0814 08:07:13.612035 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a920ee0 for it.
1884: I0814 08:07:13.612238 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.612288 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.612924 30891 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f5a94000000), and remaining 0
1884: I0814 08:07:13.621587 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a920ee0 for it.
1884: I0814 08:07:13.621691 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:13.621726 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.621764 30891 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:13.621929 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:13.622859 30891 dygraph_functions.cc:33459] Running AD API: full
1884: I0814 08:07:13.622877 30891 dygraph_functions.cc:33480] { Input: []} 
1884: I0814 08:07:13.622921 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:13.622996 30891 dygraph_functions.cc:64553] Running AD API: scale
1884: I0814 08:07:13.623020 30891 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.623073 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:13.623145 30891 dygraph_functions.cc:26170] Running AD API: exp
1884: I0814 08:07:13.623163 30891 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.623193 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:13.623378 30891 dygraph_functions.cc:72508] Running AD API: sum
1884: I0814 08:07:13.623397 30891 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.623554 30891 dygraph_functions.cc:83176] Running AD API: divide
1884: I0814 08:07:13.623579 30891 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:13.623646 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:13.625537 30891 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0814 08:07:13.625638 30891 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0814 08:07:13.625662 30891 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0814 08:07:13.625720 30891 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0814 08:07:15.120664 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.120746 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.121093 30891 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0814 08:07:15.121114 30891 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.127250 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.127288 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.128407 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.128425 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.128439 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.129246 30891 program_interpreter.cc:243] New Executor is Running.
1884: I0814 08:07:15.129259 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.129276 30891 scope.cc:202] Create variable feed
1884: I0814 08:07:15.129285 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.129294 30891 scope.cc:202] Create variable fetch
1884: I0814 08:07:15.129305 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.129318 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.129324 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.129328 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.129331 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.131791 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.132150 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.132164 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.132167 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.133834 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.133882 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.133891 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.133898 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.133908 30891 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:07:15.133913 30891 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x615c4110 type is 7
1884: I0814 08:07:15.133919 30891 scope.cc:202] Create variable x
1884: I0814 08:07:15.133922 30891 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x615b0320 type is 7
1884: I0814 08:07:15.133985 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.133991 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.133996 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.133999 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.134121 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.134145 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.134269 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.134279 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.134296 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.134464 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.134495 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.134514 30891 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.134519 30891 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x615cc100Variable Type 7
1884: I0814 08:07:15.134546 30891 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.134569 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.134622 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.134642 30891 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.136201 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.136257 30891 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.136691 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.141021 30891 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:07:15.141041 30891 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:07:15.141120 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.141148 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.141677 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: I0814 08:07:15.141745 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.141768 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.142215 30891 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: I0814 08:07:15.142274 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.142297 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.142329 30891 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.142576 30891 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:07:15.142586 30891 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:07:15.142695 30891 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0814 08:07:15.142719 30891 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.143098 30891 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:07:15.143110 30891 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:07:15.143155 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.143175 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.143365 30891 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:07:15.143375 30891 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:07:15.143410 30891 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:07:15.143429 30891 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:07:15.143445 30891 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.146164 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.146188 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.146242 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.146251 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.148156 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.148527 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.148543 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.148548 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.150336 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.150389 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.150400 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.150409 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x615f74e0 type is 7
1884: I0814 08:07:15.150417 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.150424 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x615f7850 type is 7
1884: I0814 08:07:15.150429 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.150434 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.150489 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.150497 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.150501 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.150506 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.150555 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.150574 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.150635 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.150645 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.150664 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.150899 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.150915 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.150933 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.150941 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x615fdfd0Variable Type 7
1884: I0814 08:07:15.150959 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.150977 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.151001 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.151018 30891 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.151816 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.151845 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.152019 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.162927 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: I0814 08:07:15.163137 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 08:07:15.169463 30891 pir_interpreter.cc:161] PirInterpreter(): 0x617ba540 on Place(gpu:0)
1884: I0814 08:07:15.169507 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.169536 30891 scope.cc:202] Create variable 0x617ba5401723622835169491770_inner_var_1
1884: I0814 08:07:15.169548 30891 scope.cc:202] Create variable 0x617ba5401723622835169491770_inner_var_2
1884: I0814 08:07:15.169559 30891 scope.cc:202] Create variable 0x617ba5401723622835169491770_inner_var_3
1884: I0814 08:07:15.169569 30891 scope.cc:202] Create variable 0x617ba5401723622835169491770_inner_var_4
1884: I0814 08:07:15.169579 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.169996 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.170011 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.170015 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0814 08:07:15.170058 30891 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x617b9c80
1884: 1 -> 0x617ba5401723622835169491770_inner_var_1 -> 0x617b9950
1884: 2 -> 0x617ba5401723622835169491770_inner_var_2 -> 0x617badd0
1884: 3 -> 0x617ba5401723622835169491770_inner_var_3 -> 0x617ba520
1884: 4 -> 0x617ba5401723622835169491770_inner_var_4 -> 0x617bb180
1884: 5 -> fetch0@fetch -> 0x617bb990
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.170822 30891 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0814 08:07:15.171101 30929 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.171249 30931 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.171249 30930 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.171351 30932 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.171433 30933 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.171551 30934 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.171530 30932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x617ba5401723622835169491770_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.171612 30934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.171676 30934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 08:07:15.171686 30932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x617ba5401723622835169491770_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.171736 30934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x617ba5401723622835169491770_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x617ba5401723622835169491770_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.172029 30934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x617ba5401723622835169491770_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x617ba5401723622835169491770_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 08:07:15.172112 30932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x617ba5401723622835169491770_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.172142 30932 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.173415 30932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x617ba5401723622835169491770_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x617ba5401723622835169491770_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:07:15.173461 30932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x617ba5401723622835169491770_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.173493 30932 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.174082 30932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x617ba5401723622835169491770_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:07:15.174129 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x617ba6b0) got event_name: TaskCompletion
1884: I0814 08:07:15.174162 30891 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.257959 30929 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 14191204146868911163 to 12944631260672994248 , after update, data is {current : 0, peak : 800768}.
1884: I0814 08:07:15.257990 30929 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14191204146868911163 to 8419767903331465015 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:07:15.257997 30929 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14191204146868911163 to 8419767903331465015 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:07:15.258204 30932 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8419767903331465015 to 1913625173855795099 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 08:07:15.258225 30932 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8419767903331465015 to 1913625173855795099 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 08:07:15.258428 30934 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0814 08:07:15.258445 30934 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.265451 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.265484 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.265547 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.265553 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.267369 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.267735 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.267750 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.267755 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.269280 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.269392 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.269403 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.269409 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x617b8080 type is 7
1884: I0814 08:07:15.269418 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.269420 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x617bb550 type is 7
1884: I0814 08:07:15.269425 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.269429 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.269486 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.269492 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.269496 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.269500 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.269552 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.269568 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.269631 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.269639 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.269654 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.269789 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.269799 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.269814 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.269821 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x615b2bb0Variable Type 7
1884: I0814 08:07:15.269837 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.269855 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.269877 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.269892 30891 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.271453 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.271492 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.271703 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.277825 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: I0814 08:07:15.278010 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 08:07:15.281150 30891 pir_interpreter.cc:161] PirInterpreter(): 0x615de7a0 on Place(gpu:0)
1884: I0814 08:07:15.281186 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.281209 30891 scope.cc:202] Create variable 0x615de7a01723622835281177679_inner_var_1
1884: I0814 08:07:15.281220 30891 scope.cc:202] Create variable 0x615de7a01723622835281177679_inner_var_2
1884: I0814 08:07:15.281232 30891 scope.cc:202] Create variable 0x615de7a01723622835281177679_inner_var_3
1884: I0814 08:07:15.281244 30891 scope.cc:202] Create variable 0x615de7a01723622835281177679_inner_var_4
1884: I0814 08:07:15.281255 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.281585 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.281601 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.281605 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x52f8c80
1884: 1 -> 0x615de7a01723622835281177679_inner_var_1 -> 0x63c724f0
1884: 2 -> 0x615de7a01723622835281177679_inner_var_2 -> 0x61380510
1884: 3 -> 0x615de7a01723622835281177679_inner_var_3 -> 0x615cc740
1884: 4 -> 0x615de7a01723622835281177679_inner_var_4 -> 0x52f9260
1884: 5 -> fetch0@fetch -> 0x39cfd70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.282316 30935 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.282416 30936 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.282447 30937 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.282517 30938 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.282516 30939 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.282572 30939 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615de7a01723622835281177679_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.282639 30939 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615de7a01723622835281177679_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.282699 30940 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.282722 30940 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.282747 30940 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:07:15.282784 30940 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615de7a01723622835281177679_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x615de7a01723622835281177679_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.282893 30940 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615de7a01723622835281177679_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x615de7a01723622835281177679_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.282972 30939 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615de7a01723622835281177679_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.283016 30939 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.286587 30939 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615de7a01723622835281177679_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x615de7a01723622835281177679_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.286638 30939 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x615de7a01723622835281177679_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.286660 30939 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.289621 30939 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x615de7a01723622835281177679_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.289665 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x615de910) got event_name: TaskCompletion
1884: I0814 08:07:15.289685 30891 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.329028 30935 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 12944631260672994248 to 8748220093554807323 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 08:07:15.329048 30935 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12944631260672994248 to 7632337268549823683 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:07:15.329053 30935 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12944631260672994248 to 7632337268549823683 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:07:15.329216 30939 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7632337268549823683 to 1913625173855795099 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 08:07:15.329227 30939 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7632337268549823683 to 1913625173855795099 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 08:07:15.329419 30940 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.333909 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.333933 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.333990 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.333997 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.335728 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.336081 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.336094 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.336099 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.337620 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.337716 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.337728 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.337733 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x53220d0 type is 7
1884: I0814 08:07:15.337741 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.337746 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x53184e0 type is 7
1884: I0814 08:07:15.337750 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.337755 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.337810 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.337816 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.337819 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.337822 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.337874 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.337888 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.337945 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.337953 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.337967 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.338004 30891 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.338122 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.338183 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.338193 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.338208 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.338215 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63f00110Variable Type 7
1884: I0814 08:07:15.338232 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.338249 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.338271 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.338285 30891 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.338524 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.338548 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.338748 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.339562 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: I0814 08:07:15.339751 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 08:07:15.342801 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63c12580 on Place(gpu:0)
1884: I0814 08:07:15.342833 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.342856 30891 scope.cc:202] Create variable 0x63c125801723622835342825706_inner_var_1
1884: I0814 08:07:15.342867 30891 scope.cc:202] Create variable 0x63c125801723622835342825706_inner_var_2
1884: I0814 08:07:15.342878 30891 scope.cc:202] Create variable 0x63c125801723622835342825706_inner_var_3
1884: I0814 08:07:15.342890 30891 scope.cc:202] Create variable 0x63c125801723622835342825706_inner_var_4
1884: I0814 08:07:15.342901 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.343236 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.343252 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.343256 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62981d0
1884: 1 -> 0x63c125801723622835342825706_inner_var_1 -> 0x617558f0
1884: 2 -> 0x63c125801723622835342825706_inner_var_2 -> 0x45389f50
1884: 3 -> 0x63c125801723622835342825706_inner_var_3 -> 0x3734780
1884: 4 -> 0x63c125801723622835342825706_inner_var_4 -> 0x651fc00
1884: 5 -> fetch0@fetch -> 0x615cb5e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.343937 30941 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.344029 30942 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.344066 30943 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.344098 30944 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.344137 30945 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.344182 30946 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.344182 30945 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63c125801723622835342825706_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.344205 30946 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.344239 30946 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 08:07:15.344244 30945 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63c125801723622835342825706_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.344272 30946 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63c125801723622835342825706_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63c125801723622835342825706_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.344327 30946 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.344446 30946 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.344473 30946 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63c125801723622835342825706_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63c125801723622835342825706_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 08:07:15.344537 30945 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63c125801723622835342825706_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.344559 30945 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.344859 30945 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63c125801723622835342825706_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63c125801723622835342825706_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:07:15.344885 30945 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63c125801723622835342825706_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.344903 30945 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.344918 30945 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63c125801723622835342825706_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:07:15.344951 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x63c126f0) got event_name: TaskCompletion
1884: I0814 08:07:15.344974 30891 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.385586 30941 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 8748220093554807323 to 12944631260672994248 , after update, data is {current : 0, peak : 3328}.
1884: I0814 08:07:15.385609 30941 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8748220093554807323 to 12944631260672994248 , after update, data is {current : -804, peak : 2000}.
1884: I0814 08:07:15.385614 30941 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8748220093554807323 to 12944631260672994248 , after update, data is {current : -804, peak : 2000}.
1884: I0814 08:07:15.385823 30945 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1644672459677850030 to 12944631260672994248 , after update, data is {current : 800, peak : 2000}.
1884: I0814 08:07:15.385835 30945 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1644672459677850030 to 12944631260672994248 , after update, data is {current : 800, peak : 2000}.
1884: I0814 08:07:15.386008 30946 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 08:07:15.386019 30946 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 08:07:15.386024 30946 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.390854 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.390877 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.390934 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.390941 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.392724 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.393078 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.393092 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.393097 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.394627 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.394731 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.394743 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.394750 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5318c40 type is 7
1884: I0814 08:07:15.394759 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.394763 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x52dfe60 type is 7
1884: I0814 08:07:15.394767 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.394771 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.394826 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.394833 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.394836 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.394840 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.394891 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.394904 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.394965 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.394974 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.394987 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.395231 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.395244 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.395259 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.395267 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x615ad910Variable Type 7
1884: I0814 08:07:15.395282 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.395306 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.395329 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.395344 30891 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.396147 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.396175 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.396389 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.401156 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: I0814 08:07:15.401342 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 08:07:15.404386 30891 pir_interpreter.cc:161] PirInterpreter(): 0x615ccba0 on Place(gpu:0)
1884: I0814 08:07:15.404421 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.404443 30891 scope.cc:202] Create variable 0x615ccba01723622835404412943_inner_var_1
1884: I0814 08:07:15.404455 30891 scope.cc:202] Create variable 0x615ccba01723622835404412943_inner_var_2
1884: I0814 08:07:15.404466 30891 scope.cc:202] Create variable 0x615ccba01723622835404412943_inner_var_3
1884: I0814 08:07:15.404477 30891 scope.cc:202] Create variable 0x615ccba01723622835404412943_inner_var_4
1884: I0814 08:07:15.404489 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.404815 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.404830 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.404834 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5331880
1884: 1 -> 0x615ccba01723622835404412943_inner_var_1 -> 0x615c58a0
1884: 2 -> 0x615ccba01723622835404412943_inner_var_2 -> 0x615e5cc0
1884: 3 -> 0x615ccba01723622835404412943_inner_var_3 -> 0x615e4f50
1884: 4 -> 0x615ccba01723622835404412943_inner_var_4 -> 0x6397eab0
1884: 5 -> fetch0@fetch -> 0x615c4bc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.405530 30947 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.405627 30948 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.405645 30949 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.405721 30950 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.405730 30951 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.405782 30952 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.405786 30950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ccba01723622835404412943_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.405804 30952 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.405844 30952 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 08:07:15.405877 30950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ccba01723622835404412943_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.405908 30952 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615ccba01723622835404412943_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x615ccba01723622835404412943_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.406111 30952 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615ccba01723622835404412943_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x615ccba01723622835404412943_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 08:07:15.406189 30950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615ccba01723622835404412943_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.406221 30950 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.407474 30950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615ccba01723622835404412943_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x615ccba01723622835404412943_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:07:15.407528 30950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x615ccba01723622835404412943_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.407552 30950 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.408205 30950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x615ccba01723622835404412943_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:07:15.408246 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x615ccd10) got event_name: TaskCompletion
1884: I0814 08:07:15.408267 30891 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.444778 30947 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 12944631260672994248 to 8748220093554807323 , after update, data is {current : 0, peak : 800768}.
1884: I0814 08:07:15.444799 30947 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 12944631260672994248 to 15841787502719477104 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:07:15.444804 30947 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 12944631260672994248 to 15841787502719477104 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:07:15.444975 30950 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 15841787502719477104 to 1913625173855795099 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 08:07:15.444988 30950 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 15841787502719477104 to 1913625173855795099 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 08:07:15.445175 30952 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0814 08:07:15.445189 30952 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.450726 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.450753 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.450810 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.450817 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.452584 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.452945 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.452960 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.452965 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.454504 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.454594 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.454607 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.454612 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63963c90 type is 7
1884: I0814 08:07:15.454622 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.454625 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3a80ef0 type is 7
1884: I0814 08:07:15.454629 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.454633 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.454689 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.454695 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.454699 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.454702 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.454753 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.454767 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.454828 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.454835 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.454849 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.454973 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.454983 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.454998 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.455004 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6369cd10Variable Type 7
1884: I0814 08:07:15.455020 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.455039 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.455056 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.455071 30891 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.456629 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.456662 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.456872 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.461714 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: I0814 08:07:15.461905 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 08:07:15.464972 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63f0b600 on Place(gpu:0)
1884: I0814 08:07:15.465006 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.465029 30891 scope.cc:202] Create variable 0x63f0b6001723622835464997482_inner_var_1
1884: I0814 08:07:15.465040 30891 scope.cc:202] Create variable 0x63f0b6001723622835464997482_inner_var_2
1884: I0814 08:07:15.465052 30891 scope.cc:202] Create variable 0x63f0b6001723622835464997482_inner_var_3
1884: I0814 08:07:15.465062 30891 scope.cc:202] Create variable 0x63f0b6001723622835464997482_inner_var_4
1884: I0814 08:07:15.465072 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.465409 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.465425 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.465430 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x63c505e0
1884: 1 -> 0x63f0b6001723622835464997482_inner_var_1 -> 0x615bb4d0
1884: 2 -> 0x63f0b6001723622835464997482_inner_var_2 -> 0x6272810
1884: 3 -> 0x63f0b6001723622835464997482_inner_var_3 -> 0x6397f060
1884: 4 -> 0x63f0b6001723622835464997482_inner_var_4 -> 0x636ad460
1884: 5 -> fetch0@fetch -> 0x654f3f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.466130 30953 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.466228 30954 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.466255 30955 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.466312 30956 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.466356 30957 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.466406 30958 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.466400 30957 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63f0b6001723622835464997482_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.466434 30958 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.466466 30957 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63f0b6001723622835464997482_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.466476 30958 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:07:15.466512 30958 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63f0b6001723622835464997482_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63f0b6001723622835464997482_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.466652 30958 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63f0b6001723622835464997482_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63f0b6001723622835464997482_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.466714 30957 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63f0b6001723622835464997482_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.466742 30957 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.469570 30957 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63f0b6001723622835464997482_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63f0b6001723622835464997482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.469619 30957 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63f0b6001723622835464997482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.469643 30957 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.471640 30957 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63f0b6001723622835464997482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:07:15.471690 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x63f0b770) got event_name: TaskCompletion
1884: I0814 08:07:15.471711 30891 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.510202 30953 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8748220093554807323 to 12944631260672994248 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 08:07:15.510221 30953 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8748220093554807323 to 1644672459677850030 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:07:15.510227 30953 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8748220093554807323 to 1644672459677850030 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:07:15.510407 30957 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1644672459677850030 to 1913625173855795099 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 08:07:15.510418 30957 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1644672459677850030 to 1913625173855795099 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 08:07:15.510612 30958 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 12944631260672994248 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.515038 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.515066 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.515120 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.515128 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.516863 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.517215 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.517230 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.517235 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.518761 30891 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:07:15.518853 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.518867 30891 scope.cc:202] Create variable Out
1884: I0814 08:07:15.518872 30891 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63c80bd0 type is 7
1884: I0814 08:07:15.518882 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.518884 30891 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6159d950 type is 7
1884: I0814 08:07:15.518888 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.518893 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.518946 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.518954 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.518957 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.518960 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.519011 30891 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.519024 30891 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.519085 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.519094 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.519109 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.519147 30891 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.519268 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.519326 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.519338 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.519353 30891 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.519361 30891 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63c95d60Variable Type 7
1884: I0814 08:07:15.519376 30891 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.519394 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.519416 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.519430 30891 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.519567 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.519590 30891 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.519783 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.520550 30891 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a975320 for it.
1884: I0814 08:07:15.520741 30891 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a937a00 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 08:07:15.523763 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63583c50 on Place(gpu:0)
1884: I0814 08:07:15.523795 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.523818 30891 scope.cc:202] Create variable 0x63583c501723622835523786833_inner_var_1
1884: I0814 08:07:15.523828 30891 scope.cc:202] Create variable 0x63583c501723622835523786833_inner_var_2
1884: I0814 08:07:15.523840 30891 scope.cc:202] Create variable 0x63583c501723622835523786833_inner_var_3
1884: I0814 08:07:15.523851 30891 scope.cc:202] Create variable 0x63583c501723622835523786833_inner_var_4
1884: I0814 08:07:15.523859 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.524178 30891 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:07:15.524192 30891 scope.cc:202] Create variable X
1884: I0814 08:07:15.524196 30891 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x615c47f0
1884: 1 -> 0x63583c501723622835523786833_inner_var_1 -> 0x617bd0a0
1884: 2 -> 0x63583c501723622835523786833_inner_var_2 -> 0x18b70a80
1884: 3 -> 0x63583c501723622835523786833_inner_var_3 -> 0x3c4ad80
1884: 4 -> 0x63583c501723622835523786833_inner_var_4 -> 0x617b9610
1884: 5 -> fetch0@fetch -> 0x617b87f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:07:15.524891 30959 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.524977 30960 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.525017 30961 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.525048 30962 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.525099 30963 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.525133 30963 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63583c501723622835523786833_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.525189 30963 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63583c501723622835523786833_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.525249 30964 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.525271 30964 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.525296 30964 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 08:07:15.525331 30964 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63583c501723622835523786833_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63583c501723622835523786833_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.525369 30964 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.525483 30964 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.525511 30964 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63583c501723622835523786833_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63583c501723622835523786833_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 08:07:15.525573 30963 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63583c501723622835523786833_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.525611 30963 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.525736 30963 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63583c501723622835523786833_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63583c501723622835523786833_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:07:15.525765 30963 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63583c501723622835523786833_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.525784 30963 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.525797 30963 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63583c501723622835523786833_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:07:15.525830 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x63583dc0) got event_name: TaskCompletion
1884: I0814 08:07:15.525852 30891 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.560307 30959 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 12944631260672994248 to 8748220093554807323 , after update, data is {current : 0, peak : 10240}.
1884: I0814 08:07:15.560328 30959 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12944631260672994248 to 8748220093554807323 , after update, data is {current : -804, peak : 8000}.
1884: I0814 08:07:15.560333 30959 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12944631260672994248 to 8748220093554807323 , after update, data is {current : -804, peak : 8000}.
1884: I0814 08:07:15.560603 30963 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 7632337268549823683 to 8748220093554807323 , after update, data is {current : 800, peak : 8000}.
1884: I0814 08:07:15.560616 30963 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 7632337268549823683 to 8748220093554807323 , after update, data is {current : 800, peak : 8000}.
1884: I0814 08:07:15.560734 30964 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 08:07:15.560746 30964 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 08:07:15.560751 30964 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 8748220093554807323 to 1913625173855795099 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:07:15.567525 30891 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0814 08:07:15.567576 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 08:07:15.568655 30891 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:07:15.569484 30891 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0814 08:07:15.569515 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 08:07:15.570814 30891 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0814 08:07:15.570839 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:07:15.571533 30891 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0814 08:07:15.572505 30891 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0814 08:07:15.572532 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:07:15.573855 30891 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0814 08:07:15.573879 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:07:15.574465 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.574491 30891 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:07:15.574497 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.574503 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.576491 30891 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0814 08:07:15.576517 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:07:15.577471 30891 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0814 08:07:15.577498 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 08:07:15.578513 30891 pybind.cc:1827] need skip: 0
1884: I0814 08:07:15.578810 30891 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:07:15.580574 30891 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:07:15.584123 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.584142 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.584146 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.586165 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.586185 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.586194 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.586200 30891 scope.cc:202] Create variable learning_rate_0
1884: I0814 08:07:15.586206 30891 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6247ea80 type is 7
1884: I0814 08:07:15.586210 30891 scope.cc:202] Create variable linear_0.b_0
1884: I0814 08:07:15.586213 30891 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6247fda0 type is 7
1884: I0814 08:07:15.586217 30891 scope.cc:202] Create variable linear_0.w_0
1884: I0814 08:07:15.586220 30891 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6247fe20 type is 7
1884: I0814 08:07:15.586283 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.586290 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.586293 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.586297 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.586359 30891 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586372 30891 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586395 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 08:07:15.586529 30891 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586539 30891 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586601 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.586645 30891 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586654 30891 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.586679 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.587711 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.589046 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.589495 30891 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:07:15.589730 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.590030 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.590245 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.590261 30891 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:07:15.590338 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.590345 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.590349 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.590449 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.590461 30891 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:07:15.592098 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.593425 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.594516 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.594708 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.594722 30891 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 08:07:15.594727 30891 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x63c797c0 type is 7
1884: I0814 08:07:15.594734 30891 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:07:15.594738 30891 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x63c79440 type is 7
1884: I0814 08:07:15.594743 30891 scope.cc:202] Create variable cast_0.tmp_0
1884: I0814 08:07:15.594746 30891 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x6238af40 type is 7
1884: I0814 08:07:15.594751 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.594755 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.594759 30891 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 08:07:15.594763 30891 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x63c7a920 type is 7
1884: I0814 08:07:15.594767 30891 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6247ea80 type is 7
1884: I0814 08:07:15.594771 30891 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6247fda0 type is 7
1884: I0814 08:07:15.594775 30891 scope.cc:202] Create variable linear_0.tmp_0
1884: I0814 08:07:15.594779 30891 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x63c7a900 type is 7
1884: I0814 08:07:15.594782 30891 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 08:07:15.594785 30891 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x63c7ae60 type is 7
1884: I0814 08:07:15.594790 30891 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6247fe20 type is 7
1884: I0814 08:07:15.594794 30891 scope.cc:202] Create variable mean_0.tmp_0
1884: I0814 08:07:15.594797 30891 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x63c7b0d0 type is 7
1884: I0814 08:07:15.594801 30891 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0814 08:07:15.594805 30891 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x63c7b310 type is 7
1884: I0814 08:07:15.594810 30891 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:07:15.594812 30891 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x63c7b570 type is 7
1884: I0814 08:07:15.594899 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.594915 30891 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:07:15.594974 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.594981 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.594985 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.594988 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.595038 30891 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.595050 30891 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.595067 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 08:07:15.595181 30891 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.595192 30891 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.595211 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:07:15.595283 30891 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:07:15.595371 30891 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0814 08:07:15.596535 30891 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596557 30891 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596621 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.596684 30891 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596694 30891 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596709 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:07:15.596731 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.596771 30891 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596781 30891 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596792 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:07:15.596879 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596889 30891 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.596902 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.597009 30891 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.597087 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.597142 30891 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597151 30891 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597165 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:07:15.597198 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.597249 30891 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597258 30891 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597272 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 08:07:15.597386 30891 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597397 30891 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597417 30891 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.597463 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.597471 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.597487 30891 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.597494 30891 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x635975a0Variable Type 7
1884: I0814 08:07:15.597510 30891 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.597528 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.597548 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597561 30891 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.597600 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.597625 30891 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:07:15.597651 30891 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.597659 30891 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.597672 30891 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:07:15.597678 30891 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62357bd0Variable Type 7
1884: I0814 08:07:15.597690 30891 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:07:15.597702 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.597718 30891 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.597729 30891 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.597761 30891 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:07:15.597775 30891 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0814 08:07:15.598225 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:07:15.598259 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:07:15.598277 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:07:15.598318 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:07:15.598352 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.598371 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:07:15.602818 30891 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 08:07:15.602852 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:07:15.603586 30891 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 08:07:15.603610 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:07:15.603974 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.605680 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.606540 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.606664 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.607177 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.608081 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.610154 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.611202 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.613061 30891 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0814 08:07:15.613822 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.613839 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.613843 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.615059 30891 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:07:15.615077 30891 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x52ee7e0 type is 9
1884: I0814 08:07:15.615084 30891 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x52eb910 type is 10
1884: I0814 08:07:15.615092 30891 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6247fda0 type is 7
1884: I0814 08:07:15.615095 30891 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6247fe20 type is 7
1884: I0814 08:07:15.615099 30891 scope.cc:202] Create variable saved_params
1884: I0814 08:07:15.615103 30891 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x6241dc20 type is 17
1884: I0814 08:07:15.615130 30891 interpreter_util.cc:594] Static build: 0
1884: I0814 08:07:15.615137 30891 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:07:15.615140 30891 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:07:15.615144 30891 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:07:15.615185 30891 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.615197 30891 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:07:15.615947 30891 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 08:07:15.615989 30891 analysis_predictor.cc:433] Predictor::init()
1884: I0814 08:07:15.616048 30891 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0814 08:07:15.617270 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.617336 30891 scope.cc:202] Create variable feed
1884: I0814 08:07:15.617343 30891 naive_executor.cc:189] 0x63868d80 Create persistable variable feed, which pointer is 0x6386aaf0
1884: I0814 08:07:15.617348 30891 scope.cc:202] Create variable fetch
1884: I0814 08:07:15.617352 30891 naive_executor.cc:189] 0x63868d80 Create persistable variable fetch, which pointer is 0x6386a990
1884: I0814 08:07:15.617355 30891 scope.cc:202] Create variable linear_0.b_0
1884: I0814 08:07:15.617358 30891 naive_executor.cc:189] 0x63868d80 Create persistable variable linear_0.b_0, which pointer is 0x63868fa0
1884: I0814 08:07:15.617363 30891 scope.cc:202] Create variable linear_0.w_0
1884: I0814 08:07:15.617367 30891 naive_executor.cc:189] 0x63868d80 Create persistable variable linear_0.w_0, which pointer is 0x63867b70
1884: I0814 08:07:15.617383 30891 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0814 08:07:15.617730 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.617815 30891 program_converter.cc:296] is_legacy_program : 0
1884: I0814 08:07:15.617862 30891 executor.cc:183] Old Executor is Running.
1884: I0814 08:07:15.617933 30891 executor.cc:92] Creating Variables for block 0
1884: I0814 08:07:15.617940 30891 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0814 08:07:15.617944 30891 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x63868fa0 type is 7
1884: I0814 08:07:15.617946 30891 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0814 08:07:15.617949 30891 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x63867b70 type is 7
1884: I0814 08:07:15.617981 30891 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.618057 30891 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0814 08:07:15.618099 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.618105 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.618239 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.618355 30891 graph.cc:149] create OpNode by feed
1884: I0814 08:07:15.618393 30891 graph.cc:149] create OpNode by matmul_v2
1884: I0814 08:07:15.618409 30891 graph.cc:149] create OpNode by elementwise_add
1884: I0814 08:07:15.618424 30891 graph.cc:149] create OpNode by abs
1884: I0814 08:07:15.618435 30891 graph.cc:149] create OpNode by assign_value
1884: I0814 08:07:15.618451 30891 graph.cc:149] create OpNode by multinomial
1884: I0814 08:07:15.618461 30891 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:07:15.618475 30891 graph.cc:149] create OpNode by scale
1884: I0814 08:07:15.618489 30891 graph.cc:149] create OpNode by scale
1884: I0814 08:07:15.618500 30891 graph.cc:149] create OpNode by fetch
1884: I0814 08:07:15.618516 30891 graph.cc:149] create OpNode by fetch
1884: I0814 08:07:15.618536 30891 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0814 08:07:15.619760 30891 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0814 08:07:15.619768 30891 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0814 08:07:15.619841 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.619848 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0814 08:07:15.619961 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620211 30891 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 08:07:15.620270 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620275 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0814 08:07:15.620316 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620321 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0814 08:07:15.620361 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620420 30891 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:07:15.620452 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620457 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0814 08:07:15.620476 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620487 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.620510 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620515 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0814 08:07:15.620553 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620573 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.620597 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620602 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0814 08:07:15.620644 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620719 30891 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:07:15.620748 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620754 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0814 08:07:15.620783 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.620802 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.620824 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.620829 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0814 08:07:15.620857 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621007 30891 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 08:07:15.621035 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.621040 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0814 08:07:15.621070 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621088 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.621109 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.621114 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0814 08:07:15.621135 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621150 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.621170 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.621174 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0814 08:07:15.621197 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621210 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.621232 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.621237 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0814 08:07:15.621261 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621343 30891 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 08:07:15.621379 30891 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:07:15.621394 30891 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:07:15.621408 30891 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0814 08:07:15.621433 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.621438 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0814 08:07:15.621459 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.621500 30891 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 08:07:15.621519 30891 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:07:15.621531 30891 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:07:15.621543 30891 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:07:15.621572 30891 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 08:07:15.621583 30891 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0814 08:07:15.622761 30891 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:07:15.622807 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.622814 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0814 08:07:15.622840 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.622860 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.622886 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.622891 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0814 08:07:15.622913 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.622962 30891 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:07:15.622992 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.622997 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0814 08:07:15.623015 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.623030 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.623052 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.623057 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0814 08:07:15.623090 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.623176 30891 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0814 08:07:15.623205 30891 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:07:15.623220 30891 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:07:15.623235 30891 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:07:15.623250 30891 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 08:07:15.623265 30891 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 08:07:15.623281 30891 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0814 08:07:15.623309 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.623382 30891 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0814 08:07:15.623405 30891 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:07:15.623418 30891 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:07:15.623431 30891 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:07:15.623446 30891 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 08:07:15.623461 30891 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 08:07:15.623476 30891 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0814 08:07:15.623518 30891 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 08:07:15.623791 30891 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:07:15.623821 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.623826 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0814 08:07:15.623874 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.623934 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.623967 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624013 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624040 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624081 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624105 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624142 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624163 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624195 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624213 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624243 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624259 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624285 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624298 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624326 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624338 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624356 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624382 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624387 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0814 08:07:15.624411 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624452 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624477 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624482 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0814 08:07:15.624491 30891 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 08:07:15.624495 30891 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:07:15.624541 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624562 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624588 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624591 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0814 08:07:15.624600 30891 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 08:07:15.624604 30891 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:07:15.624644 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624665 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624691 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624696 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0814 08:07:15.624703 30891 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 08:07:15.624706 30891 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:07:15.624738 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624756 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624778 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624783 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0814 08:07:15.624791 30891 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 08:07:15.624794 30891 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:07:15.624831 30891 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:07:15.624851 30891 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:07:15.624874 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624879 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0814 08:07:15.624890 30891 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0814 08:07:15.624933 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.624938 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0814 08:07:15.625007 30891 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:07:15.625030 30891 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.625047 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:07:15.625102 30891 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0814 08:07:15.625119 30891 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:07:15.625145 30891 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:07:15.625169 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.625173 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0814 08:07:15.626116 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.626130 30891 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0814 08:07:15.626180 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.626186 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.626796 30891 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0814 08:07:15.627007 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.627079 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.627084 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.627498 30891 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:07:15.627718 30891 graph.h:183] deleting __fuse_statis__
1884: I0814 08:07:15.627727 30891 graph.h:183] deleting pass_recorder
1884: I0814 08:07:15.627732 30891 graph.h:183] deleting stale_program_op_descs
1884: I0814 08:07:15.627816 30891 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0814 08:07:15.627826 30891 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 08:07:15.627830 30891 naive_executor.cc:195] 0x63868d80 Create variable abs_0.tmp_0, which pointer is 0x617d0f10
1884: I0814 08:07:15.627835 30891 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 08:07:15.627841 30891 naive_executor.cc:195] 0x63868d80 Create variable gaussian_0.tmp_0, which pointer is 0x63cfa1a0
1884: I0814 08:07:15.627853 30891 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 08:07:15.627857 30891 naive_executor.cc:195] 0x63868d80 Create variable linear_0.tmp_1, which pointer is 0x63c0f7f0
1884: I0814 08:07:15.627861 30891 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:07:15.627864 30891 naive_executor.cc:195] 0x63868d80 Create variable multinomial_0.tmp_0, which pointer is 0x63c0f290
1884: I0814 08:07:15.627867 30891 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0814 08:07:15.627871 30891 naive_executor.cc:195] 0x63868d80 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x63c0f590
1884: I0814 08:07:15.627873 30891 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0814 08:07:15.627876 30891 naive_executor.cc:195] 0x63868d80 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x63c0db90
1884: I0814 08:07:15.627882 30891 scope.cc:202] Create variable feed
1884: I0814 08:07:15.627885 30891 scope.cc:202] Create variable fetch
1884: I0814 08:07:15.627905 30891 naive_executor.cc:46] NaiveExecutor init with scope 0x63868d80
1884: I0814 08:07:15.627911 30891 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0814 08:07:15.628103 30891 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:07:15.628118 30891 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:07:15.628144 30891 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0814 08:07:15.628150 30891 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0814 08:07:15.628158 30891 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.628191 30891 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:07:15.628407 30891 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.628424 30891 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:07:15.628469 30891 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.628494 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0814 08:07:15.679088 30891 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:07:15.679165 30891 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.679188 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:07:15.679239 30891 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:07:15.679267 30891 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.679289 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:07:15.679356 30891 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 08:07:15.679395 30891 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.679411 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:07:15.679463 30891 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:07:15.679491 30891 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:07:15.679505 30891 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:07:15.679538 30891 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 08:07:15.679555 30891 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.679582 30891 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:07:15.679605 30891 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 08:07:15.679955 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.679966 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0814 08:07:15.722831 30891 pir_interpreter.cc:161] PirInterpreter(): 0x6385fca0 on Place(gpu:0)
1884: I0814 08:07:15.722873 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_0
1884: I0814 08:07:15.722890 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_1
1884: I0814 08:07:15.722899 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_2
1884: I0814 08:07:15.722908 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_3
1884: I0814 08:07:15.722937 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_4
1884: I0814 08:07:15.722951 30891 scope.cc:202] Create variable 0x6385fca01723622835722861113_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x6385fca01723622835722861113_inner_var_0 -> 0x5331140
1884: 1 -> 0x6385fca01723622835722861113_inner_var_1 -> 0x5331af0
1884: 2 -> 0x6385fca01723622835722861113_inner_var_2 -> 0x530db30
1884: 3 -> linear_1.w_0 -> 0x617ba500
1884: 4 -> linear_1.b_0 -> 0x615c4cf0
1884: 5 -> learning_rate_1 -> 0x63597b10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 08:07:15.723785 30965 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.723802 30966 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.723826 30967 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.723871 30968 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.723920 30969 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.723922 30967 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.723935 30966 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.723917 30968 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.723949 30969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.724011 30969 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.724009 30966 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.724014 30967 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.724009 30968 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6385fca01723622835722861113_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:07:15.724057 30969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0814 08:07:15.724081 30969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.724102 30969 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.724114 30969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:07:15.724129 30969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6385fca01723622835722861113_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6385fca01723622835722861113_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6385fca01723622835722861113_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.724193 30969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6385fca01723622835722861113_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6385fca01723622835722861113_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6385fca01723622835722861113_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0814 08:07:15.724254 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x6385fe10) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0814 08:07:15.726246 30891 pir_interpreter.cc:161] PirInterpreter(): 0x615e7590 on Place(gpu:0)
1884: I0814 08:07:15.726279 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_1
1884: I0814 08:07:15.726295 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_4
1884: I0814 08:07:15.726311 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_5
1884: I0814 08:07:15.726320 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_6
1884: I0814 08:07:15.726341 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_7
1884: I0814 08:07:15.726352 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_8
1884: I0814 08:07:15.726362 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_9
1884: I0814 08:07:15.726388 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_10
1884: I0814 08:07:15.726397 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_11
1884: I0814 08:07:15.726404 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_12
1884: I0814 08:07:15.726414 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_13
1884: I0814 08:07:15.726423 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_14
1884: I0814 08:07:15.726433 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_15
1884: I0814 08:07:15.726440 30891 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:07:15.726451 30891 scope.cc:202] Create variable 0x615e75901723622835726268251_inner_var_17
1884: I0814 08:07:15.726460 30891 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x63597b10
1884: 1 -> 0x615e75901723622835726268251_inner_var_1 -> 0x615dcf70
1884: 2 -> linear_1.b_0 -> 0x615c4cf0
1884: 3 -> linear_1.w_0 -> 0x617ba500
1884: 4 -> 0x615e75901723622835726268251_inner_var_4 -> 0x6235ef50
1884: 5 -> 0x615e75901723622835726268251_inner_var_5 -> 0x61594880
1884: 6 -> 0x615e75901723622835726268251_inner_var_6 -> 0x615948c0
1884: 7 -> 0x615e75901723622835726268251_inner_var_7 -> 0x62444a60
1884: 8 -> 0x615e75901723622835726268251_inner_var_8 -> 0x61451400
1884: 9 -> 0x615e75901723622835726268251_inner_var_9 -> 0x617bb7c0
1884: 10 -> 0x615e75901723622835726268251_inner_var_10 -> 0x4538dfb0
1884: 11 -> 0x615e75901723622835726268251_inner_var_11 -> 0x613a04c0
1884: 12 -> 0x615e75901723622835726268251_inner_var_12 -> 0x615778b0
1884: 13 -> 0x615e75901723622835726268251_inner_var_13 -> 0x617b82d0
1884: 14 -> 0x615e75901723622835726268251_inner_var_14 -> 0x617bb3b0
1884: 15 -> 0x615e75901723622835726268251_inner_var_15 -> 0x45394e00
1884: 16 -> fetch0@fetch -> 0x62478170
1884: 17 -> 0x615e75901723622835726268251_inner_var_17 -> 0x615b29d0
1884: 18 -> fetch1@fetch -> 0x615cc6f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0814 08:07:15.728118 30970 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.728229 30971 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.728240 30972 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.728348 30973 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.728396 30973 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728408 30972 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728420 30974 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:07:15.728427 30973 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:07:15.728441 30972 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.728456 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728485 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:07:15.728511 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728549 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.728605 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:07:15.728621 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:07:15.728682 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:07:15.728700 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x615e75901723622835726268251_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728742 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x615e75901723622835726268251_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:07:15.728768 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x615e75901723622835726268251_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728806 30974 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:07:15.728844 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x615e75901723622835726268251_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.728873 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x615e75901723622835726268251_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x615e75901723622835726268251_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728915 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.728932 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x615e75901723622835726268251_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x615e75901723622835726268251_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.728963 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x615e75901723622835726268251_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728986 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.728989 30972 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615e75901723622835726268251_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.728999 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x615e75901723622835726268251_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.729017 30972 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.729017 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x615e75901723622835726268251_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729043 30974 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.729096 30972 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615e75901723622835726268251_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.729130 30972 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x615e75901723622835726268251_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729151 30974 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.729153 30972 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.729175 30972 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x615e75901723622835726268251_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.729204 30974 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.729269 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.729290 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x615e75901723622835726268251_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x615e75901723622835726268251_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.729332 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x615e75901723622835726268251_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729338 30972 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615e75901723622835726268251_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729355 30972 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:07:15.729362 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.729379 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x615e75901723622835726268251_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.729389 30972 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x615e75901723622835726268251_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.729396 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x615e75901723622835726268251_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729408 30972 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x615e75901723622835726268251_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729424 30972 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.729436 30972 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x615e75901723622835726268251_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.729475 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x615e75901723622835726268251_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:07:15.729494 30974 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x615e75901723622835726268251_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615e75901723622835726268251_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.729523 30974 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:07:15.729537 30974 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x615e75901723622835726268251_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615e75901723622835726268251_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x615e75901723622835726268251_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:07:15.729571 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x615e7700) got event_name: TaskCompletion
1884: I0814 08:07:15.729601 30891 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.729635 30891 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 08:07:15.742106 30891 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 08:07:15.742161 30891 analysis_predictor.cc:433] Predictor::init()
1884: I0814 08:07:15.742952 30891 scope.cc:202] Create variable linear_1.b_0
1884: I0814 08:07:15.743011 30891 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0814 08:07:15.743470 30891 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357435226750"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357435226750"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0814 08:07:15.743659 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63597080 on Place(cpu)
1884: I0814 08:07:15.743680 30891 scope.cc:202] Create variable 0x635970801723622835743674393_inner_var_0
1884: I0814 08:07:15.743706 30891 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357435226750"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236228357435226750 -> 0x62711950
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0814 08:07:15.743844 30891 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0814 08:07:15.743988 30975 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.744125 30976 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.744175 30977 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.744246 30978 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.744239 30976 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236228357435226750:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.744285 30976 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236228357435226750:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:07:15.744309 30979 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.744318 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x635971f0) got event_name: TaskCompletion
1884: I0814 08:07:15.744599 30976 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10991261041970238006 to 156893843977272642 , after update, data is {current : -4, peak : 104}.
1884: I0814 08:07:15.744608 30976 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10991261041970238006 to 156893843977272642 , after update, data is {current : -4, peak : 104}.
1884: I0814 08:07:15.744717 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.744724 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357448256671"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357448256671"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 08:07:15.745038 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63597080 on Place(cpu)
1884: I0814 08:07:15.745064 30891 scope.cc:202] Create variable 0x635970801723622835745055186_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357448256671"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236228357448256671 -> 0x63ce54e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 08:07:15.745328 30980 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.745391 30981 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.745414 30982 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.745441 30983 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.745466 30983 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236228357448256671:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.745479 30984 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.745517 30983 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236228357448256671:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.745543 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x635971f0) got event_name: TaskCompletion
1884: I0814 08:07:15.745719 30983 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 131179841540030010 to 156893843977272642 , after update, data is {current : 4, peak : 104}.
1884: I0814 08:07:15.745728 30983 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 131179841540030010 to 156893843977272642 , after update, data is {current : 4, peak : 104}.
1884: I0814 08:07:15.745829 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.745836 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236228357448256671",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236228357459229402"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236228357448256671",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236228357459229402"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 08:07:15.746083 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63597080 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236228357448256671",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236228357459229402"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236228357459229402 -> 0x63ce54e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0814 08:07:15.746354 30985 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.746400 30986 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.746417 30987 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.746451 30988 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.746477 30989 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.746475 30988 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.746515 30988 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.746541 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x635971f0) got event_name: TaskCompletion
1884: I0814 08:07:15.746805 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.746812 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357468921583"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357468921583"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0814 08:07:15.747022 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63597080 on Place(cpu)
1884: I0814 08:07:15.747041 30891 scope.cc:202] Create variable 0x635970801723622835747035596_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236228357468921583"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236228357468921583 -> 0x615ccc80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 08:07:15.747227 30990 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.747272 30991 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:07:15.747291 30992 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:07:15.747332 30993 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:07:15.747354 30994 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:07:15.747352 30993 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236228357468921583:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.747391 30993 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236228357468921583:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:07:15.747413 30891 pir_interpreter.cc:1766] main_thread_blocker_(0x635971f0) got event_name: TaskCompletion
1884: I0814 08:07:15.747579 30993 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 131179841540030010 to 156893843977272642 , after update, data is {current : 8, peak : 104}.
1884: I0814 08:07:15.747588 30993 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 131179841540030010 to 156893843977272642 , after update, data is {current : 8, peak : 104}.
1884: I0814 08:07:15.747696 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.747704 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.747769 30891 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0814 08:07:15.747835 30891 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0814 08:07:15.747874 30891 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357468921583"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357459229402"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357468921583"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357459229402"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0814 08:07:15.748610 30891 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0814 08:07:15.748642 30891 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0814 08:07:15.748703 30891 pir_interpreter.cc:161] PirInterpreter(): 0x63597080 on Place(cpu)
1884: I0814 08:07:15.748735 30891 scope.cc:202] Create variable feed_name_0
1884: I0814 08:07:15.748751 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_5
1884: I0814 08:07:15.748783 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_6
1884: I0814 08:07:15.748796 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_7
1884: I0814 08:07:15.748804 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_8
1884: I0814 08:07:15.748833 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_9
1884: I0814 08:07:15.748847 30891 scope.cc:202] Create variable 0x635970801723622835748717571_inner_var_10
1884: I0814 08:07:15.748873 30891 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.748898 30891 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:07:15.749032 30891 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.749047 30891 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357468921583"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236228357459229402"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236228357468921583 -> 0x615ccc80
1884: 1 -> constant_folding@_17236228357459229402 -> 0x63ce54e0
1884: 2 -> linear_1.b_0 -> 0x62711990
1884: 3 -> linear_1.w_0 -> 0x63c6c040
1884: 4 -> feed_name_0 -> 0x61595c60
1884: 5 -> 0x635970801723622835748717571_inner_var_5 -> 0x453892e0
1884: 6 -> 0x635970801723622835748717571_inner_var_6 -> 0x63ce5420
1884: 7 -> 0x635970801723622835748717571_inner_var_7 -> 0x615cd190
1884: 8 -> 0x635970801723622835748717571_inner_var_8 -> 0x63c71e60
1884: 9 -> fetch_name_0 -> 0x623688e0
1884: 10 -> fetch_name_1 -> 0x4538b880
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 08:07:15.749644 30891 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0814 08:07:15.749723 30995 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:07:15.749718 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.749775 30891 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:07:15.749799 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.749831 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x635970801723622835748717571_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x635970801723622835748717571_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.749872 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x635970801723622835748717571_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.749910 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.749933 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.749950 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.749986 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236228357459229402:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.750011 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236228357468921583:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.750042 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236228357468921583:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x635970801723622835748717571_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:07:15.750069 30891 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236228357468921583:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:07:15.750099 30891 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236228357468921583:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x635970801723622835748717571_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:07:15.750128 30891 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:07:15.750149 30891 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:07:15.750172 30891 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 08:07:15.750330 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.750339 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.750392 30995 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 10991261041970238006 to 156893843977272642 , after update, data is {current : -184, peak : 104}.
1884: I0814 08:07:15.750401 30995 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 10991261041970238006 to 156893843977272642 , after update, data is {current : -184, peak : 104}.
1884: I0814 08:07:15.750447 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.750455 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.752454 30891 mmap_allocator.cc:348] PID: 30891, MemoryMapFdSet: set size - 0
1884: I0814 08:07:15.764262 30891 mmap_allocator.cc:348] PID: 30891, MemoryMapFdSet: set size - 0
1884: I0814 08:07:15.833664 30966 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7632337268549823683 to 156893843977272642 , after update, data is {current : -180, peak : 104}.
1884: I0814 08:07:15.833684 30966 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7632337268549823683 to 156893843977272642 , after update, data is {current : -180, peak : 104}.
1884: I0814 08:07:15.833699 30968 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3334861516841814415 to 156893843977272642 , after update, data is {current : -164, peak : 104}.
1884: I0814 08:07:15.833712 30968 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3334861516841814415 to 156893843977272642 , after update, data is {current : -164, peak : 104}.
1884: I0814 08:07:15.833770 30967 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 15841787502719477104 to 156893843977272642 , after update, data is {current : -160, peak : 104}.
1884: I0814 08:07:15.833782 30967 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 15841787502719477104 to 156893843977272642 , after update, data is {current : -160, peak : 104}.
1884: I0814 08:07:15.833946 30969 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 5220922062324274725 to 156893843977272642 , after update, data is {current : -184, peak : 104}.
1884: I0814 08:07:15.833958 30969 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 5220922062324274725 to 156893843977272642 , after update, data is {current : -184, peak : 104}.
1884: I0814 08:07:15.833964 30969 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 5220922062324274725 to 11883872660040484040 , after update, data is {current : 256, peak : 768}.
1884: I0814 08:07:15.834296 30973 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17013477932878528959 to 156893843977272642 , after update, data is {current : -168, peak : 104}.
1884: I0814 08:07:15.834313 30973 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17013477932878528959 to 156893843977272642 , after update, data is {current : -168, peak : 104}.
1884: I0814 08:07:15.834362 30972 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 11883872660040484040 to 156893843977272642 , after update, data is {current : 768, peak : 1536}.
1884: I0814 08:07:15.834371 30972 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 11883872660040484040 to 156893843977272642 , after update, data is {current : 28, peak : 268}.
1884: I0814 08:07:15.834378 30972 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 11883872660040484040 to 156893843977272642 , after update, data is {current : 28, peak : 268}.
1884: I0814 08:07:15.834533 30974 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 156893843977272642 to 1913625173855795099 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 08:07:15.834543 30974 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 156893843977272642 to 1913625173855795099 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 08:07:15.834548 30974 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 156893843977272642 to 1913625173855795099 , after update, data is {current : 1536, peak : 2401024}.
1884: I0814 08:07:15.981154 30891 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:07:15.981187 30891 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:07:15.981237 30891 mmap_allocator.cc:348] PID: 30891, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   12.25 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.42 sec
