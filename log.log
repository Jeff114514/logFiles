UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:31:13.790221 17597 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:31:14.576776 17597 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=cublaslt_device_best_config,cublaslt_exhaustive_search_times,host_trace_level,use_auto_growth_v2,max_inplace_grad_add,enable_interpretercore_launch_cinn,tracer_onednn_ops_on,enable_all2all_use_fp16,alloc_fill_value,new_executor_use_cuda_graph,prim_backward,enable_pir_in_executor_trace_run,enable_cse_in_dy2st,set_to_1d,call_stack_level,multiple_of_cupti_buffer_size,executor_log_deps_every_microseconds,pir_apply_inplace_pass,allow_cinn_ops,logging_pir_py_code_dump_symbolic_dims,accuracy_check_atol_fp32,log_memory_stats,enable_pir_with_pt_in_dy2st,cuda_dir,manually_trans_conv_filter,cinn_compile_thread_num,initial_gpu_memory_in_mb,use_cuda_malloc_async_allocator,use_system_allocator,enable_neighbor_list_use_uva,gpugraph_enable_gpu_direct_access,prim_enabled,free_when_no_cache_hit,prim_all,enable_pir_api,enable_dump_main_program,reader_queue_speed_test_mode,new_executor_sequential_run,save_static_runtime_data,enable_tracker_all2all,win_cuda_bin_dir,nccl_blocking_wait,gpugraph_merge_grads_segment_size,tensorrt_dir,gpu_allocator_retry_time,nccl_dir,new_executor_serial_run,pir_broadcast_tree_limit,gpugraph_parallel_copyer_split_maxsize,gpu_memory_limit_mb,enable_auto_rdma_trans,cudnn_exhaustive_search_times,print_allocator_trace_info,gpugraph_offload_param_extends,convert_all_blocks,jit_engine_type,nvidia_package_dir,enable_gpu_memory_usage_log_mb,allreduce_record_one_event,fraction_of_cpu_memory_to_use,gpugraph_parallel_stream_num,enable_sparse_inner_gather,gpugraph_enable_segment_merge_grads,gpugraph_load_node_list_into_hbm,multi_node_sample_use_gpu_table,tracer_profile_fname,use_pinned_memory,query_dest_rank_by_multi_node,cse_max_count,use_shm_cache,npu_storage_format,sync_nccl_allreduce,lapack_dir,run_kp_kernel,use_stride_kernel,enable_blaslt_global_search,cusparse_dir,accuracy_check_atol_bf16,new_executor_use_local_scope,pir_apply_shape_optimization_pass,enable_gpu_memory_usage_log,accuracy_check_rtol_fp16,gpugraph_offload_param_stat,gpugraph_hbm_table_load_factor,use_xqa_optim,op_dir,all_blocks_convert_trt,paddle_num_threads,benchmark_nccl,mkl_dir,enable_pir_in_executor,print_sub_graph_dir,cudnn_deterministic,enable_exit_when_partial_worker,cinn_subgraph_graphviz_dir,eager_delete_tensor_gb,use_auto_growth_pinned_allocator,prim_forward_blacklist,enable_fusion_fallback,prim_enable_dynamic,logging_trunc_pir_py_code,apply_pass_to_program,use_cinn,cuda_memory_async_pool_realease_threshold,cupti_dir,enable_auto_detect_gpu_topo,cudnn_batchnorm_spatial_persistent,gpugraph_dedup_pull_push_mode,ir_inplace_kernel_blacklist,fraction_of_cuda_pinned_memory_to_use,get_host_by_name_time,dump_chunk_info,use_virtual_memory_auto_growth,graph_load_in_parallel,pir_subgraph_saving_dir,logging_pir_py_code_int_tensor_element_limit,use_fast_math,initial_cpu_memory_in_mb,pir_debug,benchmark,gpugraph_storage_mode,free_idle_chunk,check_nan_inf_level,mklml_dir,dist_threadpool_size,graph_neighbor_size_percent,auto_free_cudagraph_allocations_on_launch,sync_after_alloc,accuracy_check_rtol_bf16,search_cache_max_number,reallocate_gpu_memory_in_mb,new_executor_static_build,check_nan_inf,fleet_executor_with_standalone,conv_workspace_size_limit,dygraph_debug,memory_fraction_of_eager_deletion,graph_embedding_split_infer_mode,selected_gpus,use_mkldnn,deny_cinn_ops,enable_cinn_accuracy_check,enable_record_memory,enable_dependency_builder_debug_info,enable_adjust_op_order,enable_cinn_auto_tune,accuracy_check_atol_fp16,gpugraph_sparse_table_storage_mode,prim_skip_dynamic,use_cuda_managed_memory,static_executor_perfstat_filepath,prim_forward,inner_op_parallelism,enable_cinn_compile_cache,auto_growth_chunk_size_in_mb,curand_dir,init_allocated_mem,pinned_memory_as_cpu_backend,fast_eager_deletion_mode,enable_collect_shape,enable_fuse_parallel_matmul_pass,prim_check_ops,static_runtime_data_save_path,tracer_onednn_ops_off,cublas_dir,gpugraph_enable_print_op_debug,tensor_operants_mode,cusparselt_dir,gpugraph_enable_hbm_table_collision_stat,fuse_parameter_memory_size,check_infer_symbolic,disable_dyshape_in_train,local_exe_sub_scope_limit,sort_sum_gradient,cudnn_exhaustive_search,dataloader_use_file_descriptor,allocator_strategy,gemm_use_half_precision_compute_type,fuse_parameter_groups_size,enable_api_kernel_fallback,use_autotune,check_kernel_launch,enable_unused_var_check,enable_opt_get_features,enable_graph_multi_node_sampling,conv2d_disable_cudnn,low_precision_op_list,embedding_deterministic,custom_device_mem_record,new_executor_use_inplace,gpugraph_offload_gather_copy_maxsize,print_ir,cusolver_dir,enable_cublas_tensor_op_math,use_stream_safe_cuda_allocator,fraction_of_gpu_memory_to_use,eager_delete_scope,gpugraph_force_device_batch_num_equal,dynamic_static_unified_comm,graph_get_neighbor_id,trt_ibuilder_cache,graph_metapath_split_opt,cudnn_dir,cache_inference_while_scope,gpugraph_slot_feasign_max_num,gpugraph_debug_gpu_memory,einsum_opt,accuracy_check_rtol_fp32,enable_async_trace,add_dependency_for_communication_op,async_trace_count,logging_pir_py_code_dir,cuda_malloc_async_pool_memory_throttle_ratio 
1884: I0815 04:31:14.576886 17597 init.cc:108] After Parse: argc is 2
1884: I0815 04:31:24.891435 17597 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:31:24.891490 17597 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:31:24.892165 17597 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:31:24.892696 17597 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:31:24.893548 17597 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:31:24.893646 17597 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:31:24.893743 17597 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:31:24.894416 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6d8fa00000), and remaining 0
1884: I0815 04:31:24.894774 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:24.894841 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:24.894929 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6d8fa00200), and remaining 0
1884: I0815 04:31:24.894956 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6d8fa00400), and remaining 0
1884: I0815 04:31:24.898717 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6d8fa00600), and remaining 0
1884: I0815 04:31:24.898859 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6d8fa00800), and remaining 0
1884: I0815 04:31:24.898926 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f6d8fa00a00), and remaining 0
1884: I0815 04:31:24.899019 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:24.899039 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:24.899114 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:24.899127 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:24.900592 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adb00e0 for it.
1884: I0815 04:31:24.900750 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:24.900776 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:24.900832 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f6d8fa00e00), and remaining 0
1884: I0815 04:31:24.900913 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f6d8fac4400), and remaining 0
1884: I0815 04:31:25.030831 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adb00e0 for it.
1884: I0815 04:31:25.031005 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:25.031041 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.031630 17597 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f6d8fc00000), and remaining 0
1884: I0815 04:31:25.038686 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adb00e0 for it.
1884: I0815 04:31:25.038772 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:25.038800 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.038832 17597 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:25.038995 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:25.039983 17597 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:31:25.040001 17597 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:31:25.040048 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:25.040127 17597 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:31:25.040150 17597 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.040210 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:25.040292 17597 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:31:25.040319 17597 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.040357 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:25.040560 17597 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:31:25.040577 17597 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.040746 17597 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:31:25.040771 17597 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:25.040845 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:25.044755 17597 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:31:25.044873 17597 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:31:25.044898 17597 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:31:25.044962 17597 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:31:26.381047 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.381129 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.381484 17597 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:31:26.381507 17597 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.387177 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.387215 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.388314 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.388334 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.388348 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.389102 17597 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:31:26.389117 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.389132 17597 scope.cc:202] Create variable feed
1884: I0815 04:31:26.389142 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.389150 17597 scope.cc:202] Create variable fetch
1884: I0815 04:31:26.389155 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.389168 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.389173 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.389176 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.389179 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.391556 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.391899 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.391912 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.391916 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.393582 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.393630 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.393639 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.393646 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.393652 17597 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:31:26.393663 17597 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61a03070 type is 7
1884: I0815 04:31:26.393671 17597 scope.cc:202] Create variable x
1884: I0815 04:31:26.393673 17597 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x61a00a60 type is 7
1884: I0815 04:31:26.393729 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.393736 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.393740 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.393743 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.393862 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.393884 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.394006 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.394016 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.394033 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.394191 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.394218 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.394239 17597 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.394244 17597 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61a17cf0Variable Type 7
1884: I0815 04:31:26.394264 17597 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.394287 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.394347 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.394367 17597 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.395579 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.395635 17597 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.396018 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.399919 17597 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:31:26.399940 17597 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:31:26.400027 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.400054 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.400565 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae98120 for it.
1884: I0815 04:31:26.400635 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.400658 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.401109 17597 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ae98120 for it.
1884: I0815 04:31:26.401170 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.401193 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.401217 17597 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.401497 17597 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:31:26.401510 17597 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:31:26.401631 17597 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:31:26.401656 17597 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.402045 17597 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:31:26.402057 17597 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:31:26.402099 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.402118 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.402318 17597 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:31:26.402329 17597 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:31:26.402366 17597 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:31:26.402384 17597 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:31:26.402401 17597 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.405059 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.405082 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.405134 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.405143 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.406999 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.407367 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.407383 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.407388 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.411149 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.411204 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.411214 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.411219 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61a431c0 type is 7
1884: I0815 04:31:26.411226 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.411231 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61a43530 type is 7
1884: I0815 04:31:26.411235 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.411240 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.411316 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.411324 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.411327 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.411330 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.411398 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.411414 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.411471 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.411480 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.411494 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.411788 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.411805 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.411823 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.411828 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61a49bf0Variable Type 7
1884: I0815 04:31:26.411849 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.411864 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.411907 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.411921 17597 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.412616 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.412642 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.412811 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.425729 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae98120 for it.
1884: I0815 04:31:26.425916 17597 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adce7e0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:31:26.432214 17597 pir_interpreter.cc:161] PirInterpreter(): 0x61c06150 on Place(gpu:0)
1884: I0815 04:31:26.432255 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.432284 17597 scope.cc:202] Create variable 0x61c061501723696286432239618_inner_var_1
1884: I0815 04:31:26.432296 17597 scope.cc:202] Create variable 0x61c061501723696286432239618_inner_var_2
1884: I0815 04:31:26.432314 17597 scope.cc:202] Create variable 0x61c061501723696286432239618_inner_var_3
1884: I0815 04:31:26.432325 17597 scope.cc:202] Create variable 0x61c061501723696286432239618_inner_var_4
1884: I0815 04:31:26.432333 17597 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:31:26.432747 17597 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:31:26.432765 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.432768 17597 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:31:26.432812 17597 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61c060b0
1884: 1 -> 0x61c061501723696286432239618_inner_var_1 -> 0x61c06130
1884: 2 -> 0x61c061501723696286432239618_inner_var_2 -> 0x61c06a00
1884: 3 -> 0x61c061501723696286432239618_inner_var_3 -> 0x61c05890
1884: 4 -> 0x61c061501723696286432239618_inner_var_4 -> 0x61c06db0
1884: 5 -> fetch0@fetch -> 0x61c075c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:31:26.433557 17597 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:31:26.433809 17635 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.433954 17636 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.433997 17637 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.434026 17638 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.434103 17639 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.434114 17638 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61c061501723696286432239618_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.434185 17640 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.434226 17638 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61c061501723696286432239618_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.434233 17640 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.434278 17640 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:31:26.434330 17640 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61c061501723696286432239618_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61c061501723696286432239618_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.434517 17640 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61c061501723696286432239618_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61c061501723696286432239618_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:31:26.434588 17638 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61c061501723696286432239618_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.434612 17638 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.435822 17638 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61c061501723696286432239618_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61c061501723696286432239618_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:31:26.435861 17638 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61c061501723696286432239618_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.435887 17638 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.436453 17638 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61c061501723696286432239618_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:31:26.436492 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x61c062c0) got event_name: TaskCompletion
1884: I0815 04:31:26.436517 17597 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.510304 17635 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 2674682991962447340 to 2728054576369675340 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:31:26.510321 17635 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 2674682991962447340 to 5697889645413499531 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:31:26.510329 17635 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 2674682991962447340 to 5697889645413499531 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:31:26.510504 17638 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 5697889645413499531 to 14369536172427389571 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:31:26.510522 17638 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 5697889645413499531 to 14369536172427389571 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:31:26.510696 17640 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 2728054576369675340 to 14369536172427389571 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:31:26.510711 17640 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 2728054576369675340 to 14369536172427389571 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:31:26.516537 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.516564 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.516626 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.516634 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.518419 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.518777 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.518792 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.518797 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.520323 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.520416 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.520428 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.520434 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5349ee0 type is 7
1884: I0815 04:31:26.520442 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.520445 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61a16a30 type is 7
1884: I0815 04:31:26.520450 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.520454 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.520509 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.520514 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.520519 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.520521 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.520572 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.520588 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.520653 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.520663 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.520675 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.520807 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.520818 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.520833 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.520839 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61bdd500Variable Type 7
1884: I0815 04:31:26.520854 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.520872 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.520893 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.520907 17597 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.522532 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.522568 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.522773 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.527176 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce7e0 for it.
1884: I0815 04:31:26.527369 17597 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae98120 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:31:26.530436 17597 pir_interpreter.cc:161] PirInterpreter(): 0x61a13b00 on Place(gpu:0)
1884: I0815 04:31:26.530470 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.530494 17597 scope.cc:202] Create variable 0x61a13b001723696286530461527_inner_var_1
1884: I0815 04:31:26.530505 17597 scope.cc:202] Create variable 0x61a13b001723696286530461527_inner_var_2
1884: I0815 04:31:26.530516 17597 scope.cc:202] Create variable 0x61a13b001723696286530461527_inner_var_3
1884: I0815 04:31:26.530527 17597 scope.cc:202] Create variable 0x61a13b001723696286530461527_inner_var_4
1884: I0815 04:31:26.530539 17597 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:31:26.530860 17597 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:31:26.530876 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.530879 17597 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5322a20
1884: 1 -> 0x61a13b001723696286530461527_inner_var_1 -> 0x534a320
1884: 2 -> 0x61a13b001723696286530461527_inner_var_2 -> 0x4e03240
1884: 3 -> 0x61a13b001723696286530461527_inner_var_3 -> 0x640bf310
1884: 4 -> 0x61a13b001723696286530461527_inner_var_4 -> 0x61c04200
1884: 5 -> fetch0@fetch -> 0x61a07180
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:31:26.531576 17641 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.531656 17642 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.531679 17643 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.531709 17644 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.531757 17645 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.531785 17646 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.531795 17645 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a13b001723696286530461527_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.531812 17646 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.531863 17646 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:31:26.531879 17645 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a13b001723696286530461527_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.531917 17646 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a13b001723696286530461527_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a13b001723696286530461527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.532052 17646 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a13b001723696286530461527_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a13b001723696286530461527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:31:26.532112 17645 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a13b001723696286530461527_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.532138 17645 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.534929 17645 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a13b001723696286530461527_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61a13b001723696286530461527_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:31:26.534977 17645 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61a13b001723696286530461527_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.535002 17645 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.537031 17645 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61a13b001723696286530461527_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:31:26.537082 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x61a13c70) got event_name: TaskCompletion
1884: I0815 04:31:26.537108 17597 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.575124 17641 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 2728054576369675340 to 16973530630261042933 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:31:26.575135 17641 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2728054576369675340 to 13340423173674060691 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:31:26.575140 17641 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2728054576369675340 to 13340423173674060691 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:31:26.575291 17645 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13340423173674060691 to 14369536172427389571 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:31:26.575306 17645 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13340423173674060691 to 14369536172427389571 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:31:26.575459 17646 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 16973530630261042933 to 14369536172427389571 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:31:26.579255 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.579279 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.579339 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.579346 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.581164 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.581502 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.581516 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.581521 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.583001 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.583129 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.583142 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.583148 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x2e53590 type is 7
1884: I0815 04:31:26.583158 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.583161 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x35f1ec0 type is 7
1884: I0815 04:31:26.583165 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.583169 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.583222 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.583230 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.583232 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.583236 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.583281 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.583293 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.583357 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.583366 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.583380 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.583420 17597 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.583622 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.583683 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.583693 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.583709 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.583715 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3a9d660Variable Type 7
1884: I0815 04:31:26.583731 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.583750 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.583770 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.583784 17597 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.584080 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.584102 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.584287 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.585049 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae98120 for it.
1884: I0815 04:31:26.585237 17597 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adce7e0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:31:26.588239 17597 pir_interpreter.cc:161] PirInterpreter(): 0x64236430 on Place(gpu:0)
1884: I0815 04:31:26.588271 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.588294 17597 scope.cc:202] Create variable 0x642364301723696286588263372_inner_var_1
1884: I0815 04:31:26.588313 17597 scope.cc:202] Create variable 0x642364301723696286588263372_inner_var_2
1884: I0815 04:31:26.588325 17597 scope.cc:202] Create variable 0x642364301723696286588263372_inner_var_3
1884: I0815 04:31:26.588336 17597 scope.cc:202] Create variable 0x642364301723696286588263372_inner_var_4
1884: I0815 04:31:26.588348 17597 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:31:26.588673 17597 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:31:26.588689 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.588693 17597 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x1ae13d00
1884: 1 -> 0x642364301723696286588263372_inner_var_1 -> 0x6433c6e0
1884: 2 -> 0x642364301723696286588263372_inner_var_2 -> 0x61a16ef0
1884: 3 -> 0x642364301723696286588263372_inner_var_3 -> 0x534efc0
1884: 4 -> 0x642364301723696286588263372_inner_var_4 -> 0x61bdba80
1884: 5 -> fetch0@fetch -> 0x5371a40
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:31:26.589365 17647 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.589442 17648 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.589483 17649 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.589507 17650 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.589545 17651 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.589584 17652 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.589581 17651 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642364301723696286588263372_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.589604 17652 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.589629 17651 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642364301723696286588263372_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.589633 17652 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:31:26.589660 17652 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x642364301723696286588263372_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x642364301723696286588263372_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.589695 17652 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.589830 17652 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.589857 17652 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x642364301723696286588263372_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x642364301723696286588263372_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:31:26.589913 17651 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642364301723696286588263372_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.589937 17651 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.590256 17651 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642364301723696286588263372_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x642364301723696286588263372_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:31:26.590283 17651 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x642364301723696286588263372_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.590308 17651 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.590322 17651 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x642364301723696286588263372_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:31:26.590359 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x642365a0) got event_name: TaskCompletion
1884: I0815 04:31:26.590381 17597 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.621909 17647 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 16973530630261042933 to 2728054576369675340 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:31:26.621920 17647 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 16973530630261042933 to 2728054576369675340 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:31:26.621927 17647 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 16973530630261042933 to 2728054576369675340 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:31:26.622123 17651 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12800182798909144744 to 2728054576369675340 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:31:26.622131 17651 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12800182798909144744 to 2728054576369675340 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:31:26.622318 17652 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2728054576369675340 to 14369536172427389571 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:31:26.622328 17652 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2728054576369675340 to 14369536172427389571 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:31:26.622334 17652 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 2728054576369675340 to 14369536172427389571 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:31:26.626173 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.626196 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.626243 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.626251 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.627810 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.628137 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.628150 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.628155 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.629652 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.629727 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.629739 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.629745 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61c0fe00 type is 7
1884: I0815 04:31:26.629753 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.629757 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5322370 type is 7
1884: I0815 04:31:26.629761 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.629765 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.629817 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.629823 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.629827 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.629832 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.629875 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.629887 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.629940 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.629949 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.629963 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.630203 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.630215 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.630231 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.630237 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x643403e0Variable Type 7
1884: I0815 04:31:26.630254 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.630270 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.630291 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.630313 17597 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.631043 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.631071 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.631256 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.670025 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.670051 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.670101 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.670109 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.671743 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.672067 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.672080 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.672084 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.673584 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.673668 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.673679 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.673684 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5362a70 type is 7
1884: I0815 04:31:26.673698 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.673702 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x63a8af10 type is 7
1884: I0815 04:31:26.673707 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.673710 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.673763 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.673768 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.673772 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.673775 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.673820 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.673833 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.673887 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.673895 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.673908 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.674036 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.674048 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.674062 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.674068 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63848610Variable Type 7
1884: I0815 04:31:26.674082 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.674099 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.674118 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.674131 17597 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.675799 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.675834 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.676033 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.721383 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.721410 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.721460 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.721468 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.723083 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.723420 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.723434 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.723439 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.724931 17597 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:31:26.725018 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.725030 17597 scope.cc:202] Create variable Out
1884: I0815 04:31:26.725035 17597 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x643ac640 type is 7
1884: I0815 04:31:26.725044 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.725049 17597 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61a196b0 type is 7
1884: I0815 04:31:26.725054 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.725057 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.725111 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.725116 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.725121 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.725123 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.725172 17597 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.725185 17597 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.725242 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.725251 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.725265 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.725764 17597 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.725899 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.725963 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.725975 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.725992 17597 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.725997 17597 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63650990Variable Type 7
1884: I0815 04:31:26.726015 17597 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.726032 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.726056 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.726070 17597 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.726176 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.726198 17597 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.726398 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.727144 17597 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x64341340 for it.
1884: I0815 04:31:26.727394 17597 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adce7e0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:31:26.730408 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(gpu:0)
1884: I0815 04:31:26.730441 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.730464 17597 scope.cc:202] Create variable 0x619f89601723696286730433335_inner_var_1
1884: I0815 04:31:26.730475 17597 scope.cc:202] Create variable 0x619f89601723696286730433335_inner_var_2
1884: I0815 04:31:26.730487 17597 scope.cc:202] Create variable 0x619f89601723696286730433335_inner_var_3
1884: I0815 04:31:26.730497 17597 scope.cc:202] Create variable 0x619f89601723696286730433335_inner_var_4
1884: I0815 04:31:26.730509 17597 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:31:26.730835 17597 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:31:26.730850 17597 scope.cc:202] Create variable X
1884: I0815 04:31:26.730854 17597 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x638481e0
1884: 1 -> 0x619f89601723696286730433335_inner_var_1 -> 0x63848260
1884: 2 -> 0x619f89601723696286730433335_inner_var_2 -> 0x6406bf50
1884: 3 -> 0x619f89601723696286730433335_inner_var_3 -> 0x619f6910
1884: 4 -> 0x619f89601723696286730433335_inner_var_4 -> 0x643ae540
1884: 5 -> fetch0@fetch -> 0x643c2500
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:31:26.731534 17653 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.731613 17654 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.731686 17655 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.731683 17656 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.731714 17657 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.731752 17657 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x619f89601723696286730433335_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.731814 17657 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x619f89601723696286730433335_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.731875 17658 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.731894 17658 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.731923 17658 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:31:26.731951 17658 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x619f89601723696286730433335_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286730433335_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.731992 17658 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.732116 17658 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.732143 17658 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x619f89601723696286730433335_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286730433335_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:31:26.732206 17657 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x619f89601723696286730433335_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.732234 17657 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.732390 17657 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x619f89601723696286730433335_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x619f89601723696286730433335_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:31:26.732415 17657 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x619f89601723696286730433335_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.732432 17657 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.732446 17657 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x619f89601723696286730433335_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:31:26.732475 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x619f8ad0) got event_name: TaskCompletion
1884: I0815 04:31:26.732499 17597 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.764195 17653 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 2728054576369675340 to 16973530630261042933 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:31:26.764207 17653 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2728054576369675340 to 16973530630261042933 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:31:26.764212 17653 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2728054576369675340 to 16973530630261042933 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:31:26.764431 17657 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13340423173674060691 to 16973530630261042933 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:31:26.764448 17657 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13340423173674060691 to 16973530630261042933 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:31:26.764607 17658 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 16973530630261042933 to 14369536172427389571 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:31:26.764617 17658 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 16973530630261042933 to 14369536172427389571 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:31:26.764622 17658 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 16973530630261042933 to 14369536172427389571 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:31:26.770805 17597 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:31:26.770854 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:31:26.771904 17597 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:31:26.772755 17597 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:31:26.772785 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:31:26.774086 17597 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:31:26.774111 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:31:26.774799 17597 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:31:26.775769 17597 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:31:26.775795 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:31:26.777084 17597 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:31:26.777107 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:31:26.777698 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.777725 17597 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:31:26.777732 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.777738 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.779625 17597 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:31:26.779651 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:31:26.780589 17597 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:31:26.780617 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:31:26.781592 17597 pybind.cc:1827] need skip: 0
1884: I0815 04:31:26.781881 17597 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:31:26.783614 17597 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:31:26.787137 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.787154 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.787159 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.789108 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.789126 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.789134 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.789140 17597 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:31:26.789144 17597 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x640b1ba0 type is 7
1884: I0815 04:31:26.789149 17597 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:31:26.789152 17597 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x640b2040 type is 7
1884: I0815 04:31:26.789157 17597 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:31:26.789160 17597 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x640b1fe0 type is 7
1884: I0815 04:31:26.789223 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.789229 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.789233 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.789237 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.789283 17597 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789296 17597 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789325 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:31:26.789446 17597 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789456 17597 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789518 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.789563 17597 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789572 17597 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.789597 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.790565 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.791885 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.792336 17597 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:31:26.792551 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.792838 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.793045 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.793062 17597 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:31:26.793126 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.793133 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.793138 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.793238 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.793249 17597 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:31:26.794775 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.796084 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.797158 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.797348 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.797361 17597 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:31:26.797367 17597 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x640c7790 type is 7
1884: I0815 04:31:26.797375 17597 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:31:26.797379 17597 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x640c7510 type is 7
1884: I0815 04:31:26.797384 17597 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:31:26.797386 17597 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x640c7600 type is 7
1884: I0815 04:31:26.797391 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.797395 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.797400 17597 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:31:26.797403 17597 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x640c8bd0 type is 7
1884: I0815 04:31:26.797407 17597 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x640b1ba0 type is 7
1884: I0815 04:31:26.797411 17597 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x640b2040 type is 7
1884: I0815 04:31:26.797415 17597 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:31:26.797418 17597 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x640c8bb0 type is 7
1884: I0815 04:31:26.797423 17597 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:31:26.797426 17597 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x640c9080 type is 7
1884: I0815 04:31:26.797430 17597 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x640b1fe0 type is 7
1884: I0815 04:31:26.797433 17597 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:31:26.797436 17597 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x640c92f0 type is 7
1884: I0815 04:31:26.797441 17597 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:31:26.797443 17597 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x640c9530 type is 7
1884: I0815 04:31:26.797447 17597 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:31:26.797451 17597 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x640c9790 type is 7
1884: I0815 04:31:26.797530 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.797544 17597 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:31:26.797600 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.797606 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.797610 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.797614 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.797657 17597 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.797667 17597 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.797683 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:31:26.797778 17597 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.797789 17597 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.797806 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:31:26.797878 17597 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:31:26.798085 17597 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:31:26.799235 17597 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799257 17597 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799327 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.799387 17597 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799399 17597 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799415 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:31:26.799439 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.799476 17597 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799485 17597 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799497 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:31:26.799577 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799587 17597 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799599 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.799698 17597 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.799782 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.799836 17597 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799846 17597 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799860 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:31:26.799893 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.799945 17597 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799954 17597 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.799968 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:31:26.800077 17597 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.800088 17597 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.800107 17597 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.800148 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.800156 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.800171 17597 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.800177 17597 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64237f90Variable Type 7
1884: I0815 04:31:26.800194 17597 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.800209 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.800228 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.800241 17597 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.800280 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.800311 17597 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:31:26.800338 17597 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.800345 17597 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.800357 17597 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:31:26.800364 17597 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64239fb0Variable Type 7
1884: I0815 04:31:26.800376 17597 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:31:26.800388 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.800405 17597 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.800416 17597 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.800448 17597 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:31:26.800460 17597 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:31:26.800853 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:31:26.800886 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:31:26.800904 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:31:26.800935 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:31:26.800967 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.800985 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:31:26.804728 17597 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:31:26.804764 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:31:26.805444 17597 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:31:26.805465 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:31:26.805776 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.807415 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.808226 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.808344 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.808845 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.809736 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.811817 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.812865 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.814682 17597 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:31:26.815448 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.815464 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.815469 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.816622 17597 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:31:26.816640 17597 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5335bd0 type is 9
1884: I0815 04:31:26.816646 17597 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3301530 type is 10
1884: I0815 04:31:26.816653 17597 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x640b2040 type is 7
1884: I0815 04:31:26.816658 17597 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x640b1fe0 type is 7
1884: I0815 04:31:26.816661 17597 scope.cc:202] Create variable saved_params
1884: I0815 04:31:26.816664 17597 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x61a397e0 type is 17
1884: I0815 04:31:26.816691 17597 interpreter_util.cc:594] Static build: 0
1884: I0815 04:31:26.816696 17597 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:31:26.816700 17597 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:31:26.816704 17597 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:31:26.816738 17597 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.816749 17597 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:31:26.817430 17597 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:31:26.817468 17597 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:31:26.817517 17597 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:31:26.818678 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.818735 17597 scope.cc:202] Create variable feed
1884: I0815 04:31:26.818743 17597 naive_executor.cc:189] 0x635f7010 Create persistable variable feed, which pointer is 0x636dd390
1884: I0815 04:31:26.818748 17597 scope.cc:202] Create variable fetch
1884: I0815 04:31:26.818751 17597 naive_executor.cc:189] 0x635f7010 Create persistable variable fetch, which pointer is 0x635f7180
1884: I0815 04:31:26.818755 17597 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:31:26.818758 17597 naive_executor.cc:189] 0x635f7010 Create persistable variable linear_0.b_0, which pointer is 0x643cceb0
1884: I0815 04:31:26.818763 17597 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:31:26.818766 17597 naive_executor.cc:189] 0x635f7010 Create persistable variable linear_0.w_0, which pointer is 0x6401fd40
1884: I0815 04:31:26.818781 17597 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:31:26.819121 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.819206 17597 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:31:26.819253 17597 executor.cc:183] Old Executor is Running.
1884: I0815 04:31:26.819327 17597 executor.cc:92] Creating Variables for block 0
1884: I0815 04:31:26.819335 17597 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:31:26.819339 17597 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x643cceb0 type is 7
1884: I0815 04:31:26.819341 17597 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:31:26.819343 17597 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x6401fd40 type is 7
1884: I0815 04:31:26.819375 17597 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.819444 17597 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:31:26.819484 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.819490 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:26.819620 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.819725 17597 graph.cc:149] create OpNode by feed
1884: I0815 04:31:26.819761 17597 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:31:26.819775 17597 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:31:26.819790 17597 graph.cc:149] create OpNode by abs
1884: I0815 04:31:26.819801 17597 graph.cc:149] create OpNode by assign_value
1884: I0815 04:31:26.819818 17597 graph.cc:149] create OpNode by multinomial
1884: I0815 04:31:26.819828 17597 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:31:26.819844 17597 graph.cc:149] create OpNode by scale
1884: I0815 04:31:26.819855 17597 graph.cc:149] create OpNode by scale
1884: I0815 04:31:26.819867 17597 graph.cc:149] create OpNode by fetch
1884: I0815 04:31:26.819885 17597 graph.cc:149] create OpNode by fetch
1884: I0815 04:31:26.819904 17597 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:31:26.821074 17597 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:31:26.821082 17597 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:31:26.821153 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821159 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:31:26.821267 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.821527 17597 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:31:26.821588 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821592 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:31:26.821625 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821630 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:31:26.821669 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.821728 17597 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:31:26.821758 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821763 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:31:26.821781 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.821794 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.821815 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821820 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:31:26.821857 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.821878 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.821900 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.821905 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:31:26.821947 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822024 17597 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:31:26.822050 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822055 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:31:26.822086 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822105 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.822127 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822131 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:31:26.822160 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822330 17597 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:31:26.822361 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822366 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:31:26.822396 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822412 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.822435 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822440 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:31:26.822460 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822475 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.822496 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822501 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:31:26.822523 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822537 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.822558 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822562 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:31:26.822585 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822651 17597 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:31:26.822685 17597 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:31:26.822700 17597 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:31:26.822711 17597 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:31:26.822734 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.822736 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:31:26.822758 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.822796 17597 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:31:26.822816 17597 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:31:26.822829 17597 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:31:26.822839 17597 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:31:26.822870 17597 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:31:26.822880 17597 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:31:26.824035 17597 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:31:26.824080 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.824087 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:31:26.824112 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.824132 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.824158 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.824163 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:31:26.824187 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.824235 17597 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:31:26.824263 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.824268 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:31:26.824286 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.824308 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.824331 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.824337 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:31:26.824369 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.824455 17597 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:31:26.824481 17597 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:31:26.824496 17597 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:31:26.824510 17597 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:31:26.824525 17597 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:31:26.824540 17597 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:31:26.824554 17597 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:31:26.824577 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.824649 17597 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:31:26.824671 17597 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:31:26.824683 17597 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:31:26.824697 17597 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:31:26.824709 17597 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:31:26.824724 17597 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:31:26.824739 17597 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:31:26.824784 17597 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:31:26.825053 17597 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:31:26.825083 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.825088 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:31:26.825134 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825193 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825227 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825273 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825306 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825348 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825372 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825410 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825430 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825464 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825481 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825512 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825527 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825554 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825567 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825590 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825601 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825618 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825644 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.825649 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:31:26.825673 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825712 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825737 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.825742 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:31:26.825752 17597 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:31:26.825754 17597 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:31:26.825802 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825824 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825848 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.825853 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:31:26.825861 17597 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:31:26.825865 17597 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:31:26.825904 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.825927 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.825951 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.825955 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:31:26.825963 17597 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:31:26.825966 17597 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:31:26.825999 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.826016 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.826038 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.826043 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:31:26.826050 17597 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:31:26.826053 17597 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:31:26.826090 17597 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:31:26.826110 17597 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:31:26.826133 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.826138 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:31:26.826149 17597 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:31:26.826187 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.826192 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:31:26.826260 17597 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:31:26.826279 17597 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.826296 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:31:26.826352 17597 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:31:26.826370 17597 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:31:26.826395 17597 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:31:26.826418 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.826423 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:31:26.827265 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.827278 17597 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:31:26.827332 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.827339 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:26.827937 17597 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:31:26.828146 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.828217 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.828222 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:26.828635 17597 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:31:26.828840 17597 graph.h:183] deleting __fuse_statis__
1884: I0815 04:31:26.828847 17597 graph.h:183] deleting pass_recorder
1884: I0815 04:31:26.828853 17597 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:31:26.828931 17597 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:31:26.828940 17597 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:31:26.828943 17597 naive_executor.cc:195] 0x635f7010 Create variable abs_0.tmp_0, which pointer is 0x640dcb20
1884: I0815 04:31:26.828949 17597 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:31:26.828953 17597 naive_executor.cc:195] 0x635f7010 Create variable gaussian_0.tmp_0, which pointer is 0x63a90640
1884: I0815 04:31:26.828963 17597 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:31:26.828969 17597 naive_executor.cc:195] 0x635f7010 Create variable linear_0.tmp_1, which pointer is 0x641c8090
1884: I0815 04:31:26.828972 17597 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:31:26.828974 17597 naive_executor.cc:195] 0x635f7010 Create variable multinomial_0.tmp_0, which pointer is 0x641c7b30
1884: I0815 04:31:26.828979 17597 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:31:26.828980 17597 naive_executor.cc:195] 0x635f7010 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x641c7e30
1884: I0815 04:31:26.828984 17597 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:31:26.828986 17597 naive_executor.cc:195] 0x635f7010 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x6374b800
1884: I0815 04:31:26.828992 17597 scope.cc:202] Create variable feed
1884: I0815 04:31:26.828996 17597 scope.cc:202] Create variable fetch
1884: I0815 04:31:26.829016 17597 naive_executor.cc:46] NaiveExecutor init with scope 0x635f7010
1884: I0815 04:31:26.829021 17597 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:31:26.829206 17597 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:31:26.829219 17597 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:31:26.829246 17597 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:31:26.829252 17597 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:31:26.829260 17597 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.829289 17597 helper.h:475] Init predictor : [cpu current allocated memory: 3.05348MB], [cpu current reserved memory: 3.05348MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:31:26.829499 17597 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.829515 17597 helper.h:475] before run : [cpu current allocated memory: 3.05353MB], [cpu current reserved memory: 3.05353MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:31:26.829558 17597 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.829582 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:31:26.833240 17597 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:31:26.833340 17597 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.833369 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:31:26.833432 17597 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:31:26.833464 17597 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.833490 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:31:26.833556 17597 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:31:26.833599 17597 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.833616 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:31:26.833668 17597 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:31:26.833698 17597 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:31:26.833714 17597 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:31:26.833750 17597 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:31:26.833767 17597 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.833797 17597 helper.h:475] after run : [cpu current allocated memory: 3.05401MB], [cpu current reserved memory: 3.05401MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:31:26.833819 17597 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:31:26.834275 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.834285 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:31:26.880498 17597 pir_interpreter.cc:161] PirInterpreter(): 0x63a70c70 on Place(gpu:0)
1884: I0815 04:31:26.880537 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_0
1884: I0815 04:31:26.880553 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_1
1884: I0815 04:31:26.880563 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_2
1884: I0815 04:31:26.880571 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_3
1884: I0815 04:31:26.880609 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_4
1884: I0815 04:31:26.880622 17597 scope.cc:202] Create variable 0x63a70c701723696286880525795_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x63a70c701723696286880525795_inner_var_0 -> 0x61a2af10
1884: 1 -> 0x63a70c701723696286880525795_inner_var_1 -> 0x635867f0
1884: 2 -> 0x63a70c701723696286880525795_inner_var_2 -> 0x534c840
1884: 3 -> linear_1.w_0 -> 0x63ba5410
1884: 4 -> linear_1.b_0 -> 0x63848240
1884: 5 -> learning_rate_1 -> 0x635f5e20
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:31:26.881431 17659 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.881448 17660 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.881474 17661 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.881516 17662 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.881548 17663 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.881551 17660 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881548 17662 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881551 17661 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881575 17663 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881608 17660 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.881632 17663 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.881623 17662 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.881624 17661 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63a70c701723696286880525795_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:31:26.881676 17663 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:31:26.881700 17663 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881717 17663 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.881729 17663 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:31:26.881743 17663 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x63a70c701723696286880525795_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63a70c701723696286880525795_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63a70c701723696286880525795_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.881827 17663 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x63a70c701723696286880525795_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63a70c701723696286880525795_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63a70c701723696286880525795_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:31:26.881881 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x63a70de0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:31:26.883878 17597 pir_interpreter.cc:161] PirInterpreter(): 0x4d3146a0 on Place(gpu:0)
1884: I0815 04:31:26.883911 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_1
1884: I0815 04:31:26.883927 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_4
1884: I0815 04:31:26.883936 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_5
1884: I0815 04:31:26.883944 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_6
1884: I0815 04:31:26.883966 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_7
1884: I0815 04:31:26.883978 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_8
1884: I0815 04:31:26.883988 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_9
1884: I0815 04:31:26.884016 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_10
1884: I0815 04:31:26.884025 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_11
1884: I0815 04:31:26.884033 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_12
1884: I0815 04:31:26.884042 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_13
1884: I0815 04:31:26.884050 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_14
1884: I0815 04:31:26.884058 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_15
1884: I0815 04:31:26.884068 17597 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:31:26.884076 17597 scope.cc:202] Create variable 0x4d3146a01723696286883899968_inner_var_17
1884: I0815 04:31:26.884084 17597 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x635f5e20
1884: 1 -> 0x4d3146a01723696286883899968_inner_var_1 -> 0x42842f00
1884: 2 -> linear_1.b_0 -> 0x63848240
1884: 3 -> linear_1.w_0 -> 0x63ba5410
1884: 4 -> 0x4d3146a01723696286883899968_inner_var_4 -> 0x68ae380
1884: 5 -> 0x4d3146a01723696286883899968_inner_var_5 -> 0x61a152d0
1884: 6 -> 0x4d3146a01723696286883899968_inner_var_6 -> 0x63826e90
1884: 7 -> 0x4d3146a01723696286883899968_inner_var_7 -> 0x63826f90
1884: 8 -> 0x4d3146a01723696286883899968_inner_var_8 -> 0x63a84460
1884: 9 -> 0x4d3146a01723696286883899968_inner_var_9 -> 0x6443bb60
1884: 10 -> 0x4d3146a01723696286883899968_inner_var_10 -> 0x61a26aa0
1884: 11 -> 0x4d3146a01723696286883899968_inner_var_11 -> 0x63ba2c20
1884: 12 -> 0x4d3146a01723696286883899968_inner_var_12 -> 0x61c062d0
1884: 13 -> 0x4d3146a01723696286883899968_inner_var_13 -> 0x61c03b60
1884: 14 -> 0x4d3146a01723696286883899968_inner_var_14 -> 0x62d5110
1884: 15 -> 0x4d3146a01723696286883899968_inner_var_15 -> 0x643c1d10
1884: 16 -> fetch0@fetch -> 0x63865540
1884: 17 -> 0x4d3146a01723696286883899968_inner_var_17 -> 0x61a1acf0
1884: 18 -> fetch1@fetch -> 0x45131930
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:31:26.885718 17664 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.885792 17665 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.885867 17666 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.885936 17667 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.885969 17666 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.885972 17665 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886004 17666 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:31:26.886018 17665 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.886023 17668 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:31:26.886078 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886108 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:31:26.886137 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886178 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.886241 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:31:26.886258 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:31:26.886322 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:31:26.886340 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x4d3146a01723696286883899968_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886379 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x4d3146a01723696286883899968_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:31:26.886404 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x4d3146a01723696286883899968_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886435 17668 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:31:26.886471 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x4d3146a01723696286883899968_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.886498 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x4d3146a01723696286883899968_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x4d3146a01723696286883899968_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886534 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.886554 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x4d3146a01723696286883899968_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x4d3146a01723696286883899968_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.886584 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x4d3146a01723696286883899968_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886605 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.886605 17665 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d3146a01723696286883899968_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886615 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x4d3146a01723696286883899968_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.886627 17665 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.886631 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x4d3146a01723696286883899968_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886654 17668 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.886693 17665 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d3146a01723696286883899968_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.886722 17665 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4d3146a01723696286883899968_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886740 17665 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.886741 17668 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.886754 17665 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4d3146a01723696286883899968_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.886782 17668 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.886854 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.886874 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d3146a01723696286883899968_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x4d3146a01723696286883899968_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.886906 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886914 17665 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886927 17665 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:31:26.886931 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.886956 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.886960 17665 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d3146a01723696286883899968_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.886973 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x4d3146a01723696286883899968_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886978 17665 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4d3146a01723696286883899968_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.886991 17665 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.887003 17665 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4d3146a01723696286883899968_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.887042 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x4d3146a01723696286883899968_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:31:26.887063 17668 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x4d3146a01723696286883899968_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4d3146a01723696286883899968_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.887085 17668 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:31:26.887101 17668 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x4d3146a01723696286883899968_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4d3146a01723696286883899968_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4d3146a01723696286883899968_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:31:26.887138 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x4d314810) got event_name: TaskCompletion
1884: I0815 04:31:26.887168 17597 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.887202 17597 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:31:26.893095 17597 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:31:26.893144 17597 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:31:26.893864 17597 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:31:26.893915 17597 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:31:26.894402 17597 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868944601700"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868944601700"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:31:26.894603 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(cpu)
1884: I0815 04:31:26.894625 17597 scope.cc:202] Create variable 0x619f89601723696286894618888_inner_var_0
1884: I0815 04:31:26.894654 17597 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868944601700"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236962868944601700 -> 0x61a33c90
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:31:26.894800 17597 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:31:26.894937 17669 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.895083 17671 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.895082 17670 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.895152 17672 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.895156 17670 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236962868944601700:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.895215 17670 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236962868944601700:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:31:26.895222 17673 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.895248 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x619f8ad0) got event_name: TaskCompletion
1884: I0815 04:31:26.895517 17670 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11166891072268078134 to 13643486491906691280 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:31:26.895527 17670 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11166891072268078134 to 13643486491906691280 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:31:26.895640 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.895649 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868957745561"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868957745561"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:31:26.895999 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(cpu)
1884: I0815 04:31:26.896024 17597 scope.cc:202] Create variable 0x619f89601723696286896014689_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868957745561"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236962868957745561 -> 0x61a067f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:31:26.896284 17674 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.896348 17675 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.896364 17676 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.896422 17678 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.896436 17677 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.896438 17676 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236962868957745561:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.896513 17676 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236962868957745561:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.896540 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x619f8ad0) got event_name: TaskCompletion
1884: I0815 04:31:26.896711 17676 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 3524136937063757526 to 13643486491906691280 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:31:26.896719 17676 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 3524136937063757526 to 13643486491906691280 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:31:26.896826 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.896832 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236962868957745561",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236962868969192742"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236962868957745561",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236962868969192742"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:31:26.897076 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236962868957745561",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236962868969192742"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236962868969192742 -> 0x61a067f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:31:26.897367 17679 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.897425 17680 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.897445 17681 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.897478 17682 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.897504 17683 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.897502 17682 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.897531 17682 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.897553 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x619f8ad0) got event_name: TaskCompletion
1884: I0815 04:31:26.897818 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.897826 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868979052143"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868979052143"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:31:26.898037 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(cpu)
1884: I0815 04:31:26.898056 17597 scope.cc:202] Create variable 0x619f89601723696286898051100_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236962868979052143"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236962868979052143 -> 0x6364b080
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:31:26.898247 17684 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.898325 17685 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:31:26.898348 17686 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:31:26.898377 17687 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:31:26.898406 17688 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:31:26.898403 17687 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236962868979052143:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.898449 17687 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236962868979052143:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:31:26.898475 17597 pir_interpreter.cc:1766] main_thread_blocker_(0x619f8ad0) got event_name: TaskCompletion
1884: I0815 04:31:26.898641 17687 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 18043262396355597542 to 13643486491906691280 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:31:26.898650 17687 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 18043262396355597542 to 13643486491906691280 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:31:26.898741 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.898748 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:26.898815 17597 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:31:26.898888 17597 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:31:26.898931 17597 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868979052143"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868969192742"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868979052143"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868969192742"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:31:26.899650 17597 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:31:26.899667 17597 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:31:26.899706 17597 pir_interpreter.cc:161] PirInterpreter(): 0x619f8960 on Place(cpu)
1884: I0815 04:31:26.899736 17597 scope.cc:202] Create variable feed_name_0
1884: I0815 04:31:26.899751 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_5
1884: I0815 04:31:26.899773 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_6
1884: I0815 04:31:26.899785 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_7
1884: I0815 04:31:26.899794 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_8
1884: I0815 04:31:26.899814 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_9
1884: I0815 04:31:26.899827 17597 scope.cc:202] Create variable 0x619f89601723696286899720991_inner_var_10
1884: I0815 04:31:26.899848 17597 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.899868 17597 helper.h:475] Init predictor : [cpu current allocated memory: 3.05385MB], [cpu current reserved memory: 3.05385MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:31:26.899995 17597 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.900012 17597 helper.h:475] before run : [cpu current allocated memory: 3.0539MB], [cpu current reserved memory: 3.0539MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868979052143"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236962868969192742"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236962868979052143 -> 0x6364b080
1884: 1 -> constant_folding@_17236962868969192742 -> 0x61a067f0
1884: 2 -> linear_1.b_0 -> 0x63847f00
1884: 3 -> linear_1.w_0 -> 0x643cd610
1884: 4 -> feed_name_0 -> 0x619fdf90
1884: 5 -> 0x619f89601723696286899720991_inner_var_5 -> 0x640bc580
1884: 6 -> 0x619f89601723696286899720991_inner_var_6 -> 0x61a33e70
1884: 7 -> 0x619f89601723696286899720991_inner_var_7 -> 0x61a2dea0
1884: 8 -> 0x619f89601723696286899720991_inner_var_8 -> 0x643ce220
1884: 9 -> fetch_name_0 -> 0x61a11a90
1884: 10 -> fetch_name_1 -> 0x643a48d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:31:26.900609 17597 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:31:26.900671 17689 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:31:26.900665 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.900717 17597 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:31:26.900738 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.900769 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x619f89601723696286899720991_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x619f89601723696286899720991_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.900810 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x619f89601723696286899720991_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.900843 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.900864 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.900880 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.900913 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236962868969192742:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.900938 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236962868979052143:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.900967 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236962868979052143:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x619f89601723696286899720991_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:31:26.900995 17597 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236962868979052143:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:31:26.901023 17597 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236962868979052143:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x619f89601723696286899720991_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:31:26.901051 17597 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:31:26.901072 17597 helper.h:475] after run : [cpu current allocated memory: 3.05408MB], [cpu current reserved memory: 3.05408MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:31:26.901093 17597 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:31:26.901223 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.901232 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:26.901279 17689 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 11166891072268078134 to 13643486491906691280 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:31:26.901288 17689 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 11166891072268078134 to 13643486491906691280 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:31:26.901330 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:26.901338 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EE...
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2976, in check_output_customized
1884:     checker(outs)
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 82, in verify_output
1884:     print(self.calc_output())
1884: TypeError: calc_output() missing 1 required positional argument: 'place'
1884: 
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2976, in check_output_customized
1884:     checker(outs)
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 82, in verify_output
1884:     print(self.calc_output())
1884: TypeError: calc_output() missing 1 required positional argument: 'place'
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.471s
1884: 
1884: FAILED (errors=2, skipped=3)
1884: 
1884: I0815 04:31:26.903486 17597 mmap_allocator.cc:348] PID: 17597, MemoryMapFdSet: set size - 0
1884: I0815 04:31:26.916055 17597 mmap_allocator.cc:348] PID: 17597, MemoryMapFdSet: set size - 0
1884: I0815 04:31:26.986799 17660 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13340423173674060691 to 13643486491906691280 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:31:26.986814 17660 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13340423173674060691 to 13643486491906691280 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:31:26.986837 17661 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10612537056882985011 to 13643486491906691280 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:31:26.986850 17661 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10612537056882985011 to 13643486491906691280 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:31:26.986887 17662 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5697889645413499531 to 13643486491906691280 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:31:26.986908 17662 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5697889645413499531 to 13643486491906691280 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:31:26.987049 17663 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 12800182798909144744 to 13643486491906691280 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:31:26.987061 17663 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 12800182798909144744 to 13643486491906691280 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:31:26.987068 17663 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 12800182798909144744 to 11434449934605407536 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:31:26.987377 17665 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 11434449934605407536 to 13643486491906691280 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:31:26.987386 17665 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 11434449934605407536 to 13643486491906691280 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:31:26.987391 17665 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 11434449934605407536 to 13643486491906691280 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:31:26.987416 17666 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14806409784238778518 to 13643486491906691280 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:31:26.987425 17666 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14806409784238778518 to 13643486491906691280 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:31:26.987623 17668 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 13643486491906691280 to 14369536172427389571 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:31:26.987633 17668 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 13643486491906691280 to 14369536172427389571 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:31:26.987638 17668 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 13643486491906691280 to 14369536172427389571 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:31:27.120838 17597 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:31:27.120864 17597 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:31:27.120905 17597 mmap_allocator.cc:348] PID: 17597, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   14.40 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  14.58 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
