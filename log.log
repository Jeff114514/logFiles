UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:27:08.111961 17479 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:27:08.893806 17479 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=free_idle_chunk,search_cache_max_number,graph_embedding_split_infer_mode,new_executor_use_local_scope,gpugraph_hbm_table_load_factor,all_blocks_convert_trt,prim_forward,use_virtual_memory_auto_growth,dygraph_debug,fraction_of_cuda_pinned_memory_to_use,jit_engine_type,accuracy_check_rtol_bf16,pinned_memory_as_cpu_backend,npu_storage_format,cupti_dir,enable_gpu_memory_usage_log_mb,cudnn_deterministic,gpugraph_enable_hbm_table_collision_stat,graph_neighbor_size_percent,gpugraph_offload_param_stat,gpugraph_force_device_batch_num_equal,sync_after_alloc,benchmark_nccl,low_precision_op_list,enable_neighbor_list_use_uva,manually_trans_conv_filter,pir_apply_inplace_pass,eager_delete_scope,cusolver_dir,allreduce_record_one_event,prim_skip_dynamic,enable_pir_api,selected_gpus,ir_inplace_kernel_blacklist,enable_pir_with_pt_in_dy2st,inner_op_parallelism,gpugraph_sparse_table_storage_mode,use_xqa_optim,sync_nccl_allreduce,enable_cublas_tensor_op_math,cudnn_exhaustive_search_times,cse_max_count,enable_cinn_accuracy_check,run_kp_kernel,enable_cinn_auto_tune,print_sub_graph_dir,cublaslt_device_best_config,prim_check_ops,gpu_allocator_retry_time,enable_collect_shape,gpugraph_enable_segment_merge_grads,pir_subgraph_saving_dir,eager_delete_tensor_gb,fraction_of_cpu_memory_to_use,static_runtime_data_save_path,accuracy_check_rtol_fp32,conv_workspace_size_limit,enable_record_memory,use_cuda_managed_memory,prim_enabled,new_executor_use_cuda_graph,enable_cse_in_dy2st,alloc_fill_value,cache_inference_while_scope,memory_fraction_of_eager_deletion,apply_pass_to_program,enable_unused_var_check,gpugraph_enable_gpu_direct_access,logging_trunc_pir_py_code,enable_blaslt_global_search,dist_threadpool_size,enable_graph_multi_node_sampling,fleet_executor_with_standalone,use_autotune,gpugraph_parallel_copyer_split_maxsize,allocator_strategy,custom_device_mem_record,tracer_onednn_ops_off,multi_node_sample_use_gpu_table,trt_ibuilder_cache,prim_all,cublaslt_exhaustive_search_times,print_allocator_trace_info,gpugraph_slot_feasign_max_num,free_when_no_cache_hit,use_stream_safe_cuda_allocator,local_exe_sub_scope_limit,lapack_dir,cinn_compile_thread_num,mkl_dir,cudnn_batchnorm_spatial_persistent,use_auto_growth_pinned_allocator,check_nan_inf_level,gpugraph_storage_mode,gpugraph_merge_grads_segment_size,accuracy_check_atol_fp32,enable_sparse_inner_gather,graph_get_neighbor_id,nvidia_package_dir,convert_all_blocks,query_dest_rank_by_multi_node,embedding_deterministic,cuda_malloc_async_pool_memory_throttle_ratio,win_cuda_bin_dir,pir_apply_shape_optimization_pass,dump_chunk_info,enable_pir_in_executor,cinn_subgraph_graphviz_dir,enable_all2all_use_fp16,logging_pir_py_code_dump_symbolic_dims,enable_dependency_builder_debug_info,enable_auto_detect_gpu_topo,check_nan_inf,deny_cinn_ops,tracer_onednn_ops_on,multiple_of_cupti_buffer_size,curand_dir,cusparse_dir,reader_queue_speed_test_mode,enable_interpretercore_launch_cinn,reallocate_gpu_memory_in_mb,tensor_operants_mode,cudnn_exhaustive_search,gpugraph_enable_print_op_debug,logging_pir_py_code_dir,logging_pir_py_code_int_tensor_element_limit,new_executor_serial_run,use_cinn,gemm_use_half_precision_compute_type,use_shm_cache,dataloader_use_file_descriptor,graph_metapath_split_opt,auto_growth_chunk_size_in_mb,conv2d_disable_cudnn,enable_pir_in_executor_trace_run,use_mkldnn,initial_gpu_memory_in_mb,gpugraph_parallel_stream_num,save_static_runtime_data,fuse_parameter_memory_size,enable_tracker_all2all,set_to_1d,op_dir,gpugraph_dedup_pull_push_mode,nccl_dir,add_dependency_for_communication_op,new_executor_static_build,accuracy_check_rtol_fp16,prim_backward,fast_eager_deletion_mode,log_memory_stats,new_executor_use_inplace,get_host_by_name_time,cudnn_dir,pir_broadcast_tree_limit,new_executor_sequential_run,check_kernel_launch,accuracy_check_atol_bf16,tensorrt_dir,cuda_memory_async_pool_realease_threshold,enable_exit_when_partial_worker,use_system_allocator,print_ir,prim_enable_dynamic,use_cuda_malloc_async_allocator,async_trace_count,fuse_parameter_groups_size,pir_debug,prim_forward_blacklist,fraction_of_gpu_memory_to_use,gpugraph_debug_gpu_memory,enable_dump_main_program,init_allocated_mem,enable_cinn_compile_cache,auto_free_cudagraph_allocations_on_launch,use_stride_kernel,enable_gpu_memory_usage_log,einsum_opt,max_inplace_grad_add,enable_fusion_fallback,allow_cinn_ops,graph_load_in_parallel,executor_log_deps_every_microseconds,static_executor_perfstat_filepath,mklml_dir,use_auto_growth_v2,disable_dyshape_in_train,call_stack_level,accuracy_check_atol_fp16,initial_cpu_memory_in_mb,cusparselt_dir,use_pinned_memory,sort_sum_gradient,enable_adjust_op_order,enable_auto_rdma_trans,cublas_dir,use_fast_math,host_trace_level,nccl_blocking_wait,paddle_num_threads,cuda_dir,enable_opt_get_features,benchmark,tracer_profile_fname,dynamic_static_unified_comm,enable_api_kernel_fallback,gpugraph_offload_gather_copy_maxsize,gpugraph_load_node_list_into_hbm,gpugraph_offload_param_extends,enable_fuse_parallel_matmul_pass,enable_async_trace,check_infer_symbolic,gpu_memory_limit_mb 
1884: I0815 04:27:08.893922 17479 init.cc:108] After Parse: argc is 2
1884: I0815 04:27:17.377611 17479 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:27:17.377650 17479 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:27:17.378361 17479 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:27:17.378705 17479 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:27:17.379544 17479 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:27:17.379650 17479 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:27:17.379745 17479 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:27:17.380462 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f488c000000), and remaining 0
1884: I0815 04:27:17.380760 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.380836 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.380941 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f488c000200), and remaining 0
1884: I0815 04:27:17.380975 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f488c000400), and remaining 0
1884: I0815 04:27:17.384706 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f488c000600), and remaining 0
1884: I0815 04:27:17.384835 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f488c000800), and remaining 0
1884: I0815 04:27:17.384928 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f488c000a00), and remaining 0
1884: I0815 04:27:17.385018 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.385039 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.385104 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.385118 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.385985 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a87e0e0 for it.
1884: I0815 04:27:17.386132 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.386155 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.386209 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f488c000e00), and remaining 0
1884: I0815 04:27:17.386287 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f488c0c4400), and remaining 0
1884: I0815 04:27:17.513579 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a87e0e0 for it.
1884: I0815 04:27:17.513788 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.513836 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.514498 17479 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f488c200000), and remaining 0
1884: I0815 04:27:17.529238 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a87e0e0 for it.
1884: I0815 04:27:17.529353 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:17.529383 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.529423 17479 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:17.529621 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:17.530567 17479 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:27:17.530586 17479 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:27:17.530633 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:17.530722 17479 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:27:17.530747 17479 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.530802 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:17.530903 17479 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:27:17.530921 17479 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.530954 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:17.531239 17479 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:27:17.531258 17479 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.531427 17479 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:27:17.531453 17479 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:17.531522 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:17.533659 17479 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:27:17.533761 17479 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:27:17.533782 17479 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:27:17.533833 17479 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:27:18.984982 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:18.985070 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:18.985479 17479 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:27:18.985504 17479 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:18.991082 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:18.991118 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:18.992331 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:18.992352 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:18.992369 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:18.993187 17479 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:27:18.993201 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:18.993224 17479 scope.cc:202] Create variable feed
1884: I0815 04:27:18.993234 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:18.993243 17479 scope.cc:202] Create variable fetch
1884: I0815 04:27:18.993249 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:18.993261 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:18.993266 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:18.993270 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:18.993274 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:18.995682 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:18.996018 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:18.996031 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:18.996035 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:18.997717 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:18.997766 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:18.997774 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:18.997781 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:18.997788 17479 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:27:18.997797 17479 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61f83200 type is 7
1884: I0815 04:27:18.997802 17479 scope.cc:202] Create variable x
1884: I0815 04:27:18.997807 17479 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x61f81cc0 type is 7
1884: I0815 04:27:18.997864 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:18.997870 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:18.997874 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:18.997879 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:18.998021 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:18.998049 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:18.998183 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:18.998191 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:18.998207 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:18.998386 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:18.998417 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:18.998438 17479 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:18.998443 17479 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61f89d20Variable Type 7
1884: I0815 04:27:18.998468 17479 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:18.998490 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:18.998543 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:18.998567 17479 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:18.999817 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:18.999874 17479 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.000283 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.005906 17479 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:27:19.005925 17479 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:27:19.006017 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:19.006045 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.006580 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: I0815 04:27:19.006651 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:19.006675 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.007128 17479 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: I0815 04:27:19.007189 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:19.007211 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.007236 17479 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.007526 17479 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:27:19.007537 17479 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:27:19.007660 17479 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:27:19.007683 17479 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.008073 17479 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:27:19.008085 17479 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:27:19.008127 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:19.008147 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.008340 17479 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:27:19.008351 17479 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:27:19.008388 17479 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:27:19.008407 17479 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:27:19.008423 17479 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.011119 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.011142 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.011194 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.011204 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.013103 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.013469 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.013484 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.013489 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.015278 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.015336 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.015347 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.015354 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61fb55e0 type is 7
1884: I0815 04:27:19.015364 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.015369 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61fb5950 type is 7
1884: I0815 04:27:19.015374 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.015379 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.015435 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.015444 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.015450 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.015455 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.015501 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.015519 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.015576 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.015586 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.015604 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.015839 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.015854 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.015872 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.015879 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61fbc0d0Variable Type 7
1884: I0815 04:27:19.015898 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.015915 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.015939 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.015956 17479 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.016635 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.016665 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.016872 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.028097 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: I0815 04:27:19.028278 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:27:19.035791 17479 pir_interpreter.cc:161] PirInterpreter(): 0x61f747b0 on Place(gpu:0)
1884: I0815 04:27:19.035840 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.035871 17479 scope.cc:202] Create variable 0x61f747b01723696039035817558_inner_var_1
1884: I0815 04:27:19.035882 17479 scope.cc:202] Create variable 0x61f747b01723696039035817558_inner_var_2
1884: I0815 04:27:19.035894 17479 scope.cc:202] Create variable 0x61f747b01723696039035817558_inner_var_3
1884: I0815 04:27:19.035902 17479 scope.cc:202] Create variable 0x61f747b01723696039035817558_inner_var_4
1884: I0815 04:27:19.035914 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.036352 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.036367 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.036372 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:27:19.036417 17479 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61f74680
1884: 1 -> 0x61f747b01723696039035817558_inner_var_1 -> 0x61f74790
1884: 2 -> 0x61f747b01723696039035817558_inner_var_2 -> 0x61f74f30
1884: 3 -> 0x61f747b01723696039035817558_inner_var_3 -> 0x61f74020
1884: 4 -> 0x61f747b01723696039035817558_inner_var_4 -> 0x61f751b0
1884: 5 -> fetch0@fetch -> 0x61f630a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.037173 17479 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:27:19.037431 17517 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.037528 17518 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.037632 17519 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.037652 17520 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.037700 17521 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.037727 17519 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f747b01723696039035817558_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.037832 17522 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.037871 17519 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f747b01723696039035817558_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.037879 17522 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.037925 17522 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:27:19.037971 17522 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f747b01723696039035817558_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f747b01723696039035817558_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.038162 17522 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f747b01723696039035817558_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f747b01723696039035817558_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:27:19.038234 17519 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f747b01723696039035817558_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.038259 17519 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.039500 17519 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f747b01723696039035817558_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61f747b01723696039035817558_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:27:19.039543 17519 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f747b01723696039035817558_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.039572 17519 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.040148 17519 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f747b01723696039035817558_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:27:19.040185 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x61f74920) got event_name: TaskCompletion
1884: I0815 04:27:19.040212 17479 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.114430 17517 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 8449082692095416186 to 2143424216090434313 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:27:19.114462 17517 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8449082692095416186 to 12432396714844489770 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:27:19.114468 17517 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8449082692095416186 to 12432396714844489770 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:27:19.114655 17519 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:27:19.114668 17519 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:27:19.114854 17522 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:27:19.114871 17522 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.119412 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.119436 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.119496 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.119504 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.121219 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.121554 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.121569 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.121574 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.123119 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.123219 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.123230 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.123236 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x647027b0 type is 7
1884: I0815 04:27:19.123245 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.123248 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x64702b20 type is 7
1884: I0815 04:27:19.123253 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.123258 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.123327 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.123333 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.123339 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.123344 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.123399 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.123415 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.123477 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.123486 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.123500 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.123644 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.123654 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.123669 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.123675 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6471ad30Variable Type 7
1884: I0815 04:27:19.123692 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.123710 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.123731 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.123746 17479 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.125393 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.125430 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.125644 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.138321 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: I0815 04:27:19.138553 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:27:19.141709 17479 pir_interpreter.cc:161] PirInterpreter(): 0x6459e540 on Place(gpu:0)
1884: I0815 04:27:19.141744 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.141768 17479 scope.cc:202] Create variable 0x6459e5401723696039141735565_inner_var_1
1884: I0815 04:27:19.141779 17479 scope.cc:202] Create variable 0x6459e5401723696039141735565_inner_var_2
1884: I0815 04:27:19.141788 17479 scope.cc:202] Create variable 0x6459e5401723696039141735565_inner_var_3
1884: I0815 04:27:19.141799 17479 scope.cc:202] Create variable 0x6459e5401723696039141735565_inner_var_4
1884: I0815 04:27:19.141809 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.142128 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.142143 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.142146 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x64702d90
1884: 1 -> 0x6459e5401723696039141735565_inner_var_1 -> 0x64702e10
1884: 2 -> 0x6459e5401723696039141735565_inner_var_2 -> 0x6459e190
1884: 3 -> 0x6459e5401723696039141735565_inner_var_3 -> 0x647136f0
1884: 4 -> 0x6459e5401723696039141735565_inner_var_4 -> 0x64613600
1884: 5 -> fetch0@fetch -> 0x645b38b0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.142853 17523 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.142938 17524 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.142948 17525 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.142992 17526 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.143014 17527 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.143057 17528 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.143067 17527 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6459e5401723696039141735565_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.143085 17528 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.143126 17527 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6459e5401723696039141735565_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.143126 17528 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:27:19.143170 17528 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6459e5401723696039141735565_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6459e5401723696039141735565_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.143307 17528 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6459e5401723696039141735565_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6459e5401723696039141735565_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.143368 17527 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6459e5401723696039141735565_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.143389 17527 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.145995 17527 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6459e5401723696039141735565_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6459e5401723696039141735565_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.146040 17527 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6459e5401723696039141735565_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.146061 17527 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.148069 17527 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6459e5401723696039141735565_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.148115 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x6459e6b0) got event_name: TaskCompletion
1884: I0815 04:27:19.148137 17479 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.186693 17523 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 2143424216090434313 to 12432396714844489770 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:27:19.186713 17523 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2143424216090434313 to 13180696948244695195 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:27:19.186719 17523 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2143424216090434313 to 13180696948244695195 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:27:19.186866 17527 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13180696948244695195 to 15487614165098538368 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:27:19.186877 17527 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13180696948244695195 to 15487614165098538368 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:27:19.187044 17528 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.190753 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.190774 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.190825 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.190833 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.192430 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.192749 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.192761 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.192766 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.194286 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.194386 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.194397 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.194402 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x648ae900 type is 7
1884: I0815 04:27:19.194411 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.194415 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6471ccb0 type is 7
1884: I0815 04:27:19.194419 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.194427 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.194475 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.194481 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.194485 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.194490 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.194540 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.194553 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.194612 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.194619 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.194633 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.194667 17479 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.194799 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.194860 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.194869 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.194885 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.194891 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64606650Variable Type 7
1884: I0815 04:27:19.194907 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.194926 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.194947 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.194960 17479 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.195181 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.195202 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.195394 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.196133 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: I0815 04:27:19.196326 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:27:19.199295 17479 pir_interpreter.cc:161] PirInterpreter(): 0x648af180 on Place(gpu:0)
1884: I0815 04:27:19.199352 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.199375 17479 scope.cc:202] Create variable 0x648af1801723696039199343452_inner_var_1
1884: I0815 04:27:19.199388 17479 scope.cc:202] Create variable 0x648af1801723696039199343452_inner_var_2
1884: I0815 04:27:19.199398 17479 scope.cc:202] Create variable 0x648af1801723696039199343452_inner_var_3
1884: I0815 04:27:19.199409 17479 scope.cc:202] Create variable 0x648af1801723696039199343452_inner_var_4
1884: I0815 04:27:19.199419 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.199740 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.199754 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.199759 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x64665500
1884: 1 -> 0x648af1801723696039199343452_inner_var_1 -> 0x64665dc0
1884: 2 -> 0x648af1801723696039199343452_inner_var_2 -> 0x648cb690
1884: 3 -> 0x648af1801723696039199343452_inner_var_3 -> 0x61f639c0
1884: 4 -> 0x648af1801723696039199343452_inner_var_4 -> 0x646f8c10
1884: 5 -> fetch0@fetch -> 0x61f772c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.200455 17529 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.200531 17530 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.200551 17531 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.200595 17532 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.200627 17533 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.200675 17534 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.200656 17533 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x648af1801723696039199343452_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.200711 17534 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.200754 17533 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x648af1801723696039199343452_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.200763 17534 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:27:19.200806 17534 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x648af1801723696039199343452_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x648af1801723696039199343452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.200865 17534 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.201013 17534 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.201046 17534 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x648af1801723696039199343452_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x648af1801723696039199343452_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:27:19.201108 17533 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x648af1801723696039199343452_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.201131 17533 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.201386 17533 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x648af1801723696039199343452_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x648af1801723696039199343452_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:27:19.201413 17533 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x648af1801723696039199343452_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.201433 17533 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.201447 17533 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x648af1801723696039199343452_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:27:19.201483 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x648af2f0) got event_name: TaskCompletion
1884: I0815 04:27:19.201506 17479 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.233354 17529 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 12432396714844489770 to 2143424216090434313 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:27:19.233367 17529 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12432396714844489770 to 2143424216090434313 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:27:19.233373 17529 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12432396714844489770 to 2143424216090434313 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:27:19.233573 17533 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 10490613342170176297 to 2143424216090434313 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:27:19.233582 17533 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 10490613342170176297 to 2143424216090434313 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:27:19.233767 17534 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:27:19.233778 17534 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:27:19.233783 17534 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.237618 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.237641 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.237687 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.237694 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.239221 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.239554 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.239568 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.239573 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.241024 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.241102 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.241112 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.241117 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63eb11b0 type is 7
1884: I0815 04:27:19.241124 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.241127 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x63eb03e0 type is 7
1884: I0815 04:27:19.241132 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.241137 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.241186 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.241194 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.241197 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.241201 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.241246 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.241258 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.241323 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.241333 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.241348 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.241582 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.241593 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.241608 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.241616 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x648bc910Variable Type 7
1884: I0815 04:27:19.241632 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.241650 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.241673 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.241688 17479 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.242404 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.242434 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.242625 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.247181 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: I0815 04:27:19.247360 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:27:19.250335 17479 pir_interpreter.cc:161] PirInterpreter(): 0x648bc330 on Place(gpu:0)
1884: I0815 04:27:19.250368 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.250391 17479 scope.cc:202] Create variable 0x648bc3301723696039250360113_inner_var_1
1884: I0815 04:27:19.250402 17479 scope.cc:202] Create variable 0x648bc3301723696039250360113_inner_var_2
1884: I0815 04:27:19.250414 17479 scope.cc:202] Create variable 0x648bc3301723696039250360113_inner_var_3
1884: I0815 04:27:19.250423 17479 scope.cc:202] Create variable 0x648bc3301723696039250360113_inner_var_4
1884: I0815 04:27:19.250435 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.250751 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.250766 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.250769 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61f63580
1884: 1 -> 0x648bc3301723696039250360113_inner_var_1 -> 0x64713500
1884: 2 -> 0x648bc3301723696039250360113_inner_var_2 -> 0x63eb0a90
1884: 3 -> 0x648bc3301723696039250360113_inner_var_3 -> 0x645fd4b0
1884: 4 -> 0x648bc3301723696039250360113_inner_var_4 -> 0x645ef6e0
1884: 5 -> fetch0@fetch -> 0x648cc450
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.251454 17535 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.251531 17536 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.251554 17537 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.251598 17538 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.251621 17539 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.251667 17540 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.251662 17539 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x648bc3301723696039250360113_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.251689 17540 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.251708 17539 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x648bc3301723696039250360113_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.251727 17540 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:27:19.251757 17540 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x648bc3301723696039250360113_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x648bc3301723696039250360113_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.251933 17540 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x648bc3301723696039250360113_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x648bc3301723696039250360113_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:27:19.251988 17539 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x648bc3301723696039250360113_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.252007 17539 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.252722 17539 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x648bc3301723696039250360113_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x648bc3301723696039250360113_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:27:19.252756 17539 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x648bc3301723696039250360113_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.252775 17539 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.253247 17539 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x648bc3301723696039250360113_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:27:19.253283 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x648bc4a0) got event_name: TaskCompletion
1884: I0815 04:27:19.253309 17479 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.286610 17535 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 2143424216090434313 to 12432396714844489770 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:27:19.286623 17535 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 2143424216090434313 to 13180696948244695195 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:27:19.286628 17535 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 2143424216090434313 to 13180696948244695195 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:27:19.286783 17539 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 13180696948244695195 to 15487614165098538368 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 04:27:19.286794 17539 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 13180696948244695195 to 15487614165098538368 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 04:27:19.286967 17540 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 04:27:19.286979 17540 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.291324 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.291347 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.291399 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.291407 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.293042 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.293398 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.293412 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.293418 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.294911 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.294994 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.295004 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.295010 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6470af70 type is 7
1884: I0815 04:27:19.295018 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.295023 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6470b1c0 type is 7
1884: I0815 04:27:19.295028 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.295033 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.295084 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.295090 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.295095 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.295099 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.295148 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.295161 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.295218 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.295226 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.295239 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.295379 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.295392 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.295406 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.295413 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x647076c0Variable Type 7
1884: I0815 04:27:19.295430 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.295449 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.295468 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.295481 17479 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.297053 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.297091 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.297291 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.303324 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: I0815 04:27:19.303509 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:27:19.306561 17479 pir_interpreter.cc:161] PirInterpreter(): 0x63a9ebb0 on Place(gpu:0)
1884: I0815 04:27:19.306594 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.306617 17479 scope.cc:202] Create variable 0x63a9ebb01723696039306586369_inner_var_1
1884: I0815 04:27:19.306628 17479 scope.cc:202] Create variable 0x63a9ebb01723696039306586369_inner_var_2
1884: I0815 04:27:19.306641 17479 scope.cc:202] Create variable 0x63a9ebb01723696039306586369_inner_var_3
1884: I0815 04:27:19.306653 17479 scope.cc:202] Create variable 0x63a9ebb01723696039306586369_inner_var_4
1884: I0815 04:27:19.306663 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.306996 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.307011 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.307015 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61f566a0
1884: 1 -> 0x63a9ebb01723696039306586369_inner_var_1 -> 0x64718240
1884: 2 -> 0x63a9ebb01723696039306586369_inner_var_2 -> 0x64717bc0
1884: 3 -> 0x63a9ebb01723696039306586369_inner_var_3 -> 0x61f76ae0
1884: 4 -> 0x63a9ebb01723696039306586369_inner_var_4 -> 0x646654e0
1884: 5 -> fetch0@fetch -> 0x646f82f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.307718 17541 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.307796 17542 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.307821 17543 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.307862 17544 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.307897 17545 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.307942 17546 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.307934 17545 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a9ebb01723696039306586369_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.307969 17546 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.307984 17545 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63a9ebb01723696039306586369_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.308000 17546 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:27:19.308027 17546 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63a9ebb01723696039306586369_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63a9ebb01723696039306586369_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.308126 17546 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63a9ebb01723696039306586369_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63a9ebb01723696039306586369_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.308177 17545 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63a9ebb01723696039306586369_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.308198 17545 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.310782 17545 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63a9ebb01723696039306586369_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63a9ebb01723696039306586369_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.310825 17545 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63a9ebb01723696039306586369_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.310846 17545 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.312723 17545 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63a9ebb01723696039306586369_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:27:19.312769 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x63a9ed20) got event_name: TaskCompletion
1884: I0815 04:27:19.312793 17479 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.350421 17541 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 12432396714844489770 to 2143424216090434313 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:27:19.350433 17541 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12432396714844489770 to 10490613342170176297 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:27:19.350437 17541 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12432396714844489770 to 10490613342170176297 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:27:19.350670 17545 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 10490613342170176297 to 15487614165098538368 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 04:27:19.350679 17545 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 10490613342170176297 to 15487614165098538368 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 04:27:19.350795 17546 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 2143424216090434313 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.354465 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.354489 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.354537 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.354545 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.356105 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.356446 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.356459 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.356464 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.361807 17479 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:27:19.361914 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.361925 17479 scope.cc:202] Create variable Out
1884: I0815 04:27:19.361932 17479 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6471c280 type is 7
1884: I0815 04:27:19.361938 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.361944 17479 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x645f0c10 type is 7
1884: I0815 04:27:19.361949 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.361954 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.362015 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.362020 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.362026 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.362030 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.362078 17479 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.362092 17479 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.362149 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.362157 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.362171 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.362210 17479 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.362339 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.362394 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.362404 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.362421 17479 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.362426 17479 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63a5aa70Variable Type 7
1884: I0815 04:27:19.362444 17479 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.362462 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.362484 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.362498 17479 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.362622 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.362643 17479 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.362846 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.363593 17479 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a8af970 for it.
1884: I0815 04:27:19.363790 17479 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a911a50 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:27:19.366828 17479 pir_interpreter.cc:161] PirInterpreter(): 0x63941810 on Place(gpu:0)
1884: I0815 04:27:19.366860 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.366883 17479 scope.cc:202] Create variable 0x639418101723696039366851939_inner_var_1
1884: I0815 04:27:19.366894 17479 scope.cc:202] Create variable 0x639418101723696039366851939_inner_var_2
1884: I0815 04:27:19.366904 17479 scope.cc:202] Create variable 0x639418101723696039366851939_inner_var_3
1884: I0815 04:27:19.366914 17479 scope.cc:202] Create variable 0x639418101723696039366851939_inner_var_4
1884: I0815 04:27:19.366923 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.367245 17479 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:27:19.367259 17479 scope.cc:202] Create variable X
1884: I0815 04:27:19.367264 17479 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x4d3383f0
1884: 1 -> 0x639418101723696039366851939_inner_var_1 -> 0x4d338470
1884: 2 -> 0x639418101723696039366851939_inner_var_2 -> 0x45d443b0
1884: 3 -> 0x639418101723696039366851939_inner_var_3 -> 0x6470b510
1884: 4 -> 0x639418101723696039366851939_inner_var_4 -> 0x66da2a0
1884: 5 -> fetch0@fetch -> 0x645fdcd0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:27:19.367954 17547 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.368038 17548 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.368062 17549 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.368105 17550 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.368134 17551 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.368181 17552 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.368175 17551 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x639418101723696039366851939_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.368201 17552 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.368222 17551 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x639418101723696039366851939_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.368247 17552 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:27:19.368276 17552 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x639418101723696039366851939_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x639418101723696039366851939_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.368321 17552 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.368428 17552 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.368455 17552 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x639418101723696039366851939_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x639418101723696039366851939_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:27:19.368526 17551 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x639418101723696039366851939_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.368568 17551 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.368680 17551 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x639418101723696039366851939_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x639418101723696039366851939_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:27:19.368712 17551 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x639418101723696039366851939_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.368731 17551 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.368744 17551 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x639418101723696039366851939_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:27:19.368780 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x63941980) got event_name: TaskCompletion
1884: I0815 04:27:19.368805 17479 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.401193 17547 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 2143424216090434313 to 12432396714844489770 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:27:19.401204 17547 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2143424216090434313 to 12432396714844489770 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:27:19.401209 17547 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2143424216090434313 to 12432396714844489770 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:27:19.401412 17551 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13180696948244695195 to 12432396714844489770 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:27:19.401422 17551 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13180696948244695195 to 12432396714844489770 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:27:19.401588 17552 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 04:27:19.401597 17552 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 04:27:19.401602 17552 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 12432396714844489770 to 15487614165098538368 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:27:19.407647 17479 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:27:19.407699 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:27:19.408764 17479 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:27:19.409612 17479 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:27:19.409641 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:27:19.410933 17479 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:27:19.410959 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:27:19.411653 17479 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:27:19.412617 17479 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:27:19.412644 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:27:19.413944 17479 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:27:19.413965 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:27:19.414556 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.414584 17479 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:27:19.414592 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.414597 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.416489 17479 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:27:19.416515 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:27:19.417471 17479 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:27:19.417500 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:27:19.418460 17479 pybind.cc:1827] need skip: 0
1884: I0815 04:27:19.418764 17479 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:27:19.420502 17479 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:27:19.424034 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.424052 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.424058 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.426028 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.426048 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.426054 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.426061 17479 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:27:19.426066 17479 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63eb4b70 type is 7
1884: I0815 04:27:19.426071 17479 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:27:19.426074 17479 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63eb5cf0 type is 7
1884: I0815 04:27:19.426080 17479 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:27:19.426083 17479 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63eb5d70 type is 7
1884: I0815 04:27:19.426146 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.426152 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.426157 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.426162 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.426209 17479 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426221 17479 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426242 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:27:19.426383 17479 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426394 17479 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426465 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.426510 17479 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426519 17479 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.426545 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.427524 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.428838 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.429276 17479 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:27:19.429502 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.429790 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.429996 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.430011 17479 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:27:19.430074 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.430081 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.430086 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.430186 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.430198 17479 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:27:19.431732 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.433063 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.434141 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.434329 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.434342 17479 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:27:19.434347 17479 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x646674d0 type is 7
1884: I0815 04:27:19.434355 17479 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:27:19.434357 17479 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x635a2200 type is 7
1884: I0815 04:27:19.434362 17479 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:27:19.434365 17479 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x635a22f0 type is 7
1884: I0815 04:27:19.434370 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.434376 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.434381 17479 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:27:19.434383 17479 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x64667810 type is 7
1884: I0815 04:27:19.434388 17479 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63eb4b70 type is 7
1884: I0815 04:27:19.434392 17479 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63eb5cf0 type is 7
1884: I0815 04:27:19.434397 17479 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:27:19.434401 17479 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x646677f0 type is 7
1884: I0815 04:27:19.434406 17479 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:27:19.434412 17479 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x64667d00 type is 7
1884: I0815 04:27:19.434415 17479 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63eb5d70 type is 7
1884: I0815 04:27:19.434419 17479 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:27:19.434424 17479 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x64667f70 type is 7
1884: I0815 04:27:19.434428 17479 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:27:19.434432 17479 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x646681b0 type is 7
1884: I0815 04:27:19.434437 17479 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:27:19.434440 17479 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x64668410 type is 7
1884: I0815 04:27:19.434520 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.434533 17479 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:27:19.434590 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.434597 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.434602 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.434605 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.434650 17479 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.434661 17479 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.434677 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:27:19.434789 17479 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.434799 17479 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.434818 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:27:19.434895 17479 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:27:19.434973 17479 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:27:19.436067 17479 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436089 17479 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436151 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.436215 17479 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436226 17479 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436239 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:27:19.436262 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.436311 17479 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436321 17479 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436332 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:27:19.436416 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436425 17479 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436439 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.436540 17479 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.436617 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.436677 17479 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436687 17479 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436702 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:27:19.436738 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.436787 17479 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436797 17479 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436810 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:27:19.436918 17479 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436928 17479 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.436947 17479 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.436988 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.436996 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.437013 17479 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.437019 17479 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x635dcee0Variable Type 7
1884: I0815 04:27:19.437037 17479 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.437053 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.437072 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.437085 17479 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.437124 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.437148 17479 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:27:19.437173 17479 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.437181 17479 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.437193 17479 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:27:19.437199 17479 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x635de600Variable Type 7
1884: I0815 04:27:19.437212 17479 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:27:19.437224 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.437238 17479 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.437249 17479 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.437283 17479 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:27:19.437295 17479 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:27:19.437700 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:27:19.437733 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:27:19.437752 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:27:19.437783 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:27:19.437814 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.437831 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:27:19.441677 17479 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:27:19.441712 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:27:19.442409 17479 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:27:19.442431 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:27:19.442746 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.444387 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.445199 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.445327 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.445859 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.446763 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.448846 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.449888 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.451745 17479 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:27:19.452517 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.452533 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.452538 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.453692 17479 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:27:19.453709 17479 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x526afe0 type is 9
1884: I0815 04:27:19.453716 17479 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5252840 type is 10
1884: I0815 04:27:19.453722 17479 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63eb5cf0 type is 7
1884: I0815 04:27:19.453727 17479 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63eb5d70 type is 7
1884: I0815 04:27:19.453732 17479 scope.cc:202] Create variable saved_params
1884: I0815 04:27:19.453734 17479 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x63db0330 type is 17
1884: I0815 04:27:19.453764 17479 interpreter_util.cc:594] Static build: 0
1884: I0815 04:27:19.453770 17479 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:27:19.453774 17479 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:27:19.453778 17479 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:27:19.453812 17479 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.453823 17479 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:27:19.454547 17479 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:27:19.454591 17479 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:27:19.454643 17479 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:27:19.455861 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.455919 17479 scope.cc:202] Create variable feed
1884: I0815 04:27:19.455927 17479 naive_executor.cc:189] 0x64550350 Create persistable variable feed, which pointer is 0x635a8820
1884: I0815 04:27:19.455932 17479 scope.cc:202] Create variable fetch
1884: I0815 04:27:19.455935 17479 naive_executor.cc:189] 0x64550350 Create persistable variable fetch, which pointer is 0x6454eca0
1884: I0815 04:27:19.455940 17479 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:27:19.455942 17479 naive_executor.cc:189] 0x64550350 Create persistable variable linear_0.b_0, which pointer is 0x6454ea50
1884: I0815 04:27:19.455947 17479 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:27:19.455950 17479 naive_executor.cc:189] 0x64550350 Create persistable variable linear_0.w_0, which pointer is 0x64550de0
1884: I0815 04:27:19.455966 17479 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:27:19.456295 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.456400 17479 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:27:19.456457 17479 executor.cc:183] Old Executor is Running.
1884: I0815 04:27:19.456532 17479 executor.cc:92] Creating Variables for block 0
1884: I0815 04:27:19.456539 17479 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:27:19.456542 17479 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x6454ea50 type is 7
1884: I0815 04:27:19.456547 17479 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:27:19.456549 17479 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x64550de0 type is 7
1884: I0815 04:27:19.456579 17479 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.456652 17479 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:27:19.456696 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.456702 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.456835 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.456943 17479 graph.cc:149] create OpNode by feed
1884: I0815 04:27:19.456981 17479 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:27:19.456998 17479 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:27:19.457012 17479 graph.cc:149] create OpNode by abs
1884: I0815 04:27:19.457022 17479 graph.cc:149] create OpNode by assign_value
1884: I0815 04:27:19.457039 17479 graph.cc:149] create OpNode by multinomial
1884: I0815 04:27:19.457051 17479 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:27:19.457064 17479 graph.cc:149] create OpNode by scale
1884: I0815 04:27:19.457077 17479 graph.cc:149] create OpNode by scale
1884: I0815 04:27:19.457088 17479 graph.cc:149] create OpNode by fetch
1884: I0815 04:27:19.457106 17479 graph.cc:149] create OpNode by fetch
1884: I0815 04:27:19.457126 17479 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:27:19.458441 17479 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:27:19.458449 17479 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:27:19.458524 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.458531 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:27:19.458643 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.458894 17479 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:27:19.458953 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.458958 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:27:19.458992 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.458997 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:27:19.459040 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459101 17479 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:27:19.459133 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459138 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:27:19.459157 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459172 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459194 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459199 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:27:19.459240 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459262 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459285 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459290 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:27:19.459345 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459419 17479 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:27:19.459450 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459455 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:27:19.459487 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459507 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459528 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459534 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:27:19.459564 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459712 17479 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:27:19.459740 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459745 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:27:19.459777 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459794 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459817 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459823 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:27:19.459846 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459859 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459882 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459887 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:27:19.459909 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.459923 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.459944 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.459949 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:27:19.459975 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.460043 17479 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:27:19.460078 17479 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:27:19.460093 17479 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:27:19.460106 17479 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:27:19.460130 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.460136 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:27:19.460160 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.460201 17479 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:27:19.460222 17479 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:27:19.460234 17479 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:27:19.460247 17479 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:27:19.460276 17479 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:27:19.460287 17479 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:27:19.461619 17479 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:27:19.461668 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.461674 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:27:19.461700 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.461721 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.461747 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.461753 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:27:19.461776 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.461827 17479 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:27:19.461855 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.461861 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:27:19.461881 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.461897 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.461920 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.461925 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:27:19.461959 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.462044 17479 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:27:19.462066 17479 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:27:19.462081 17479 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:27:19.462096 17479 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:27:19.462111 17479 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:27:19.462126 17479 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:27:19.462142 17479 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:27:19.462164 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.462239 17479 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:27:19.462260 17479 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:27:19.462273 17479 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:27:19.462286 17479 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:27:19.462308 17479 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:27:19.462322 17479 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:27:19.462337 17479 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:27:19.462381 17479 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:27:19.462666 17479 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:27:19.462697 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.462702 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:27:19.462750 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.462810 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.462846 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.462891 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.462919 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.462961 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.462985 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463023 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463044 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463078 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463097 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463127 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463143 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463171 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463183 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463207 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463217 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463235 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463260 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463265 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:27:19.463291 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463341 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463367 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463373 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:27:19.463385 17479 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:27:19.463388 17479 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:27:19.463438 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463460 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463485 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463490 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:27:19.463501 17479 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:27:19.463505 17479 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:27:19.463544 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463567 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463590 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463595 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:27:19.463603 17479 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:27:19.463606 17479 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:27:19.463639 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463658 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463680 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463685 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:27:19.463696 17479 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:27:19.463697 17479 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:27:19.463734 17479 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:27:19.463754 17479 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:27:19.463778 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463783 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:27:19.463795 17479 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:27:19.463837 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.463842 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:27:19.463915 17479 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:27:19.463935 17479 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.463953 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:27:19.464004 17479 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:27:19.464022 17479 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:27:19.464049 17479 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:27:19.464071 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.464076 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:27:19.464956 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.464972 17479 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:27:19.465025 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.465031 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.465643 17479 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:27:19.465854 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.465926 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.465932 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.466346 17479 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:27:19.466552 17479 graph.h:183] deleting __fuse_statis__
1884: I0815 04:27:19.466559 17479 graph.h:183] deleting pass_recorder
1884: I0815 04:27:19.466564 17479 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:27:19.466645 17479 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:27:19.466653 17479 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:27:19.466657 17479 naive_executor.cc:195] 0x64550350 Create variable abs_0.tmp_0, which pointer is 0x63d99070
1884: I0815 04:27:19.466663 17479 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:27:19.466666 17479 naive_executor.cc:195] 0x64550350 Create variable gaussian_0.tmp_0, which pointer is 0x63941e60
1884: I0815 04:27:19.466677 17479 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:27:19.466682 17479 naive_executor.cc:195] 0x64550350 Create variable linear_0.tmp_1, which pointer is 0x63d8bee0
1884: I0815 04:27:19.466687 17479 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:27:19.466691 17479 naive_executor.cc:195] 0x64550350 Create variable multinomial_0.tmp_0, which pointer is 0x63d8b980
1884: I0815 04:27:19.466694 17479 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:27:19.466697 17479 naive_executor.cc:195] 0x64550350 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x63d8bc80
1884: I0815 04:27:19.466701 17479 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:27:19.466704 17479 naive_executor.cc:195] 0x64550350 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x63d8a280
1884: I0815 04:27:19.466711 17479 scope.cc:202] Create variable feed
1884: I0815 04:27:19.466715 17479 scope.cc:202] Create variable fetch
1884: I0815 04:27:19.466735 17479 naive_executor.cc:46] NaiveExecutor init with scope 0x64550350
1884: I0815 04:27:19.466742 17479 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:27:19.466928 17479 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:27:19.466941 17479 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:27:19.466969 17479 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:27:19.466974 17479 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:27:19.466981 17479 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.467013 17479 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 04:27:19.467214 17479 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.467229 17479 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 04:27:19.467275 17479 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.467324 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:27:19.477728 17479 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:27:19.477840 17479 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.477869 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:27:19.477941 17479 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:27:19.477972 17479 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.477998 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:27:19.478075 17479 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:27:19.478119 17479 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.478137 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:27:19.478189 17479 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:27:19.478219 17479 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:27:19.478235 17479 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:27:19.478271 17479 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:27:19.478289 17479 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.478332 17479 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 04:27:19.478358 17479 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:27:19.478833 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.478845 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:27:19.534572 17479 pir_interpreter.cc:161] PirInterpreter(): 0x4e4eaa50 on Place(gpu:0)
1884: I0815 04:27:19.534615 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_0
1884: I0815 04:27:19.534631 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_1
1884: I0815 04:27:19.534641 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_2
1884: I0815 04:27:19.534651 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_3
1884: I0815 04:27:19.534677 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_4
1884: I0815 04:27:19.534693 17479 scope.cc:202] Create variable 0x4e4eaa501723696039534602780_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x4e4eaa501723696039534602780_inner_var_0 -> 0x526a100
1884: 1 -> 0x4e4eaa501723696039534602780_inner_var_1 -> 0x61f8aa20
1884: 2 -> 0x4e4eaa501723696039534602780_inner_var_2 -> 0x61f9f010
1884: 3 -> linear_1.w_0 -> 0x64702b80
1884: 4 -> linear_1.b_0 -> 0x63960640
1884: 5 -> learning_rate_1 -> 0x636ba4d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:27:19.535584 17553 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.535604 17554 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.535629 17555 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.535660 17556 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.535701 17557 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.535694 17555 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535701 17556 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535707 17554 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535734 17557 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535750 17555 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:27:19.535771 17556 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.535773 17554 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4e4eaa501723696039534602780_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.535795 17557 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.535836 17557 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:27:19.535861 17557 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535880 17557 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.535892 17557 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:27:19.535905 17557 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x4e4eaa501723696039534602780_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4e4eaa501723696039534602780_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4e4eaa501723696039534602780_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.535971 17557 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x4e4eaa501723696039534602780_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4e4eaa501723696039534602780_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4e4eaa501723696039534602780_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:27:19.536031 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x4e4eabc0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:27:19.538105 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64703c10 on Place(gpu:0)
1884: I0815 04:27:19.538141 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_1
1884: I0815 04:27:19.538156 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_4
1884: I0815 04:27:19.538166 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_5
1884: I0815 04:27:19.538173 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_6
1884: I0815 04:27:19.538197 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_7
1884: I0815 04:27:19.538208 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_8
1884: I0815 04:27:19.538218 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_9
1884: I0815 04:27:19.538244 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_10
1884: I0815 04:27:19.538254 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_11
1884: I0815 04:27:19.538260 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_12
1884: I0815 04:27:19.538270 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_13
1884: I0815 04:27:19.538278 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_14
1884: I0815 04:27:19.538287 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_15
1884: I0815 04:27:19.538295 17479 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:27:19.538323 17479 scope.cc:202] Create variable 0x64703c101723696039538127301_inner_var_17
1884: I0815 04:27:19.538332 17479 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x636ba4d0
1884: 1 -> 0x64703c101723696039538127301_inner_var_1 -> 0x63a8bb40
1884: 2 -> linear_1.b_0 -> 0x63960640
1884: 3 -> linear_1.w_0 -> 0x64702b80
1884: 4 -> 0x64703c101723696039538127301_inner_var_4 -> 0x646f83b0
1884: 5 -> 0x64703c101723696039538127301_inner_var_5 -> 0x635e0870
1884: 6 -> 0x64703c101723696039538127301_inner_var_6 -> 0x645cc1d0
1884: 7 -> 0x64703c101723696039538127301_inner_var_7 -> 0x45d4cea0
1884: 8 -> 0x64703c101723696039538127301_inner_var_8 -> 0x6241290
1884: 9 -> 0x64703c101723696039538127301_inner_var_9 -> 0x63eb6490
1884: 10 -> 0x64703c101723696039538127301_inner_var_10 -> 0x63da5770
1884: 11 -> 0x64703c101723696039538127301_inner_var_11 -> 0x43ffcfc0
1884: 12 -> 0x64703c101723696039538127301_inner_var_12 -> 0x61f64a20
1884: 13 -> 0x64703c101723696039538127301_inner_var_13 -> 0x694ad90
1884: 14 -> 0x64703c101723696039538127301_inner_var_14 -> 0x61f781b0
1884: 15 -> 0x64703c101723696039538127301_inner_var_15 -> 0x63da5790
1884: 16 -> fetch0@fetch -> 0x636c1c20
1884: 17 -> 0x64703c101723696039538127301_inner_var_17 -> 0x6214d7d0
1884: 18 -> fetch1@fetch -> 0x648bbbf0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:27:19.540021 17558 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.540148 17559 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.540153 17560 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.540221 17561 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.540242 17560 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540249 17559 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540306 17562 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:27:19.540305 17559 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.540297 17560 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:27:19.540341 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540374 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:27:19.540397 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540434 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.540464 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:27:19.540480 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:27:19.540534 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:27:19.540550 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x64703c101723696039538127301_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540588 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x64703c101723696039538127301_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:27:19.540614 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x64703c101723696039538127301_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540652 17562 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:27:19.540683 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x64703c101723696039538127301_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.540711 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x64703c101723696039538127301_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x64703c101723696039538127301_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540752 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.540768 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x64703c101723696039538127301_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x64703c101723696039538127301_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.540799 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x64703c101723696039538127301_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540822 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.540822 17560 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64703c101723696039538127301_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540832 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x64703c101723696039538127301_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.540848 17560 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.540848 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x64703c101723696039538127301_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540874 17562 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.540915 17560 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64703c101723696039538127301_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.540946 17560 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x64703c101723696039538127301_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.540956 17562 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.540967 17560 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.540982 17560 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x64703c101723696039538127301_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.540995 17562 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.541054 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.541070 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64703c101723696039538127301_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x64703c101723696039538127301_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.541103 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x64703c101723696039538127301_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.541110 17560 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64703c101723696039538127301_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.541126 17560 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:27:19.541132 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.541146 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x64703c101723696039538127301_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.541162 17560 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64703c101723696039538127301_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.541164 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x64703c101723696039538127301_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.541188 17560 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x64703c101723696039538127301_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.541203 17560 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.541218 17560 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x64703c101723696039538127301_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.541245 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x64703c101723696039538127301_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:27:19.541265 17562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x64703c101723696039538127301_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64703c101723696039538127301_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.541292 17562 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:27:19.541312 17562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x64703c101723696039538127301_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64703c101723696039538127301_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x64703c101723696039538127301_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:27:19.541350 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x64703d80) got event_name: TaskCompletion
1884: I0815 04:27:19.541375 17479 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.541410 17479 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:27:19.547049 17479 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:27:19.547096 17479 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:27:19.547775 17479 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:27:19.547822 17479 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:27:19.548269 17479 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395483259980"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395483259980"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:27:19.548458 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64665520 on Place(cpu)
1884: I0815 04:27:19.548478 17479 scope.cc:202] Create variable 0x646655201723696039548472006_inner_var_0
1884: I0815 04:27:19.548503 17479 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395483259980"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236960395483259980 -> 0x63942790
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:27:19.548633 17479 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:27:19.548750 17563 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.548921 17564 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.548930 17565 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.549008 17566 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.549005 17564 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236960395483259980:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.549047 17564 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236960395483259980:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:27:19.549052 17567 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.549074 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x64665690) got event_name: TaskCompletion
1884: I0815 04:27:19.549340 17564 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17859849439902291264 to 531030038121051903 , after update, data is {current : 20, peak : 20}.
1884: I0815 04:27:19.549350 17564 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17859849439902291264 to 531030038121051903 , after update, data is {current : 20, peak : 20}.
1884: I0815 04:27:19.549407 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.549414 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395494807651"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395494807651"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:27:19.549613 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64665520 on Place(cpu)
1884: I0815 04:27:19.549633 17479 scope.cc:202] Create variable 0x646655201723696039549626507_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395494807651"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236960395494807651 -> 0x61f73c80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:27:19.549840 17568 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.549904 17569 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.549938 17570 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.549960 17571 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.549984 17572 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.549983 17570 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236960395494807651:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.550050 17570 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236960395494807651:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.550077 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x64665690) got event_name: TaskCompletion
1884: I0815 04:27:19.550269 17570 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 9111186282280381049 to 531030038121051903 , after update, data is {current : 28, peak : 28}.
1884: I0815 04:27:19.550278 17570 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 9111186282280381049 to 531030038121051903 , after update, data is {current : 28, peak : 28}.
1884: I0815 04:27:19.550388 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.550396 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236960395494807651",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236960395504748752"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236960395494807651",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236960395504748752"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:27:19.550621 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64665520 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236960395494807651",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236960395504748752"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236960395504748752 -> 0x61f73c80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:27:19.550881 17573 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.550940 17574 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.550959 17575 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.550994 17576 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.551023 17577 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.551014 17576 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.551043 17576 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.551069 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x64665690) got event_name: TaskCompletion
1884: I0815 04:27:19.551363 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.551371 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395514488333"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395514488333"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:27:19.551576 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64665520 on Place(cpu)
1884: I0815 04:27:19.551594 17479 scope.cc:202] Create variable 0x646655201723696039551589110_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236960395514488333"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236960395514488333 -> 0x63d83710
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:27:19.551795 17578 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.551832 17579 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:27:19.551863 17580 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:27:19.551891 17581 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:27:19.551923 17582 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:27:19.551918 17581 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236960395514488333:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.551955 17581 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236960395514488333:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:27:19.551981 17479 pir_interpreter.cc:1766] main_thread_blocker_(0x64665690) got event_name: TaskCompletion
1884: I0815 04:27:19.552157 17581 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13634841278306427929 to 531030038121051903 , after update, data is {current : 32, peak : 32}.
1884: I0815 04:27:19.552166 17581 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13634841278306427929 to 531030038121051903 , after update, data is {current : 32, peak : 32}.
1884: I0815 04:27:19.552263 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.552270 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.552341 17479 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:27:19.552397 17479 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:27:19.552433 17479 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395514488333"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395504748752"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395514488333"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395504748752"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:27:19.553088 17479 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:27:19.553107 17479 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:27:19.553138 17479 pir_interpreter.cc:161] PirInterpreter(): 0x64665520 on Place(cpu)
1884: I0815 04:27:19.553169 17479 scope.cc:202] Create variable feed_name_0
1884: I0815 04:27:19.553184 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_5
1884: I0815 04:27:19.553205 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_6
1884: I0815 04:27:19.553217 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_7
1884: I0815 04:27:19.553226 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_8
1884: I0815 04:27:19.553246 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_9
1884: I0815 04:27:19.553257 17479 scope.cc:202] Create variable 0x646655201723696039553153457_inner_var_10
1884: I0815 04:27:19.553280 17479 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.553309 17479 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 04:27:19.553433 17479 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.553448 17479 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395514488333"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236960395504748752"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236960395514488333 -> 0x63d83710
1884: 1 -> constant_folding@_17236960395504748752 -> 0x61f73c80
1884: 2 -> linear_1.b_0 -> 0x639d8590
1884: 3 -> linear_1.w_0 -> 0x6454d040
1884: 4 -> feed_name_0 -> 0x64663a30
1884: 5 -> 0x646655201723696039553153457_inner_var_5 -> 0x636d1530
1884: 6 -> 0x646655201723696039553153457_inner_var_6 -> 0x636b9b50
1884: 7 -> 0x646655201723696039553153457_inner_var_7 -> 0x61f768d0
1884: 8 -> 0x646655201723696039553153457_inner_var_8 -> 0x63eb9cc0
1884: 9 -> fetch_name_0 -> 0x636a4810
1884: 10 -> fetch_name_1 -> 0x646f0b30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:27:19.553995 17479 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:27:19.554064 17583 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:27:19.554057 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554111 17479 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:27:19.554131 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.554163 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x646655201723696039553153457_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x646655201723696039553153457_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554203 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x646655201723696039553153457_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.554236 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554257 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.554272 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554312 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236960395504748752:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.554338 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236960395514488333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554368 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236960395514488333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x646655201723696039553153457_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:27:19.554394 17479 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236960395514488333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:27:19.554423 17479 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236960395514488333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x646655201723696039553153457_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:27:19.554452 17479 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:27:19.554473 17479 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 04:27:19.554495 17479 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:27:19.554612 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.554620 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.554670 17583 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 17859849439902291264 to 531030038121051903 , after update, data is {current : -160, peak : 32}.
1884: I0815 04:27:19.554679 17583 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 17859849439902291264 to 531030038121051903 , after update, data is {current : -160, peak : 32}.
1884: I0815 04:27:19.554723 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.554731 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.557083 17479 mmap_allocator.cc:348] PID: 17479, MemoryMapFdSet: set size - 0
1884: I0815 04:27:19.569949 17479 mmap_allocator.cc:348] PID: 17479, MemoryMapFdSet: set size - 0
1884: I0815 04:27:19.672860 17555 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 4193809585671281229 to 531030038121051903 , after update, data is {current : -144, peak : 32}.
1884: I0815 04:27:19.672922 17555 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 4193809585671281229 to 531030038121051903 , after update, data is {current : -144, peak : 32}.
1884: I0815 04:27:19.672930 17556 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1416695420805570877 to 531030038121051903 , after update, data is {current : -140, peak : 32}.
1884: I0815 04:27:19.672966 17554 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13180696948244695195 to 531030038121051903 , after update, data is {current : -136, peak : 32}.
1884: I0815 04:27:19.672971 17556 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1416695420805570877 to 531030038121051903 , after update, data is {current : -140, peak : 32}.
1884: I0815 04:27:19.672994 17554 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13180696948244695195 to 531030038121051903 , after update, data is {current : -136, peak : 32}.
1884: I0815 04:27:19.673492 17557 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 7225759158082735973 to 531030038121051903 , after update, data is {current : -160, peak : 32}.
1884: I0815 04:27:19.673511 17557 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 7225759158082735973 to 531030038121051903 , after update, data is {current : -160, peak : 32}.
1884: I0815 04:27:19.673524 17557 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 7225759158082735973 to 12577647897817424502 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:27:19.674474 17559 thread_data_registry.h:135] Add data {current : -160, peak : 32} from thread 531030038121051903 to 12577647897817424502 , after update, data is {current : 48, peak : 280}.
1884: I0815 04:27:19.674489 17559 thread_data_registry.h:135] Add data {current : -160, peak : 32} from thread 531030038121051903 to 12577647897817424502 , after update, data is {current : 48, peak : 280}.
1884: I0815 04:27:19.674510 17560 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 12577647897817424502 to 18388137733616234996 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:27:19.674518 17560 thread_data_registry.h:135] Add data {current : 48, peak : 280} from thread 12577647897817424502 to 18388137733616234996 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:27:19.674523 17560 thread_data_registry.h:135] Add data {current : 48, peak : 280} from thread 12577647897817424502 to 18388137733616234996 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:27:19.675091 17562 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 18388137733616234996 to 15487614165098538368 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 04:27:19.675100 17562 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 18388137733616234996 to 15487614165098538368 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 04:27:19.675105 17562 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 18388137733616234996 to 15487614165098538368 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:27:19.841940 17479 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:27:19.841970 17479 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:27:19.842021 17479 mmap_allocator.cc:348] PID: 17479, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   12.82 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.99 sec
