UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:00:27.662060  6751 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:00:28.464519  6751 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=auto_free_cudagraph_allocations_on_launch,prim_forward_blacklist,enable_cublas_tensor_op_math,use_xqa_optim,dygraph_debug,free_idle_chunk,use_autotune,check_kernel_launch,allocator_strategy,cusparselt_dir,reader_queue_speed_test_mode,cudnn_exhaustive_search,cusparse_dir,curand_dir,nccl_dir,query_dest_rank_by_multi_node,call_stack_level,eager_delete_scope,enable_neighbor_list_use_uva,allow_cinn_ops,use_cinn,reallocate_gpu_memory_in_mb,dump_chunk_info,einsum_opt,cusolver_dir,all_blocks_convert_trt,initial_gpu_memory_in_mb,prim_all,new_executor_serial_run,paddle_num_threads,enable_api_kernel_fallback,dynamic_static_unified_comm,check_nan_inf_level,enable_dependency_builder_debug_info,benchmark_nccl,pir_debug,run_kp_kernel,ir_inplace_kernel_blacklist,conv2d_disable_cudnn,use_mkldnn,enable_fuse_parallel_matmul_pass,allreduce_record_one_event,fleet_executor_with_standalone,init_allocated_mem,low_precision_op_list,mkl_dir,cinn_subgraph_graphviz_dir,gpugraph_dedup_pull_push_mode,enable_gpu_memory_usage_log,enable_blaslt_global_search,sync_nccl_allreduce,gpugraph_offload_gather_copy_maxsize,fraction_of_gpu_memory_to_use,gpugraph_enable_segment_merge_grads,max_inplace_grad_add,use_auto_growth_v2,pir_apply_inplace_pass,accuracy_check_rtol_bf16,selected_gpus,multi_node_sample_use_gpu_table,cache_inference_while_scope,graph_embedding_split_infer_mode,prim_skip_dynamic,free_when_no_cache_hit,use_stride_kernel,check_infer_symbolic,use_shm_cache,logging_pir_py_code_int_tensor_element_limit,enable_auto_rdma_trans,gemm_use_half_precision_compute_type,npu_storage_format,cupti_dir,fraction_of_cuda_pinned_memory_to_use,enable_cse_in_dy2st,local_exe_sub_scope_limit,accuracy_check_atol_fp32,gpugraph_sparse_table_storage_mode,cudnn_batchnorm_spatial_persistent,gpugraph_parallel_stream_num,tensorrt_dir,gpugraph_force_device_batch_num_equal,gpugraph_storage_mode,cse_max_count,executor_log_deps_every_microseconds,convert_all_blocks,logging_pir_py_code_dir,enable_unused_var_check,print_sub_graph_dir,enable_tracker_all2all,accuracy_check_rtol_fp32,set_to_1d,conv_workspace_size_limit,eager_delete_tensor_gb,graph_get_neighbor_id,cudnn_exhaustive_search_times,enable_graph_multi_node_sampling,gpu_memory_limit_mb,gpu_allocator_retry_time,gpugraph_slot_feasign_max_num,new_executor_static_build,gpugraph_parallel_copyer_split_maxsize,async_trace_count,enable_all2all_use_fp16,prim_enable_dynamic,cinn_compile_thread_num,graph_neighbor_size_percent,cublaslt_device_best_config,gpugraph_enable_hbm_table_collision_stat,enable_pir_in_executor_trace_run,new_executor_use_cuda_graph,fraction_of_cpu_memory_to_use,mklml_dir,gpugraph_enable_print_op_debug,trt_ibuilder_cache,cuda_memory_async_pool_realease_threshold,cuda_malloc_async_pool_memory_throttle_ratio,cudnn_dir,fuse_parameter_groups_size,logging_trunc_pir_py_code,prim_forward,enable_pir_in_executor,custom_device_mem_record,pinned_memory_as_cpu_backend,enable_pir_api,cublas_dir,logging_pir_py_code_dump_symbolic_dims,deny_cinn_ops,enable_fusion_fallback,accuracy_check_atol_fp16,inner_op_parallelism,static_executor_perfstat_filepath,enable_cinn_compile_cache,jit_engine_type,cuda_dir,print_allocator_trace_info,embedding_deterministic,pir_apply_shape_optimization_pass,lapack_dir,log_memory_stats,new_executor_sequential_run,win_cuda_bin_dir,memory_fraction_of_eager_deletion,use_pinned_memory,enable_pir_with_pt_in_dy2st,enable_cinn_accuracy_check,dataloader_use_file_descriptor,auto_growth_chunk_size_in_mb,get_host_by_name_time,prim_check_ops,enable_exit_when_partial_worker,pir_subgraph_saving_dir,fast_eager_deletion_mode,gpugraph_enable_gpu_direct_access,cublaslt_exhaustive_search_times,cudnn_deterministic,accuracy_check_atol_bf16,gpugraph_offload_param_stat,enable_async_trace,initial_cpu_memory_in_mb,enable_gpu_memory_usage_log_mb,save_static_runtime_data,enable_adjust_op_order,graph_load_in_parallel,enable_collect_shape,tracer_profile_fname,gpugraph_debug_gpu_memory,fuse_parameter_memory_size,sort_sum_gradient,graph_metapath_split_opt,apply_pass_to_program,accuracy_check_rtol_fp16,alloc_fill_value,prim_enabled,gpugraph_offload_param_extends,use_cuda_malloc_async_allocator,new_executor_use_local_scope,sync_after_alloc,disable_dyshape_in_train,nccl_blocking_wait,search_cache_max_number,static_runtime_data_save_path,new_executor_use_inplace,use_cuda_managed_memory,host_trace_level,enable_sparse_inner_gather,use_system_allocator,enable_record_memory,dist_threadpool_size,tensor_operants_mode,use_stream_safe_cuda_allocator,enable_auto_detect_gpu_topo,prim_backward,gpugraph_merge_grads_segment_size,enable_interpretercore_launch_cinn,gpugraph_hbm_table_load_factor,gpugraph_load_node_list_into_hbm,use_virtual_memory_auto_growth,enable_opt_get_features,add_dependency_for_communication_op,manually_trans_conv_filter,tracer_onednn_ops_off,multiple_of_cupti_buffer_size,tracer_onednn_ops_on,benchmark,check_nan_inf,use_auto_growth_pinned_allocator,use_fast_math,nvidia_package_dir,enable_dump_main_program,enable_cinn_auto_tune,pir_broadcast_tree_limit,op_dir,print_ir 
1884: I0815 04:00:28.464632  6751 init.cc:108] After Parse: argc is 2
1884: I0815 04:00:36.907361  6751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:00:36.907421  6751 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:00:36.908180  6751 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:00:36.908803  6751 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:00:36.909649  6751 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:00:36.909760  6751 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:00:36.909857  6751 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:00:36.910616  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f312f600000), and remaining 0
1884: I0815 04:00:36.910965  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:36.911038  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:36.911131  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f312f600200), and remaining 0
1884: I0815 04:00:36.911165  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f312f600400), and remaining 0
1884: I0815 04:00:36.915251  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f312f600600), and remaining 0
1884: I0815 04:00:36.915429  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f312f600800), and remaining 0
1884: I0815 04:00:36.915505  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f312f600a00), and remaining 0
1884: I0815 04:00:36.915627  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:36.915653  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:36.915742  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:36.915758  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:36.917268  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aef6210 for it.
1884: I0815 04:00:36.917444  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:36.917474  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:36.917536  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f312f600e00), and remaining 0
1884: I0815 04:00:36.917629  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f312f6c4400), and remaining 0
1884: I0815 04:00:37.056533  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aef6210 for it.
1884: I0815 04:00:37.056771  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:37.056825  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.057552  6751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f312f800000), and remaining 0
1884: I0815 04:00:37.066785  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aef6210 for it.
1884: I0815 04:00:37.066915  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:37.066955  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.067003  6751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:37.067211  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:37.068228  6751 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:00:37.068248  6751 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:00:37.068313  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:37.068414  6751 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:00:37.068444  6751 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.068512  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:37.068614  6751 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:00:37.068636  6751 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.068678  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:37.068903  6751 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:00:37.068925  6751 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.069121  6751 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:00:37.069150  6751 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:37.069234  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:37.073179  6751 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:00:37.073311  6751 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:00:37.073343  6751 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:00:37.073418  6751 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:00:38.556599  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.556689  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.557098  6751 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:00:38.557121  6751 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.563287  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.563354  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.564530  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.564550  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.564566  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.565428  6751 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:00:38.565445  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.565462  6751 scope.cc:202] Create variable feed
1884: I0815 04:00:38.565471  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.565482  6751 scope.cc:202] Create variable fetch
1884: I0815 04:00:38.565485  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.565500  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.565503  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.565507  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.565510  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.568121  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.568518  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.568533  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.568538  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.570413  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.570474  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.570484  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.570494  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.570508  6751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:00:38.570521  6751 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x618ff240 type is 7
1884: I0815 04:00:38.570528  6751 scope.cc:202] Create variable x
1884: I0815 04:00:38.570538  6751 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x618ff390 type is 7
1884: I0815 04:00:38.570605  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.570613  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.570617  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.570621  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.570763  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.570791  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.570927  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.570940  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.570964  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.571173  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.571206  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.571231  6751 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.571239  6751 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x619073e0Variable Type 7
1884: I0815 04:00:38.571266  6751 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.571295  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.571362  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.571385  6751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.574457  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.574523  6751 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.575021  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.581799  6751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:00:38.581825  6751 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:00:38.581960  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.582001  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.582571  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: I0815 04:00:38.582674  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.582700  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.583169  6751 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: I0815 04:00:38.583240  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.583266  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.583297  6751 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.583608  6751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:00:38.583621  6751 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:00:38.583755  6751 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:00:38.583782  6751 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.584194  6751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:00:38.584208  6751 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:00:38.584259  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.584281  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.584486  6751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:00:38.584497  6751 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:00:38.584543  6751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:00:38.584564  6751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:00:38.584586  6751 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.587647  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.587677  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.587745  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.587756  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.590000  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.590416  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.590435  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.590441  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.592432  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.592500  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.592514  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.592527  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61932890 type is 7
1884: I0815 04:00:38.592537  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.592550  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61932c00 type is 7
1884: I0815 04:00:38.592557  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.592571  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.592634  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.592643  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.592648  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.592653  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.592716  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.592741  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.592821  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.592834  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.592857  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.593199  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.593218  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.593241  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.593251  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61939360Variable Type 7
1884: I0815 04:00:38.593273  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.593297  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.593339  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.593362  6751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.594122  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.594157  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.594381  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.606160  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: I0815 04:00:38.606467  6751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:00:38.613528  6751 pir_interpreter.cc:161] PirInterpreter(): 0x61af53c0 on Place(gpu:0)
1884: I0815 04:00:38.613581  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.613619  6751 scope.cc:202] Create variable 0x61af53c01723694438613562461_inner_var_1
1884: I0815 04:00:38.613634  6751 scope.cc:202] Create variable 0x61af53c01723694438613562461_inner_var_2
1884: I0815 04:00:38.613648  6751 scope.cc:202] Create variable 0x61af53c01723694438613562461_inner_var_3
1884: I0815 04:00:38.613665  6751 scope.cc:202] Create variable 0x61af53c01723694438613562461_inner_var_4
1884: I0815 04:00:38.613679  6751 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:00:38.614195  6751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:00:38.614212  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.614220  6751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:00:38.614277  6751 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61af45d0
1884: 1 -> 0x61af53c01723694438613562461_inner_var_1 -> 0x61af47d0
1884: 2 -> 0x61af53c01723694438613562461_inner_var_2 -> 0x61af5c50
1884: 3 -> 0x61af53c01723694438613562461_inner_var_3 -> 0x61af53a0
1884: 4 -> 0x61af53c01723694438613562461_inner_var_4 -> 0x61af6000
1884: 5 -> fetch0@fetch -> 0x61af6810
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:00:38.615141  6751 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:00:38.615447  6789 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:38.615669  6790 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:38.615676  6791 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:38.615799  6792 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:38.615809  6793 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:38.615855  6790 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61af53c01723694438613562461_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.615924  6794 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:38.615983  6794 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.616014  6790 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61af53c01723694438613562461_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:38.616043  6794 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:00:38.616101  6794 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61af53c01723694438613562461_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61af53c01723694438613562461_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.616392  6794 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61af53c01723694438613562461_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61af53c01723694438613562461_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:00:38.616472  6790 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61af53c01723694438613562461_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.616499  6790 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.617740  6790 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61af53c01723694438613562461_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61af53c01723694438613562461_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:00:38.617787  6790 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61af53c01723694438613562461_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.617817  6790 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.618408  6790 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61af53c01723694438613562461_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:00:38.618458  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x61af5530) got event_name: TaskCompletion
1884: I0815 04:00:38.618500  6751 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.696703  6789 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 15755936466032542851 to 7384116453199295403 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:00:38.696733  6789 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 15755936466032542851 to 2377958819657618922 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:00:38.696740  6789 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 15755936466032542851 to 2377958819657618922 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:00:38.697018  6790 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2377958819657618922 to 11392407368413149006 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:00:38.697037  6790 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2377958819657618922 to 11392407368413149006 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:00:38.697201  6794 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 7384116453199295403 to 11392407368413149006 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:00:38.697213  6794 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 7384116453199295403 to 11392407368413149006 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:00:38.703575  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.703610  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.703681  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.703691  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.705627  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.706014  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.706030  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.706035  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.707719  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.707842  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.707855  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.707863  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x570c9d0 type is 7
1884: I0815 04:00:38.707872  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.707886  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x56d5f00 type is 7
1884: I0815 04:00:38.707891  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.707897  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.707957  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.707964  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.707968  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.707973  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.708034  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.708053  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.708130  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.708141  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.708159  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.708360  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.708374  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.708393  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.708401  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61902640Variable Type 7
1884: I0815 04:00:38.708421  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.708442  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.708468  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.708487  6751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.710196  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.710243  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.710495  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.717602  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: I0815 04:00:38.717823  6751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:00:38.721227  6751 pir_interpreter.cc:161] PirInterpreter(): 0x63f8b8b0 on Place(gpu:0)
1884: I0815 04:00:38.721269  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.721297  6751 scope.cc:202] Create variable 0x63f8b8b01723694438721257707_inner_var_1
1884: I0815 04:00:38.721321  6751 scope.cc:202] Create variable 0x63f8b8b01723694438721257707_inner_var_2
1884: I0815 04:00:38.721343  6751 scope.cc:202] Create variable 0x63f8b8b01723694438721257707_inner_var_3
1884: I0815 04:00:38.721357  6751 scope.cc:202] Create variable 0x63f8b8b01723694438721257707_inner_var_4
1884: I0815 04:00:38.721372  6751 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:00:38.721769  6751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:00:38.721786  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.721794  6751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61af4150
1884: 1 -> 0x63f8b8b01723694438721257707_inner_var_1 -> 0x2fb1a50
1884: 2 -> 0x63f8b8b01723694438721257707_inner_var_2 -> 0x619348c0
1884: 3 -> 0x63f8b8b01723694438721257707_inner_var_3 -> 0x618efb10
1884: 4 -> 0x63f8b8b01723694438721257707_inner_var_4 -> 0x36a0530
1884: 5 -> fetch0@fetch -> 0x56f6850
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:00:38.722604  6795 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:38.722684  6796 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:38.722708  6797 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:38.722738  6798 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:38.722774  6799 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:38.722820  6800 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:38.722823  6799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63f8b8b01723694438721257707_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.722852  6800 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.722906  6800 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:00:38.722913  6799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63f8b8b01723694438721257707_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:38.722966  6800 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63f8b8b01723694438721257707_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63f8b8b01723694438721257707_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.723112  6800 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63f8b8b01723694438721257707_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63f8b8b01723694438721257707_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:00:38.723178  6799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63f8b8b01723694438721257707_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.723205  6799 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.725903  6799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63f8b8b01723694438721257707_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63f8b8b01723694438721257707_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:00:38.725962  6799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63f8b8b01723694438721257707_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.725989  6799 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.728000  6799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63f8b8b01723694438721257707_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:00:38.728061  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x63f8ba20) got event_name: TaskCompletion
1884: I0815 04:00:38.728098  6751 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.768085  6795 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 7384116453199295403 to 2615954813785320905 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:00:38.768114  6795 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 7384116453199295403 to 13556316358673641595 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:00:38.768122  6795 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 7384116453199295403 to 13556316358673641595 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:00:38.768401  6799 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13556316358673641595 to 11392407368413149006 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:00:38.768426  6799 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13556316358673641595 to 11392407368413149006 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:00:38.768555  6800 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 2615954813785320905 to 11392407368413149006 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:00:38.772872  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.772908  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.772976  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.772985  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.774894  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.775277  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.775293  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.775306  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.776962  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.777094  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.777108  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.777122  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61af7c80 type is 7
1884: I0815 04:00:38.777132  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.777140  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x56dbcf0 type is 7
1884: I0815 04:00:38.777146  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.777151  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.777213  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.777221  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.777225  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.777230  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.777287  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.777314  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.777388  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.777400  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.777417  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.777477  6751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.777652  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:38.777729  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.777741  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.777760  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.777767  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x56eea30Variable Type 7
1884: I0815 04:00:38.777786  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.777807  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.777835  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.777854  6751 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.778075  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.778101  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.778323  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.779196  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: I0815 04:00:38.779421  6751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:00:38.782770  6751 pir_interpreter.cc:161] PirInterpreter(): 0x618e4140 on Place(gpu:0)
1884: I0815 04:00:38.782809  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.782836  6751 scope.cc:202] Create variable 0x618e41401723694438782798550_inner_var_1
1884: I0815 04:00:38.782850  6751 scope.cc:202] Create variable 0x618e41401723694438782798550_inner_var_2
1884: I0815 04:00:38.782867  6751 scope.cc:202] Create variable 0x618e41401723694438782798550_inner_var_3
1884: I0815 04:00:38.782883  6751 scope.cc:202] Create variable 0x618e41401723694438782798550_inner_var_4
1884: I0815 04:00:38.782897  6751 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:00:38.783281  6751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:00:38.783298  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.783314  6751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x618d7ba0
1884: 1 -> 0x618e41401723694438782798550_inner_var_1 -> 0x618e1630
1884: 2 -> 0x618e41401723694438782798550_inner_var_2 -> 0x640333a0
1884: 3 -> 0x618e41401723694438782798550_inner_var_3 -> 0x56c20d0
1884: 4 -> 0x618e41401723694438782798550_inner_var_4 -> 0x61af4ae0
1884: 5 -> fetch0@fetch -> 0x1af53e10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:00:38.784085  6801 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:38.784178  6802 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:38.784183  6803 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:38.784216  6804 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:38.784252  6805 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:38.784291  6806 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:38.784276  6805 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x618e41401723694438782798550_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.784322  6806 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.784351  6805 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x618e41401723694438782798550_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:38.784360  6806 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:00:38.784401  6806 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x618e41401723694438782798550_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x618e41401723694438782798550_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.784452  6806 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.784585  6806 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:38.784615  6806 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x618e41401723694438782798550_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x618e41401723694438782798550_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:00:38.784672  6805 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x618e41401723694438782798550_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.784694  6805 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.784952  6805 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x618e41401723694438782798550_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x618e41401723694438782798550_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:00:38.784977  6805 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x618e41401723694438782798550_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.784996  6805 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.785008  6805 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x618e41401723694438782798550_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:00:38.785039  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x618e42b0) got event_name: TaskCompletion
1884: I0815 04:00:38.785068  6751 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.823161  6801 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 2615954813785320905 to 7384116453199295403 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:00:38.823191  6801 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2615954813785320905 to 7384116453199295403 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:00:38.823197  6801 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2615954813785320905 to 7384116453199295403 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:00:38.823478  6805 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 14021460290891867851 to 7384116453199295403 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:00:38.823500  6805 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 14021460290891867851 to 7384116453199295403 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:00:38.823619  6806 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 7384116453199295403 to 11392407368413149006 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:00:38.823634  6806 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 7384116453199295403 to 11392407368413149006 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:00:38.823639  6806 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 7384116453199295403 to 11392407368413149006 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:00:38.828243  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.828270  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.828346  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.828356  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.830338  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.830720  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.830736  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.830742  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.832430  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.832553  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.832566  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.832581  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x1af4cae0 type is 7
1884: I0815 04:00:38.832592  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.832595  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x570be00 type is 7
1884: I0815 04:00:38.832600  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.832617  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.832679  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.832687  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.832691  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.832696  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.832758  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.832777  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.832855  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.832865  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.832883  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.833182  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.833197  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.833215  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.833223  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x570daf0Variable Type 7
1884: I0815 04:00:38.833242  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.833264  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.833292  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.833318  6751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.834041  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.834079  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.834319  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.837364  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: I0815 04:00:38.878043  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.878082  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.878149  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.878157  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.880110  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.880499  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.880515  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.880522  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.882194  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.882328  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.882341  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.882359  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x618ea6f0 type is 7
1884: I0815 04:00:38.882375  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.882386  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61917220 type is 7
1884: I0815 04:00:38.882391  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.882397  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.882457  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.882465  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.882469  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.882473  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.882532  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.882551  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.882627  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.882637  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.882654  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.882833  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.882845  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.882866  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.882874  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6292e670Variable Type 7
1884: I0815 04:00:38.882892  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.882915  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.882941  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.882959  6751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.884630  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.884675  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.884912  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.890200  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: I0815 04:00:38.929453  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.929489  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.929556  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.929566  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.931448  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.931820  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.931835  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.931841  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.933495  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.933615  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.933629  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.933638  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63743650 type is 7
1884: I0815 04:00:38.933650  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.933655  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x642a89b0 type is 7
1884: I0815 04:00:38.933660  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.933667  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.933724  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.933732  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.933737  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.933741  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.933799  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.933818  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.933890  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.933902  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.933919  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.933971  6751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.934119  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:38.934182  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.934195  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.934213  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.934221  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x627e9a20Variable Type 7
1884: I0815 04:00:38.934240  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.934262  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.934289  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.934321  6751 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.934389  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.934417  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.934634  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.935509  6751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1af12200 for it.
1884: I0815 04:00:38.935755  6751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1afd99d0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:00:38.939079  6751 pir_interpreter.cc:161] PirInterpreter(): 0x61900f50 on Place(gpu:0)
1884: I0815 04:00:38.939119  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.939146  6751 scope.cc:202] Create variable 0x61900f501723694438939109675_inner_var_1
1884: I0815 04:00:38.939162  6751 scope.cc:202] Create variable 0x61900f501723694438939109675_inner_var_2
1884: I0815 04:00:38.939177  6751 scope.cc:202] Create variable 0x61900f501723694438939109675_inner_var_3
1884: I0815 04:00:38.939193  6751 scope.cc:202] Create variable 0x61900f501723694438939109675_inner_var_4
1884: I0815 04:00:38.939208  6751 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:00:38.939631  6751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:00:38.939651  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.939654  6751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x642b23d0
1884: 1 -> 0x61900f501723694438939109675_inner_var_1 -> 0x642b2450
1884: 2 -> 0x61900f501723694438939109675_inner_var_2 -> 0x642af730
1884: 3 -> 0x61900f501723694438939109675_inner_var_3 -> 0x63735490
1884: 4 -> 0x61900f501723694438939109675_inner_var_4 -> 0x64229c00
1884: 5 -> fetch0@fetch -> 0x61903a70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:00:38.940433  6807 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:38.940508  6808 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:38.940526  6809 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:38.940557  6810 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:38.940583  6811 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:38.940626  6812 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:38.940623  6811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61900f501723694438939109675_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.940652  6812 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.940691  6811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61900f501723694438939109675_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:38.940696  6812 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:00:38.940728  6812 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61900f501723694438939109675_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61900f501723694438939109675_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.940774  6812 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.940898  6812 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:38.940927  6812 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61900f501723694438939109675_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61900f501723694438939109675_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:00:38.940991  6811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61900f501723694438939109675_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.941013  6811 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.941156  6811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61900f501723694438939109675_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61900f501723694438939109675_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:00:38.941182  6811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61900f501723694438939109675_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:38.941198  6811 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.941212  6811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61900f501723694438939109675_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:00:38.941243  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x619010c0) got event_name: TaskCompletion
1884: I0815 04:00:38.941272  6751 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:00:38.942907  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.942934  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.942999  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:38.943011  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.945055  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:38.945514  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.945533  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.945539  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.947589  6751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:00:38.947696  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:38.947711  6751 scope.cc:202] Create variable Out
1884: I0815 04:00:38.947726  6751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x642a9d70 type is 7
1884: I0815 04:00:38.947742  6751 scope.cc:202] Create variable X
1884: I0815 04:00:38.947753  6751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x618d59e0 type is 7
1884: I0815 04:00:38.947760  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:38.947767  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:38.947844  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:38.947852  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:38.947857  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:38.947862  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:38.947923  6751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.947947  6751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.948026  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.948043  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.948067  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:38.948117  6751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.948243  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:38.948314  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.948330  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:38.948351  6751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:38.948361  6751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6375ae40Variable Type 7
1884: I0815 04:00:38.948383  6751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:38.948408  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.948438  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:38.948458  6751 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:38.948510  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:38.948541  6751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:38.948782  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:38.985958  6807 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 7384116453199295403 to 2615954813785320905 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:00:38.985983  6807 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 7384116453199295403 to 2615954813785320905 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:00:38.985989  6807 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 7384116453199295403 to 2615954813785320905 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:00:38.986297  6811 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13556316358673641595 to 2615954813785320905 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:00:38.986332  6811 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13556316358673641595 to 2615954813785320905 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:00:38.986450  6812 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 2615954813785320905 to 11392407368413149006 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:00:38.986464  6812 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 2615954813785320905 to 11392407368413149006 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:00:38.986469  6812 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 2615954813785320905 to 11392407368413149006 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:00:38.994012  6751 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:00:38.994074  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:00:38.995262  6751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:00:38.996168  6751 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:00:38.996201  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:00:38.997617  6751 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:00:38.997646  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:00:38.998394  6751 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:00:38.999460  6751 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:00:38.999490  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:00:39.000919  6751 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:00:39.000944  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:00:39.001588  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.001619  6751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:00:39.001627  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.001636  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:39.003798  6751 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:00:39.003826  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:00:39.004884  6751 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:00:39.004917  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:00:39.005970  6751 pybind.cc:1827] need skip: 0
1884: I0815 04:00:39.006310  6751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:00:39.008258  6751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:00:39.012485  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.012506  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.012511  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.014698  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:39.014720  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:39.014739  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:39.014745  6751 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:00:39.014772  6751 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x636ac4c0 type is 7
1884: I0815 04:00:39.014777  6751 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:00:39.014781  6751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x636ad770 type is 7
1884: I0815 04:00:39.014787  6751 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:00:39.014801  6751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x636ad7f0 type is 7
1884: I0815 04:00:39.014874  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:39.014882  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.014886  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.014890  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.014956  6751 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.014976  6751 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.015007  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:00:39.015164  6751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.015177  6751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.015249  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.015316  6751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.015327  6751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.015360  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.016472  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.017968  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:39.018457  6751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:00:39.018695  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.019012  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.019237  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.019258  6751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:00:39.019341  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.019351  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.019354  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.019461  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.019475  6751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:00:39.021173  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.022616  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.023823  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.024034  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:39.024050  6751 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:00:39.024067  6751 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x636d4740 type is 7
1884: I0815 04:00:39.024077  6751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:00:39.024087  6751 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x636d44c0 type is 7
1884: I0815 04:00:39.024092  6751 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:00:39.024096  6751 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x636d45b0 type is 7
1884: I0815 04:00:39.024102  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:39.024108  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:39.024114  6751 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:00:39.024117  6751 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x636d5b30 type is 7
1884: I0815 04:00:39.024123  6751 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x636ac4c0 type is 7
1884: I0815 04:00:39.024139  6751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x636ad770 type is 7
1884: I0815 04:00:39.024145  6751 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:00:39.024158  6751 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x636d5b10 type is 7
1884: I0815 04:00:39.024163  6751 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:00:39.024168  6751 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x636d6070 type is 7
1884: I0815 04:00:39.024173  6751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x636ad7f0 type is 7
1884: I0815 04:00:39.024178  6751 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:00:39.024183  6751 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x636d62e0 type is 7
1884: I0815 04:00:39.024187  6751 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:00:39.024201  6751 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x636d6520 type is 7
1884: I0815 04:00:39.024206  6751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:00:39.024211  6751 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x636d6780 type is 7
1884: I0815 04:00:39.024314  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.024333  6751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:00:39.024396  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:39.024405  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.024410  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.024413  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.024479  6751 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.024497  6751 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.024520  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:00:39.024681  6751 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.024693  6751 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.024718  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:00:39.024803  6751 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:00:39.024892  6751 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:00:39.026194  6751 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026217  6751 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026314  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.026391  6751 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026403  6751 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026419  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:00:39.026449  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.026499  6751 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026510  6751 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026525  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:00:39.026623  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026635  6751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026651  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:39.026788  6751 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.026887  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.026953  6751 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026965  6751 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.026981  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:00:39.027022  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.027086  6751 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027097  6751 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027114  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:00:39.027238  6751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027251  6751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027276  6751 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.027335  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.027347  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.027366  6751 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:39.027375  6751 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x640f2fe0Variable Type 7
1884: I0815 04:00:39.027395  6751 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:39.027416  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:39.027441  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027457  6751 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.027504  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:39.027532  6751 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:00:39.027565  6751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.027575  6751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.027590  6751 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:00:39.027598  6751 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x640f3450Variable Type 7
1884: I0815 04:00:39.027613  6751 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:00:39.027627  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:39.027647  6751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.027662  6751 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.027699  6751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:00:39.027715  6751 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:00:39.028173  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:00:39.028213  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:00:39.028234  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:00:39.028271  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:00:39.028326  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:39.028348  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:00:39.034464  6751 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:00:39.034510  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:00:39.035338  6751 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:00:39.035365  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:00:39.035773  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.037628  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.038492  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.038614  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.039137  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.040161  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.042490  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.043658  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.045738  6751 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:00:39.046666  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.046685  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.046691  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.048031  6751 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:00:39.048055  6751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x570d5a0 type is 9
1884: I0815 04:00:39.048075  6751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x56f5270 type is 10
1884: I0815 04:00:39.048082  6751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x636ad770 type is 7
1884: I0815 04:00:39.048094  6751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x636ad7f0 type is 7
1884: I0815 04:00:39.048101  6751 scope.cc:202] Create variable saved_params
1884: I0815 04:00:39.048105  6751 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x6423ae70 type is 17
1884: I0815 04:00:39.048138  6751 interpreter_util.cc:594] Static build: 0
1884: I0815 04:00:39.048146  6751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:00:39.048151  6751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:00:39.048153  6751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:00:39.048213  6751 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.048233  6751 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:00:39.049116  6751 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:00:39.049161  6751 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:00:39.049227  6751 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:00:39.050448  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.050511  6751 scope.cc:202] Create variable feed
1884: I0815 04:00:39.050521  6751 naive_executor.cc:189] 0x6423d940 Create persistable variable feed, which pointer is 0x6290c800
1884: I0815 04:00:39.050529  6751 scope.cc:202] Create variable fetch
1884: I0815 04:00:39.050546  6751 naive_executor.cc:189] 0x6423d940 Create persistable variable fetch, which pointer is 0x6290c6a0
1884: I0815 04:00:39.050551  6751 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:00:39.050555  6751 naive_executor.cc:189] 0x6423d940 Create persistable variable linear_0.b_0, which pointer is 0x6423f580
1884: I0815 04:00:39.050562  6751 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:00:39.050565  6751 naive_executor.cc:189] 0x6423d940 Create persistable variable linear_0.w_0, which pointer is 0x629121c0
1884: I0815 04:00:39.050585  6751 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:00:39.050954  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.051043  6751 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:00:39.051100  6751 executor.cc:183] Old Executor is Running.
1884: I0815 04:00:39.051182  6751 executor.cc:92] Creating Variables for block 0
1884: I0815 04:00:39.051192  6751 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:00:39.051195  6751 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x6423f580 type is 7
1884: I0815 04:00:39.051199  6751 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:00:39.051203  6751 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x629121c0 type is 7
1884: I0815 04:00:39.051246  6751 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.051350  6751 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:00:39.051402  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.051409  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.051548  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.051661  6751 graph.cc:149] create OpNode by feed
1884: I0815 04:00:39.051703  6751 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:00:39.051723  6751 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:00:39.051741  6751 graph.cc:149] create OpNode by abs
1884: I0815 04:00:39.051754  6751 graph.cc:149] create OpNode by assign_value
1884: I0815 04:00:39.051774  6751 graph.cc:149] create OpNode by multinomial
1884: I0815 04:00:39.051784  6751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:00:39.051800  6751 graph.cc:149] create OpNode by scale
1884: I0815 04:00:39.051815  6751 graph.cc:149] create OpNode by scale
1884: I0815 04:00:39.051826  6751 graph.cc:149] create OpNode by fetch
1884: I0815 04:00:39.051843  6751 graph.cc:149] create OpNode by fetch
1884: I0815 04:00:39.051868  6751 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:00:39.053118  6751 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:00:39.053126  6751 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:00:39.053207  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.053215  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:00:39.053346  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.053601  6751 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:00:39.053663  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.053670  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:00:39.053709  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.053715  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:00:39.053761  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.053822  6751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:00:39.053857  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.053864  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:00:39.053884  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.053897  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.053922  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.053928  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:00:39.053972  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.053995  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.054020  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054028  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:00:39.054075  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054150  6751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:00:39.054183  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054189  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:00:39.054224  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054245  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.054270  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054276  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:00:39.054313  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054464  6751 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:00:39.054495  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054502  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:00:39.054536  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054554  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.054579  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054585  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:00:39.054608  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054625  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.054649  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054656  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:00:39.054680  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054695  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.054719  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054725  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:00:39.054751  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054819  6751 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:00:39.054854  6751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:00:39.054870  6751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:00:39.054884  6751 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:00:39.054911  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.054917  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:00:39.054942  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.054984  6751 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:00:39.055004  6751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:00:39.055016  6751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:00:39.055029  6751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:00:39.055063  6751 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:00:39.055075  6751 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:00:39.056325  6751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:00:39.056375  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.056382  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:00:39.056411  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.056432  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.056460  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.056466  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:00:39.056492  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.056543  6751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:00:39.056576  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.056581  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:00:39.056602  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.056618  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.056643  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.056649  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:00:39.056687  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.056773  6751 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:00:39.056803  6751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:00:39.056818  6751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:00:39.056834  6751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:00:39.056850  6751 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:00:39.056865  6751 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:00:39.056881  6751 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:00:39.056907  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.056982  6751 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:00:39.057005  6751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:00:39.057019  6751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:00:39.057034  6751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:00:39.057049  6751 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:00:39.057065  6751 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:00:39.057080  6751 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:00:39.057126  6751 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:00:39.057420  6751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:00:39.057453  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.057459  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:00:39.057509  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057571  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057607  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057654  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057683  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057726  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057751  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057790  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057812  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057847  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057866  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057897  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057915  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057942  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057957  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.057982  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.057994  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058014  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058043  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058049  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:00:39.058076  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058120  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058148  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058156  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:00:39.058166  6751 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:00:39.058171  6751 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:00:39.058223  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058246  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058275  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058281  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:00:39.058291  6751 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:00:39.058295  6751 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:00:39.058346  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058368  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058396  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058403  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:00:39.058413  6751 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:00:39.058415  6751 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:00:39.058452  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058472  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058497  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058503  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:00:39.058513  6751 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:00:39.058516  6751 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:00:39.058558  6751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:00:39.058580  6751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:00:39.058606  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058614  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:00:39.058626  6751 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:00:39.058674  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058681  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:00:39.058759  6751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:00:39.058790  6751 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.058816  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:00:39.058899  6751 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:00:39.058923  6751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:00:39.058956  6751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:00:39.058982  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.058988  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:00:39.059922  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.059940  6751 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:00:39.059995  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.060002  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.060624  6751 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:00:39.060842  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.060917  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.060925  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.061367  6751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:00:39.061590  6751 graph.h:183] deleting __fuse_statis__
1884: I0815 04:00:39.061601  6751 graph.h:183] deleting pass_recorder
1884: I0815 04:00:39.061606  6751 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:00:39.061789  6751 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:00:39.061800  6751 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:00:39.061807  6751 naive_executor.cc:195] 0x6423d940 Create variable abs_0.tmp_0, which pointer is 0x63f7c620
1884: I0815 04:00:39.061815  6751 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:00:39.061820  6751 naive_executor.cc:195] 0x6423d940 Create variable gaussian_0.tmp_0, which pointer is 0x63fa5c80
1884: I0815 04:00:39.061833  6751 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:00:39.061839  6751 naive_executor.cc:195] 0x6423d940 Create variable linear_0.tmp_1, which pointer is 0x63fb0c70
1884: I0815 04:00:39.061843  6751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:00:39.061847  6751 naive_executor.cc:195] 0x6423d940 Create variable multinomial_0.tmp_0, which pointer is 0x63fb0710
1884: I0815 04:00:39.061851  6751 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:00:39.061854  6751 naive_executor.cc:195] 0x6423d940 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x63fb0a10
1884: I0815 04:00:39.061859  6751 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:00:39.061862  6751 naive_executor.cc:195] 0x6423d940 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x63faf010
1884: I0815 04:00:39.061870  6751 scope.cc:202] Create variable feed
1884: I0815 04:00:39.061877  6751 scope.cc:202] Create variable fetch
1884: I0815 04:00:39.061901  6751 naive_executor.cc:46] NaiveExecutor init with scope 0x6423d940
1884: I0815 04:00:39.061908  6751 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:00:39.061998  6751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:00:39.062016  6751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:00:39.062050  6751 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:00:39.062057  6751 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:00:39.062067  6751 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.062106  6751 helper.h:475] Init predictor : [cpu current allocated memory: 3.05348MB], [cpu current reserved memory: 3.05348MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:00:39.062373  6751 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.062392  6751 helper.h:475] before run : [cpu current allocated memory: 3.05353MB], [cpu current reserved memory: 3.05353MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:00:39.062453  6751 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.062484  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:00:39.111248  6751 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:00:39.111380  6751 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.111413  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:00:39.111491  6751 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:00:39.111529  6751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.111557  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:00:39.111631  6751 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:00:39.111682  6751 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.111703  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:00:39.111763  6751 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:00:39.111799  6751 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:00:39.111817  6751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:00:39.111858  6751 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:00:39.111881  6751 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.111917  6751 helper.h:475] after run : [cpu current allocated memory: 3.05401MB], [cpu current reserved memory: 3.05401MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:00:39.111946  6751 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:00:39.112406  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.112419  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:00:39.151752  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642b0540 on Place(gpu:0)
1884: I0815 04:00:39.151799  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_0
1884: I0815 04:00:39.151820  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_1
1884: I0815 04:00:39.151831  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_2
1884: I0815 04:00:39.151845  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_3
1884: I0815 04:00:39.151880  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_4
1884: I0815 04:00:39.151897  6751 scope.cc:202] Create variable 0x642b05401723694439151786685_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x642b05401723694439151786685_inner_var_0 -> 0x6372f0a0
1884: 1 -> 0x642b05401723694439151786685_inner_var_1 -> 0x636b5cc0
1884: 2 -> 0x642b05401723694439151786685_inner_var_2 -> 0x6430e460
1884: 3 -> linear_1.w_0 -> 0x456c7c70
1884: 4 -> linear_1.b_0 -> 0x618e3f20
1884: 5 -> learning_rate_1 -> 0x3fc36d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:00:39.152776  6813 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.152853  6814 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.152848  6816 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.152882  6817 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:39.152886  6814 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.152894  6813 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.152925  6816 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.152933  6817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.152964  6814 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.152964  6813 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.152978  6816 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x642b05401723694439151786685_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:00:39.152997  6817 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.153040  6817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:00:39.153064  6817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.153081  6817 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.153093  6817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:00:39.153106  6817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x642b05401723694439151786685_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642b05401723694439151786685_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642b05401723694439151786685_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.153168  6817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x642b05401723694439151786685_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642b05401723694439151786685_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642b05401723694439151786685_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:00:39.153223  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x642b06b0) got event_name: TaskCompletion
1884: I0815 04:00:39.153333  6815 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:00:39.155536  6751 pir_interpreter.cc:161] PirInterpreter(): 0x64342980 on Place(gpu:0)
1884: I0815 04:00:39.155580  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_1
1884: I0815 04:00:39.155601  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_4
1884: I0815 04:00:39.155612  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_5
1884: I0815 04:00:39.155624  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_6
1884: I0815 04:00:39.155652  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_7
1884: I0815 04:00:39.155668  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_8
1884: I0815 04:00:39.155680  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_9
1884: I0815 04:00:39.155714  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_10
1884: I0815 04:00:39.155727  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_11
1884: I0815 04:00:39.155740  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_12
1884: I0815 04:00:39.155752  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_13
1884: I0815 04:00:39.155766  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_14
1884: I0815 04:00:39.155778  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_15
1884: I0815 04:00:39.155789  6751 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:00:39.155804  6751 scope.cc:202] Create variable 0x643429801723694439155563482_inner_var_17
1884: I0815 04:00:39.155815  6751 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x3fc36d0
1884: 1 -> 0x643429801723694439155563482_inner_var_1 -> 0x5ae7ed40
1884: 2 -> linear_1.b_0 -> 0x618e3f20
1884: 3 -> linear_1.w_0 -> 0x456c7c70
1884: 4 -> 0x643429801723694439155563482_inner_var_4 -> 0x5ae7ec60
1884: 5 -> 0x643429801723694439155563482_inner_var_5 -> 0x629133e0
1884: 6 -> 0x643429801723694439155563482_inner_var_6 -> 0x64351f50
1884: 7 -> 0x643429801723694439155563482_inner_var_7 -> 0x63fa4430
1884: 8 -> 0x643429801723694439155563482_inner_var_8 -> 0x6401b5b0
1884: 9 -> 0x643429801723694439155563482_inner_var_9 -> 0x64352010
1884: 10 -> 0x643429801723694439155563482_inner_var_10 -> 0x456be040
1884: 11 -> 0x643429801723694439155563482_inner_var_11 -> 0x6367d7e0
1884: 12 -> 0x643429801723694439155563482_inner_var_12 -> 0x640f3150
1884: 13 -> 0x643429801723694439155563482_inner_var_13 -> 0x640f2730
1884: 14 -> 0x643429801723694439155563482_inner_var_14 -> 0x6423c150
1884: 15 -> 0x643429801723694439155563482_inner_var_15 -> 0x6381e590
1884: 16 -> fetch0@fetch -> 0x6375c150
1884: 17 -> 0x643429801723694439155563482_inner_var_17 -> 0x63fa4390
1884: 18 -> fetch1@fetch -> 0x642e3950
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:00:39.157666  6818 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.157817  6819 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.157869  6820 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:39.157891  6821 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.157895  6819 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.157900  6820 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.157925  6819 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:00:39.157930  6820 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.158006  6822 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:00:39.158044  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158071  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:00:39.158095  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158138  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.158180  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:00:39.158195  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:00:39.158250  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:00:39.158267  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x643429801723694439155563482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158313  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x643429801723694439155563482_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:00:39.158340  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x643429801723694439155563482_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158377  6822 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:00:39.158411  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x643429801723694439155563482_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.158438  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x643429801723694439155563482_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x643429801723694439155563482_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158478  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.158492  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x643429801723694439155563482_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x643429801723694439155563482_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.158522  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x643429801723694439155563482_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158542  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.158555  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x643429801723694439155563482_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.158557  6820 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x643429801723694439155563482_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158572  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x643429801723694439155563482_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158601  6820 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.158607  6822 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.158691  6822 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.158721  6820 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x643429801723694439155563482_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.158759  6822 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.158761  6820 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x643429801723694439155563482_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158779  6820 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:00:39.158795  6820 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x643429801723694439155563482_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.158824  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.158843  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x643429801723694439155563482_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x643429801723694439155563482_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.158877  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x643429801723694439155563482_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158885  6820 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x643429801723694439155563482_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158900  6820 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:00:39.158903  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.158916  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x643429801723694439155563482_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.158933  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x643429801723694439155563482_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158936  6820 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x643429801723694439155563482_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.158954  6820 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x643429801723694439155563482_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.158970  6820 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:00:39.158982  6820 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x643429801723694439155563482_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.159005  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x643429801723694439155563482_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:00:39.159025  6822 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x643429801723694439155563482_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x643429801723694439155563482_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.159051  6822 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:00:39.159063  6822 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x643429801723694439155563482_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x643429801723694439155563482_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x643429801723694439155563482_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:00:39.159098  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x64342af0) got event_name: TaskCompletion
1884: I0815 04:00:39.159114  6751 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:00:39.159137  6751 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:00:39.168798  6751 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:00:39.168849  6751 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:00:39.169637  6751 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:00:39.169693  6751 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:00:39.170168  6751 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391702212170"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391702212170"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:00:39.170423  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642eb990 on Place(cpu)
1884: I0815 04:00:39.170447  6751 scope.cc:202] Create variable 0x642eb9901723694439170438874_inner_var_0
1884: I0815 04:00:39.170477  6751 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391702212170"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236944391702212170 -> 0x642af4a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:00:39.170650  6751 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:00:39.170794  6823 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:39.170977  6824 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.171007  6825 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.171087  6824 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236944391702212170:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.171094  6826 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:39.171135  6824 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236944391702212170:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:00:39.171164  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x642ebb00) got event_name: TaskCompletion
1884: I0815 04:00:39.171217  6827 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.171455  6824 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3917766642329445563 to 17030668657724370064 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:00:39.171465  6824 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3917766642329445563 to 17030668657724370064 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:00:39.171521  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.171530  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391716021331"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391716021331"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:00:39.171761  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642eb990 on Place(cpu)
1884: I0815 04:00:39.171780  6751 scope.cc:202] Create variable 0x642eb9901723694439171774532_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391716021331"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236944391716021331 -> 0x63f68a50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:00:39.171999  6828 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:39.172070  6829 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.172127  6832 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.172124  6829 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236944391716021331:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.172225  6829 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236944391716021331:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.172259  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x642ebb00) got event_name: TaskCompletion
1884: I0815 04:00:39.172293  6830 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.172372  6831 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:39.172549  6829 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 15059955295858641759 to 17030668657724370064 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:00:39.172571  6829 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 15059955295858641759 to 17030668657724370064 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:00:39.172637  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.172645  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236944391716021331",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236944391727226222"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236944391716021331",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236944391727226222"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:00:39.172873  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642eb990 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236944391716021331",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236944391727226222"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236944391727226222 -> 0x63f68a50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:00:39.173130  6833 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:39.173192  6834 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.173244  6835 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.173305  6836 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:39.173269  6834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.173341  6834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.173359  6837 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.173372  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x642ebb00) got event_name: TaskCompletion
1884: I0815 04:00:39.173722  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.173728  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391738091123"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391738091123"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:00:39.173938  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642eb990 on Place(cpu)
1884: I0815 04:00:39.173955  6751 scope.cc:202] Create variable 0x642eb9901723694439173950722_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236944391738091123"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236944391738091123 -> 0x636b46d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:00:39.174152  6838 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:39.174279  6839 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:00:39.174336  6840 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:00:39.174367  6840 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236944391738091123:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.174407  6840 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236944391738091123:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:00:39.174432  6751 pir_interpreter.cc:1766] main_thread_blocker_(0x642ebb00) got event_name: TaskCompletion
1884: I0815 04:00:39.174465  6841 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:00:39.174536  6842 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:00:39.174664  6840 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 9576916677642449532 to 17030668657724370064 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:00:39.174672  6840 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 9576916677642449532 to 17030668657724370064 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:00:39.174777  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.174785  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.174851  6751 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:00:39.174913  6751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:00:39.174952  6751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391738091123"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391727226222"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391738091123"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391727226222"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:00:39.175655  6751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:00:39.175673  6751 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:00:39.175709  6751 pir_interpreter.cc:161] PirInterpreter(): 0x642eb990 on Place(cpu)
1884: I0815 04:00:39.175740  6751 scope.cc:202] Create variable feed_name_0
1884: I0815 04:00:39.175755  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_5
1884: I0815 04:00:39.175777  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_6
1884: I0815 04:00:39.175788  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_7
1884: I0815 04:00:39.175798  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_8
1884: I0815 04:00:39.175818  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_9
1884: I0815 04:00:39.175830  6751 scope.cc:202] Create variable 0x642eb9901723694439175724137_inner_var_10
1884: I0815 04:00:39.175854  6751 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.175876  6751 helper.h:475] Init predictor : [cpu current allocated memory: 3.05385MB], [cpu current reserved memory: 3.05385MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:00:39.176025  6751 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.176040  6751 helper.h:475] before run : [cpu current allocated memory: 3.0539MB], [cpu current reserved memory: 3.0539MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391738091123"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236944391727226222"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236944391738091123 -> 0x636b46d0
1884: 1 -> constant_folding@_17236944391727226222 -> 0x63f68a50
1884: 2 -> linear_1.b_0 -> 0x456ca300
1884: 3 -> linear_1.w_0 -> 0x6423d6a0
1884: 4 -> feed_name_0 -> 0x642af020
1884: 5 -> 0x642eb9901723694439175724137_inner_var_5 -> 0x63f68a70
1884: 6 -> 0x642eb9901723694439175724137_inner_var_6 -> 0x62917730
1884: 7 -> 0x642eb9901723694439175724137_inner_var_7 -> 0x63f8b800
1884: 8 -> 0x642eb9901723694439175724137_inner_var_8 -> 0x61928670
1884: 9 -> fetch_name_0 -> 0x642af660
1884: 10 -> fetch_name_1 -> 0x636b5050
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:00:39.176640  6751 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:00:39.176705  6843 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:00:39.176699  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.176755  6751 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:00:39.176776  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.176808  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x642eb9901723694439175724137_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x642eb9901723694439175724137_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.176851  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x642eb9901723694439175724137_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.176884  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.176911  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.176927  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.176962  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236944391727226222:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.176987  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236944391738091123:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.177018  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236944391738091123:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x642eb9901723694439175724137_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:00:39.177045  6751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236944391738091123:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:00:39.177074  6751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236944391738091123:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642eb9901723694439175724137_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:00:39.177103  6751 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07951MB]
1884: I0815 04:00:39.177125  6751 helper.h:475] after run : [cpu current allocated memory: 3.05408MB], [cpu current reserved memory: 3.05408MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:00:39.177148  6751 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:00:39.177269  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.177276  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.177333  6843 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 3917766642329445563 to 17030668657724370064 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:00:39.177345  6843 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 3917766642329445563 to 17030668657724370064 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:00:39.177386  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.177394  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EEF..
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2979, in check_output_customized
1884:     outs_p = self._calc_pir_output(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1361, in _calc_pir_output
1884:     kernel_sig = self.get_kernel_signature(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1264, in get_kernel_signature
1884:     else self.append_input_output_for_dygraph(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1026, in append_input_output_for_dygraph
1884:     if (name not in np_list) and var_proto.dispensable:
1884: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
1884: 
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2979, in check_output_customized
1884:     outs_p = self._calc_pir_output(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1361, in _calc_pir_output
1884:     kernel_sig = self.get_kernel_signature(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1264, in get_kernel_signature
1884:     else self.append_input_output_for_dygraph(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1026, in append_input_output_for_dygraph
1884:     if (name not in np_list) and var_proto.dispensable:
1884: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 99 / 100 (99%)
1884: Max absolute difference: 995
1884: Max relative difference: inf
1884:  x: array([ 91,   0, 851, 327, 917, 218, 842, 774, 835, 629, 113,  94, 242,
1884:        278, 679, 633, 532, 952, 169, 642,  38, 807, 512, 317, 904, 659,
1884:        970, 485, 613, 341, 180, 117, 370, 840, 552, 237, 392, 704, 438,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.852s
1884: 
1884: FAILED (failures=1, errors=2, skipped=3)
1884: 
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [array([3, 0, 0, ..., 0, 3, 0], dtype=int64)]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [array([[0, 0, 0, ..., 3, 0, 0],
1884:        [1, 0, 3, ..., 3, 1, 0],
1884:        [3, 3, 1, ..., 2, 3, 0]], dtype=int64)]
1884: I0815 04:00:39.179531  6751 mmap_allocator.cc:348] PID: 6751, MemoryMapFdSet: set size - 0
1884: I0815 04:00:39.191366  6751 mmap_allocator.cc:348] PID: 6751, MemoryMapFdSet: set size - 0
1884: I0815 04:00:39.265928  6816 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3549384761189727402 to 17030668657724370064 , after update, data is {current : -168, peak : 104}.
1884: I0815 04:00:39.265964  6816 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3549384761189727402 to 17030668657724370064 , after update, data is {current : -168, peak : 104}.
1884: I0815 04:00:39.265973  6813 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2615954813785320905 to 17030668657724370064 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:00:39.266003  6813 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2615954813785320905 to 17030668657724370064 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:00:39.266009  6814 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13556316358673641595 to 17030668657724370064 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:00:39.266031  6814 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13556316358673641595 to 17030668657724370064 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:00:39.266407  6817 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 14021460290891867851 to 17030668657724370064 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:00:39.266423  6817 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 14021460290891867851 to 17030668657724370064 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:00:39.266429  6817 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 14021460290891867851 to 1525862569570788927 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:00:39.266690  6820 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 1525862569570788927 to 17030668657724370064 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:00:39.266700  6820 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 1525862569570788927 to 17030668657724370064 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:00:39.266703  6820 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 1525862569570788927 to 17030668657724370064 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:00:39.266760  6819 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 2108614879160138562 to 17030668657724370064 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:00:39.266769  6819 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 2108614879160138562 to 17030668657724370064 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:00:39.266968  6822 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 17030668657724370064 to 11392407368413149006 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:00:39.266978  6822 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 17030668657724370064 to 11392407368413149006 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:00:39.266983  6822 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 17030668657724370064 to 11392407368413149006 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:00:39.422735  6751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:00:39.422766  6751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:00:39.422807  6751 mmap_allocator.cc:348] PID: 6751, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   12.88 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  13.06 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
