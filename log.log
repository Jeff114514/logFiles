UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0814 09:43:19.898065 27007 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0814 09:43:20.691147 27007 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=free_idle_chunk,enable_opt_get_features,enable_graph_multi_node_sampling,prim_enable_dynamic,pir_apply_shape_optimization_pass,allocator_strategy,accuracy_check_rtol_fp32,trt_ibuilder_cache,enable_pir_with_pt_in_dy2st,cusolver_dir,cudnn_dir,win_cuda_bin_dir,prim_all,sort_sum_gradient,gpugraph_enable_hbm_table_collision_stat,benchmark_nccl,static_executor_perfstat_filepath,multi_node_sample_use_gpu_table,gpu_memory_limit_mb,tensor_operants_mode,nvidia_package_dir,use_pinned_memory,use_shm_cache,print_allocator_trace_info,dist_threadpool_size,auto_free_cudagraph_allocations_on_launch,fraction_of_cpu_memory_to_use,enable_dependency_builder_debug_info,check_nan_inf,benchmark,gpugraph_dedup_pull_push_mode,use_stride_kernel,init_allocated_mem,async_trace_count,local_exe_sub_scope_limit,new_executor_serial_run,use_virtual_memory_auto_growth,pir_subgraph_saving_dir,eager_delete_scope,use_cuda_malloc_async_allocator,gpugraph_offload_param_stat,multiple_of_cupti_buffer_size,auto_growth_chunk_size_in_mb,save_static_runtime_data,add_dependency_for_communication_op,enable_api_kernel_fallback,npu_storage_format,manually_trans_conv_filter,initial_gpu_memory_in_mb,use_fast_math,use_mkldnn,conv_workspace_size_limit,sync_after_alloc,jit_engine_type,initial_cpu_memory_in_mb,graph_load_in_parallel,logging_trunc_pir_py_code,get_host_by_name_time,cinn_compile_thread_num,enable_dump_main_program,enable_pir_in_executor_trace_run,all_blocks_convert_trt,gpugraph_load_node_list_into_hbm,static_runtime_data_save_path,cinn_subgraph_graphviz_dir,logging_pir_py_code_dump_symbolic_dims,gpugraph_sparse_table_storage_mode,set_to_1d,graph_embedding_split_infer_mode,enable_cinn_auto_tune,pir_broadcast_tree_limit,check_kernel_launch,tracer_onednn_ops_off,enable_cinn_accuracy_check,cudnn_exhaustive_search,use_auto_growth_pinned_allocator,disable_dyshape_in_train,accuracy_check_rtol_fp16,memory_fraction_of_eager_deletion,accuracy_check_atol_fp32,gpugraph_parallel_copyer_split_maxsize,gpugraph_slot_feasign_max_num,use_stream_safe_cuda_allocator,new_executor_use_cuda_graph,prim_backward,logging_pir_py_code_dir,alloc_fill_value,enable_blaslt_global_search,enable_cse_in_dy2st,query_dest_rank_by_multi_node,inner_op_parallelism,enable_sparse_inner_gather,use_cuda_managed_memory,new_executor_use_inplace,deny_cinn_ops,gpugraph_merge_grads_segment_size,lapack_dir,fast_eager_deletion_mode,gpugraph_debug_gpu_memory,logging_pir_py_code_int_tensor_element_limit,einsum_opt,dygraph_debug,mkl_dir,enable_async_trace,graph_metapath_split_opt,enable_neighbor_list_use_uva,print_sub_graph_dir,pir_debug,use_xqa_optim,cuda_malloc_async_pool_memory_throttle_ratio,cuda_memory_async_pool_realease_threshold,prim_check_ops,new_executor_use_local_scope,cusparselt_dir,fuse_parameter_memory_size,search_cache_max_number,gpugraph_offload_param_extends,max_inplace_grad_add,enable_auto_detect_gpu_topo,gpugraph_storage_mode,curand_dir,embedding_deterministic,reader_queue_speed_test_mode,executor_log_deps_every_microseconds,use_auto_growth_v2,enable_all2all_use_fp16,enable_cinn_compile_cache,enable_pir_api,low_precision_op_list,use_cinn,enable_interpretercore_launch_cinn,enable_gpu_memory_usage_log,conv2d_disable_cudnn,cudnn_exhaustive_search_times,cache_inference_while_scope,prim_forward_blacklist,host_trace_level,enable_unused_var_check,enable_fuse_parallel_matmul_pass,cublas_dir,dump_chunk_info,enable_adjust_op_order,use_autotune,allow_cinn_ops,use_system_allocator,graph_neighbor_size_percent,free_when_no_cache_hit,gemm_use_half_precision_compute_type,gpugraph_enable_segment_merge_grads,tracer_onednn_ops_on,nccl_blocking_wait,prim_forward,dynamic_static_unified_comm,prim_skip_dynamic,gpugraph_force_device_batch_num_equal,pinned_memory_as_cpu_backend,enable_collect_shape,ir_inplace_kernel_blacklist,apply_pass_to_program,custom_device_mem_record,pir_apply_inplace_pass,run_kp_kernel,prim_enabled,gpugraph_hbm_table_load_factor,accuracy_check_rtol_bf16,fraction_of_gpu_memory_to_use,enable_gpu_memory_usage_log_mb,enable_exit_when_partial_worker,convert_all_blocks,gpugraph_enable_print_op_debug,new_executor_static_build,enable_tracker_all2all,allreduce_record_one_event,new_executor_sequential_run,tensorrt_dir,cupti_dir,gpugraph_offload_gather_copy_maxsize,tracer_profile_fname,gpugraph_parallel_stream_num,cse_max_count,check_nan_inf_level,op_dir,selected_gpus,cudnn_deterministic,cublaslt_device_best_config,sync_nccl_allreduce,log_memory_stats,accuracy_check_atol_fp16,accuracy_check_atol_bf16,dataloader_use_file_descriptor,fuse_parameter_groups_size,enable_pir_in_executor,enable_auto_rdma_trans,enable_record_memory,print_ir,mklml_dir,gpu_allocator_retry_time,nccl_dir,cublaslt_exhaustive_search_times,call_stack_level,eager_delete_tensor_gb,fraction_of_cuda_pinned_memory_to_use,fleet_executor_with_standalone,cusparse_dir,enable_cublas_tensor_op_math,graph_get_neighbor_id,gpugraph_enable_gpu_direct_access,enable_fusion_fallback,cudnn_batchnorm_spatial_persistent,cuda_dir,check_infer_symbolic,paddle_num_threads,reallocate_gpu_memory_in_mb 
1884: I0814 09:43:20.691258 27007 init.cc:108] After Parse: argc is 2
1884: I0814 09:43:28.736639 27007 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 09:43:28.736693 27007 dygraph_functions.cc:77659] { Input: []} 
1884: W0814 09:43:28.737401 27007 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0814 09:43:28.737947 27007 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0814 09:43:28.738776 27007 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0814 09:43:28.738866 27007 allocator_facade.cc:212] selected allocator strategy:1
1884: I0814 09:43:28.738955 27007 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0814 09:43:28.739629 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f8d31200000), and remaining 0
1884: I0814 09:43:28.739976 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.740041 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.740128 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f8d31200200), and remaining 0
1884: I0814 09:43:28.740154 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f8d31200400), and remaining 0
1884: I0814 09:43:28.743916 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f8d31200600), and remaining 0
1884: I0814 09:43:28.744057 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f8d31200800), and remaining 0
1884: I0814 09:43:28.744124 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f8d31200a00), and remaining 0
1884: I0814 09:43:28.744218 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.744239 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.744315 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.744329 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.745779 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aa8f9a0 for it.
1884: I0814 09:43:28.745927 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.745950 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.746004 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f8d31200e00), and remaining 0
1884: I0814 09:43:28.746078 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f8d312c4400), and remaining 0
1884: I0814 09:43:28.868233 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aa8f9a0 for it.
1884: I0814 09:43:28.868438 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.868484 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.869062 27007 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f8d31400000), and remaining 0
1884: I0814 09:43:28.880008 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aa8f9a0 for it.
1884: I0814 09:43:28.880106 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:28.880138 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.880177 27007 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:28.880352 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:28.881335 27007 dygraph_functions.cc:33459] Running AD API: full
1884: I0814 09:43:28.881351 27007 dygraph_functions.cc:33480] { Input: []} 
1884: I0814 09:43:28.881400 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:28.881475 27007 dygraph_functions.cc:64553] Running AD API: scale
1884: I0814 09:43:28.881502 27007 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.881562 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:28.881652 27007 dygraph_functions.cc:26170] Running AD API: exp
1884: I0814 09:43:28.881668 27007 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.881702 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:28.881902 27007 dygraph_functions.cc:72508] Running AD API: sum
1884: I0814 09:43:28.881919 27007 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.882082 27007 dygraph_functions.cc:83176] Running AD API: divide
1884: I0814 09:43:28.882109 27007 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:28.882180 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:28.886075 27007 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0814 09:43:28.886185 27007 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0814 09:43:28.886212 27007 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0814 09:43:28.886274 27007 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0814 09:43:30.228852 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.228932 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.229338 27007 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0814 09:43:30.229360 27007 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.235458 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.235495 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.236599 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.236618 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.236630 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.237401 27007 program_interpreter.cc:243] New Executor is Running.
1884: I0814 09:43:30.237413 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.237430 27007 scope.cc:202] Create variable feed
1884: I0814 09:43:30.237442 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.237455 27007 scope.cc:202] Create variable fetch
1884: I0814 09:43:30.237460 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.237470 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.237475 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.237479 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.237483 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.239861 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.240208 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.240221 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.240227 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.241909 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.241957 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.241966 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.241972 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.241978 27007 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 09:43:30.241987 27007 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x611dcc70 type is 7
1884: I0814 09:43:30.241991 27007 scope.cc:202] Create variable x
1884: I0814 09:43:30.241995 27007 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x611db740 type is 7
1884: I0814 09:43:30.242058 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.242065 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.242070 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.242074 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.242199 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.242223 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.242358 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.242370 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.242388 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.242552 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.242583 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.242601 27007 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.242609 27007 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x611e3790Variable Type 7
1884: I0814 09:43:30.242636 27007 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.242663 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.242717 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.242735 27007 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.243979 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.244035 27007 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.244438 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.248589 27007 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 09:43:30.248608 27007 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 09:43:30.248701 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.248729 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.249234 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: I0814 09:43:30.249315 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.249338 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.249801 27007 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: I0814 09:43:30.249862 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.249884 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.249909 27007 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.250160 27007 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 09:43:30.250171 27007 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 09:43:30.250288 27007 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0814 09:43:30.250319 27007 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.250695 27007 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 09:43:30.250706 27007 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 09:43:30.250747 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.250766 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.250944 27007 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 09:43:30.250953 27007 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 09:43:30.250989 27007 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 09:43:30.251006 27007 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 09:43:30.251022 27007 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.253721 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.253743 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.253798 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.253806 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.255718 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.256078 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.256091 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.256096 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.257874 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.257926 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.257937 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.257942 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6120e750 type is 7
1884: I0814 09:43:30.257952 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.257956 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6120eac0 type is 7
1884: I0814 09:43:30.257961 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.257966 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.258023 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.258029 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.258035 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.258039 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.258090 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.258109 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.258170 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.258180 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.258198 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.258464 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.258481 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.258500 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.258508 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61215170Variable Type 7
1884: I0814 09:43:30.258527 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.258545 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.258569 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.258586 27007 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.259315 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.259341 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.259516 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.273052 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: I0814 09:43:30.273267 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ab779a0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 09:43:30.279536 27007 pir_interpreter.cc:161] PirInterpreter(): 0x613d1ce0 on Place(gpu:0)
1884: I0814 09:43:30.279582 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.279616 27007 scope.cc:202] Create variable 0x613d1ce01723628610279564282_inner_var_1
1884: I0814 09:43:30.279628 27007 scope.cc:202] Create variable 0x613d1ce01723628610279564282_inner_var_2
1884: I0814 09:43:30.279639 27007 scope.cc:202] Create variable 0x613d1ce01723628610279564282_inner_var_3
1884: I0814 09:43:30.279651 27007 scope.cc:202] Create variable 0x613d1ce01723628610279564282_inner_var_4
1884: I0814 09:43:30.279659 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.280077 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.280093 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.280097 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0814 09:43:30.280143 27007 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x613d1c40
1884: 1 -> 0x613d1ce01723628610279564282_inner_var_1 -> 0x613d1cc0
1884: 2 -> 0x613d1ce01723628610279564282_inner_var_2 -> 0x613d2590
1884: 3 -> 0x613d1ce01723628610279564282_inner_var_3 -> 0x613d1420
1884: 4 -> 0x613d1ce01723628610279564282_inner_var_4 -> 0x613d2940
1884: 5 -> fetch0@fetch -> 0x613d3150
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.280911 27007 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0814 09:43:30.281141 27045 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.281323 27046 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.281344 27047 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.281411 27048 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.281463 27049 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.281486 27048 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x613d1ce01723628610279564282_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.281553 27050 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.281584 27048 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x613d1ce01723628610279564282_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.281610 27050 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.281657 27050 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 09:43:30.281705 27050 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x613d1ce01723628610279564282_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x613d1ce01723628610279564282_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.281910 27050 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x613d1ce01723628610279564282_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x613d1ce01723628610279564282_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 09:43:30.281983 27048 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x613d1ce01723628610279564282_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.282008 27048 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.283239 27048 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x613d1ce01723628610279564282_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x613d1ce01723628610279564282_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 09:43:30.283281 27048 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x613d1ce01723628610279564282_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.283313 27048 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.283898 27048 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x613d1ce01723628610279564282_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 09:43:30.283936 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x613d1e50) got event_name: TaskCompletion
1884: I0814 09:43:30.283962 27007 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.359220 27045 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 646669062748365297 to 12688117890663693436 , after update, data is {current : 0, peak : 800768}.
1884: I0814 09:43:30.359251 27045 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 646669062748365297 to 13975936553071589987 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 09:43:30.359258 27045 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 646669062748365297 to 13975936553071589987 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 09:43:30.359524 27048 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 09:43:30.359541 27048 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 09:43:30.359683 27050 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0814 09:43:30.359696 27050 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.365746 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.365773 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.365832 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.365840 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.367650 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.367997 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.368011 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.368016 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.369557 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.369652 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.369663 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.369670 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x543c7e0 type is 7
1884: I0814 09:43:30.369678 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.369681 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x611c2ff0 type is 7
1884: I0814 09:43:30.369685 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.369689 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.369746 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.369752 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.369756 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.369760 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.369813 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.369830 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.369892 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.369900 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.369915 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.370066 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.370077 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.370093 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.370100 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x611c2f50Variable Type 7
1884: I0814 09:43:30.370114 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.370133 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.370153 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.370167 27007 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.371788 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.371824 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.372033 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.376722 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ab779a0 for it.
1884: I0814 09:43:30.376900 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 09:43:30.379949 27007 pir_interpreter.cc:161] PirInterpreter(): 0x611bc750 on Place(gpu:0)
1884: I0814 09:43:30.379985 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.380009 27007 scope.cc:202] Create variable 0x611bc7501723628610379975459_inner_var_1
1884: I0814 09:43:30.380019 27007 scope.cc:202] Create variable 0x611bc7501723628610379975459_inner_var_2
1884: I0814 09:43:30.380029 27007 scope.cc:202] Create variable 0x611bc7501723628610379975459_inner_var_3
1884: I0814 09:43:30.380039 27007 scope.cc:202] Create variable 0x611bc7501723628610379975459_inner_var_4
1884: I0814 09:43:30.380049 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.380388 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.380404 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.380407 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x2f64590
1884: 1 -> 0x611bc7501723628610379975459_inner_var_1 -> 0x4d09ba20
1884: 2 -> 0x611bc7501723628610379975459_inner_var_2 -> 0x5447260
1884: 3 -> 0x611bc7501723628610379975459_inner_var_3 -> 0x542e740
1884: 4 -> 0x611bc7501723628610379975459_inner_var_4 -> 0x611dce40
1884: 5 -> fetch0@fetch -> 0x611e0fe0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.381129 27051 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.381201 27052 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.381253 27054 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.381287 27053 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.381286 27055 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.381332 27056 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.381338 27053 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611bc7501723628610379975459_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.381358 27056 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.381399 27056 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 09:43:30.381435 27053 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611bc7501723628610379975459_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.381465 27056 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611bc7501723628610379975459_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611bc7501723628610379975459_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.381573 27056 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611bc7501723628610379975459_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611bc7501723628610379975459_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.381649 27053 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611bc7501723628610379975459_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.381680 27053 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.384562 27053 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611bc7501723628610379975459_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x611bc7501723628610379975459_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.384621 27053 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x611bc7501723628610379975459_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.384647 27053 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.386721 27053 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x611bc7501723628610379975459_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.386773 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x611bc8c0) got event_name: TaskCompletion
1884: I0814 09:43:30.386798 27007 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.424203 27051 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 12688117890663693436 to 17628551397842200998 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 09:43:30.424221 27051 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12688117890663693436 to 13975936553071589987 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 09:43:30.424226 27051 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12688117890663693436 to 13975936553071589987 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 09:43:30.424495 27053 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 09:43:30.424507 27053 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 09:43:30.424644 27056 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.428635 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.428661 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.428717 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.428725 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.430919 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.431263 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.431277 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.431281 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.432811 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.432956 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.432967 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.432973 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x611fea10 type is 7
1884: I0814 09:43:30.432981 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.432986 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5436a40 type is 7
1884: I0814 09:43:30.432991 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.432996 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.433050 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.433056 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.433061 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.433065 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.433116 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.433131 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.433192 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.433199 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.433213 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.433254 27007 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.433403 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.433466 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.433477 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.433492 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.433499 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x611e1a10Variable Type 7
1884: I0814 09:43:30.433516 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.433533 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.433554 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.433570 27007 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.433844 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.433866 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.434057 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.434841 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: I0814 09:43:30.435037 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ab779a0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 09:43:30.438031 27007 pir_interpreter.cc:161] PirInterpreter(): 0x63aaf460 on Place(gpu:0)
1884: I0814 09:43:30.438066 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.438088 27007 scope.cc:202] Create variable 0x63aaf4601723628610438056543_inner_var_1
1884: I0814 09:43:30.438099 27007 scope.cc:202] Create variable 0x63aaf4601723628610438056543_inner_var_2
1884: I0814 09:43:30.438108 27007 scope.cc:202] Create variable 0x63aaf4601723628610438056543_inner_var_3
1884: I0814 09:43:30.438118 27007 scope.cc:202] Create variable 0x63aaf4601723628610438056543_inner_var_4
1884: I0814 09:43:30.438127 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.438470 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.438486 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.438490 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x611f9510
1884: 1 -> 0x63aaf4601723628610438056543_inner_var_1 -> 0x613db350
1884: 2 -> 0x63aaf4601723628610438056543_inner_var_2 -> 0x613a71e0
1884: 3 -> 0x63aaf4601723628610438056543_inner_var_3 -> 0x5466130
1884: 4 -> 0x63aaf4601723628610438056543_inner_var_4 -> 0x3d356d0
1884: 5 -> fetch0@fetch -> 0x5479ff0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.439162 27057 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.439265 27059 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.439262 27058 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.439317 27060 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.439348 27061 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.439394 27062 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.439386 27061 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63aaf4601723628610438056543_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.439425 27062 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.439440 27061 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63aaf4601723628610438056543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.439461 27062 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 09:43:30.439496 27062 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63aaf4601723628610438056543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63aaf4601723628610438056543_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.439538 27062 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.439659 27062 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.439687 27062 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63aaf4601723628610438056543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63aaf4601723628610438056543_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 09:43:30.439751 27061 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63aaf4601723628610438056543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.439782 27061 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.440078 27061 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63aaf4601723628610438056543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63aaf4601723628610438056543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 09:43:30.440102 27061 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63aaf4601723628610438056543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.440120 27061 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.440132 27061 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63aaf4601723628610438056543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 09:43:30.440163 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x63aaf5d0) got event_name: TaskCompletion
1884: I0814 09:43:30.440182 27007 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.472306 27057 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 17628551397842200998 to 12688117890663693436 , after update, data is {current : 0, peak : 3328}.
1884: I0814 09:43:30.472322 27057 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17628551397842200998 to 12688117890663693436 , after update, data is {current : -804, peak : 2000}.
1884: I0814 09:43:30.472328 27057 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17628551397842200998 to 12688117890663693436 , after update, data is {current : -804, peak : 2000}.
1884: I0814 09:43:30.472541 27061 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1446525784980733918 to 12688117890663693436 , after update, data is {current : 800, peak : 2000}.
1884: I0814 09:43:30.472554 27061 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1446525784980733918 to 12688117890663693436 , after update, data is {current : 800, peak : 2000}.
1884: I0814 09:43:30.472730 27062 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 09:43:30.472743 27062 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 09:43:30.472748 27062 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.476936 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.476960 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.477013 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.477021 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.478703 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.479043 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.479056 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.479061 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.480581 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.480670 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.480681 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.480687 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x542c610 type is 7
1884: I0814 09:43:30.480695 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.480697 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x613d3dd0 type is 7
1884: I0814 09:43:30.480703 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.480708 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.480763 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.480769 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.480773 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.480777 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.480826 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.480841 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.480902 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.480911 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.480923 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.481148 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.481161 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.481177 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.481184 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x634647a0Variable Type 7
1884: I0814 09:43:30.481199 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.481218 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.481240 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.481253 27007 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.481950 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.481979 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.482179 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.484597 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ab779a0 for it.
1884: I0814 09:43:30.484771 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 09:43:30.487794 27007 pir_interpreter.cc:161] PirInterpreter(): 0x611ef6e0 on Place(gpu:0)
1884: I0814 09:43:30.487829 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.487854 27007 scope.cc:202] Create variable 0x611ef6e01723628610487819843_inner_var_1
1884: I0814 09:43:30.487864 27007 scope.cc:202] Create variable 0x611ef6e01723628610487819843_inner_var_2
1884: I0814 09:43:30.487874 27007 scope.cc:202] Create variable 0x611ef6e01723628610487819843_inner_var_3
1884: I0814 09:43:30.487883 27007 scope.cc:202] Create variable 0x611ef6e01723628610487819843_inner_var_4
1884: I0814 09:43:30.487892 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.488217 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.488231 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.488235 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x611c3980
1884: 1 -> 0x611ef6e01723628610487819843_inner_var_1 -> 0x611c3a00
1884: 2 -> 0x611ef6e01723628610487819843_inner_var_2 -> 0x6357d810
1884: 3 -> 0x611ef6e01723628610487819843_inner_var_3 -> 0x611c2f30
1884: 4 -> 0x611ef6e01723628610487819843_inner_var_4 -> 0x613d0360
1884: 5 -> fetch0@fetch -> 0x44f976e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.488931 27063 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.489010 27064 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.489039 27065 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.489104 27066 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.489104 27067 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.489152 27068 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.489148 27067 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611ef6e01723628610487819843_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.489177 27068 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.489198 27067 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611ef6e01723628610487819843_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.489217 27068 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 09:43:30.489250 27068 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611ef6e01723628610487819843_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611ef6e01723628610487819843_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.489434 27068 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611ef6e01723628610487819843_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611ef6e01723628610487819843_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 09:43:30.489491 27067 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611ef6e01723628610487819843_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.489511 27067 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.490702 27067 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611ef6e01723628610487819843_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x611ef6e01723628610487819843_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 09:43:30.490742 27067 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x611ef6e01723628610487819843_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.490762 27067 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.491276 27067 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x611ef6e01723628610487819843_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 09:43:30.491322 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x611ef850) got event_name: TaskCompletion
1884: I0814 09:43:30.491341 27007 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.524996 27063 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 12688117890663693436 to 17628551397842200998 , after update, data is {current : 0, peak : 800768}.
1884: I0814 09:43:30.525015 27063 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 12688117890663693436 to 2983379759911535213 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 09:43:30.525020 27063 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 12688117890663693436 to 2983379759911535213 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 09:43:30.525180 27067 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2983379759911535213 to 2937561976808406820 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 09:43:30.525192 27067 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 2983379759911535213 to 2937561976808406820 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 09:43:30.525380 27068 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0814 09:43:30.525393 27068 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.530079 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.530105 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.530164 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.530171 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.531913 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.532263 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.532276 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.532281 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.533813 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.533905 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.533915 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.533921 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x611dcbd0 type is 7
1884: I0814 09:43:30.533929 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.533934 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3412530 type is 7
1884: I0814 09:43:30.533939 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.533943 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.533998 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.534005 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.534009 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.534013 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.534065 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.534080 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.534142 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.534149 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.534163 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.534307 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.534319 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.534335 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.534343 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x611f7920Variable Type 7
1884: I0814 09:43:30.534358 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.534375 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.534394 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.534408 27007 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.536022 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.536055 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.536264 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.540863 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x61431c50 for it.
1884: I0814 09:43:30.541050 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 09:43:30.544103 27007 pir_interpreter.cc:161] PirInterpreter(): 0x611f66e0 on Place(gpu:0)
1884: I0814 09:43:30.544138 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.544162 27007 scope.cc:202] Create variable 0x611f66e01723628610544129220_inner_var_1
1884: I0814 09:43:30.544173 27007 scope.cc:202] Create variable 0x611f66e01723628610544129220_inner_var_2
1884: I0814 09:43:30.544181 27007 scope.cc:202] Create variable 0x611f66e01723628610544129220_inner_var_3
1884: I0814 09:43:30.544191 27007 scope.cc:202] Create variable 0x611f66e01723628610544129220_inner_var_4
1884: I0814 09:43:30.544200 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.544538 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.544553 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.544557 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6346bf10
1884: 1 -> 0x611f66e01723628610544129220_inner_var_1 -> 0x61112d70
1884: 2 -> 0x611f66e01723628610544129220_inner_var_2 -> 0x611d0bd0
1884: 3 -> 0x611f66e01723628610544129220_inner_var_3 -> 0x5b088ec0
1884: 4 -> 0x611f66e01723628610544129220_inner_var_4 -> 0x5aa99b30
1884: 5 -> fetch0@fetch -> 0x611de230
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.545250 27069 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.545343 27070 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.545367 27071 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.545419 27072 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.545476 27073 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.545500 27072 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611f66e01723628610544129220_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.545537 27074 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.545588 27074 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.545609 27072 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611f66e01723628610544129220_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.545635 27074 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 09:43:30.545676 27074 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611f66e01723628610544129220_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611f66e01723628610544129220_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.545840 27074 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x611f66e01723628610544129220_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x611f66e01723628610544129220_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.545902 27072 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611f66e01723628610544129220_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.545928 27072 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.548714 27072 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x611f66e01723628610544129220_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x611f66e01723628610544129220_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.548763 27072 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x611f66e01723628610544129220_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.548789 27072 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.550849 27072 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x611f66e01723628610544129220_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 09:43:30.550899 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x611f6850) got event_name: TaskCompletion
1884: I0814 09:43:30.550923 27007 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.596108 27069 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 17628551397842200998 to 12688117890663693436 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 09:43:30.596130 27069 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 17628551397842200998 to 13975936553071589987 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 09:43:30.596136 27069 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 17628551397842200998 to 13975936553071589987 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 09:43:30.596314 27072 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 09:43:30.596329 27072 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 13975936553071589987 to 2937561976808406820 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 09:43:30.596508 27074 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 12688117890663693436 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.600526 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.600557 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.600611 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.600618 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.602355 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.602698 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.602711 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.602716 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.604241 27007 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 09:43:30.604346 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.604357 27007 scope.cc:202] Create variable Out
1884: I0814 09:43:30.604363 27007 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x611fb810 type is 7
1884: I0814 09:43:30.604372 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.604375 27007 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x613d2790 type is 7
1884: I0814 09:43:30.604380 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.604384 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.604439 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.604444 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.604449 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.604452 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.604504 27007 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.604518 27007 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.604580 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.604588 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.604602 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.604647 27007 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.604779 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.604833 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.604842 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.604858 27007 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.604866 27007 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x611dab90Variable Type 7
1884: I0814 09:43:30.604880 27007 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.604899 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.604920 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.604935 27007 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.605033 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.605057 27007 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.605254 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.606034 27007 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44fa7f60 for it.
1884: I0814 09:43:30.606249 27007 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x61431c50 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 09:43:30.609243 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6301c490 on Place(gpu:0)
1884: I0814 09:43:30.609279 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.609310 27007 scope.cc:202] Create variable 0x6301c4901723628610609269526_inner_var_1
1884: I0814 09:43:30.609323 27007 scope.cc:202] Create variable 0x6301c4901723628610609269526_inner_var_2
1884: I0814 09:43:30.609331 27007 scope.cc:202] Create variable 0x6301c4901723628610609269526_inner_var_3
1884: I0814 09:43:30.609342 27007 scope.cc:202] Create variable 0x6301c4901723628610609269526_inner_var_4
1884: I0814 09:43:30.609351 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.609678 27007 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 09:43:30.609694 27007 scope.cc:202] Create variable X
1884: I0814 09:43:30.609697 27007 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61216540
1884: 1 -> 0x6301c4901723628610609269526_inner_var_1 -> 0x613d0d30
1884: 2 -> 0x6301c4901723628610609269526_inner_var_2 -> 0x611f17f0
1884: 3 -> 0x6301c4901723628610609269526_inner_var_3 -> 0x50e1e20
1884: 4 -> 0x6301c4901723628610609269526_inner_var_4 -> 0x3ae0140
1884: 5 -> fetch0@fetch -> 0x1aae6790
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 09:43:30.610401 27075 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.610481 27076 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.610502 27077 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.610538 27078 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.610565 27079 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.610610 27080 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.610603 27079 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6301c4901723628610609269526_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.610634 27080 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.610657 27079 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6301c4901723628610609269526_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.610666 27080 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 09:43:30.610695 27080 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6301c4901723628610609269526_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6301c4901723628610609269526_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.610736 27080 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.610844 27080 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.610869 27080 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6301c4901723628610609269526_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6301c4901723628610609269526_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 09:43:30.610934 27079 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6301c4901723628610609269526_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.610955 27079 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.611096 27079 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6301c4901723628610609269526_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6301c4901723628610609269526_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 09:43:30.611122 27079 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6301c4901723628610609269526_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.611143 27079 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.611157 27079 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6301c4901723628610609269526_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 09:43:30.611191 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x6301c600) got event_name: TaskCompletion
1884: I0814 09:43:30.611212 27007 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.643329 27075 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 12688117890663693436 to 17628551397842200998 , after update, data is {current : 0, peak : 10240}.
1884: I0814 09:43:30.643344 27075 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12688117890663693436 to 17628551397842200998 , after update, data is {current : -804, peak : 8000}.
1884: I0814 09:43:30.643350 27075 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12688117890663693436 to 17628551397842200998 , after update, data is {current : -804, peak : 8000}.
1884: I0814 09:43:30.643553 27079 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 2983379759911535213 to 17628551397842200998 , after update, data is {current : 800, peak : 8000}.
1884: I0814 09:43:30.643565 27079 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 2983379759911535213 to 17628551397842200998 , after update, data is {current : 800, peak : 8000}.
1884: I0814 09:43:30.643724 27080 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 09:43:30.643738 27080 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 09:43:30.643743 27080 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 17628551397842200998 to 2937561976808406820 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 09:43:30.650182 27007 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0814 09:43:30.650234 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 09:43:30.651332 27007 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 09:43:30.652179 27007 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0814 09:43:30.652208 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 09:43:30.653534 27007 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0814 09:43:30.653558 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 09:43:30.654258 27007 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0814 09:43:30.655243 27007 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0814 09:43:30.655267 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 09:43:30.656625 27007 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0814 09:43:30.656647 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 09:43:30.657234 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.657259 27007 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 09:43:30.657265 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.657272 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.659245 27007 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0814 09:43:30.659268 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 09:43:30.660229 27007 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0814 09:43:30.660257 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 09:43:30.661242 27007 pybind.cc:1827] need skip: 0
1884: I0814 09:43:30.661550 27007 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 09:43:30.663314 27007 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 09:43:30.666919 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.666937 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.666942 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.668943 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.668963 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.668972 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.668977 27007 scope.cc:202] Create variable learning_rate_0
1884: I0814 09:43:30.668984 27007 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x611be010 type is 7
1884: I0814 09:43:30.668988 27007 scope.cc:202] Create variable linear_0.b_0
1884: I0814 09:43:30.668992 27007 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63b25e10 type is 7
1884: I0814 09:43:30.668998 27007 scope.cc:202] Create variable linear_0.w_0
1884: I0814 09:43:30.669000 27007 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x611bdf80 type is 7
1884: I0814 09:43:30.669064 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.669070 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.669075 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.669078 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.669133 27007 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669148 27007 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669173 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 09:43:30.669322 27007 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669332 27007 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669397 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.669446 27007 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669456 27007 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.669482 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.670478 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.671816 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.672260 27007 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 09:43:30.672492 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.672784 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.673002 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.673018 27007 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 09:43:30.673081 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.673089 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.673094 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.673189 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.673202 27007 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 09:43:30.674782 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.676113 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.677202 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.677399 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.677412 27007 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 09:43:30.677417 27007 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x6346c8c0 type is 7
1884: I0814 09:43:30.677423 27007 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 09:43:30.677428 27007 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x62c7d450 type is 7
1884: I0814 09:43:30.677433 27007 scope.cc:202] Create variable cast_0.tmp_0
1884: I0814 09:43:30.677435 27007 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x62c7d540 type is 7
1884: I0814 09:43:30.677439 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.677444 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.677448 27007 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 09:43:30.677451 27007 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x6346cc90 type is 7
1884: I0814 09:43:30.677455 27007 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x611be010 type is 7
1884: I0814 09:43:30.677460 27007 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63b25e10 type is 7
1884: I0814 09:43:30.677464 27007 scope.cc:202] Create variable linear_0.tmp_0
1884: I0814 09:43:30.677469 27007 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x6346cc70 type is 7
1884: I0814 09:43:30.677474 27007 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 09:43:30.677476 27007 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x6346d1d0 type is 7
1884: I0814 09:43:30.677479 27007 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x611bdf80 type is 7
1884: I0814 09:43:30.677484 27007 scope.cc:202] Create variable mean_0.tmp_0
1884: I0814 09:43:30.677486 27007 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x6346d440 type is 7
1884: I0814 09:43:30.677490 27007 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0814 09:43:30.677493 27007 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x6346d680 type is 7
1884: I0814 09:43:30.677497 27007 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 09:43:30.677500 27007 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6346d8e0 type is 7
1884: I0814 09:43:30.677583 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.677597 27007 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 09:43:30.677654 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.677660 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.677665 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.677668 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.677716 27007 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.677727 27007 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.677742 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 09:43:30.677867 27007 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.677878 27007 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.677897 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 09:43:30.677971 27007 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 09:43:30.678048 27007 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0814 09:43:30.679181 27007 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679200 27007 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679263 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.679339 27007 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679352 27007 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679366 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 09:43:30.679391 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.679431 27007 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679440 27007 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679452 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 09:43:30.679539 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679548 27007 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679561 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.679673 27007 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.679754 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.679814 27007 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679824 27007 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679838 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 09:43:30.679874 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.679927 27007 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679936 27007 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.679950 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 09:43:30.680068 27007 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.680078 27007 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.680099 27007 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.680142 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.680150 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.680166 27007 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.680173 27007 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63015670Variable Type 7
1884: I0814 09:43:30.680189 27007 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.680207 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.680225 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.680238 27007 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.680279 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.680310 27007 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 09:43:30.680336 27007 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.680346 27007 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.680358 27007 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 09:43:30.680364 27007 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6300faa0Variable Type 7
1884: I0814 09:43:30.680377 27007 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 09:43:30.680389 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.680403 27007 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.680415 27007 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.680447 27007 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 09:43:30.680461 27007 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0814 09:43:30.680876 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 09:43:30.680912 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 09:43:30.680928 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 09:43:30.680960 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 09:43:30.680992 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.681010 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 09:43:30.685021 27007 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 09:43:30.685057 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 09:43:30.685760 27007 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 09:43:30.685782 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 09:43:30.686105 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.687772 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.688616 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.688735 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.689236 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.690138 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.692201 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.693250 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.695119 27007 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0814 09:43:30.695849 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.695865 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.695870 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.697080 27007 interpreter_util.cc:1169] Creating Variables
1884: I0814 09:43:30.697098 27007 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x36db520 type is 9
1884: I0814 09:43:30.697104 27007 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f14240 type is 10
1884: I0814 09:43:30.697109 27007 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63b25e10 type is 7
1884: I0814 09:43:30.697113 27007 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x611bdf80 type is 7
1884: I0814 09:43:30.697118 27007 scope.cc:202] Create variable saved_params
1884: I0814 09:43:30.697122 27007 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x62c637a0 type is 17
1884: I0814 09:43:30.697150 27007 interpreter_util.cc:594] Static build: 0
1884: I0814 09:43:30.697156 27007 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 09:43:30.697160 27007 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 09:43:30.697165 27007 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 09:43:30.697202 27007 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.697214 27007 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 09:43:30.697940 27007 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 09:43:30.697979 27007 analysis_predictor.cc:433] Predictor::init()
1884: I0814 09:43:30.698037 27007 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0814 09:43:30.699193 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.699250 27007 scope.cc:202] Create variable feed
1884: I0814 09:43:30.699256 27007 naive_executor.cc:189] 0x63b16460 Create persistable variable feed, which pointer is 0x63891b40
1884: I0814 09:43:30.699263 27007 scope.cc:202] Create variable fetch
1884: I0814 09:43:30.699265 27007 naive_executor.cc:189] 0x63b16460 Create persistable variable fetch, which pointer is 0x638919e0
1884: I0814 09:43:30.699270 27007 scope.cc:202] Create variable linear_0.b_0
1884: I0814 09:43:30.699272 27007 naive_executor.cc:189] 0x63b16460 Create persistable variable linear_0.b_0, which pointer is 0x6300d510
1884: I0814 09:43:30.699277 27007 scope.cc:202] Create variable linear_0.w_0
1884: I0814 09:43:30.699280 27007 naive_executor.cc:189] 0x63b16460 Create persistable variable linear_0.w_0, which pointer is 0x63b16540
1884: I0814 09:43:30.699296 27007 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0814 09:43:30.699656 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.699741 27007 program_converter.cc:296] is_legacy_program : 0
1884: I0814 09:43:30.699785 27007 executor.cc:183] Old Executor is Running.
1884: I0814 09:43:30.699854 27007 executor.cc:92] Creating Variables for block 0
1884: I0814 09:43:30.699862 27007 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0814 09:43:30.699867 27007 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x6300d510 type is 7
1884: I0814 09:43:30.699869 27007 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0814 09:43:30.699872 27007 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x63b16540 type is 7
1884: I0814 09:43:30.699905 27007 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.699981 27007 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0814 09:43:30.700023 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.700028 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.700162 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.700269 27007 graph.cc:149] create OpNode by feed
1884: I0814 09:43:30.700313 27007 graph.cc:149] create OpNode by matmul_v2
1884: I0814 09:43:30.700330 27007 graph.cc:149] create OpNode by elementwise_add
1884: I0814 09:43:30.700345 27007 graph.cc:149] create OpNode by abs
1884: I0814 09:43:30.700356 27007 graph.cc:149] create OpNode by assign_value
1884: I0814 09:43:30.700371 27007 graph.cc:149] create OpNode by multinomial
1884: I0814 09:43:30.700381 27007 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 09:43:30.700394 27007 graph.cc:149] create OpNode by scale
1884: I0814 09:43:30.700407 27007 graph.cc:149] create OpNode by scale
1884: I0814 09:43:30.700418 27007 graph.cc:149] create OpNode by fetch
1884: I0814 09:43:30.700435 27007 graph.cc:149] create OpNode by fetch
1884: I0814 09:43:30.700455 27007 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0814 09:43:30.701628 27007 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0814 09:43:30.701637 27007 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0814 09:43:30.701704 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.701710 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0814 09:43:30.701828 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702069 27007 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 09:43:30.702133 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702138 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0814 09:43:30.702172 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702178 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0814 09:43:30.702214 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702273 27007 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 09:43:30.702311 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702316 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0814 09:43:30.702332 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702344 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.702369 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702374 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0814 09:43:30.702417 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702438 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.702461 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702466 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0814 09:43:30.702505 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702582 27007 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 09:43:30.702610 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702615 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0814 09:43:30.702647 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702667 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.702689 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702693 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0814 09:43:30.702721 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702867 27007 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 09:43:30.702896 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702901 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0814 09:43:30.702934 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.702950 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.702973 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.702978 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0814 09:43:30.703001 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.703016 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.703037 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.703042 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0814 09:43:30.703065 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.703078 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.703099 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.703104 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0814 09:43:30.703130 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.703199 27007 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 09:43:30.703228 27007 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 09:43:30.703243 27007 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 09:43:30.703256 27007 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0814 09:43:30.703281 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.703286 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0814 09:43:30.703315 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.703356 27007 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 09:43:30.703377 27007 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 09:43:30.703388 27007 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 09:43:30.703400 27007 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 09:43:30.703429 27007 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 09:43:30.703438 27007 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0814 09:43:30.704592 27007 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 09:43:30.704635 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.704641 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0814 09:43:30.704667 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.704689 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.704715 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.704718 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0814 09:43:30.704746 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.704793 27007 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 09:43:30.704823 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.704828 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0814 09:43:30.704849 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.704864 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.704887 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.704891 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0814 09:43:30.704922 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705004 27007 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0814 09:43:30.705024 27007 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 09:43:30.705039 27007 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 09:43:30.705053 27007 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 09:43:30.705068 27007 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 09:43:30.705082 27007 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 09:43:30.705097 27007 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0814 09:43:30.705121 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705189 27007 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0814 09:43:30.705209 27007 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 09:43:30.705222 27007 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 09:43:30.705235 27007 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 09:43:30.705250 27007 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 09:43:30.705262 27007 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 09:43:30.705276 27007 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0814 09:43:30.705329 27007 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 09:43:30.705597 27007 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 09:43:30.705626 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.705631 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0814 09:43:30.705678 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705736 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.705772 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705819 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.705847 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705889 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.705914 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.705951 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.705972 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706007 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706024 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706055 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706071 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706099 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706111 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706135 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706146 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706163 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706188 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706193 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0814 09:43:30.706223 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706260 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706285 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706290 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0814 09:43:30.706328 27007 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 09:43:30.706333 27007 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 09:43:30.706379 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706401 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706427 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706432 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0814 09:43:30.706444 27007 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 09:43:30.706446 27007 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 09:43:30.706486 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706508 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706532 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706537 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0814 09:43:30.706547 27007 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 09:43:30.706550 27007 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 09:43:30.706583 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706600 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706622 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706627 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0814 09:43:30.706636 27007 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 09:43:30.706638 27007 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 09:43:30.706678 27007 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 09:43:30.706698 27007 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 09:43:30.706722 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706727 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0814 09:43:30.706739 27007 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0814 09:43:30.706780 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.706785 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0814 09:43:30.706852 27007 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 09:43:30.706872 27007 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.706889 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 09:43:30.706944 27007 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0814 09:43:30.706961 27007 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 09:43:30.706990 27007 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 09:43:30.707011 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.707016 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0814 09:43:30.707885 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.707902 27007 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0814 09:43:30.707953 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.707958 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.708571 27007 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0814 09:43:30.708781 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.708850 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.708856 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.709260 27007 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 09:43:30.709470 27007 graph.h:183] deleting __fuse_statis__
1884: I0814 09:43:30.709478 27007 graph.h:183] deleting pass_recorder
1884: I0814 09:43:30.709484 27007 graph.h:183] deleting stale_program_op_descs
1884: I0814 09:43:30.709564 27007 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0814 09:43:30.709573 27007 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 09:43:30.709578 27007 naive_executor.cc:195] 0x63b16460 Create variable abs_0.tmp_0, which pointer is 0x634549c0
1884: I0814 09:43:30.709583 27007 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 09:43:30.709586 27007 naive_executor.cc:195] 0x63b16460 Create variable gaussian_0.tmp_0, which pointer is 0x639e8190
1884: I0814 09:43:30.709597 27007 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 09:43:30.709602 27007 naive_executor.cc:195] 0x63b16460 Create variable linear_0.tmp_1, which pointer is 0x62d123c0
1884: I0814 09:43:30.709606 27007 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 09:43:30.709609 27007 naive_executor.cc:195] 0x63b16460 Create variable multinomial_0.tmp_0, which pointer is 0x62d11e60
1884: I0814 09:43:30.709612 27007 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0814 09:43:30.709615 27007 naive_executor.cc:195] 0x63b16460 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62d12160
1884: I0814 09:43:30.709619 27007 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0814 09:43:30.709622 27007 naive_executor.cc:195] 0x63b16460 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62d10760
1884: I0814 09:43:30.709630 27007 scope.cc:202] Create variable feed
1884: I0814 09:43:30.709633 27007 scope.cc:202] Create variable fetch
1884: I0814 09:43:30.709656 27007 naive_executor.cc:46] NaiveExecutor init with scope 0x63b16460
1884: I0814 09:43:30.709662 27007 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0814 09:43:30.709851 27007 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 09:43:30.709864 27007 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 09:43:30.709892 27007 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0814 09:43:30.709896 27007 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0814 09:43:30.709903 27007 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.709934 27007 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 09:43:30.710134 27007 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.710148 27007 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 09:43:30.710193 27007 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.710220 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0814 09:43:30.735474 27007 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0814 09:43:30.735522 27007 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.735538 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 09:43:30.735572 27007 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 09:43:30.735595 27007 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.735611 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 09:43:30.735654 27007 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 09:43:30.735687 27007 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.735699 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 09:43:30.735739 27007 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 09:43:30.735760 27007 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 09:43:30.735771 27007 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 09:43:30.735798 27007 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 09:43:30.735812 27007 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.735831 27007 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 09:43:30.735848 27007 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 09:43:30.736114 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.736121 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0814 09:43:30.782124 27007 pir_interpreter.cc:161] PirInterpreter(): 0x611e1fd0 on Place(gpu:0)
1884: I0814 09:43:30.782164 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_0
1884: I0814 09:43:30.782181 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_1
1884: I0814 09:43:30.782191 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_2
1884: I0814 09:43:30.782199 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_3
1884: I0814 09:43:30.782222 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_4
1884: I0814 09:43:30.782236 27007 scope.cc:202] Create variable 0x611e1fd01723628610782153505_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x611e1fd01723628610782153505_inner_var_0 -> 0x611e32f0
1884: 1 -> 0x611e1fd01723628610782153505_inner_var_1 -> 0x60f7c840
1884: 2 -> 0x611e1fd01723628610782153505_inner_var_2 -> 0x639e9880
1884: 3 -> linear_1.w_0 -> 0x611fbfd0
1884: 4 -> linear_1.b_0 -> 0x613ac550
1884: 5 -> learning_rate_1 -> 0x639e9620
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 09:43:30.783042 27081 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.783056 27082 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.783077 27083 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.783111 27084 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.783150 27085 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.783149 27084 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783160 27083 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783177 27085 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783171 27082 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783201 27084 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 09:43:30.783216 27083 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.783226 27085 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.783224 27082 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x611e1fd01723628610782153505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.783272 27085 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0814 09:43:30.783294 27085 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783317 27085 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.783329 27085 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 09:43:30.783345 27085 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x611e1fd01723628610782153505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x611e1fd01723628610782153505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x611e1fd01723628610782153505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.783401 27085 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x611e1fd01723628610782153505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x611e1fd01723628610782153505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x611e1fd01723628610782153505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0814 09:43:30.783455 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x611e2140) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0814 09:43:30.785440 27007 pir_interpreter.cc:161] PirInterpreter(): 0x63014e00 on Place(gpu:0)
1884: I0814 09:43:30.785475 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_1
1884: I0814 09:43:30.785491 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_4
1884: I0814 09:43:30.785501 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_5
1884: I0814 09:43:30.785507 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_6
1884: I0814 09:43:30.785530 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_7
1884: I0814 09:43:30.785542 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_8
1884: I0814 09:43:30.785548 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_9
1884: I0814 09:43:30.785576 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_10
1884: I0814 09:43:30.785586 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_11
1884: I0814 09:43:30.785594 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_12
1884: I0814 09:43:30.785604 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_13
1884: I0814 09:43:30.785612 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_14
1884: I0814 09:43:30.785621 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_15
1884: I0814 09:43:30.785629 27007 scope.cc:202] Create variable fetch0@fetch
1884: I0814 09:43:30.785640 27007 scope.cc:202] Create variable 0x63014e001723628610785462853_inner_var_17
1884: I0814 09:43:30.785650 27007 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x639e9620
1884: 1 -> 0x63014e001723628610785462853_inner_var_1 -> 0x44f97600
1884: 2 -> linear_1.b_0 -> 0x613ac550
1884: 3 -> linear_1.w_0 -> 0x611fbfd0
1884: 4 -> 0x63014e001723628610785462853_inner_var_4 -> 0x639fb700
1884: 5 -> 0x63014e001723628610785462853_inner_var_5 -> 0x631575a0
1884: 6 -> 0x63014e001723628610785462853_inner_var_6 -> 0x63176250
1884: 7 -> 0x63014e001723628610785462853_inner_var_7 -> 0x611fab70
1884: 8 -> 0x63014e001723628610785462853_inner_var_8 -> 0x63157310
1884: 9 -> 0x63014e001723628610785462853_inner_var_9 -> 0x613f0130
1884: 10 -> 0x63014e001723628610785462853_inner_var_10 -> 0x613a7160
1884: 11 -> 0x63014e001723628610785462853_inner_var_11 -> 0x611e3580
1884: 12 -> 0x63014e001723628610785462853_inner_var_12 -> 0x611cbb50
1884: 13 -> 0x63014e001723628610785462853_inner_var_13 -> 0x6aae310
1884: 14 -> 0x63014e001723628610785462853_inner_var_14 -> 0x4442ad80
1884: 15 -> 0x63014e001723628610785462853_inner_var_15 -> 0x613d3540
1884: 16 -> fetch0@fetch -> 0x611cee20
1884: 17 -> 0x63014e001723628610785462853_inner_var_17 -> 0x42f2bfc0
1884: 18 -> fetch1@fetch -> 0x42f2a400
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0814 09:43:30.787268 27086 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.787384 27087 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.787400 27088 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.787464 27089 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.787472 27088 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787477 27087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787494 27088 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 09:43:30.787503 27090 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 09:43:30.787505 27087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.787549 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787585 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 09:43:30.787612 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787658 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.787707 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 09:43:30.787724 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 09:43:30.787783 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 09:43:30.787801 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63014e001723628610785462853_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787842 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63014e001723628610785462853_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 09:43:30.787869 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63014e001723628610785462853_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.787902 27090 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 09:43:30.787940 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63014e001723628610785462853_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.787967 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63014e001723628610785462853_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63014e001723628610785462853_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788005 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.788020 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63014e001723628610785462853_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63014e001723628610785462853_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.788050 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x63014e001723628610785462853_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788071 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.788071 27087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63014e001723628610785462853_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788084 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x63014e001723628610785462853_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.788094 27087 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.788100 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63014e001723628610785462853_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788123 27090 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.788166 27087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63014e001723628610785462853_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.788197 27087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63014e001723628610785462853_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788211 27090 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.788215 27087 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.788229 27087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63014e001723628610785462853_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.788259 27090 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.788328 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.788347 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63014e001723628610785462853_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63014e001723628610785462853_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.788381 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x63014e001723628610785462853_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788388 27087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63014e001723628610785462853_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788403 27087 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 09:43:30.788408 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.788426 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x63014e001723628610785462853_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.788434 27087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63014e001723628610785462853_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.788445 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x63014e001723628610785462853_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788453 27087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63014e001723628610785462853_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788466 27087 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.788481 27087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63014e001723628610785462853_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.788515 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x63014e001723628610785462853_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 09:43:30.788537 27090 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63014e001723628610785462853_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63014e001723628610785462853_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.788563 27090 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 09:43:30.788575 27090 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63014e001723628610785462853_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63014e001723628610785462853_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63014e001723628610785462853_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 09:43:30.788610 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x63014f70) got event_name: TaskCompletion
1884: I0814 09:43:30.788633 27007 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.788661 27007 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 09:43:30.794014 27007 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 09:43:30.794060 27007 analysis_predictor.cc:433] Predictor::init()
1884: I0814 09:43:30.794734 27007 scope.cc:202] Create variable linear_1.b_0
1884: I0814 09:43:30.794781 27007 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0814 09:43:30.795200 27007 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107952522990"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107952522990"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0814 09:43:30.795392 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6345ea20 on Place(cpu)
1884: I0814 09:43:30.795413 27007 scope.cc:202] Create variable 0x6345ea201723628610795406056_inner_var_0
1884: I0814 09:43:30.795439 27007 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107952522990"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236286107952522990 -> 0x638a0550
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0814 09:43:30.795574 27007 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0814 09:43:30.795691 27091 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.795805 27092 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.795823 27093 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.795892 27092 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236286107952522990:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.795918 27094 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.795928 27095 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.795924 27092 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236286107952522990:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 09:43:30.795946 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x6345eb90) got event_name: TaskCompletion
1884: I0814 09:43:30.796164 27092 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 12201711216000862655 to 3972107547345422068 , after update, data is {current : -4, peak : 104}.
1884: I0814 09:43:30.796172 27092 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 12201711216000862655 to 3972107547345422068 , after update, data is {current : -4, peak : 104}.
1884: I0814 09:43:30.796241 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.796249 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107963408101"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107963408101"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 09:43:30.796479 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6345ea20 on Place(cpu)
1884: I0814 09:43:30.796499 27007 scope.cc:202] Create variable 0x6345ea201723628610796493509_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107963408101"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236286107963408101 -> 0x613efca0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 09:43:30.796718 27096 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.796754 27097 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.796782 27098 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.796802 27099 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.796829 27100 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.796826 27099 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236286107963408101:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.796877 27099 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236286107963408101:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.796902 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x6345eb90) got event_name: TaskCompletion
1884: I0814 09:43:30.797066 27099 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11674972687230054370 to 3972107547345422068 , after update, data is {current : 4, peak : 104}.
1884: I0814 09:43:30.797075 27099 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11674972687230054370 to 3972107547345422068 , after update, data is {current : 4, peak : 104}.
1884: I0814 09:43:30.797173 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.797180 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236286107963408101",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236286107972528502"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236286107963408101",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236286107972528502"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 09:43:30.797411 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6345ea20 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236286107963408101",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236286107972528502"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236286107972528502 -> 0x613efca0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0814 09:43:30.797662 27101 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.797716 27102 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.797734 27103 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.797767 27104 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.797791 27105 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.797788 27104 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.797812 27104 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.797835 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x6345eb90) got event_name: TaskCompletion
1884: I0814 09:43:30.798110 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.798115 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107981857693"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107981857693"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0814 09:43:30.798323 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6345ea20 on Place(cpu)
1884: I0814 09:43:30.798343 27007 scope.cc:202] Create variable 0x6345ea201723628610798337718_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236286107981857693"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236286107981857693 -> 0x63176ce0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 09:43:30.798552 27106 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.798593 27107 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 09:43:30.798609 27108 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 09:43:30.798637 27109 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 09:43:30.798664 27110 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 09:43:30.798660 27109 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236286107981857693:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.798694 27109 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236286107981857693:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 09:43:30.798719 27007 pir_interpreter.cc:1766] main_thread_blocker_(0x6345eb90) got event_name: TaskCompletion
1884: I0814 09:43:30.798882 27109 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 11674972687230054370 to 3972107547345422068 , after update, data is {current : 8, peak : 104}.
1884: I0814 09:43:30.798890 27109 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 11674972687230054370 to 3972107547345422068 , after update, data is {current : 8, peak : 104}.
1884: I0814 09:43:30.798981 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.798988 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.799048 27007 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0814 09:43:30.799103 27007 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0814 09:43:30.799137 27007 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107981857693"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107972528502"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107981857693"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107972528502"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0814 09:43:30.799806 27007 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0814 09:43:30.799824 27007 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0814 09:43:30.799856 27007 pir_interpreter.cc:161] PirInterpreter(): 0x6345ea20 on Place(cpu)
1884: I0814 09:43:30.799886 27007 scope.cc:202] Create variable feed_name_0
1884: I0814 09:43:30.799901 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_5
1884: I0814 09:43:30.799922 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_6
1884: I0814 09:43:30.799933 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_7
1884: I0814 09:43:30.799943 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_8
1884: I0814 09:43:30.799963 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_9
1884: I0814 09:43:30.799974 27007 scope.cc:202] Create variable 0x6345ea201723628610799871406_inner_var_10
1884: I0814 09:43:30.799999 27007 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.800020 27007 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 09:43:30.800138 27007 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.800153 27007 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107981857693"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236286107972528502"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236286107981857693 -> 0x63176ce0
1884: 1 -> constant_folding@_17236286107972528502 -> 0x613efca0
1884: 2 -> linear_1.b_0 -> 0x611e3f80
1884: 3 -> linear_1.w_0 -> 0x62d52410
1884: 4 -> feed_name_0 -> 0x612003a0
1884: 5 -> 0x6345ea201723628610799871406_inner_var_5 -> 0x63897030
1884: 6 -> 0x6345ea201723628610799871406_inner_var_6 -> 0x44f96bf0
1884: 7 -> 0x6345ea201723628610799871406_inner_var_7 -> 0x611e3c60
1884: 8 -> 0x6345ea201723628610799871406_inner_var_8 -> 0x6388fc30
1884: 9 -> fetch_name_0 -> 0x638a0a50
1884: 10 -> fetch_name_1 -> 0x613efa50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 09:43:30.800724 27007 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0814 09:43:30.800788 27111 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 09:43:30.800783 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.800833 27007 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 09:43:30.800858 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.800889 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x6345ea201723628610799871406_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x6345ea201723628610799871406_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.800928 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x6345ea201723628610799871406_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.800966 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.800987 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.801005 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.801035 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236286107972528502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.801062 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236286107981857693:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.801092 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236286107981857693:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6345ea201723628610799871406_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 09:43:30.801121 27007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236286107981857693:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 09:43:30.801148 27007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236286107981857693:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6345ea201723628610799871406_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 09:43:30.801175 27007 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 09:43:30.801196 27007 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 09:43:30.801219 27007 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 09:43:30.801353 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.801362 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.801414 27111 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 12201711216000862655 to 3972107547345422068 , after update, data is {current : -184, peak : 104}.
1884: I0814 09:43:30.801421 27111 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 12201711216000862655 to 3972107547345422068 , after update, data is {current : -184, peak : 104}.
1884: I0814 09:43:30.801457 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:30.801465 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:30.803263 27007 mmap_allocator.cc:348] PID: 27007, MemoryMapFdSet: set size - 0
1884: I0814 09:43:30.814961 27007 mmap_allocator.cc:348] PID: 27007, MemoryMapFdSet: set size - 0
1884: I0814 09:43:30.882812 27082 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2983379759911535213 to 3972107547345422068 , after update, data is {current : -180, peak : 104}.
1884: I0814 09:43:30.882848 27082 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2983379759911535213 to 3972107547345422068 , after update, data is {current : -180, peak : 104}.
1884: I0814 09:43:30.882855 27084 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18089707341252561786 to 3972107547345422068 , after update, data is {current : -164, peak : 104}.
1884: I0814 09:43:30.882879 27084 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18089707341252561786 to 3972107547345422068 , after update, data is {current : -164, peak : 104}.
1884: I0814 09:43:30.882884 27083 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5966808960206016351 to 3972107547345422068 , after update, data is {current : -160, peak : 104}.
1884: I0814 09:43:30.882903 27083 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5966808960206016351 to 3972107547345422068 , after update, data is {current : -160, peak : 104}.
1884: I0814 09:43:30.883031 27085 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 13590641181493478747 to 3972107547345422068 , after update, data is {current : -184, peak : 104}.
1884: I0814 09:43:30.883046 27085 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 13590641181493478747 to 3972107547345422068 , after update, data is {current : -184, peak : 104}.
1884: I0814 09:43:30.883052 27085 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 13590641181493478747 to 3972107547345422068 , after update, data is {current : 1280, peak : 1536}.
1884: I0814 09:43:30.883327 27087 thread_data_registry.h:135] Add data {current : -512, peak : 0} from thread 3459243091002959541 to 3972107547345422068 , after update, data is {current : 768, peak : 1536}.
1884: I0814 09:43:30.883337 27087 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 3459243091002959541 to 3972107547345422068 , after update, data is {current : 12, peak : 268}.
1884: I0814 09:43:30.883343 27087 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 3459243091002959541 to 3972107547345422068 , after update, data is {current : 12, peak : 268}.
1884: I0814 09:43:30.883404 27088 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 7004495507929671157 to 3972107547345422068 , after update, data is {current : 28, peak : 268}.
1884: I0814 09:43:30.883412 27088 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 7004495507929671157 to 3972107547345422068 , after update, data is {current : 28, peak : 268}.
1884: I0814 09:43:30.883538 27090 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 3972107547345422068 to 2937561976808406820 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 09:43:30.883550 27090 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 3972107547345422068 to 2937561976808406820 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 09:43:30.883555 27090 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 3972107547345422068 to 2937561976808406820 , after update, data is {current : 1536, peak : 2401024}.
1884: I0814 09:43:31.026710 27007 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 09:43:31.026738 27007 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 09:43:31.026789 27007 mmap_allocator.cc:348] PID: 27007, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   12.31 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.49 sec
