UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:36:07.622335 17927 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:36:08.519821 17927 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=enable_collect_shape,run_kp_kernel,cublaslt_exhaustive_search_times,nccl_dir,enable_auto_rdma_trans,prim_forward,use_cinn,enable_blaslt_global_search,enable_async_trace,tracer_onednn_ops_on,fraction_of_gpu_memory_to_use,gpugraph_dedup_pull_push_mode,eager_delete_scope,print_ir,enable_tracker_all2all,cusparse_dir,reader_queue_speed_test_mode,lapack_dir,use_system_allocator,prim_all,gpugraph_offload_gather_copy_maxsize,enable_gpu_memory_usage_log,allreduce_record_one_event,fraction_of_cpu_memory_to_use,fraction_of_cuda_pinned_memory_to_use,cuda_malloc_async_pool_memory_throttle_ratio,initial_cpu_memory_in_mb,prim_enable_dynamic,alloc_fill_value,check_nan_inf,tracer_onednn_ops_off,accuracy_check_atol_fp32,use_cuda_managed_memory,gpugraph_load_node_list_into_hbm,tracer_profile_fname,gpugraph_offload_param_extends,static_runtime_data_save_path,sync_after_alloc,use_shm_cache,get_host_by_name_time,gpugraph_parallel_stream_num,low_precision_op_list,gemm_use_half_precision_compute_type,gpugraph_enable_print_op_debug,multi_node_sample_use_gpu_table,enable_exit_when_partial_worker,cuda_dir,gpugraph_enable_hbm_table_collision_stat,free_when_no_cache_hit,check_infer_symbolic,pir_apply_inplace_pass,multiple_of_cupti_buffer_size,jit_engine_type,all_blocks_convert_trt,prim_backward,enable_pir_with_pt_in_dy2st,einsum_opt,ir_inplace_kernel_blacklist,gpu_memory_limit_mb,custom_device_mem_record,pir_subgraph_saving_dir,convert_all_blocks,enable_dependency_builder_debug_info,use_virtual_memory_auto_growth,gpugraph_slot_feasign_max_num,dataloader_use_file_descriptor,nccl_blocking_wait,enable_pir_in_executor_trace_run,log_memory_stats,apply_pass_to_program,enable_neighbor_list_use_uva,accuracy_check_atol_bf16,gpugraph_force_device_batch_num_equal,use_autotune,cusparselt_dir,enable_gpu_memory_usage_log_mb,enable_unused_var_check,new_executor_sequential_run,benchmark_nccl,enable_record_memory,logging_pir_py_code_int_tensor_element_limit,conv2d_disable_cudnn,fleet_executor_with_standalone,dist_threadpool_size,op_dir,async_trace_count,gpugraph_parallel_copyer_split_maxsize,enable_fuse_parallel_matmul_pass,gpu_allocator_retry_time,manually_trans_conv_filter,enable_cinn_auto_tune,use_pinned_memory,use_xqa_optim,graph_embedding_split_infer_mode,enable_auto_detect_gpu_topo,reallocate_gpu_memory_in_mb,executor_log_deps_every_microseconds,mklml_dir,new_executor_use_local_scope,enable_opt_get_features,allow_cinn_ops,enable_pir_api,cudnn_batchnorm_spatial_persistent,disable_dyshape_in_train,enable_graph_multi_node_sampling,selected_gpus,fast_eager_deletion_mode,gpugraph_enable_segment_merge_grads,new_executor_serial_run,prim_enabled,enable_pir_in_executor,enable_fusion_fallback,enable_cinn_compile_cache,mkl_dir,enable_adjust_op_order,cublaslt_device_best_config,enable_api_kernel_fallback,enable_cublas_tensor_op_math,conv_workspace_size_limit,query_dest_rank_by_multi_node,prim_check_ops,add_dependency_for_communication_op,npu_storage_format,pir_debug,cudnn_deterministic,cudnn_exhaustive_search,accuracy_check_rtol_fp16,cinn_subgraph_graphviz_dir,new_executor_use_cuda_graph,local_exe_sub_scope_limit,enable_sparse_inner_gather,logging_trunc_pir_py_code,cache_inference_while_scope,print_sub_graph_dir,gpugraph_enable_gpu_direct_access,enable_all2all_use_fp16,use_stride_kernel,initial_gpu_memory_in_mb,curand_dir,call_stack_level,cinn_compile_thread_num,prim_forward_blacklist,cusolver_dir,accuracy_check_rtol_bf16,use_auto_growth_pinned_allocator,win_cuda_bin_dir,free_idle_chunk,use_mkldnn,fuse_parameter_memory_size,gpugraph_debug_gpu_memory,gpugraph_storage_mode,pinned_memory_as_cpu_backend,paddle_num_threads,cse_max_count,sync_nccl_allreduce,use_auto_growth_v2,graph_get_neighbor_id,gpugraph_merge_grads_segment_size,enable_cinn_accuracy_check,tensor_operants_mode,pir_broadcast_tree_limit,dump_chunk_info,cupti_dir,save_static_runtime_data,benchmark,auto_growth_chunk_size_in_mb,print_allocator_trace_info,tensorrt_dir,cublas_dir,host_trace_level,logging_pir_py_code_dir,sort_sum_gradient,deny_cinn_ops,dynamic_static_unified_comm,prim_skip_dynamic,nvidia_package_dir,gpugraph_offload_param_stat,new_executor_static_build,fuse_parameter_groups_size,accuracy_check_rtol_fp32,init_allocated_mem,new_executor_use_inplace,static_executor_perfstat_filepath,enable_dump_main_program,cudnn_dir,use_fast_math,use_stream_safe_cuda_allocator,inner_op_parallelism,graph_load_in_parallel,cudnn_exhaustive_search_times,auto_free_cudagraph_allocations_on_launch,use_cuda_malloc_async_allocator,allocator_strategy,enable_interpretercore_launch_cinn,set_to_1d,enable_cse_in_dy2st,memory_fraction_of_eager_deletion,max_inplace_grad_add,check_nan_inf_level,gpugraph_sparse_table_storage_mode,embedding_deterministic,dygraph_debug,trt_ibuilder_cache,graph_neighbor_size_percent,logging_pir_py_code_dump_symbolic_dims,accuracy_check_atol_fp16,eager_delete_tensor_gb,gpugraph_hbm_table_load_factor,search_cache_max_number,pir_apply_shape_optimization_pass,check_kernel_launch,graph_metapath_split_opt,cuda_memory_async_pool_realease_threshold 
1884: I0815 04:36:08.519934 17927 init.cc:108] After Parse: argc is 2
1884: I0815 04:36:17.645092 17927 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:36:17.645149 17927 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:36:17.645833 17927 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:36:17.646229 17927 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:36:17.647063 17927 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:36:17.647161 17927 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:36:17.647255 17927 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:36:17.647934 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f957c000000), and remaining 0
1884: I0815 04:36:17.648260 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.648339 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.648423 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f957c000200), and remaining 0
1884: I0815 04:36:17.648452 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f957c000400), and remaining 0
1884: I0815 04:36:17.652208 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f957c000600), and remaining 0
1884: I0815 04:36:17.652364 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f957c000800), and remaining 0
1884: I0815 04:36:17.652436 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f957c000a00), and remaining 0
1884: I0815 04:36:17.652536 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.652562 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.652639 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.652653 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.653956 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a66e9e0 for it.
1884: I0815 04:36:17.654114 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.654139 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.654196 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f957c000e00), and remaining 0
1884: I0815 04:36:17.654273 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f957c0c4400), and remaining 0
1884: I0815 04:36:17.769799 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a66e9e0 for it.
1884: I0815 04:36:17.770020 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.770063 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.774760 17927 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f957c200000), and remaining 0
1884: I0815 04:36:17.786937 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a66e9e0 for it.
1884: I0815 04:36:17.787106 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:17.787147 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.787199 17927 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:17.787401 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:17.788434 17927 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:36:17.788450 17927 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:36:17.788499 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:17.788576 17927 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:36:17.788600 17927 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.788654 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:17.788738 17927 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:36:17.788753 17927 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.788781 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:17.788980 17927 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:36:17.788995 17927 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.789153 17927 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:36:17.789175 17927 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:17.789242 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:17.791675 17927 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:36:17.791805 17927 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:36:17.791831 17927 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:36:17.791893 17927 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:36:19.349931 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.349995 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.350255 17927 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:36:19.350275 17927 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.355357 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.355402 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.356514 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.356535 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.356549 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.357347 17927 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:36:19.357360 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.357378 17927 scope.cc:202] Create variable feed
1884: I0815 04:36:19.357388 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.357396 17927 scope.cc:202] Create variable fetch
1884: I0815 04:36:19.357405 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.357417 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.357473 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.357479 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.357483 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.359977 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.360353 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.360366 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.360371 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.362116 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.362177 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.362186 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.362195 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.362210 17927 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:36:19.362216 17927 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6169a6a0 type is 7
1884: I0815 04:36:19.362228 17927 scope.cc:202] Create variable x
1884: I0815 04:36:19.362231 17927 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x6169a7d0 type is 7
1884: I0815 04:36:19.362295 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.362309 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.362315 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.362318 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.362453 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.362478 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.362603 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.362613 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.362628 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.362823 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.362852 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.362872 17927 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.362879 17927 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x616a2810Variable Type 7
1884: I0815 04:36:19.362902 17927 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.362926 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.362977 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.362996 17927 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.364203 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.364259 17927 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.364684 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.376348 17927 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:36:19.376380 17927 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:36:19.376581 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.376621 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.377168 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: I0815 04:36:19.377413 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.377439 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.377923 17927 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: I0815 04:36:19.377988 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.378011 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.378038 17927 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.378345 17927 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:36:19.378356 17927 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:36:19.378490 17927 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:36:19.378515 17927 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.378917 17927 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:36:19.378929 17927 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:36:19.378970 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.378989 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.379171 17927 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:36:19.379180 17927 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:36:19.379215 17927 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:36:19.379232 17927 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:36:19.379249 17927 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.382344 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.382376 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.382442 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.382452 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.384708 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.385110 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.385128 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.385133 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.387005 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.387077 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.387089 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.387096 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x616ce220 type is 7
1884: I0815 04:36:19.387113 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.387117 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x616ce590 type is 7
1884: I0815 04:36:19.387122 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.387136 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.387197 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.387204 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.387209 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.387214 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.387284 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.387312 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.387392 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.387403 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.387421 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.387742 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.387759 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.387778 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.387786 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x616d4cf0Variable Type 7
1884: I0815 04:36:19.387806 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.387827 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.387856 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.387873 17927 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.388628 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.388662 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.388881 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.400394 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: I0815 04:36:19.400688 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:36:19.407490 17927 pir_interpreter.cc:161] PirInterpreter(): 0x61891d30 on Place(gpu:0)
1884: I0815 04:36:19.407541 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.407575 17927 scope.cc:202] Create variable 0x61891d301723696579407523533_inner_var_1
1884: I0815 04:36:19.407588 17927 scope.cc:202] Create variable 0x61891d301723696579407523533_inner_var_2
1884: I0815 04:36:19.407598 17927 scope.cc:202] Create variable 0x61891d301723696579407523533_inner_var_3
1884: I0815 04:36:19.407608 17927 scope.cc:202] Create variable 0x61891d301723696579407523533_inner_var_4
1884: I0815 04:36:19.407617 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:19.408125 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:19.408141 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.408145 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:36:19.408200 17927 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61891c90
1884: 1 -> 0x61891d301723696579407523533_inner_var_1 -> 0x61891d10
1884: 2 -> 0x61891d301723696579407523533_inner_var_2 -> 0x61892620
1884: 3 -> 0x61891d301723696579407523533_inner_var_3 -> 0x61891470
1884: 4 -> 0x61891d301723696579407523533_inner_var_4 -> 0x618929d0
1884: 5 -> fetch0@fetch -> 0x618931e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:19.408996 17927 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:36:19.409247 17965 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:19.423382 17968 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:19.424360 17966 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:19.424387 17967 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:19.424645 17967 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61891d301723696579407523533_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.424770 17967 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61891d301723696579407523533_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:19.425412 17969 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:19.429383 17970 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:19.429481 17970 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.429560 17970 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:36:19.429625 17970 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61891d301723696579407523533_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61891d301723696579407523533_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.429930 17970 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61891d301723696579407523533_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61891d301723696579407523533_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:36:19.431398 17969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61891d301723696579407523533_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.431478 17969 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.432814 17969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61891d301723696579407523533_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61891d301723696579407523533_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:36:19.432885 17969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61891d301723696579407523533_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.432925 17969 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.433537 17969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61891d301723696579407523533_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:36:19.435382 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x61891ea0) got event_name: TaskCompletion
1884: I0815 04:36:19.435451 17927 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.521451 17965 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 7723008416669337931 to 17205326545672764295 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:36:19.521499 17965 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7723008416669337931 to 15710721543576414051 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 04:36:19.521507 17965 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7723008416669337931 to 15710721543576414051 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 04:36:19.521692 17967 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4493206447664969044 to 15710721543576414051 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 04:36:19.521718 17967 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4493206447664969044 to 15710721543576414051 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 04:36:19.522364 17969 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 15710721543576414051 to 11344686167747026504 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:36:19.522392 17969 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 15710721543576414051 to 11344686167747026504 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:36:19.531412 17970 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:36:19.531455 17970 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:19.541023 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.541123 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.541208 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.541227 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.543160 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.543609 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.543663 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.543682 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.545284 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.545464 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.545491 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.545509 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x616b7f90 type is 7
1884: I0815 04:36:19.545528 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.545542 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x616b83c0 type is 7
1884: I0815 04:36:19.545555 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.545569 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.545643 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.545658 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.545672 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.545686 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.545758 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.545787 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.545871 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.545890 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.545917 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.546104 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.546129 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.546156 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.546171 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x502b260Variable Type 7
1884: I0815 04:36:19.546198 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.546226 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.546257 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.546283 17927 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.547986 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.548053 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.548317 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.560338 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: I0815 04:36:19.560701 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:36:19.564287 17927 pir_interpreter.cc:161] PirInterpreter(): 0x616be380 on Place(gpu:0)
1884: I0815 04:36:19.564358 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.564399 17927 scope.cc:202] Create variable 0x616be3801723696579564344737_inner_var_1
1884: I0815 04:36:19.564422 17927 scope.cc:202] Create variable 0x616be3801723696579564344737_inner_var_2
1884: I0815 04:36:19.564446 17927 scope.cc:202] Create variable 0x616be3801723696579564344737_inner_var_3
1884: I0815 04:36:19.564469 17927 scope.cc:202] Create variable 0x616be3801723696579564344737_inner_var_4
1884: I0815 04:36:19.564493 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:19.564910 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:19.564954 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.564971 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x63e72f00
1884: 1 -> 0x616be3801723696579564344737_inner_var_1 -> 0x61892da0
1884: 2 -> 0x616be3801723696579564344737_inner_var_2 -> 0x5019580
1884: 3 -> 0x616be3801723696579564344737_inner_var_3 -> 0x504b3e0
1884: 4 -> 0x616be3801723696579564344737_inner_var_4 -> 0x45464f60
1884: 5 -> fetch0@fetch -> 0x502ac80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:19.566340 17973 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:19.566351 17971 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:19.566390 17973 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x616be3801723696579564344737_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.566464 17973 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x616be3801723696579564344737_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:19.567339 17976 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:19.567397 17976 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.567456 17976 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:36:19.567515 17976 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x616be3801723696579564344737_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x616be3801723696579564344737_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.567669 17976 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x616be3801723696579564344737_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x616be3801723696579564344737_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.568378 17973 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x616be3801723696579564344737_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.568429 17973 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.566340 17975 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:19.571322 17973 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x616be3801723696579564344737_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x616be3801723696579564344737_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.571388 17973 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x616be3801723696579564344737_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.571413 17973 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.566351 17974 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:19.566351 17972 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:19.577535 17973 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x616be3801723696579564344737_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.581382 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x616be4f0) got event_name: TaskCompletion
1884: I0815 04:36:19.581446 17927 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.630386 17971 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 17205326545672764295 to 3760932628131425615 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:36:19.630429 17971 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 17205326545672764295 to 1767850734524999313 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:36:19.630434 17971 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 17205326545672764295 to 1767850734524999313 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:36:19.637380 17973 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1767850734524999313 to 11344686167747026504 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:36:19.637416 17973 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1767850734524999313 to 11344686167747026504 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:36:19.644380 17976 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:19.652004 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.652103 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.652189 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.652206 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.654083 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.654531 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.654572 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.654589 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.656270 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.656455 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.656483 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.656502 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x502ab20 type is 7
1884: I0815 04:36:19.656520 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.656533 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5028100 type is 7
1884: I0815 04:36:19.656546 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.656560 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.656631 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.656646 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.656661 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.656674 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.656747 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.656776 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.656857 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.656878 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.656905 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.656983 17927 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.657151 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:19.657228 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.657251 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.657280 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.657294 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5028870Variable Type 7
1884: I0815 04:36:19.657332 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.657361 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.657397 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.657423 17927 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.657585 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.657620 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.657845 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.658746 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: I0815 04:36:19.659013 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:36:19.666721 17927 pir_interpreter.cc:161] PirInterpreter(): 0x616b3200 on Place(gpu:0)
1884: I0815 04:36:19.666785 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.666826 17927 scope.cc:202] Create variable 0x616b32001723696579666773579_inner_var_1
1884: I0815 04:36:19.666848 17927 scope.cc:202] Create variable 0x616b32001723696579666773579_inner_var_2
1884: I0815 04:36:19.666872 17927 scope.cc:202] Create variable 0x616b32001723696579666773579_inner_var_3
1884: I0815 04:36:19.666894 17927 scope.cc:202] Create variable 0x616b32001723696579666773579_inner_var_4
1884: I0815 04:36:19.666918 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:19.667383 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:19.667431 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.667448 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x63d56400
1884: 1 -> 0x616b32001723696579666773579_inner_var_1 -> 0x504c7a0
1884: 2 -> 0x616b32001723696579666773579_inner_var_2 -> 0x63d57270
1884: 3 -> 0x616b32001723696579666773579_inner_var_3 -> 0x5017a20
1884: 4 -> 0x616b32001723696579666773579_inner_var_4 -> 0x44fee850
1884: 5 -> fetch0@fetch -> 0x50180d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:19.669337 17979 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:19.669354 17982 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:19.669384 17979 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x616b32001723696579666773579_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.669430 17982 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.669469 17979 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x616b32001723696579666773579_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:19.669492 17982 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:36:19.669530 17982 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x616b32001723696579666773579_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x616b32001723696579666773579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.669596 17982 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.669772 17982 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:19.669804 17982 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x616b32001723696579666773579_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x616b32001723696579666773579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:36:19.672356 17978 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:19.672431 17978 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x616b32001723696579666773579_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.672492 17978 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.672601 17978 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x616b32001723696579666773579_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x616b32001723696579666773579_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:36:19.672634 17978 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x616b32001723696579666773579_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.672653 17978 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.672664 17978 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x616b32001723696579666773579_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:36:19.669339 17981 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:19.669354 17980 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:19.676447 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x616b3370) got event_name: TaskCompletion
1884: I0815 04:36:19.676514 17927 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.669337 17977 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:19.720686 17977 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 3760932628131425615 to 17205326545672764295 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:36:19.720726 17977 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 3760932628131425615 to 4493206447664969044 , after update, data is {current : 796, peak : 1600}.
1884: I0815 04:36:19.720732 17977 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 3760932628131425615 to 4493206447664969044 , after update, data is {current : 796, peak : 1600}.
1884: I0815 04:36:19.727385 17978 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 4493206447664969044 to 17205326545672764295 , after update, data is {current : 796, peak : 2000}.
1884: I0815 04:36:19.727425 17978 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 4493206447664969044 to 17205326545672764295 , after update, data is {current : 796, peak : 2000}.
1884: I0815 04:36:19.729377 17979 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8413886954196386544 to 17205326545672764295 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:36:19.729408 17979 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8413886954196386544 to 17205326545672764295 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:36:19.738400 17982 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:36:19.738441 17982 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:36:19.738447 17982 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:19.746462 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.746503 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.746568 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.746573 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.748453 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.748850 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.748862 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.748867 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.750471 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.750597 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.750607 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.750613 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5029ad0 type is 7
1884: I0815 04:36:19.750622 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.750624 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x44778ef0 type is 7
1884: I0815 04:36:19.750628 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.750633 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.750691 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.750696 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.750700 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.750705 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.750764 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.750780 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.750846 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.750854 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.750867 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.751129 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.751139 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.751155 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.751159 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x616aed60Variable Type 7
1884: I0815 04:36:19.751176 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.751194 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.751214 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.751228 17927 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.755539 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.755610 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.755877 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.759493 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.759526 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.759593 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.759599 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.761617 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.762050 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.762061 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.762068 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.763952 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.764132 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.764142 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.764150 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63ebf6e0 type is 7
1884: I0815 04:36:19.764159 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.764163 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x63ebf200 type is 7
1884: I0815 04:36:19.764168 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.764173 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.764240 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.764246 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.764250 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.764254 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.764335 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.764351 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.764442 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.764458 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.764475 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.771559 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.771596 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.771624 17927 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:36:19.772146 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.772157 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.772229 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.772233 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.772835 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: I0815 04:36:19.773068 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:36:19.776021 17927 pir_interpreter.cc:161] PirInterpreter(): 0x63ebf780 on Place(gpu:0)
1884: I0815 04:36:19.776053 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.776074 17927 scope.cc:202] Create variable 0x63ebf7801723696579776045063_inner_var_1
1884: I0815 04:36:19.776082 17927 scope.cc:202] Create variable 0x63ebf7801723696579776045063_inner_var_2
1884: I0815 04:36:19.776088 17927 scope.cc:202] Create variable 0x63ebf7801723696579776045063_inner_var_3
1884: I0815 04:36:19.776095 17927 scope.cc:202] Create variable 0x63ebf7801723696579776045063_inner_var_4
1884: I0815 04:36:19.776103 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:19.776460 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:19.776479 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.776481 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x616b5380
1884: 1 -> 0x63ebf7801723696579776045063_inner_var_1 -> 0x616b5400
1884: 2 -> 0x63ebf7801723696579776045063_inner_var_2 -> 0x616bf500
1884: 3 -> 0x63ebf7801723696579776045063_inner_var_3 -> 0x63e5f730
1884: 4 -> 0x63ebf7801723696579776045063_inner_var_4 -> 0x61686ef0
1884: 5 -> fetch0@fetch -> 0x61673a20
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:19.777344 17986 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:19.777351 17983 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:19.777384 17986 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ebf7801723696579776045063_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.777452 17986 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ebf7801723696579776045063_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:19.777352 17987 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:19.779361 17988 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:19.779407 17988 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.779457 17988 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:36:19.779497 17988 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63ebf7801723696579776045063_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63ebf7801723696579776045063_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.779737 17988 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63ebf7801723696579776045063_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63ebf7801723696579776045063_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:36:19.777352 17985 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:19.781411 17985 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ebf7801723696579776045063_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.781471 17985 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.782758 17985 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ebf7801723696579776045063_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x63ebf7801723696579776045063_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:36:19.782817 17985 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63ebf7801723696579776045063_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.782838 17985 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.783423 17985 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63ebf7801723696579776045063_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:36:19.784334 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x63ebf8f0) got event_name: TaskCompletion
1884: I0815 04:36:19.784377 17927 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.777351 17984 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:19.788074 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.788103 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.788172 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.788178 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.794435 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.794886 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.794900 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.794906 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.796840 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.796911 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.796921 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.796928 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x633e6b70 type is 7
1884: I0815 04:36:19.796937 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.796941 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x63308e80 type is 7
1884: I0815 04:36:19.796947 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.796952 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.797020 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.797025 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.797030 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.797035 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.797106 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.797127 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.797209 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.797217 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.797232 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.800227 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.800244 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.800262 17927 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:36:19.800786 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.800798 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.800868 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.800870 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.853387 17983 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 17205326545672764295 to 3760932628131425615 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:36:19.853432 17983 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 17205326545672764295 to 8413886954196386544 , after update, data is {current : -800000, peak : 4}.
1884: I0815 04:36:19.853438 17983 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 17205326545672764295 to 8413886954196386544 , after update, data is {current : -800000, peak : 4}.
1884: I0815 04:36:19.857369 17986 thread_data_registry.h:135] Add data {current : -800000, peak : 4} from thread 8413886954196386544 to 1767850734524999313 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 04:36:19.857404 17986 thread_data_registry.h:135] Add data {current : -800000, peak : 4} from thread 8413886954196386544 to 1767850734524999313 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 04:36:19.859369 17985 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 1767850734524999313 to 11344686167747026504 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 04:36:19.859406 17985 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 1767850734524999313 to 11344686167747026504 , after update, data is {current : 4000832, peak : 5600832}.
1884: I0815 04:36:19.863368 17988 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 3205136, peak : 3207136}.
1884: I0815 04:36:19.863401 17988 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:19.872709 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.872747 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.872810 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.872815 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.874717 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.875123 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.875134 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.875139 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.881013 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.881088 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.881096 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.881104 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63ebec00 type is 7
1884: I0815 04:36:19.881111 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.881115 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6186c3c0 type is 7
1884: I0815 04:36:19.881119 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.881124 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.881194 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.881199 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.881204 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.881209 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.881271 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.881287 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.881366 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.881372 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.881387 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.881546 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.881561 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.881577 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:19.881582 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61695d80Variable Type 7
1884: I0815 04:36:19.881599 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:19.881615 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.881635 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:19.881649 17927 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.883366 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:19.883415 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:19.883667 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:19.893512 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.893553 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.893625 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.893631 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.895671 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.896113 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.896126 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.896131 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.898032 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.898173 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.898183 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.898191 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x633e3eb0 type is 7
1884: I0815 04:36:19.898200 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.898203 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6341be70 type is 7
1884: I0815 04:36:19.898208 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.898214 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.898283 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.898288 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.898293 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.898298 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.898378 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.898394 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.898475 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.898490 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.898506 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.914508 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.914553 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.914582 17927 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:36:19.915534 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.915552 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.915637 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.915639 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.916239 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: I0815 04:36:19.916484 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:36:19.919384 17927 pir_interpreter.cc:161] PirInterpreter(): 0x6167d5b0 on Place(gpu:0)
1884: I0815 04:36:19.919416 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.919437 17927 scope.cc:202] Create variable 0x6167d5b01723696579919407128_inner_var_1
1884: I0815 04:36:19.919445 17927 scope.cc:202] Create variable 0x6167d5b01723696579919407128_inner_var_2
1884: I0815 04:36:19.919452 17927 scope.cc:202] Create variable 0x6167d5b01723696579919407128_inner_var_3
1884: I0815 04:36:19.919467 17927 scope.cc:202] Create variable 0x6167d5b01723696579919407128_inner_var_4
1884: I0815 04:36:19.919476 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:19.919822 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:19.919831 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.919834 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x616985c0
1884: 1 -> 0x6167d5b01723696579919407128_inner_var_1 -> 0x61698620
1884: 2 -> 0x6167d5b01723696579919407128_inner_var_2 -> 0x6168db60
1884: 3 -> 0x6167d5b01723696579919407128_inner_var_3 -> 0x631788b0
1884: 4 -> 0x6167d5b01723696579919407128_inner_var_4 -> 0x6169e2e0
1884: 5 -> fetch0@fetch -> 0x63e92bd0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:19.921334 17990 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:19.921334 17989 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:19.921352 17993 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:19.921378 17990 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6167d5b01723696579919407128_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.921447 17990 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6167d5b01723696579919407128_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:19.921334 17994 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:19.921337 17992 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:19.925388 17994 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.925444 17994 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:36:19.925483 17994 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6167d5b01723696579919407128_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6167d5b01723696579919407128_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.925622 17994 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6167d5b01723696579919407128_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6167d5b01723696579919407128_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.921339 17991 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:19.926364 17991 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6167d5b01723696579919407128_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.926402 17991 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:19.929242 17991 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6167d5b01723696579919407128_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6167d5b01723696579919407128_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.929314 17991 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6167d5b01723696579919407128_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:19.929338 17991 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.935434 17991 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6167d5b01723696579919407128_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:36:19.936326 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x6167d720) got event_name: TaskCompletion
1884: I0815 04:36:19.936362 17927 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:36:19.947052 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.947094 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.947165 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:19.947171 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.949129 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:19.949568 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.949580 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.949585 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.951481 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:19.951548 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:19.951558 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:19.951566 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6315a530 type is 7
1884: I0815 04:36:19.951575 17927 scope.cc:202] Create variable X
1884: I0815 04:36:19.951579 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6315ab60 type is 7
1884: I0815 04:36:19.951584 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:19.951589 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:19.951659 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:19.951664 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:19.951669 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:19.951674 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:19.951743 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.951759 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.951841 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.951848 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.951864 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:19.966854 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.966902 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:19.966930 17927 fetch_v2_op.cc:138] Fetch variable Out's 0 column.
1884: I0815 04:36:19.967957 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.967976 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:19.968051 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:19.968055 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.015374 17989 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 3760932628131425615 to 17205326545672764295 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:36:20.015411 17989 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 3760932628131425615 to 8413886954196386544 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 04:36:20.015417 17989 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 3760932628131425615 to 8413886954196386544 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 04:36:20.027375 17991 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 8413886954196386544 to 4493206447664969044 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 04:36:20.027412 17991 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 8413886954196386544 to 4493206447664969044 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 04:36:20.031355 17990 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 4493206447664969044 to 11344686167747026504 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 04:36:20.031383 17990 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 4493206447664969044 to 11344686167747026504 , after update, data is {current : 6400896, peak : 11200896}.
1884: I0815 04:36:20.033414 17994 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 17205326545672764295 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:20.041728 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:20.041764 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.041823 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:20.041829 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.043681 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:20.044088 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.044098 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.044103 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.049991 17927 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:36:20.050067 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:20.050076 17927 scope.cc:202] Create variable Out
1884: I0815 04:36:20.050083 17927 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4cc5e20 type is 7
1884: I0815 04:36:20.050091 17927 scope.cc:202] Create variable X
1884: I0815 04:36:20.050094 17927 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3c710f0 type is 7
1884: I0815 04:36:20.050098 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:20.050102 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:20.050179 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:20.050184 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.050189 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.050194 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.050258 17927 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.050273 17927 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.050352 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.050359 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.050376 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.050429 17927 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.050580 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.050627 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.050635 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.050652 17927 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:20.050657 17927 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x616b2c90Variable Type 7
1884: I0815 04:36:20.050676 17927 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:20.050694 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.050715 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.050727 17927 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.050853 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:20.050875 17927 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:20.051090 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.052037 17927 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a6b85b0 for it.
1884: I0815 04:36:20.052271 17927 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a686830 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:36:20.055572 17927 pir_interpreter.cc:161] PirInterpreter(): 0x63e92fd0 on Place(gpu:0)
1884: I0815 04:36:20.055608 17927 scope.cc:202] Create variable X
1884: I0815 04:36:20.055630 17927 scope.cc:202] Create variable 0x63e92fd01723696580055597262_inner_var_1
1884: I0815 04:36:20.055639 17927 scope.cc:202] Create variable 0x63e92fd01723696580055597262_inner_var_2
1884: I0815 04:36:20.055648 17927 scope.cc:202] Create variable 0x63e92fd01723696580055597262_inner_var_3
1884: I0815 04:36:20.055657 17927 scope.cc:202] Create variable 0x63e92fd01723696580055597262_inner_var_4
1884: I0815 04:36:20.055665 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:20.056041 17927 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:36:20.056052 17927 scope.cc:202] Create variable X
1884: I0815 04:36:20.056056 17927 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x616c9a30
1884: 1 -> 0x63e92fd01723696580055597262_inner_var_1 -> 0x61868d10
1884: 2 -> 0x63e92fd01723696580055597262_inner_var_2 -> 0x5066a40
1884: 3 -> 0x63e92fd01723696580055597262_inner_var_3 -> 0x6403e260
1884: 4 -> 0x63e92fd01723696580055597262_inner_var_4 -> 0x50339b0
1884: 5 -> fetch0@fetch -> 0x454478a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:36:20.057337 17999 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.057356 18000 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:20.057372 17999 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63e92fd01723696580055597262_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.057439 17999 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63e92fd01723696580055597262_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.057442 18000 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.057502 18000 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:36:20.057543 18000 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63e92fd01723696580055597262_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63e92fd01723696580055597262_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.057600 18000 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.057763 18000 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.057796 18000 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63e92fd01723696580055597262_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63e92fd01723696580055597262_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:36:20.057354 17995 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.061334 17996 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.061369 17999 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63e92fd01723696580055597262_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.061430 17999 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.061523 17999 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63e92fd01723696580055597262_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63e92fd01723696580055597262_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:36:20.061566 17999 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63e92fd01723696580055597262_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.061584 17999 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.061597 17999 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63e92fd01723696580055597262_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:36:20.062325 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x63e93140) got event_name: TaskCompletion
1884: I0815 04:36:20.062359 17927 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.057354 17998 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.057354 17997 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.115386 17995 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 17205326545672764295 to 3760932628131425615 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:36:20.115432 17995 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17205326545672764295 to 3760932628131425615 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:36:20.115439 17995 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17205326545672764295 to 3760932628131425615 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:36:20.119370 17999 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 4493206447664969044 to 3760932628131425615 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:36:20.119416 17999 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 4493206447664969044 to 3760932628131425615 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:36:20.123397 18000 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 04:36:20.123432 18000 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 6401600, peak : 11200896}.
1884: I0815 04:36:20.123438 18000 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 3760932628131425615 to 11344686167747026504 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:36:20.133656 17927 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:36:20.133723 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:36:20.134964 17927 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:36:20.140035 17927 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:36:20.140082 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:36:20.141651 17927 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:36:20.141674 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:36:20.142402 17927 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:36:20.143447 17927 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:36:20.143473 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:36:20.144907 17927 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:36:20.144927 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:36:20.145560 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.145586 17927 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:36:20.145589 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.145596 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.147768 17927 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:36:20.147799 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:36:20.152951 17927 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:36:20.152993 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:36:20.154054 17927 pybind.cc:1827] need skip: 0
1884: I0815 04:36:20.154388 17927 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:36:20.156405 17927 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:36:20.160251 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.160275 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.160281 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.166638 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:20.166668 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:20.166677 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:20.166682 17927 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:36:20.166689 17927 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62c98b00 type is 7
1884: I0815 04:36:20.166693 17927 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:36:20.166697 17927 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62c98dd0 type is 7
1884: I0815 04:36:20.166700 17927 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:36:20.166703 17927 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62c98d80 type is 7
1884: I0815 04:36:20.166786 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:20.166792 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.166796 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.166801 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.166870 17927 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.166886 17927 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.166909 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:36:20.167074 17927 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.167083 17927 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.167148 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.167186 17927 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.167193 17927 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.167218 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.168464 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.170027 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:20.170498 17927 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:36:20.170737 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.171051 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.171272 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.171288 17927 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:36:20.171370 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.171376 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.171382 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.171487 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.171504 17927 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:36:20.173101 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.174489 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.179757 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.179996 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:20.180009 17927 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:36:20.180017 17927 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x62f7b6e0 type is 7
1884: I0815 04:36:20.180024 17927 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:36:20.180028 17927 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x62f7aa10 type is 7
1884: I0815 04:36:20.180032 17927 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:36:20.180035 17927 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x62f7ab00 type is 7
1884: I0815 04:36:20.180039 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:20.180044 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:20.180048 17927 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:36:20.180051 17927 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x61894b10 type is 7
1884: I0815 04:36:20.180054 17927 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62c98b00 type is 7
1884: I0815 04:36:20.180058 17927 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62c98dd0 type is 7
1884: I0815 04:36:20.180063 17927 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:36:20.180065 17927 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x61894af0 type is 7
1884: I0815 04:36:20.180069 17927 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:36:20.180073 17927 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x61894fb0 type is 7
1884: I0815 04:36:20.180075 17927 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62c98d80 type is 7
1884: I0815 04:36:20.180079 17927 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:36:20.180083 17927 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x61895220 type is 7
1884: I0815 04:36:20.180085 17927 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:36:20.180088 17927 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x61895460 type is 7
1884: I0815 04:36:20.180092 17927 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:36:20.180095 17927 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x618956c0 type is 7
1884: I0815 04:36:20.180208 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.180224 17927 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:36:20.180290 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:20.180296 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.180327 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.180332 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.180408 17927 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.180424 17927 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.180444 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:36:20.180604 17927 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.180619 17927 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.180639 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:36:20.180723 17927 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:36:20.180845 17927 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:36:20.181999 17927 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182017 17927 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182089 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.182144 17927 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182152 17927 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182164 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:36:20.182189 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.182222 17927 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182229 17927 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182241 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:36:20.182346 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182353 17927 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182366 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.182482 17927 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.182561 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.182614 17927 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182622 17927 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182636 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:36:20.182668 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.182713 17927 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182719 17927 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182732 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:36:20.182837 17927 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182844 17927 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.182868 17927 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.182912 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.182919 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.182934 17927 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:20.182940 17927 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63ec7900Variable Type 7
1884: I0815 04:36:20.182961 17927 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:20.182979 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.182997 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.183008 17927 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.183045 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:20.183069 17927 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:36:20.183094 17927 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.183100 17927 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.183112 17927 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:36:20.183116 17927 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61896560Variable Type 7
1884: I0815 04:36:20.183130 17927 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:36:20.183138 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.183151 17927 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.183161 17927 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.183192 17927 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:36:20.183202 17927 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:36:20.183665 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:36:20.183699 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:36:20.183715 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:36:20.183748 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:36:20.183780 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.183796 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:36:20.188717 17927 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:36:20.188768 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:36:20.193759 17927 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:36:20.193800 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:36:20.194278 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.196126 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.197005 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.197126 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.197657 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.198634 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.200987 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.202075 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.204083 17927 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:36:20.209098 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.209127 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.209132 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.210496 17927 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:36:20.210520 17927 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5066760 type is 9
1884: I0815 04:36:20.210529 17927 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x318ff50 type is 10
1884: I0815 04:36:20.210534 17927 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62c98dd0 type is 7
1884: I0815 04:36:20.210537 17927 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62c98d80 type is 7
1884: I0815 04:36:20.210542 17927 scope.cc:202] Create variable saved_params
1884: I0815 04:36:20.210546 17927 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x62b82620 type is 17
1884: I0815 04:36:20.210579 17927 interpreter_util.cc:594] Static build: 0
1884: I0815 04:36:20.210583 17927 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:36:20.210587 17927 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:36:20.210590 17927 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:36:20.210650 17927 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.210665 17927 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:36:20.211544 17927 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:36:20.211583 17927 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:36:20.211639 17927 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:36:20.212915 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.212975 17927 scope.cc:202] Create variable feed
1884: I0815 04:36:20.212981 17927 naive_executor.cc:189] 0x63d6c5b0 Create persistable variable feed, which pointer is 0x63d5bc30
1884: I0815 04:36:20.212986 17927 scope.cc:202] Create variable fetch
1884: I0815 04:36:20.212989 17927 naive_executor.cc:189] 0x63d6c5b0 Create persistable variable fetch, which pointer is 0x63dd7150
1884: I0815 04:36:20.212992 17927 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:36:20.212996 17927 naive_executor.cc:189] 0x63d6c5b0 Create persistable variable linear_0.b_0, which pointer is 0x6306ed80
1884: I0815 04:36:20.213001 17927 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:36:20.213002 17927 naive_executor.cc:189] 0x63d6c5b0 Create persistable variable linear_0.w_0, which pointer is 0x6306f1a0
1884: I0815 04:36:20.213019 17927 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:36:20.213393 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.213483 17927 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:36:20.213538 17927 executor.cc:183] Old Executor is Running.
1884: I0815 04:36:20.213584 17927 executor.cc:92] Creating Variables for block 0
1884: I0815 04:36:20.213590 17927 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:36:20.213593 17927 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x6306ed80 type is 7
1884: I0815 04:36:20.213596 17927 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:36:20.213599 17927 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x6306f1a0 type is 7
1884: I0815 04:36:20.213639 17927 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.213728 17927 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:36:20.213769 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.213773 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.213923 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.214030 17927 graph.cc:149] create OpNode by feed
1884: I0815 04:36:20.214067 17927 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:36:20.214080 17927 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:36:20.214093 17927 graph.cc:149] create OpNode by abs
1884: I0815 04:36:20.214102 17927 graph.cc:149] create OpNode by assign_value
1884: I0815 04:36:20.214116 17927 graph.cc:149] create OpNode by multinomial
1884: I0815 04:36:20.214125 17927 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:36:20.214140 17927 graph.cc:149] create OpNode by scale
1884: I0815 04:36:20.214150 17927 graph.cc:149] create OpNode by scale
1884: I0815 04:36:20.214161 17927 graph.cc:149] create OpNode by fetch
1884: I0815 04:36:20.214176 17927 graph.cc:149] create OpNode by fetch
1884: I0815 04:36:20.214197 17927 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:36:20.215422 17927 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:36:20.215428 17927 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:36:20.215507 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.215512 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:36:20.215623 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.215875 17927 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:36:20.215934 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.215937 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:36:20.215970 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.215973 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:36:20.216014 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216073 17927 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:36:20.216104 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216106 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:36:20.216122 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216135 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216156 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216159 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:36:20.216197 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216215 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216238 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216239 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:36:20.216281 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216360 17927 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:36:20.216388 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216392 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:36:20.216421 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216439 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216459 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216462 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:36:20.216490 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216639 17927 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:36:20.216665 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216668 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:36:20.216697 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216711 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216732 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216735 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:36:20.216753 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216766 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216786 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216789 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:36:20.216810 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216822 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.216843 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.216845 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:36:20.216868 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.216933 17927 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:36:20.216964 17927 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:36:20.216976 17927 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:36:20.216989 17927 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:36:20.217010 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.217013 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:36:20.217034 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.217072 17927 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:36:20.217090 17927 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:36:20.217099 17927 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:36:20.217109 17927 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:36:20.217140 17927 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:36:20.217149 17927 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:36:20.222440 17927 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:36:20.222530 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.222537 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:36:20.222574 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.222599 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.222625 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.222628 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:36:20.222652 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.222707 17927 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:36:20.222738 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.222743 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:36:20.222761 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.222775 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.222796 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.222800 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:36:20.222838 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.222923 17927 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:36:20.222954 17927 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:36:20.222967 17927 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:36:20.222981 17927 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:36:20.222994 17927 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:36:20.223006 17927 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:36:20.223021 17927 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:36:20.223042 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223111 17927 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:36:20.223132 17927 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:36:20.223143 17927 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:36:20.223155 17927 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:36:20.223167 17927 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:36:20.223181 17927 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:36:20.223193 17927 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:36:20.223237 17927 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:36:20.223563 17927 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:36:20.223598 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.223601 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:36:20.223650 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223709 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.223742 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223785 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.223810 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223850 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.223871 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223906 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.223924 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223955 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.223971 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.223999 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224012 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224037 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224048 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224069 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224078 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224094 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224118 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224121 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:36:20.224146 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224185 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224210 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224212 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:36:20.224221 17927 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:36:20.224223 17927 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:36:20.224273 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224293 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224323 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224326 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:36:20.224334 17927 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:36:20.224337 17927 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:36:20.224376 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224396 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224419 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224422 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:36:20.224429 17927 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:36:20.224431 17927 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:36:20.224463 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224478 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224499 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224503 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:36:20.224509 17927 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:36:20.224512 17927 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:36:20.224550 17927 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:36:20.224567 17927 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:36:20.224588 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224591 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:36:20.224603 17927 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:36:20.224650 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224653 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:36:20.224741 17927 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:36:20.224774 17927 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.224797 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:36:20.224874 17927 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:36:20.224895 17927 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:36:20.224925 17927 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:36:20.224946 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.224948 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:36:20.225947 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.225961 17927 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:36:20.226015 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.226019 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.226646 17927 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:36:20.226856 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.226928 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.226931 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.227348 17927 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:36:20.227589 17927 graph.h:183] deleting __fuse_statis__
1884: I0815 04:36:20.227602 17927 graph.h:183] deleting pass_recorder
1884: I0815 04:36:20.227608 17927 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:36:20.227699 17927 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:36:20.227710 17927 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:36:20.227712 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable abs_0.tmp_0, which pointer is 0x63fdcf90
1884: I0815 04:36:20.227720 17927 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:36:20.227722 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable gaussian_0.tmp_0, which pointer is 0x62c385f0
1884: I0815 04:36:20.227735 17927 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:36:20.227737 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable linear_0.tmp_1, which pointer is 0x62c82e50
1884: I0815 04:36:20.227741 17927 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:36:20.227743 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable multinomial_0.tmp_0, which pointer is 0x62c828f0
1884: I0815 04:36:20.227747 17927 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:36:20.227749 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62c82bf0
1884: I0815 04:36:20.227753 17927 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:36:20.227756 17927 naive_executor.cc:195] 0x63d6c5b0 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62c811f0
1884: I0815 04:36:20.227761 17927 scope.cc:202] Create variable feed
1884: I0815 04:36:20.227766 17927 scope.cc:202] Create variable fetch
1884: I0815 04:36:20.227787 17927 naive_executor.cc:46] NaiveExecutor init with scope 0x63d6c5b0
1884: I0815 04:36:20.227789 17927 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:36:20.228005 17927 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:36:20.228019 17927 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:36:20.228049 17927 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:36:20.228051 17927 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:36:20.228060 17927 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.228097 17927 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:36:20.228363 17927 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.228377 17927 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:36:20.228440 17927 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.228471 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:36:20.406348 17927 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:36:20.406484 17927 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.406512 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:36:20.406592 17927 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:36:20.406627 17927 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.406656 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:36:20.406713 17927 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:36:20.406759 17927 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.406777 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:36:20.406831 17927 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:36:20.406862 17927 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:36:20.406878 17927 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:36:20.406914 17927 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:36:20.406932 17927 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.406970 17927 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:36:20.406996 17927 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:36:20.407516 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.407528 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:36:20.446902 17927 pir_interpreter.cc:161] PirInterpreter(): 0x62f7b810 on Place(gpu:0)
1884: I0815 04:36:20.446944 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_0
1884: I0815 04:36:20.446961 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_1
1884: I0815 04:36:20.446967 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_2
1884: I0815 04:36:20.446978 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_3
1884: I0815 04:36:20.447006 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_4
1884: I0815 04:36:20.447018 17927 scope.cc:202] Create variable 0x62f7b8101723696580446932132_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x62f7b8101723696580446932132_inner_var_0 -> 0x63078270
1884: 1 -> 0x62f7b8101723696580446932132_inner_var_1 -> 0x63dd69a0
1884: 2 -> 0x62f7b8101723696580446932132_inner_var_2 -> 0x63e72e80
1884: 3 -> linear_1.w_0 -> 0x61529590
1884: 4 -> linear_1.b_0 -> 0x62ca1b20
1884: 5 -> learning_rate_1 -> 0x616c0030
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:36:20.448341 18005 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:20.448338 18004 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.448405 18004 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448410 18005 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448484 18005 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.448477 18004 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.448498 18004 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448511 18004 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:36:20.448521 18004 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448531 18004 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62f7b8101723696580446932132_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.448527 18005 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:36:20.448551 18005 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448566 18005 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.448578 18005 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:36:20.448590 18005 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x62f7b8101723696580446932132_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62f7b8101723696580446932132_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62f7b8101723696580446932132_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.448650 18005 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x62f7b8101723696580446932132_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62f7b8101723696580446932132_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62f7b8101723696580446932132_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:36:20.448705 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x62f7b980) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:36:20.450824 17927 pir_interpreter.cc:161] PirInterpreter(): 0x63d0c150 on Place(gpu:0)
1884: I0815 04:36:20.450867 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_1
1884: I0815 04:36:20.450883 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_4
1884: I0815 04:36:20.450899 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_5
1884: I0815 04:36:20.450906 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_6
1884: I0815 04:36:20.450928 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_7
1884: I0815 04:36:20.450937 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_8
1884: I0815 04:36:20.450944 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_9
1884: I0815 04:36:20.450971 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_10
1884: I0815 04:36:20.450978 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_11
1884: I0815 04:36:20.450985 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_12
1884: I0815 04:36:20.450992 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_13
1884: I0815 04:36:20.451000 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_14
1884: I0815 04:36:20.451006 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_15
1884: I0815 04:36:20.451013 17927 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:36:20.451021 17927 scope.cc:202] Create variable 0x63d0c1501723696580450847819_inner_var_17
1884: I0815 04:36:20.451028 17927 scope.cc:202] Create variable fetch1@fetch
1884: I0815 04:36:20.448338 18001 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x616c0030
1884: 1 -> 0x63d0c1501723696580450847819_inner_var_1 -> 0x62c1c720
1884: 2 -> linear_1.b_0 -> 0x62ca1b20
1884: 3 -> linear_1.w_0 -> 0x61529590
1884: 4 -> 0x63d0c1501723696580450847819_inner_var_4 -> 0x6301c2b0
1884: 5 -> 0x63d0c1501723696580450847819_inner_var_5 -> 0x63cb3560
1884: 6 -> 0x63d0c1501723696580450847819_inner_var_6 -> 0x6301ca10
1884: 7 -> 0x63d0c1501723696580450847819_inner_var_7 -> 0x63d1ae10
1884: 8 -> 0x63d0c1501723696580450847819_inner_var_8 -> 0x62c1c8f0
1884: 9 -> 0x63d0c1501723696580450847819_inner_var_9 -> 0x63046c20
1884: 10 -> 0x63d0c1501723696580450847819_inner_var_10 -> 0x633e59c0
1884: 11 -> 0x63d0c1501723696580450847819_inner_var_11 -> 0x38c64a90
1884: 12 -> 0x63d0c1501723696580450847819_inner_var_12 -> 0x1a6cc380
1884: 13 -> 0x63d0c1501723696580450847819_inner_var_13 -> 0x1a62ace0
1884: 14 -> 0x63d0c1501723696580450847819_inner_var_14 -> 0x61699b70
1884: 15 -> 0x63d0c1501723696580450847819_inner_var_15 -> 0x1a694480
1884: 16 -> fetch0@fetch -> 0x63cb5db0
1884: 17 -> 0x63d0c1501723696580450847819_inner_var_17 -> 0x6168b070
1884: 18 -> fetch1@fetch -> 0x63d446e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:36:20.448338 18002 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.448338 18003 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.453404 18009 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.453451 18009 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.453526 18009 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.453547 18009 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.453563 18009 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:36:20.453323 18006 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.457346 18008 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.457347 18010 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:36:20.461403 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.461474 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:36:20.461513 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.461585 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.461642 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:36:20.461658 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:36:20.461721 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:36:20.461738 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63d0c1501723696580450847819_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.461781 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63d0c1501723696580450847819_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:36:20.461813 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63d0c1501723696580450847819_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.461850 18010 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:36:20.461886 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63d0c1501723696580450847819_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.461916 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63d0c1501723696580450847819_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63d0c1501723696580450847819_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.461958 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.461974 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63d0c1501723696580450847819_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63d0c1501723696580450847819_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.462009 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x63d0c1501723696580450847819_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462031 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.462042 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x63d0c1501723696580450847819_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.462059 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63d0c1501723696580450847819_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462091 18010 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.462179 18010 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.462221 18010 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.462285 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.457352 18007 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.462311 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63d0c1501723696580450847819_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63d0c1501723696580450847819_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.462345 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462373 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.462347 18008 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d0c1501723696580450847819_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462388 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.462399 18008 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.462383 18007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462406 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x63d0c1501723696580450847819_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462419 18007 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:36:20.462483 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x63d0c1501723696580450847819_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:36:20.462492 18008 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d0c1501723696580450847819_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.462505 18010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63d0c1501723696580450847819_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63d0c1501723696580450847819_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462531 18010 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:36:20.462528 18008 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63d0c1501723696580450847819_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462546 18008 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.462543 18010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63d0c1501723696580450847819_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63d0c1501723696580450847819_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:36:20.462533 18007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d0c1501723696580450847819_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63d0c1501723696580450847819_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.462560 18008 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63d0c1501723696580450847819_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.462582 18007 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63d0c1501723696580450847819_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.462604 18007 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.462621 18007 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63d0c1501723696580450847819_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.462677 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x63d0c2c0) got event_name: TaskCompletion
1884: I0815 04:36:20.462694 17927 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.462716 17927 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:36:20.469192 17927 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:36:20.469244 17927 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:36:20.469955 17927 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:36:20.470011 17927 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:36:20.470504 17927 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804705525680"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804705525680"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:36:20.470728 17927 pir_interpreter.cc:161] PirInterpreter(): 0x618ad2f0 on Place(cpu)
1884: I0815 04:36:20.470755 17927 scope.cc:202] Create variable 0x618ad2f01723696580470745660_inner_var_0
1884: I0815 04:36:20.470786 17927 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804705525680"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236965804705525680 -> 0x630487c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:36:20.470952 17927 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:36:20.477424 18011 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.477563 18015 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.477602 18015 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236965804705525680:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.477687 18015 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236965804705525680:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:36:20.477761 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x618ad460) got event_name: TaskCompletion
1884: I0815 04:36:20.477813 18012 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.479358 18014 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.480357 18013 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.481480 18015 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13214512416237096926 to 11361946176809675993 , after update, data is {current : 88, peak : 144}.
1884: I0815 04:36:20.481501 18015 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13214512416237096926 to 11361946176809675993 , after update, data is {current : 88, peak : 144}.
1884: I0815 04:36:20.481600 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.481604 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804817550911"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804817550911"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:36:20.482017 17927 pir_interpreter.cc:161] PirInterpreter(): 0x618ad2f0 on Place(cpu)
1884: I0815 04:36:20.482046 17927 scope.cc:202] Create variable 0x618ad2f01723696580482034582_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965804817550911"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236965804817550911 -> 0x640415a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:36:20.483335 18017 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.483340 18018 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.483337 18020 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.483379 18017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236965804817550911:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.483477 18017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236965804817550911:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.486323 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x618ad460) got event_name: TaskCompletion
1884: I0815 04:36:20.483346 18016 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.487332 18019 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.495373 18017 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1346112869155597571 to 11361946176809675993 , after update, data is {current : 96, peak : 144}.
1884: I0815 04:36:20.495410 18017 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1346112869155597571 to 11361946176809675993 , after update, data is {current : 96, peak : 144}.
1884: I0815 04:36:20.496413 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.496433 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236965804817550911",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236965804966239672"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236965804817550911",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236965804966239672"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:36:20.496896 17927 pir_interpreter.cc:161] PirInterpreter(): 0x618ad2f0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236965804817550911",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236965804966239672"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236965804966239672 -> 0x640415a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:36:20.497352 18021 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.498327 18022 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.498337 18023 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.498371 18022 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.498330 18024 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.498448 18022 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.501328 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x618ad460) got event_name: TaskCompletion
1884: I0815 04:36:20.501335 18025 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.511428 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.511453 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965805116502873"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965805116502873"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:36:20.511926 17927 pir_interpreter.cc:161] PirInterpreter(): 0x618ad2f0 on Place(cpu)
1884: I0815 04:36:20.511967 17927 scope.cc:202] Create variable 0x618ad2f01723696580511946328_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236965805116502873"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236965805116502873 -> 0x633e4450
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:36:20.512346 18026 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.513321 18030 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:36:20.513330 18028 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:36:20.513331 18027 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:36:20.513357 18030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236965805116502873:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.513443 18030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236965805116502873:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:36:20.513475 17927 pir_interpreter.cc:1766] main_thread_blocker_(0x618ad460) got event_name: TaskCompletion
1884: I0815 04:36:20.513607 18029 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:36:20.517397 18030 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 340229169406226843 to 11361946176809675993 , after update, data is {current : 100, peak : 144}.
1884: I0815 04:36:20.517414 18030 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 340229169406226843 to 11361946176809675993 , after update, data is {current : 100, peak : 144}.
1884: I0815 04:36:20.521385 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.521413 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.521557 17927 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:36:20.521643 17927 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:36:20.521680 17927 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965805116502873"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965804966239672"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965805116502873"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965804966239672"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:36:20.522537 17927 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:36:20.522558 17927 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:36:20.522603 17927 pir_interpreter.cc:161] PirInterpreter(): 0x618ad2f0 on Place(cpu)
1884: I0815 04:36:20.522648 17927 scope.cc:202] Create variable feed_name_0
1884: I0815 04:36:20.522665 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_5
1884: I0815 04:36:20.522687 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_6
1884: I0815 04:36:20.522696 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_7
1884: I0815 04:36:20.522703 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_8
1884: I0815 04:36:20.522722 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_9
1884: I0815 04:36:20.522733 17927 scope.cc:202] Create variable 0x618ad2f01723696580522622009_inner_var_10
1884: I0815 04:36:20.522758 17927 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.522783 17927 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:36:20.522965 17927 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.522979 17927 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965805116502873"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236965804966239672"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236965805116502873 -> 0x633e4450
1884: 1 -> constant_folding@_17236965804966239672 -> 0x640415a0
1884: 2 -> linear_1.b_0 -> 0x630487e0
1884: 3 -> linear_1.w_0 -> 0x6315b360
1884: 4 -> feed_name_0 -> 0x6340dc70
1884: 5 -> 0x618ad2f01723696580522622009_inner_var_5 -> 0x62c3a2c0
1884: 6 -> 0x618ad2f01723696580522622009_inner_var_6 -> 0x63dd0420
1884: 7 -> 0x618ad2f01723696580522622009_inner_var_7 -> 0x63dd4b80
1884: 8 -> 0x618ad2f01723696580522622009_inner_var_8 -> 0x633e7ae0
1884: 9 -> fetch_name_0 -> 0x633e7c60
1884: 10 -> fetch_name_1 -> 0x62cadf60
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:36:20.523636 17927 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:36:20.523725 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.523793 17927 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:36:20.523818 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.523849 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x618ad2f01723696580522622009_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x618ad2f01723696580522622009_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.523888 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x618ad2f01723696580522622009_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.523916 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.523937 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.523952 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.523984 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236965804966239672:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.524003 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236965805116502873:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.524032 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236965805116502873:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x618ad2f01723696580522622009_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:36:20.524053 17927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236965805116502873:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:36:20.524081 17927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236965805116502873:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x618ad2f01723696580522622009_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:36:20.524106 17927 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 04:36:20.524124 17927 helper.h:475] after run : [cpu current allocated memory: 6.10602MB], [cpu current reserved memory: 6.10602MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 12.9708MB]
1884: I0815 04:36:20.524144 17927 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:36:20.524284 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.524289 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.524358 18031 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:36:20.524462 18031 thread_data_registry.h:135] Add data {current : -312, peak : 0} from thread 340229169406226843 to 11361946176809675993 , after update, data is {current : -212, peak : 144}.
1884: I0815 04:36:20.524471 18031 thread_data_registry.h:135] Add data {current : -312, peak : 0} from thread 340229169406226843 to 11361946176809675993 , after update, data is {current : -212, peak : 144}.
1884: I0815 04:36:20.524560 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.524570 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [<paddle.base.libpaddle.Tensor object at 0x7f9550db2d30>]
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [<paddle.base.libpaddle.Tensor object at 0x7f9550d712f0>]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [<paddle.base.libpaddle.Tensor object at 0x7f9550d713b0>]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [<paddle.base.libpaddle.Tensor object at 0x7f9550d9dbf0>]
1884: I0815 04:36:20.526480 17927 mmap_allocator.cc:348] PID: 17927, MemoryMapFdSet: set size - 0
1884: I0815 04:36:20.536361 17927 mmap_allocator.cc:348] PID: 17927, MemoryMapFdSet: set size - 0
1884: I0815 04:36:20.609027 18004 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 1767850734524999313 to 11361946176809675993 , after update, data is {current : -188, peak : 144}.
1884: I0815 04:36:20.609062 18004 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 1767850734524999313 to 11361946176809675993 , after update, data is {current : -188, peak : 144}.
1884: I0815 04:36:20.614367 18005 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 15710721543576414051 to 11361946176809675993 , after update, data is {current : -212, peak : 144}.
1884: I0815 04:36:20.614398 18005 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 15710721543576414051 to 11361946176809675993 , after update, data is {current : -212, peak : 144}.
1884: I0815 04:36:20.614404 18005 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 15710721543576414051 to 11361946176809675993 , after update, data is {current : 512, peak : 768}.
1884: I0815 04:36:20.614631 18007 thread_data_registry.h:135] Add data {current : 512, peak : 768} from thread 11361946176809675993 to 9120854850087487538 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:36:20.614643 18007 thread_data_registry.h:135] Add data {current : -212, peak : 144} from thread 11361946176809675993 to 9120854850087487538 , after update, data is {current : -92, peak : 240}.
1884: I0815 04:36:20.614648 18007 thread_data_registry.h:135] Add data {current : -212, peak : 144} from thread 11361946176809675993 to 9120854850087487538 , after update, data is {current : -92, peak : 240}.
1884: I0815 04:36:20.615373 18009 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 14872056678069870560 to 9120854850087487538 , after update, data is {current : -72, peak : 240}.
1884: I0815 04:36:20.615391 18009 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 14872056678069870560 to 9120854850087487538 , after update, data is {current : -72, peak : 240}.
1884: I0815 04:36:20.615397 18008 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 9120854850087487538 to 12118520888283153041 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:36:20.615406 18008 thread_data_registry.h:135] Add data {current : -72, peak : 240} from thread 9120854850087487538 to 12118520888283153041 , after update, data is {current : -92, peak : 240}.
1884: I0815 04:36:20.615413 18008 thread_data_registry.h:135] Add data {current : -72, peak : 240} from thread 9120854850087487538 to 12118520888283153041 , after update, data is {current : -92, peak : 240}.
1884: I0815 04:36:20.622366 18010 thread_data_registry.h:135] Add data {current : -92, peak : 240} from thread 12118520888283153041 to 11344686167747026504 , after update, data is {current : 6401792, peak : 11200896}.
1884: I0815 04:36:20.622402 18010 thread_data_registry.h:135] Add data {current : -92, peak : 240} from thread 12118520888283153041 to 11344686167747026504 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 04:36:20.622408 18010 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 12118520888283153041 to 11344686167747026504 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:36:20.819541 17927 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:36:20.819576 17927 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:36:20.819619 17927 mmap_allocator.cc:348] PID: 17927, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   14.43 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  14.64 sec
