UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
<<<<<<< HEAD
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0814 08:04:06.204996 30751 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0814 08:04:07.005236 30751 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=gpugraph_debug_gpu_memory,inner_op_parallelism,gpugraph_load_node_list_into_hbm,accuracy_check_rtol_fp16,enable_dump_main_program,logging_pir_py_code_int_tensor_element_limit,enable_pir_in_executor,enable_adjust_op_order,pir_subgraph_saving_dir,free_idle_chunk,accuracy_check_atol_bf16,cuda_memory_async_pool_realease_threshold,ir_inplace_kernel_blacklist,low_precision_op_list,gpugraph_hbm_table_load_factor,enable_cse_in_dy2st,new_executor_sequential_run,enable_async_trace,logging_pir_py_code_dump_symbolic_dims,enable_interpretercore_launch_cinn,new_executor_use_inplace,cse_max_count,auto_free_cudagraph_allocations_on_launch,cudnn_dir,all_blocks_convert_trt,search_cache_max_number,enable_neighbor_list_use_uva,static_runtime_data_save_path,check_infer_symbolic,enable_graph_multi_node_sampling,cuda_dir,nccl_blocking_wait,enable_exit_when_partial_worker,run_kp_kernel,sync_nccl_allreduce,enable_collect_shape,sort_sum_gradient,use_shm_cache,cublaslt_device_best_config,enable_fuse_parallel_matmul_pass,enable_dependency_builder_debug_info,gpugraph_enable_segment_merge_grads,cusparselt_dir,enable_cinn_accuracy_check,win_cuda_bin_dir,use_cuda_managed_memory,cudnn_batchnorm_spatial_persistent,use_stream_safe_cuda_allocator,local_exe_sub_scope_limit,cinn_compile_thread_num,enable_opt_get_features,log_memory_stats,selected_gpus,nccl_dir,gpugraph_offload_param_stat,graph_load_in_parallel,enable_pir_in_executor_trace_run,use_autotune,allreduce_record_one_event,enable_auto_detect_gpu_topo,new_executor_use_cuda_graph,cusolver_dir,cudnn_deterministic,cudnn_exhaustive_search,enable_tracker_all2all,gpugraph_force_device_batch_num_equal,use_mkldnn,dygraph_debug,pinned_memory_as_cpu_backend,enable_sparse_inner_gather,multi_node_sample_use_gpu_table,prim_check_ops,graph_neighbor_size_percent,fraction_of_gpu_memory_to_use,prim_enable_dynamic,cublas_dir,gpugraph_offload_gather_copy_maxsize,gpugraph_sparse_table_storage_mode,fast_eager_deletion_mode,allow_cinn_ops,prim_forward_blacklist,prim_enabled,use_system_allocator,use_stride_kernel,tensor_operants_mode,gpu_memory_limit_mb,enable_all2all_use_fp16,trt_ibuilder_cache,initial_gpu_memory_in_mb,gpu_allocator_retry_time,nvidia_package_dir,memory_fraction_of_eager_deletion,accuracy_check_rtol_bf16,logging_trunc_pir_py_code,prim_backward,set_to_1d,call_stack_level,pir_apply_shape_optimization_pass,use_xqa_optim,executor_log_deps_every_microseconds,curand_dir,print_ir,get_host_by_name_time,host_trace_level,custom_device_mem_record,new_executor_use_local_scope,lapack_dir,print_allocator_trace_info,check_kernel_launch,conv_workspace_size_limit,benchmark_nccl,npu_storage_format,check_nan_inf,enable_record_memory,gemm_use_half_precision_compute_type,paddle_num_threads,fleet_executor_with_standalone,cusparse_dir,cuda_malloc_async_pool_memory_throttle_ratio,query_dest_rank_by_multi_node,enable_pir_with_pt_in_dy2st,prim_all,jit_engine_type,embedding_deterministic,fraction_of_cpu_memory_to_use,use_auto_growth_pinned_allocator,fraction_of_cuda_pinned_memory_to_use,fuse_parameter_groups_size,enable_pir_api,cupti_dir,prim_forward,enable_cinn_auto_tune,gpugraph_offload_param_extends,use_auto_growth_v2,gpugraph_merge_grads_segment_size,dynamic_static_unified_comm,graph_get_neighbor_id,enable_api_kernel_fallback,new_executor_static_build,tracer_profile_fname,alloc_fill_value,graph_metapath_split_opt,enable_gpu_memory_usage_log_mb,enable_fusion_fallback,tensorrt_dir,apply_pass_to_program,gpugraph_enable_hbm_table_collision_stat,add_dependency_for_communication_op,gpugraph_enable_gpu_direct_access,mklml_dir,static_executor_perfstat_filepath,graph_embedding_split_infer_mode,op_dir,check_nan_inf_level,eager_delete_scope,sync_after_alloc,new_executor_serial_run,initial_cpu_memory_in_mb,free_when_no_cache_hit,mkl_dir,allocator_strategy,tracer_onednn_ops_on,benchmark,use_cinn,pir_debug,reallocate_gpu_memory_in_mb,gpugraph_storage_mode,accuracy_check_atol_fp32,disable_dyshape_in_train,fuse_parameter_memory_size,use_cuda_malloc_async_allocator,enable_blaslt_global_search,use_fast_math,reader_queue_speed_test_mode,cublaslt_exhaustive_search_times,einsum_opt,pir_apply_inplace_pass,auto_growth_chunk_size_in_mb,convert_all_blocks,eager_delete_tensor_gb,manually_trans_conv_filter,enable_auto_rdma_trans,use_virtual_memory_auto_growth,enable_gpu_memory_usage_log,gpugraph_slot_feasign_max_num,dump_chunk_info,conv2d_disable_cudnn,enable_cublas_tensor_op_math,deny_cinn_ops,enable_cinn_compile_cache,accuracy_check_rtol_fp32,max_inplace_grad_add,cinn_subgraph_graphviz_dir,init_allocated_mem,cudnn_exhaustive_search_times,prim_skip_dynamic,gpugraph_parallel_copyer_split_maxsize,accuracy_check_atol_fp16,enable_unused_var_check,use_pinned_memory,dataloader_use_file_descriptor,async_trace_count,gpugraph_enable_print_op_debug,gpugraph_dedup_pull_push_mode,pir_broadcast_tree_limit,multiple_of_cupti_buffer_size,print_sub_graph_dir,cache_inference_while_scope,dist_threadpool_size,logging_pir_py_code_dir,save_static_runtime_data,gpugraph_parallel_stream_num,tracer_onednn_ops_off 
1884: I0814 08:04:07.005357 30751 init.cc:108] After Parse: argc is 2
1884: I0814 08:04:14.572760 30751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:04:14.572796 30751 dygraph_functions.cc:77659] { Input: []} 
1884: W0814 08:04:14.573504 30751 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0814 08:04:14.573885 30751 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0814 08:04:14.574679 30751 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0814 08:04:14.574765 30751 allocator_facade.cc:212] selected allocator strategy:1
1884: I0814 08:04:14.574868 30751 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0814 08:04:14.575551 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f2a58000000), and remaining 0
1884: I0814 08:04:14.575793 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.575852 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.575928 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f2a58000200), and remaining 0
1884: I0814 08:04:14.575958 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f2a58000400), and remaining 0
1884: I0814 08:04:14.580004 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f2a58000600), and remaining 0
1884: I0814 08:04:14.580137 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f2a58000800), and remaining 0
1884: I0814 08:04:14.580200 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f2a58000a00), and remaining 0
1884: I0814 08:04:14.580279 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.580307 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.580374 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.580386 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.581244 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aaf2440 for it.
1884: I0814 08:04:14.581401 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.581425 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.581480 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f2a58000e00), and remaining 0
1884: I0814 08:04:14.581553 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f2a580c4400), and remaining 0
1884: I0814 08:04:14.704097 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aaf2440 for it.
1884: I0814 08:04:14.704325 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.704375 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.705010 30751 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f2a58200000), and remaining 0
1884: I0814 08:04:14.715499 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aaf2440 for it.
1884: I0814 08:04:14.715610 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:14.715644 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.715687 30751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:14.715889 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:14.716867 30751 dygraph_functions.cc:33459] Running AD API: full
1884: I0814 08:04:14.716885 30751 dygraph_functions.cc:33480] { Input: []} 
1884: I0814 08:04:14.716938 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:14.717028 30751 dygraph_functions.cc:64553] Running AD API: scale
1884: I0814 08:04:14.717054 30751 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.717118 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:14.717222 30751 dygraph_functions.cc:26170] Running AD API: exp
1884: I0814 08:04:14.717242 30751 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.717272 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:14.717572 30751 dygraph_functions.cc:72508] Running AD API: sum
1884: I0814 08:04:14.717593 30751 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.717779 30751 dygraph_functions.cc:83176] Running AD API: divide
1884: I0814 08:04:14.717804 30751 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:14.717880 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:14.720052 30751 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0814 08:04:14.720171 30751 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0814 08:04:14.720197 30751 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0814 08:04:14.720260 30751 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0814 08:04:16.173661 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.173753 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.174108 30751 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0814 08:04:16.174129 30751 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.179884 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.179924 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.181073 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.181092 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.181106 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.181921 30751 program_interpreter.cc:243] New Executor is Running.
1884: I0814 08:04:16.181934 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.181950 30751 scope.cc:202] Create variable feed
1884: I0814 08:04:16.181960 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.181970 30751 scope.cc:202] Create variable fetch
1884: I0814 08:04:16.181975 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.181988 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.181994 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.181998 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.182000 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.184507 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.184862 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.184875 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.184880 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.186547 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.186594 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.186602 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.186609 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.186619 30751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:04:16.186625 30751 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61ab2820 type is 7
1884: I0814 08:04:16.186632 30751 scope.cc:202] Create variable x
1884: I0814 08:04:16.186635 30751 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x61ab2970 type is 7
1884: I0814 08:04:16.186699 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.186707 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.186710 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.186714 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.186841 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.186864 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.186983 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.186995 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.187011 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.187173 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.187203 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.187224 30751 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.187230 30751 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61aba9c0Variable Type 7
1884: I0814 08:04:16.187260 30751 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.187285 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.187345 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.187367 30751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.188920 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.188977 30751 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.189404 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.193939 30751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:04:16.193959 30751 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:04:16.194041 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.194069 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.194593 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: I0814 08:04:16.194664 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.194687 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.195152 30751 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: I0814 08:04:16.195214 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.195237 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.195261 30751 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.196111 30751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:04:16.196125 30751 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:04:16.196245 30751 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0814 08:04:16.196269 30751 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.196666 30751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:04:16.196677 30751 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:04:16.196719 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.196738 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.196920 30751 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0814 08:04:16.196929 30751 dygraph_functions.cc:77659] { Input: []} 
1884: I0814 08:04:16.196964 30751 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0814 08:04:16.196981 30751 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0814 08:04:16.196997 30751 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.200153 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.200178 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.200232 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.200240 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.202142 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.202512 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.202526 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.202531 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.204315 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.204365 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.204375 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.204380 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61ae5d60 type is 7
1884: I0814 08:04:16.204391 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.204394 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61ae60d0 type is 7
1884: I0814 08:04:16.204399 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.204407 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.204465 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.204473 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.204478 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.204481 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.204531 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.204550 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.204610 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.204620 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.204638 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.204861 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.204877 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.204895 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.204902 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61aec850Variable Type 7
1884: I0814 08:04:16.204921 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.204938 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.204964 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.204981 30751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.205789 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.205816 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.205991 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.217933 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: I0814 08:04:16.218132 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 08:04:16.225622 30751 pir_interpreter.cc:161] PirInterpreter(): 0x61a949e0 on Place(gpu:0)
1884: I0814 08:04:16.225666 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.225699 30751 scope.cc:202] Create variable 0x61a949e01723622656225650845_inner_var_1
1884: I0814 08:04:16.225711 30751 scope.cc:202] Create variable 0x61a949e01723622656225650845_inner_var_2
1884: I0814 08:04:16.225723 30751 scope.cc:202] Create variable 0x61a949e01723622656225650845_inner_var_3
1884: I0814 08:04:16.225733 30751 scope.cc:202] Create variable 0x61a949e01723622656225650845_inner_var_4
1884: I0814 08:04:16.225744 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.226154 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.226171 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.226176 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0814 08:04:16.226218 30751 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61a948b0
1884: 1 -> 0x61a949e01723622656225650845_inner_var_1 -> 0x61a949c0
1884: 2 -> 0x61a949e01723622656225650845_inner_var_2 -> 0x61aa8a10
1884: 3 -> 0x61a949e01723622656225650845_inner_var_3 -> 0x61a94250
1884: 4 -> 0x61a949e01723622656225650845_inner_var_4 -> 0x61acc2b0
1884: 5 -> fetch0@fetch -> 0x61adf5f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.226989 30751 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0814 08:04:16.227217 30789 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.227386 30790 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.227439 30791 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.227551 30793 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.227555 30792 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.227573 30791 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a949e01723622656225650845_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.227707 30794 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.227699 30791 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a949e01723622656225650845_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.227764 30794 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.227811 30794 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 08:04:16.227859 30794 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a949e01723622656225650845_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a949e01723622656225650845_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.228068 30794 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a949e01723622656225650845_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a949e01723622656225650845_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 08:04:16.228137 30791 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a949e01723622656225650845_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.228163 30791 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.229707 30791 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a949e01723622656225650845_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61a949e01723622656225650845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:04:16.229748 30791 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61a949e01723622656225650845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.229774 30791 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.230545 30791 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61a949e01723622656225650845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:04:16.230582 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x61a94b50) got event_name: TaskCompletion
1884: I0814 08:04:16.230608 30751 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.314034 30789 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 8797546167441082165 to 10669983319922967812 , after update, data is {current : 0, peak : 800768}.
1884: I0814 08:04:16.314060 30789 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8797546167441082165 to 14308904636666403190 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:04:16.314066 30789 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8797546167441082165 to 14308904636666403190 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:04:16.314244 30791 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 08:04:16.314256 30791 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 800000, peak : 2400000}.
1884: I0814 08:04:16.314477 30794 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0814 08:04:16.314494 30794 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.319833 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.319856 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.319919 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.319927 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.321712 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.322042 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.322055 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.322060 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.323567 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.323662 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.323673 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.323679 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6422f500 type is 7
1884: I0814 08:04:16.323688 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.323690 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x64195900 type is 7
1884: I0814 08:04:16.323694 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.323699 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.323752 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.323758 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.323762 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.323765 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.323820 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.323836 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.323897 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.323906 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.323920 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.324050 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.324061 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.324075 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.324082 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64223420Variable Type 7
1884: I0814 08:04:16.324097 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.324116 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.324137 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.324153 30751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.325722 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.325757 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.325963 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.332520 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: I0814 08:04:16.332703 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 08:04:16.335810 30751 pir_interpreter.cc:161] PirInterpreter(): 0x63d10720 on Place(gpu:0)
1884: I0814 08:04:16.335847 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.335871 30751 scope.cc:202] Create variable 0x63d107201723622656335837752_inner_var_1
1884: I0814 08:04:16.335882 30751 scope.cc:202] Create variable 0x63d107201723622656335837752_inner_var_2
1884: I0814 08:04:16.335894 30751 scope.cc:202] Create variable 0x63d107201723622656335837752_inner_var_3
1884: I0814 08:04:16.335906 30751 scope.cc:202] Create variable 0x63d107201723622656335837752_inner_var_4
1884: I0814 08:04:16.335917 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.336257 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.336272 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.336277 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x64247110
1884: 1 -> 0x63d107201723622656335837752_inner_var_1 -> 0x64247310
1884: 2 -> 0x63d107201723622656335837752_inner_var_2 -> 0x64135b10
1884: 3 -> 0x63d107201723622656335837752_inner_var_3 -> 0x6423b1c0
1884: 4 -> 0x63d107201723622656335837752_inner_var_4 -> 0x64135c10
1884: 5 -> fetch0@fetch -> 0x61a9d660
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.337029 30795 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.337127 30796 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.337147 30797 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.337184 30798 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.337253 30799 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.337272 30799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63d107201723622656335837752_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.337335 30799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63d107201723622656335837752_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.337397 30800 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.337419 30800 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.337445 30800 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:04:16.337482 30800 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63d107201723622656335837752_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63d107201723622656335837752_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.337594 30800 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63d107201723622656335837752_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63d107201723622656335837752_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.337671 30799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d107201723622656335837752_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.337723 30799 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.341367 30799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63d107201723622656335837752_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63d107201723622656335837752_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.341418 30799 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63d107201723622656335837752_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.341439 30799 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.344290 30799 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63d107201723622656335837752_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.344341 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x63d10890) got event_name: TaskCompletion
1884: I0814 08:04:16.344363 30751 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.383987 30795 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 10669983319922967812 to 14308904636666403190 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 08:04:16.384006 30795 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:04:16.384011 30795 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:04:16.384275 30799 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7817043709061517111 to 5349754552814084333 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 08:04:16.384291 30799 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7817043709061517111 to 5349754552814084333 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0814 08:04:16.384431 30800 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.388686 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.388710 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.388765 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.388773 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.390482 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.390815 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.390830 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.390834 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.392342 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.392436 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.392446 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.392453 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x64113a70 type is 7
1884: I0814 08:04:16.392463 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.392467 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x64113c10 type is 7
1884: I0814 08:04:16.392472 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.392477 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.392530 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.392536 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.392540 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.392544 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.392596 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.392611 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.392669 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.392678 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.392691 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.392727 30751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.392845 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.392899 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.392908 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.392923 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.392930 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x643fd560Variable Type 7
1884: I0814 08:04:16.392947 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.392966 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.392988 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.393003 30751 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.393265 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.393286 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.393486 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.394260 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: I0814 08:04:16.394446 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 08:04:16.397464 30751 pir_interpreter.cc:161] PirInterpreter(): 0x64114e60 on Place(gpu:0)
1884: I0814 08:04:16.397498 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.397521 30751 scope.cc:202] Create variable 0x64114e601723622656397489389_inner_var_1
1884: I0814 08:04:16.397532 30751 scope.cc:202] Create variable 0x64114e601723622656397489389_inner_var_2
1884: I0814 08:04:16.397543 30751 scope.cc:202] Create variable 0x64114e601723622656397489389_inner_var_3
1884: I0814 08:04:16.397554 30751 scope.cc:202] Create variable 0x64114e601723622656397489389_inner_var_4
1884: I0814 08:04:16.397563 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.397893 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.397907 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.397912 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61aab390
1884: 1 -> 0x64114e601723622656397489389_inner_var_1 -> 0x61aab410
1884: 2 -> 0x64114e601723622656397489389_inner_var_2 -> 0x643f8d50
1884: 3 -> 0x64114e601723622656397489389_inner_var_3 -> 0x643eb6d0
1884: 4 -> 0x64114e601723622656397489389_inner_var_4 -> 0x64114460
1884: 5 -> fetch0@fetch -> 0x61a9c160
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.398592 30801 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.398682 30802 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.398738 30803 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.398790 30804 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.398792 30805 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.398830 30804 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64114e601723622656397489389_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.398882 30804 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64114e601723622656397489389_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.398943 30806 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.398963 30806 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.398986 30806 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 08:04:16.399017 30806 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64114e601723622656397489389_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64114e601723622656397489389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.399053 30806 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.399164 30806 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.399191 30806 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64114e601723622656397489389_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64114e601723622656397489389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 08:04:16.399258 30804 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64114e601723622656397489389_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.399289 30804 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.399552 30804 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64114e601723622656397489389_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x64114e601723622656397489389_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:04:16.399578 30804 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x64114e601723622656397489389_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.399596 30804 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.399610 30804 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x64114e601723622656397489389_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:04:16.399641 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x64114fd0) got event_name: TaskCompletion
1884: I0814 08:04:16.399663 30751 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.434181 30801 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 14308904636666403190 to 10669983319922967812 , after update, data is {current : 0, peak : 3328}.
1884: I0814 08:04:16.434197 30801 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 14308904636666403190 to 10669983319922967812 , after update, data is {current : -804, peak : 2000}.
1884: I0814 08:04:16.434203 30801 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 14308904636666403190 to 10669983319922967812 , after update, data is {current : -804, peak : 2000}.
1884: I0814 08:04:16.434383 30804 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 5349043440585914781 to 10669983319922967812 , after update, data is {current : 800, peak : 2000}.
1884: I0814 08:04:16.434394 30804 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 5349043440585914781 to 10669983319922967812 , after update, data is {current : 800, peak : 2000}.
1884: I0814 08:04:16.434569 30806 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 08:04:16.434579 30806 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0814 08:04:16.434585 30806 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.439308 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.439332 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.439388 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.439395 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.441046 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.441396 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.441409 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.441416 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.442914 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.443001 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.443012 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.443017 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61ad9ec0 type is 7
1884: I0814 08:04:16.443025 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.443028 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61ad99b0 type is 7
1884: I0814 08:04:16.443033 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.443038 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.443092 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.443099 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.443104 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.443107 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.443157 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.443171 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.443228 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.443238 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.443251 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.443459 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.443471 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.443488 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.443495 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61a9c7b0Variable Type 7
1884: I0814 08:04:16.443512 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.443531 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.443553 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.443568 30751 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.444382 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.444412 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.444617 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.447257 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: I0814 08:04:16.447448 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0814 08:04:16.450500 30751 pir_interpreter.cc:161] PirInterpreter(): 0x61a9c980 on Place(gpu:0)
1884: I0814 08:04:16.450533 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.450556 30751 scope.cc:202] Create variable 0x61a9c9801723622656450524989_inner_var_1
1884: I0814 08:04:16.450567 30751 scope.cc:202] Create variable 0x61a9c9801723622656450524989_inner_var_2
1884: I0814 08:04:16.450577 30751 scope.cc:202] Create variable 0x61a9c9801723622656450524989_inner_var_3
1884: I0814 08:04:16.450588 30751 scope.cc:202] Create variable 0x61a9c9801723622656450524989_inner_var_4
1884: I0814 08:04:16.450598 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.450919 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.450933 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.450937 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61c44620
1884: 1 -> 0x61a9c9801723622656450524989_inner_var_1 -> 0x61c44680
1884: 2 -> 0x61a9c9801723622656450524989_inner_var_2 -> 0x1aa4be60
1884: 3 -> 0x61a9c9801723622656450524989_inner_var_3 -> 0x61aa8df0
1884: 4 -> 0x61a9c9801723622656450524989_inner_var_4 -> 0x61a9c3b0
1884: 5 -> fetch0@fetch -> 0x63aaebf0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.451660 30807 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.451740 30808 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.451802 30809 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.451824 30810 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.451851 30811 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.451902 30811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a9c9801723622656450524989_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.451992 30811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61a9c9801723622656450524989_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.452064 30812 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.452088 30812 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.452126 30812 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0814 08:04:16.452163 30812 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a9c9801723622656450524989_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a9c9801723622656450524989_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.452436 30812 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61a9c9801723622656450524989_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61a9c9801723622656450524989_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0814 08:04:16.452523 30811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a9c9801723622656450524989_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.452576 30811 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.453845 30811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61a9c9801723622656450524989_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61a9c9801723622656450524989_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:04:16.453894 30811 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61a9c9801723622656450524989_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.453918 30811 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.454504 30811 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61a9c9801723622656450524989_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0814 08:04:16.454546 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x61a9caf0) got event_name: TaskCompletion
1884: I0814 08:04:16.454568 30751 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.490912 30807 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 10669983319922967812 to 14308904636666403190 , after update, data is {current : 0, peak : 800768}.
1884: I0814 08:04:16.490927 30807 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:04:16.490932 30807 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 800000, peak : 1600004}.
1884: I0814 08:04:16.491185 30811 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 7817043709061517111 to 5349754552814084333 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 08:04:16.491199 30811 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 7817043709061517111 to 5349754552814084333 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0814 08:04:16.491353 30812 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0814 08:04:16.491380 30812 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.497010 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.497035 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.497093 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.497102 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.498833 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.499181 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.499194 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.499199 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.500744 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.500838 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.500850 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.500856 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x64236cf0 type is 7
1884: I0814 08:04:16.500866 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.500869 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61aa78f0 type is 7
1884: I0814 08:04:16.500874 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.500880 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.500936 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.500941 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.500946 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.500950 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.501005 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.501020 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.501081 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.501091 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.501106 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.501232 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.501243 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.501258 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.501264 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61aae120Variable Type 7
1884: I0814 08:04:16.501281 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.501298 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.501329 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.501344 30751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.503151 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.503185 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.503407 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.507946 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: I0814 08:04:16.508127 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0814 08:04:16.511224 30751 pir_interpreter.cc:161] PirInterpreter(): 0x61aada30 on Place(gpu:0)
1884: I0814 08:04:16.511257 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.511281 30751 scope.cc:202] Create variable 0x61aada301723622656511248548_inner_var_1
1884: I0814 08:04:16.511291 30751 scope.cc:202] Create variable 0x61aada301723622656511248548_inner_var_2
1884: I0814 08:04:16.511328 30751 scope.cc:202] Create variable 0x61aada301723622656511248548_inner_var_3
1884: I0814 08:04:16.511341 30751 scope.cc:202] Create variable 0x61aada301723622656511248548_inner_var_4
1884: I0814 08:04:16.511350 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.511672 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.511687 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.511690 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fd9dc10
1884: 1 -> 0x61aada301723622656511248548_inner_var_1 -> 0x5fd9dc90
1884: 2 -> 0x61aada301723622656511248548_inner_var_2 -> 0x61aae0f0
1884: 3 -> 0x61aada301723622656511248548_inner_var_3 -> 0x61a94fe0
1884: 4 -> 0x61aada301723622656511248548_inner_var_4 -> 0x643ecf40
1884: 5 -> fetch0@fetch -> 0x61ae0810
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.512394 30813 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.512476 30814 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.512511 30815 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.512552 30816 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.512588 30817 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.512640 30818 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.512645 30817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61aada301723622656511248548_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.512663 30818 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.512699 30818 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:04:16.512705 30817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61aada301723622656511248548_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.512737 30818 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61aada301723622656511248548_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61aada301723622656511248548_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.512847 30818 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61aada301723622656511248548_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61aada301723622656511248548_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.512907 30817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61aada301723622656511248548_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.512929 30817 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.516673 30817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61aada301723622656511248548_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61aada301723622656511248548_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.516721 30817 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61aada301723622656511248548_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.516742 30817 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.519712 30817 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61aada301723622656511248548_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0814 08:04:16.519757 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x61aadba0) got event_name: TaskCompletion
1884: I0814 08:04:16.519776 30751 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.562294 30813 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 14308904636666403190 to 10669983319922967812 , after update, data is {current : 0, peak : 2400768}.
1884: I0814 08:04:16.562323 30813 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 14308904636666403190 to 16118702692421813509 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:04:16.562330 30813 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 14308904636666403190 to 16118702692421813509 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0814 08:04:16.562491 30817 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 16118702692421813509 to 5349754552814084333 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 08:04:16.562503 30817 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 16118702692421813509 to 5349754552814084333 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0814 08:04:16.562687 30818 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 10669983319922967812 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.567098 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.567119 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.567173 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.567181 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.568866 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.569208 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.569221 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.569226 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.570744 30751 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0814 08:04:16.570828 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.570840 30751 scope.cc:202] Create variable Out
1884: I0814 08:04:16.570847 30751 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x64236fc0 type is 7
1884: I0814 08:04:16.570855 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.570859 30751 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61adb370 type is 7
1884: I0814 08:04:16.570863 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.570868 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.570922 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.570928 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.570932 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.570936 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.570988 30751 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.571002 30751 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.571061 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.571069 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.571084 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.571118 30751 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.571229 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.571273 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.571283 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.571307 30751 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.571314 30751 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63552780Variable Type 7
1884: I0814 08:04:16.571331 30751 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.571350 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.571372 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.571388 30751 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.571502 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.571524 30751 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.571720 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.572482 30751 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1aabf220 for it.
1884: I0814 08:04:16.572651 30751 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1aac7bf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0814 08:04:16.575632 30751 pir_interpreter.cc:161] PirInterpreter(): 0x635521b0 on Place(gpu:0)
1884: I0814 08:04:16.575665 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.575688 30751 scope.cc:202] Create variable 0x635521b01723622656575657419_inner_var_1
1884: I0814 08:04:16.575699 30751 scope.cc:202] Create variable 0x635521b01723622656575657419_inner_var_2
1884: I0814 08:04:16.575711 30751 scope.cc:202] Create variable 0x635521b01723622656575657419_inner_var_3
1884: I0814 08:04:16.575722 30751 scope.cc:202] Create variable 0x635521b01723622656575657419_inner_var_4
1884: I0814 08:04:16.575733 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.576063 30751 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0814 08:04:16.576078 30751 scope.cc:202] Create variable X
1884: I0814 08:04:16.576082 30751 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x63552840
1884: 1 -> 0x635521b01723622656575657419_inner_var_1 -> 0x639a8a30
1884: 2 -> 0x635521b01723622656575657419_inner_var_2 -> 0x64196a20
1884: 3 -> 0x635521b01723622656575657419_inner_var_3 -> 0x63abafc0
1884: 4 -> 0x635521b01723622656575657419_inner_var_4 -> 0x6682ad0
1884: 5 -> fetch0@fetch -> 0x641139a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0814 08:04:16.576759 30819 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.576859 30820 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.576918 30822 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.576927 30821 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.576978 30823 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.577010 30824 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.577008 30823 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x635521b01723622656575657419_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.577030 30824 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.577059 30823 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x635521b01723622656575657419_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.577062 30824 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0814 08:04:16.577092 30824 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x635521b01723622656575657419_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x635521b01723622656575657419_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.577132 30824 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.577245 30824 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.577271 30824 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x635521b01723622656575657419_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x635521b01723622656575657419_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0814 08:04:16.577332 30823 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x635521b01723622656575657419_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.577355 30823 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.577493 30823 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x635521b01723622656575657419_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x635521b01723622656575657419_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:04:16.577518 30823 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x635521b01723622656575657419_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.577535 30823 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.577548 30823 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x635521b01723622656575657419_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0814 08:04:16.577577 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x63552320) got event_name: TaskCompletion
1884: I0814 08:04:16.577600 30751 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.612013 30819 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 10669983319922967812 to 14308904636666403190 , after update, data is {current : 0, peak : 10240}.
1884: I0814 08:04:16.612032 30819 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 800, peak : 1604}.
1884: I0814 08:04:16.612037 30819 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 10669983319922967812 to 7817043709061517111 , after update, data is {current : 800, peak : 1604}.
1884: I0814 08:04:16.612236 30823 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 7817043709061517111 to 14308904636666403190 , after update, data is {current : 800, peak : 8000}.
1884: I0814 08:04:16.612247 30823 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 7817043709061517111 to 14308904636666403190 , after update, data is {current : 800, peak : 8000}.
1884: I0814 08:04:16.612428 30824 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 08:04:16.612439 30824 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0814 08:04:16.612444 30824 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 14308904636666403190 to 5349754552814084333 , after update, data is {current : 0, peak : 2401024}.
1884: I0814 08:04:16.619371 30751 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0814 08:04:16.619422 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 08:04:16.620496 30751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:04:16.621328 30751 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0814 08:04:16.621356 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 08:04:16.622653 30751 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0814 08:04:16.622675 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:04:16.623351 30751 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0814 08:04:16.624325 30751 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0814 08:04:16.624351 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:04:16.625810 30751 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0814 08:04:16.625830 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:04:16.626415 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.626441 30751 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0814 08:04:16.626448 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.626456 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.628432 30751 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0814 08:04:16.628458 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:04:16.629462 30751 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0814 08:04:16.629489 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 08:04:16.630452 30751 pybind.cc:1827] need skip: 0
1884: I0814 08:04:16.630755 30751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:04:16.632568 30751 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0814 08:04:16.636229 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.636245 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.636251 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.638275 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.638295 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.638314 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.638319 30751 scope.cc:202] Create variable learning_rate_0
1884: I0814 08:04:16.638327 30751 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63699b10 type is 7
1884: I0814 08:04:16.638332 30751 scope.cc:202] Create variable linear_0.b_0
1884: I0814 08:04:16.638337 30751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x639a7450 type is 7
1884: I0814 08:04:16.638343 30751 scope.cc:202] Create variable linear_0.w_0
1884: I0814 08:04:16.638346 30751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x639a7220 type is 7
1884: I0814 08:04:16.638412 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.638418 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.638423 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.638427 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.638481 30751 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.638495 30751 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.638518 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0814 08:04:16.638643 30751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.638653 30751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.639295 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.639345 30751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.639355 30751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.639386 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.640399 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.641718 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.642151 30751 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0814 08:04:16.642392 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.642689 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.642901 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.642918 30751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:04:16.642987 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.642993 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.642997 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.643098 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.643110 30751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:04:16.644666 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.645996 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.647096 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.647282 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.647294 30751 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 08:04:16.647306 30751 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x631cab70 type is 7
1884: I0814 08:04:16.647315 30751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:04:16.647320 30751 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x631ca8f0 type is 7
1884: I0814 08:04:16.647325 30751 scope.cc:202] Create variable cast_0.tmp_0
1884: I0814 08:04:16.647329 30751 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x631ca9e0 type is 7
1884: I0814 08:04:16.647333 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.647338 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.647343 30751 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 08:04:16.647347 30751 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x631cbfb0 type is 7
1884: I0814 08:04:16.647352 30751 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63699b10 type is 7
1884: I0814 08:04:16.647357 30751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x639a7450 type is 7
1884: I0814 08:04:16.647361 30751 scope.cc:202] Create variable linear_0.tmp_0
1884: I0814 08:04:16.647364 30751 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x631cbf90 type is 7
1884: I0814 08:04:16.647368 30751 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 08:04:16.647372 30751 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x631cc4f0 type is 7
1884: I0814 08:04:16.647375 30751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x639a7220 type is 7
1884: I0814 08:04:16.647379 30751 scope.cc:202] Create variable mean_0.tmp_0
1884: I0814 08:04:16.647383 30751 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x631cc760 type is 7
1884: I0814 08:04:16.647387 30751 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0814 08:04:16.647390 30751 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x631cc9a0 type is 7
1884: I0814 08:04:16.647395 30751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:04:16.647398 30751 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x631ccc00 type is 7
1884: I0814 08:04:16.647481 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.647495 30751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:04:16.647552 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.647558 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.647563 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.647567 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.647615 30751 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.647627 30751 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.647644 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0814 08:04:16.647742 30751 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.647751 30751 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.647771 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:04:16.647843 30751 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:04:16.647922 30751 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0814 08:04:16.650892 30751 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.650913 30751 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.650978 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.651033 30751 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651044 30751 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651058 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:04:16.651082 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.651115 30751 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651124 30751 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651137 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:04:16.651224 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651234 30751 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651248 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.651362 30751 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.651437 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.651490 30751 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651501 30751 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651515 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:04:16.651547 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.651592 30751 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651602 30751 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651615 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0814 08:04:16.651716 30751 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651726 30751 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651746 30751 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.651789 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.651798 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.651814 30751 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.651821 30751 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x639abd10Variable Type 7
1884: I0814 08:04:16.651839 30751 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.651855 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.651875 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.651890 30751 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.651929 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.651952 30751 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0814 08:04:16.651978 30751 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.651986 30751 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.652000 30751 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0814 08:04:16.652006 30751 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6419a0c0Variable Type 7
1884: I0814 08:04:16.652019 30751 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0814 08:04:16.652031 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.652047 30751 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.652060 30751 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.652091 30751 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0814 08:04:16.652104 30751 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0814 08:04:16.652544 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0814 08:04:16.652578 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:04:16.652596 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:04:16.652631 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0814 08:04:16.652663 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.652681 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0814 08:04:16.656886 30751 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 08:04:16.656920 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:04:16.657617 30751 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0814 08:04:16.657639 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:04:16.657961 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.659624 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.660452 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.660565 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.661072 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.661993 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.664031 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.665081 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.666915 30751 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0814 08:04:16.667685 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.667699 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.667706 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.668871 30751 interpreter_util.cc:1169] Creating Variables
1884: I0814 08:04:16.668887 30751 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x53ecee0 type is 9
1884: I0814 08:04:16.668895 30751 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x5400c10 type is 10
1884: I0814 08:04:16.668902 30751 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x639a7450 type is 7
1884: I0814 08:04:16.668907 30751 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x639a7220 type is 7
1884: I0814 08:04:16.668912 30751 scope.cc:202] Create variable saved_params
1884: I0814 08:04:16.668915 30751 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x63a2c870 type is 17
1884: I0814 08:04:16.668943 30751 interpreter_util.cc:594] Static build: 0
1884: I0814 08:04:16.668949 30751 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0814 08:04:16.668953 30751 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0814 08:04:16.668957 30751 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0814 08:04:16.668995 30751 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.669008 30751 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0814 08:04:16.670203 30751 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 08:04:16.670244 30751 analysis_predictor.cc:433] Predictor::init()
1884: I0814 08:04:16.670306 30751 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0814 08:04:16.671653 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.671710 30751 scope.cc:202] Create variable feed
1884: I0814 08:04:16.671717 30751 naive_executor.cc:189] 0x6435d380 Create persistable variable feed, which pointer is 0x639b40f0
1884: I0814 08:04:16.671723 30751 scope.cc:202] Create variable fetch
1884: I0814 08:04:16.671726 30751 naive_executor.cc:189] 0x6435d380 Create persistable variable fetch, which pointer is 0x639b3f90
1884: I0814 08:04:16.671731 30751 scope.cc:202] Create variable linear_0.b_0
1884: I0814 08:04:16.671734 30751 naive_executor.cc:189] 0x6435d380 Create persistable variable linear_0.b_0, which pointer is 0x640820c0
1884: I0814 08:04:16.671739 30751 scope.cc:202] Create variable linear_0.w_0
1884: I0814 08:04:16.671742 30751 naive_executor.cc:189] 0x6435d380 Create persistable variable linear_0.w_0, which pointer is 0x6325edd0
1884: I0814 08:04:16.671758 30751 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0814 08:04:16.672106 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.672195 30751 program_converter.cc:296] is_legacy_program : 0
1884: I0814 08:04:16.672245 30751 executor.cc:183] Old Executor is Running.
1884: I0814 08:04:16.672329 30751 executor.cc:92] Creating Variables for block 0
1884: I0814 08:04:16.672338 30751 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0814 08:04:16.672340 30751 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x640820c0 type is 7
1884: I0814 08:04:16.672344 30751 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0814 08:04:16.672348 30751 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x6325edd0 type is 7
1884: I0814 08:04:16.672379 30751 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.672453 30751 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0814 08:04:16.672497 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.672502 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.672641 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.672865 30751 graph.cc:149] create OpNode by feed
1884: I0814 08:04:16.672904 30751 graph.cc:149] create OpNode by matmul_v2
1884: I0814 08:04:16.672919 30751 graph.cc:149] create OpNode by elementwise_add
1884: I0814 08:04:16.672933 30751 graph.cc:149] create OpNode by abs
1884: I0814 08:04:16.672945 30751 graph.cc:149] create OpNode by assign_value
1884: I0814 08:04:16.672961 30751 graph.cc:149] create OpNode by multinomial
1884: I0814 08:04:16.672971 30751 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0814 08:04:16.672986 30751 graph.cc:149] create OpNode by scale
1884: I0814 08:04:16.672999 30751 graph.cc:149] create OpNode by scale
1884: I0814 08:04:16.673010 30751 graph.cc:149] create OpNode by fetch
1884: I0814 08:04:16.673027 30751 graph.cc:149] create OpNode by fetch
1884: I0814 08:04:16.673048 30751 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0814 08:04:16.674263 30751 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0814 08:04:16.674270 30751 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0814 08:04:16.674348 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.674355 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0814 08:04:16.674468 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.674718 30751 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 08:04:16.674778 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.674784 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0814 08:04:16.674819 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.674825 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0814 08:04:16.674865 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.674925 30751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:04:16.674957 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.674963 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0814 08:04:16.674983 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.674996 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675020 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675026 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0814 08:04:16.675066 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675087 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675112 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675117 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0814 08:04:16.675160 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675235 30751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:04:16.675263 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675269 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0814 08:04:16.675315 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675335 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675359 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675364 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0814 08:04:16.675396 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675554 30751 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 08:04:16.675583 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675590 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0814 08:04:16.675621 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675638 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675662 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675668 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0814 08:04:16.675688 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675704 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675725 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675730 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0814 08:04:16.675752 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675767 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.675789 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675794 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0814 08:04:16.675818 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.675884 30751 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0814 08:04:16.675917 30751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:04:16.675933 30751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:04:16.675946 30751 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0814 08:04:16.675971 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.675976 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0814 08:04:16.676002 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.676043 30751 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0814 08:04:16.676062 30751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:04:16.676074 30751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:04:16.676086 30751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:04:16.676117 30751 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 08:04:16.676128 30751 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0814 08:04:16.677292 30751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:04:16.677347 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.677353 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0814 08:04:16.677381 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.677402 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.677428 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.677433 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0814 08:04:16.677457 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.677506 30751 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0814 08:04:16.677536 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.677541 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0814 08:04:16.677561 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.677577 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.677599 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.677604 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0814 08:04:16.677639 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.677727 30751 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0814 08:04:16.677755 30751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:04:16.677771 30751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:04:16.677788 30751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:04:16.677803 30751 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 08:04:16.677816 30751 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 08:04:16.677834 30751 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0814 08:04:16.677855 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.677928 30751 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0814 08:04:16.677951 30751 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0814 08:04:16.677964 30751 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0814 08:04:16.677978 30751 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0814 08:04:16.677992 30751 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0814 08:04:16.678006 30751 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0814 08:04:16.678021 30751 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0814 08:04:16.678066 30751 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0814 08:04:16.678334 30751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:04:16.678365 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.678370 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0814 08:04:16.678417 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678476 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678510 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678556 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678584 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678625 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678649 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678686 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678707 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678740 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678758 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678788 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678804 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678830 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678843 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678866 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678877 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678896 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.678922 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.678927 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0814 08:04:16.678954 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.678995 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.679020 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679026 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0814 08:04:16.679035 30751 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 08:04:16.679039 30751 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:04:16.679090 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.679111 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.679136 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679142 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0814 08:04:16.679153 30751 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 08:04:16.679157 30751 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:04:16.679196 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.679219 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.679242 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679248 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0814 08:04:16.679258 30751 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0814 08:04:16.679262 30751 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:04:16.679294 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.679320 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.679343 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679349 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0814 08:04:16.679359 30751 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0814 08:04:16.679363 30751 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0814 08:04:16.679399 30751 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0814 08:04:16.679420 30751 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0814 08:04:16.679445 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679450 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0814 08:04:16.679462 30751 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0814 08:04:16.679504 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679510 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0814 08:04:16.679584 30751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:04:16.679605 30751 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.679622 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0814 08:04:16.679675 30751 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0814 08:04:16.679692 30751 scope.cc:202] Create variable assign_0.tmp_0
1884: I0814 08:04:16.679719 30751 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0814 08:04:16.679744 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.679749 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0814 08:04:16.680667 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.680682 30751 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0814 08:04:16.680733 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.680740 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.681347 30751 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0814 08:04:16.681556 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.681628 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.681634 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.682036 30751 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0814 08:04:16.682251 30751 graph.h:183] deleting __fuse_statis__
1884: I0814 08:04:16.682260 30751 graph.h:183] deleting pass_recorder
1884: I0814 08:04:16.682264 30751 graph.h:183] deleting stale_program_op_descs
1884: I0814 08:04:16.682368 30751 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0814 08:04:16.682379 30751 scope.cc:202] Create variable abs_0.tmp_0
1884: I0814 08:04:16.682382 30751 naive_executor.cc:195] 0x6435d380 Create variable abs_0.tmp_0, which pointer is 0x64191c50
1884: I0814 08:04:16.682389 30751 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0814 08:04:16.682391 30751 naive_executor.cc:195] 0x6435d380 Create variable gaussian_0.tmp_0, which pointer is 0x6366a8d0
1884: I0814 08:04:16.682404 30751 scope.cc:202] Create variable linear_0.tmp_1
1884: I0814 08:04:16.682410 30751 naive_executor.cc:195] 0x6435d380 Create variable linear_0.tmp_1, which pointer is 0x6353e0b0
1884: I0814 08:04:16.682413 30751 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0814 08:04:16.682416 30751 naive_executor.cc:195] 0x6435d380 Create variable multinomial_0.tmp_0, which pointer is 0x6353db50
1884: I0814 08:04:16.682420 30751 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0814 08:04:16.682423 30751 naive_executor.cc:195] 0x6435d380 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x6353de50
1884: I0814 08:04:16.682427 30751 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0814 08:04:16.682430 30751 naive_executor.cc:195] 0x6435d380 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x6353c450
1884: I0814 08:04:16.682436 30751 scope.cc:202] Create variable feed
1884: I0814 08:04:16.682440 30751 scope.cc:202] Create variable fetch
1884: I0814 08:04:16.682461 30751 naive_executor.cc:46] NaiveExecutor init with scope 0x6435d380
1884: I0814 08:04:16.682466 30751 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0814 08:04:16.682657 30751 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0814 08:04:16.682670 30751 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0814 08:04:16.682698 30751 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0814 08:04:16.682703 30751 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0814 08:04:16.682711 30751 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.682742 30751 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:04:16.682952 30751 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.682967 30751 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:04:16.683012 30751 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.683038 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0814 08:04:16.737087 30751 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:04:16.737164 30751 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.737186 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0814 08:04:16.737237 30751 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:04:16.737267 30751 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.737288 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0814 08:04:16.737355 30751 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 08:04:16.737396 30751 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.737412 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:04:16.737462 30751 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0814 08:04:16.737488 30751 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0814 08:04:16.737504 30751 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0814 08:04:16.737536 30751 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0814 08:04:16.737555 30751 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.737581 30751 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:04:16.737603 30751 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 08:04:16.737951 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.737959 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0814 08:04:16.780925 30751 pir_interpreter.cc:161] PirInterpreter(): 0x63ab4f30 on Place(gpu:0)
1884: I0814 08:04:16.780965 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_0
1884: I0814 08:04:16.780982 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_1
1884: I0814 08:04:16.780992 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_2
1884: I0814 08:04:16.781001 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_3
1884: I0814 08:04:16.781030 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_4
1884: I0814 08:04:16.781044 30751 scope.cc:202] Create variable 0x63ab4f301723622656780953602_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x63ab4f301723622656780953602_inner_var_0 -> 0x61ae02b0
1884: 1 -> 0x63ab4f301723622656780953602_inner_var_1 -> 0x6412ce80
1884: 2 -> 0x63ab4f301723622656780953602_inner_var_2 -> 0x61aa91e0
1884: 3 -> linear_1.w_0 -> 0x631cd080
1884: 4 -> linear_1.b_0 -> 0x61aacaa0
1884: 5 -> learning_rate_1 -> 0x639ad130
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 08:04:16.781881 30825 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.781889 30826 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.781908 30827 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.781982 30827 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.781994 30826 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.782006 30829 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.782006 30825 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.782061 30828 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.782068 30826 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.782058 30829 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.782080 30825 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.782075 30827 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63ab4f301723622656780953602_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:04:16.782136 30829 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.782189 30829 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0814 08:04:16.782219 30829 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.782238 30829 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.782251 30829 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:04:16.782266 30829 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x63ab4f301723622656780953602_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab4f301723622656780953602_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab4f301723622656780953602_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.782346 30829 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x63ab4f301723622656780953602_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab4f301723622656780953602_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab4f301723622656780953602_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0814 08:04:16.782410 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x63ab50a0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0814 08:04:16.784391 30751 pir_interpreter.cc:161] PirInterpreter(): 0x64231600 on Place(gpu:0)
1884: I0814 08:04:16.784427 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_1
1884: I0814 08:04:16.784444 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_4
1884: I0814 08:04:16.784453 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_5
1884: I0814 08:04:16.784461 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_6
1884: I0814 08:04:16.784484 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_7
1884: I0814 08:04:16.784495 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_8
1884: I0814 08:04:16.784503 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_9
1884: I0814 08:04:16.784530 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_10
1884: I0814 08:04:16.784540 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_11
1884: I0814 08:04:16.784549 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_12
1884: I0814 08:04:16.784559 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_13
1884: I0814 08:04:16.784570 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_14
1884: I0814 08:04:16.784582 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_15
1884: I0814 08:04:16.784590 30751 scope.cc:202] Create variable fetch0@fetch
1884: I0814 08:04:16.784600 30751 scope.cc:202] Create variable 0x642316001723622656784414773_inner_var_17
1884: I0814 08:04:16.784610 30751 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x639ad130
1884: 1 -> 0x642316001723622656784414773_inner_var_1 -> 0x63699e70
1884: 2 -> linear_1.b_0 -> 0x61aacaa0
1884: 3 -> linear_1.w_0 -> 0x631cd080
1884: 4 -> 0x642316001723622656784414773_inner_var_4 -> 0x61adf970
1884: 5 -> 0x642316001723622656784414773_inner_var_5 -> 0x631c4a80
1884: 6 -> 0x642316001723622656784414773_inner_var_6 -> 0x639ac910
1884: 7 -> 0x642316001723622656784414773_inner_var_7 -> 0x1aa4bdf0
1884: 8 -> 0x642316001723622656784414773_inner_var_8 -> 0x61a9db50
1884: 9 -> 0x642316001723622656784414773_inner_var_9 -> 0x61aa0840
1884: 10 -> 0x642316001723622656784414773_inner_var_10 -> 0x61a644e0
1884: 11 -> 0x642316001723622656784414773_inner_var_11 -> 0x640b50a0
1884: 12 -> 0x642316001723622656784414773_inner_var_12 -> 0x637a16d0
1884: 13 -> 0x642316001723622656784414773_inner_var_13 -> 0x61aba780
1884: 14 -> 0x642316001723622656784414773_inner_var_14 -> 0x6412cd80
1884: 15 -> 0x642316001723622656784414773_inner_var_15 -> 0x61a93a20
1884: 16 -> fetch0@fetch -> 0x61aa4eb0
1884: 17 -> 0x642316001723622656784414773_inner_var_17 -> 0x4c1a2aa0
1884: 18 -> fetch1@fetch -> 0x63a2eb80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0814 08:04:16.786262 30830 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.786377 30831 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.786391 30832 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.786482 30833 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.786518 30833 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786530 30832 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786549 30834 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0814 08:04:16.786556 30833 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:04:16.786581 30832 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.786586 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786614 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:04:16.786638 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786677 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.786706 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:04:16.786722 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:04:16.786787 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0814 08:04:16.786804 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x642316001723622656784414773_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786845 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x642316001723622656784414773_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0814 08:04:16.786873 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x642316001723622656784414773_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.786911 30834 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:04:16.786949 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x642316001723622656784414773_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.786978 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x642316001723622656784414773_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x642316001723622656784414773_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787022 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.787040 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x642316001723622656784414773_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x642316001723622656784414773_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.787071 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x642316001723622656784414773_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787096 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.787098 30832 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642316001723622656784414773_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787110 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x642316001723622656784414773_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.787125 30832 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.787127 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x642316001723622656784414773_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787153 30834 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.787207 30832 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642316001723622656784414773_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.787240 30832 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x642316001723622656784414773_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787256 30834 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.787262 30832 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.787277 30832 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x642316001723622656784414773_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.787297 30834 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.787369 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.787390 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x642316001723622656784414773_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x642316001723622656784414773_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.787425 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x642316001723622656784414773_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787432 30832 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642316001723622656784414773_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787448 30832 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0814 08:04:16.787456 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.787473 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x642316001723622656784414773_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.787482 30832 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x642316001723622656784414773_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.787492 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x642316001723622656784414773_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787500 30832 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x642316001723622656784414773_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787514 30832 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.787529 30832 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x642316001723622656784414773_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.787565 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x642316001723622656784414773_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:04:16.787586 30834 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x642316001723622656784414773_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642316001723622656784414773_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.787616 30834 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0814 08:04:16.787628 30834 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x642316001723622656784414773_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x642316001723622656784414773_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x642316001723622656784414773_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0814 08:04:16.787672 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x64231770) got event_name: TaskCompletion
1884: I0814 08:04:16.787707 30751 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.787739 30751 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0814 08:04:16.794427 30751 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0814 08:04:16.794473 30751 analysis_predictor.cc:433] Predictor::init()
1884: I0814 08:04:16.795197 30751 scope.cc:202] Create variable linear_1.b_0
1884: I0814 08:04:16.795245 30751 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0814 08:04:16.795696 30751 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567957491490"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567957491490"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0814 08:04:16.795874 30751 pir_interpreter.cc:161] PirInterpreter(): 0x4ddd9ca0 on Place(cpu)
1884: I0814 08:04:16.795893 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656795887936_inner_var_0
1884: I0814 08:04:16.795919 30751 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567957491490"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236226567957491490 -> 0x61ab5400
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0814 08:04:16.796053 30751 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0814 08:04:16.796176 30835 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.796367 30836 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.796384 30837 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.796473 30838 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.796486 30837 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236226567957491490:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.796540 30837 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236226567957491490:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0814 08:04:16.796566 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x4ddd9e10) got event_name: TaskCompletion
1884: I0814 08:04:16.796578 30839 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.796833 30837 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13020447096542273593 to 7461976887711076013 , after update, data is {current : 212, peak : 268}.
1884: I0814 08:04:16.796841 30837 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13020447096542273593 to 7461976887711076013 , after update, data is {current : 212, peak : 268}.
1884: I0814 08:04:16.796897 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.796905 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567969723111"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567969723111"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 08:04:16.797106 30751 pir_interpreter.cc:161] PirInterpreter(): 0x4ddd9ca0 on Place(cpu)
1884: I0814 08:04:16.797125 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656797120108_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567969723111"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236226567969723111 -> 0x63a2d9c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 08:04:16.797377 30840 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.797433 30841 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.797461 30842 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.797495 30843 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.797525 30844 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.797519 30843 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236226567969723111:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.797585 30843 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236226567969723111:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.797612 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x4ddd9e10) got event_name: TaskCompletion
1884: I0814 08:04:16.797783 30843 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 5498039152250193594 to 7461976887711076013 , after update, data is {current : 220, peak : 268}.
1884: I0814 08:04:16.797792 30843 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 5498039152250193594 to 7461976887711076013 , after update, data is {current : 220, peak : 268}.
1884: I0814 08:04:16.797904 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.797919 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236226567969723111",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236226567980041182"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236226567969723111",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236226567980041182"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0814 08:04:16.798163 30751 pir_interpreter.cc:161] PirInterpreter(): 0x4ddd9ca0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236226567969723111",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236226567980041182"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236226567980041182 -> 0x63a2d9c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0814 08:04:16.798432 30845 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.798511 30846 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.798533 30847 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.798564 30848 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.798595 30849 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.798591 30848 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.798619 30848 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.798640 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x4ddd9e10) got event_name: TaskCompletion
1884: I0814 08:04:16.798923 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.798929 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567990055703"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567990055703"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0814 08:04:16.799136 30751 pir_interpreter.cc:161] PirInterpreter(): 0x4ddd9ca0 on Place(cpu)
1884: I0814 08:04:16.799155 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656799150086_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236226567990055703"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236226567990055703 -> 0x63665590
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0814 08:04:16.799361 30850 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.799432 30851 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0814 08:04:16.799454 30852 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0814 08:04:16.799495 30853 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0814 08:04:16.799525 30854 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0814 08:04:16.799521 30853 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236226567990055703:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.799561 30853 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236226567990055703:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0814 08:04:16.799582 30751 pir_interpreter.cc:1766] main_thread_blocker_(0x4ddd9e10) got event_name: TaskCompletion
1884: I0814 08:04:16.799749 30853 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5498039152250193594 to 7461976887711076013 , after update, data is {current : 224, peak : 268}.
1884: I0814 08:04:16.799757 30853 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5498039152250193594 to 7461976887711076013 , after update, data is {current : 224, peak : 268}.
1884: I0814 08:04:16.799852 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.799858 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.799921 30751 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0814 08:04:16.799975 30751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0814 08:04:16.800012 30751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567990055703"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567980041182"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567990055703"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567980041182"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0814 08:04:16.800678 30751 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0814 08:04:16.800694 30751 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0814 08:04:16.800730 30751 pir_interpreter.cc:161] PirInterpreter(): 0x4ddd9ca0 on Place(cpu)
1884: I0814 08:04:16.800760 30751 scope.cc:202] Create variable feed_name_0
1884: I0814 08:04:16.800774 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_5
1884: I0814 08:04:16.800796 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_6
1884: I0814 08:04:16.800808 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_7
1884: I0814 08:04:16.800817 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_8
1884: I0814 08:04:16.800840 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_9
1884: I0814 08:04:16.800853 30751 scope.cc:202] Create variable 0x4ddd9ca01723622656800744782_inner_var_10
1884: I0814 08:04:16.800875 30751 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.800896 30751 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:04:16.801017 30751 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.801033 30751 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567990055703"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236226567980041182"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236226567990055703 -> 0x63665590
1884: 1 -> constant_folding@_17236226567980041182 -> 0x63a2d9c0
1884: 2 -> linear_1.b_0 -> 0x61adefe0
1884: 3 -> linear_1.w_0 -> 0x61ac0140
1884: 4 -> feed_name_0 -> 0x636b8630
1884: 5 -> 0x4ddd9ca01723622656800744782_inner_var_5 -> 0x6412eb90
1884: 6 -> 0x4ddd9ca01723622656800744782_inner_var_6 -> 0x642b84a0
1884: 7 -> 0x4ddd9ca01723622656800744782_inner_var_7 -> 0x3bd52510
1884: 8 -> 0x4ddd9ca01723622656800744782_inner_var_8 -> 0x61a996b0
1884: 9 -> fetch_name_0 -> 0x6325f4f0
1884: 10 -> fetch_name_1 -> 0x6407ec70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0814 08:04:16.801596 30751 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0814 08:04:16.801662 30855 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0814 08:04:16.801659 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.801712 30751 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0814 08:04:16.801733 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.801764 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x4ddd9ca01723622656800744782_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x4ddd9ca01723622656800744782_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.801805 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x4ddd9ca01723622656800744782_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.801838 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.801860 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.801877 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.801911 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236226567980041182:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.801936 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236226567990055703:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.801965 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236226567990055703:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x4ddd9ca01723622656800744782_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0814 08:04:16.801992 30751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236226567990055703:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0814 08:04:16.802019 30751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236226567990055703:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4ddd9ca01723622656800744782_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0814 08:04:16.802048 30751 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0814 08:04:16.802068 30751 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0814 08:04:16.802090 30751 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0814 08:04:16.802208 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.802215 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.802264 30855 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2600351296161035989 to 7461976887711076013 , after update, data is {current : 32, peak : 268}.
1884: I0814 08:04:16.802273 30855 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2600351296161035989 to 7461976887711076013 , after update, data is {current : 32, peak : 268}.
1884: I0814 08:04:16.802316 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:16.802325 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:16.804198 30751 mmap_allocator.cc:348] PID: 30751, MemoryMapFdSet: set size - 0
1884: I0814 08:04:16.815987 30751 mmap_allocator.cc:348] PID: 30751, MemoryMapFdSet: set size - 0
1884: I0814 08:04:16.886171 30827 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 6845551151555742659 to 7461976887711076013 , after update, data is {current : 48, peak : 268}.
1884: I0814 08:04:16.886202 30827 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 6845551151555742659 to 7461976887711076013 , after update, data is {current : 48, peak : 268}.
1884: I0814 08:04:16.886211 30826 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7817043709061517111 to 7461976887711076013 , after update, data is {current : 52, peak : 268}.
1884: I0814 08:04:16.886236 30826 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7817043709061517111 to 7461976887711076013 , after update, data is {current : 52, peak : 268}.
1884: I0814 08:04:16.886242 30825 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 14308904636666403190 to 7461976887711076013 , after update, data is {current : 56, peak : 268}.
1884: I0814 08:04:16.886262 30825 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 14308904636666403190 to 7461976887711076013 , after update, data is {current : 56, peak : 268}.
1884: I0814 08:04:16.886438 30829 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 9161869083099745509 to 7461976887711076013 , after update, data is {current : 32, peak : 268}.
1884: I0814 08:04:16.886447 30829 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 9161869083099745509 to 7461976887711076013 , after update, data is {current : 32, peak : 268}.
1884: I0814 08:04:16.886453 30829 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 9161869083099745509 to 7461976887711076013 , after update, data is {current : 256, peak : 768}.
1884: I0814 08:04:16.886758 30832 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 7461976887711076013 to 11357150961042694075 , after update, data is {current : 768, peak : 1536}.
1884: I0814 08:04:16.886775 30832 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 7461976887711076013 to 11357150961042694075 , after update, data is {current : 12, peak : 268}.
1884: I0814 08:04:16.886781 30832 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 7461976887711076013 to 11357150961042694075 , after update, data is {current : 12, peak : 268}.
1884: I0814 08:04:16.886830 30833 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13789576785653846315 to 11357150961042694075 , after update, data is {current : 28, peak : 268}.
1884: I0814 08:04:16.886840 30833 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13789576785653846315 to 11357150961042694075 , after update, data is {current : 28, peak : 268}.
1884: I0814 08:04:16.887001 30834 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 11357150961042694075 to 5349754552814084333 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 08:04:16.887009 30834 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 11357150961042694075 to 5349754552814084333 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0814 08:04:16.887014 30834 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 11357150961042694075 to 5349754552814084333 , after update, data is {current : 1536, peak : 2401024}.
1884: I0814 08:04:17.024610 30751 onednn_context.cc:104] Clearing DNNL cache.
1884: I0814 08:04:17.024639 30751 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0814 08:04:17.024679 30751 mmap_allocator.cc:348] PID: 30751, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   11.95 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.13 sec
=======
test 1874
    Start 1874: test_moe_op

1874: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_moe_op"
1874: Test timeout computed to be: 10000000
1874: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1874: WARNING: Logging before InitGoogleLogging() is written to STDERR
1874: I0814 04:51:35.883327  1693 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1874: I0814 04:51:38.309484  1693 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=print_allocator_trace_info,executor_log_deps_every_microseconds,prim_all,enable_interpretercore_launch_cinn,cuda_malloc_async_pool_memory_throttle_ratio,gpugraph_load_node_list_into_hbm,sync_nccl_allreduce,npu_storage_format,manually_trans_conv_filter,enable_cinn_auto_tune,win_cuda_bin_dir,fraction_of_cuda_pinned_memory_to_use,pir_apply_shape_optimization_pass,jit_engine_type,curand_dir,use_cinn,cudnn_dir,accuracy_check_rtol_fp16,convert_all_blocks,mklml_dir,check_nan_inf_level,enable_tracker_all2all,dataloader_use_file_descriptor,embedding_deterministic,cuda_memory_async_pool_realease_threshold,host_trace_level,use_shm_cache,use_stream_safe_cuda_allocator,enable_fusion_fallback,gpugraph_hbm_table_load_factor,deny_cinn_ops,enable_cse_in_dy2st,static_runtime_data_save_path,enable_dependency_builder_debug_info,inner_op_parallelism,use_pinned_memory,enable_unused_var_check,nvidia_package_dir,prim_forward,accuracy_check_atol_bf16,use_virtual_memory_auto_growth,dist_threadpool_size,prim_enabled,graph_get_neighbor_id,add_dependency_for_communication_op,cublas_dir,allreduce_record_one_event,dynamic_static_unified_comm,gemm_use_half_precision_compute_type,enable_cublas_tensor_op_math,graph_embedding_split_infer_mode,accuracy_check_atol_fp32,initial_cpu_memory_in_mb,new_executor_static_build,use_cuda_malloc_async_allocator,gpugraph_offload_gather_copy_maxsize,conv2d_disable_cudnn,enable_sparse_inner_gather,sync_after_alloc,use_auto_growth_v2,tensorrt_dir,einsum_opt,gpu_memory_limit_mb,use_stride_kernel,run_kp_kernel,tracer_profile_fname,logging_trunc_pir_py_code,use_auto_growth_pinned_allocator,free_idle_chunk,dump_chunk_info,tracer_onednn_ops_off,multiple_of_cupti_buffer_size,reader_queue_speed_test_mode,enable_dump_main_program,enable_api_kernel_fallback,nccl_blocking_wait,cudnn_exhaustive_search_times,search_cache_max_number,enable_gpu_memory_usage_log,graph_load_in_parallel,init_allocated_mem,logging_pir_py_code_dump_symbolic_dims,use_cuda_managed_memory,print_sub_graph_dir,logging_pir_py_code_int_tensor_element_limit,enable_all2all_use_fp16,enable_graph_multi_node_sampling,graph_neighbor_size_percent,lapack_dir,nccl_dir,alloc_fill_value,use_system_allocator,enable_auto_detect_gpu_topo,cse_max_count,op_dir,benchmark,gpugraph_dedup_pull_push_mode,enable_async_trace,cudnn_batchnorm_spatial_persistent,fleet_executor_with_standalone,enable_pir_in_executor_trace_run,enable_opt_get_features,dygraph_debug,initial_gpu_memory_in_mb,low_precision_op_list,gpugraph_storage_mode,logging_pir_py_code_dir,cusparse_dir,enable_fuse_parallel_matmul_pass,memory_fraction_of_eager_deletion,cinn_subgraph_graphviz_dir,fraction_of_cpu_memory_to_use,gpugraph_parallel_copyer_split_maxsize,gpugraph_sparse_table_storage_mode,cudnn_deterministic,cublaslt_device_best_config,conv_workspace_size_limit,new_executor_use_inplace,custom_device_mem_record,gpugraph_offload_param_extends,sort_sum_gradient,enable_exit_when_partial_worker,call_stack_level,fuse_parameter_groups_size,cupti_dir,set_to_1d,eager_delete_tensor_gb,gpugraph_offload_param_stat,pir_subgraph_saving_dir,prim_enable_dynamic,gpugraph_force_device_batch_num_equal,free_when_no_cache_hit,auto_growth_chunk_size_in_mb,enable_auto_rdma_trans,paddle_num_threads,cusparselt_dir,pir_debug,enable_pir_in_executor,graph_metapath_split_opt,gpugraph_enable_hbm_table_collision_stat,prim_check_ops,static_executor_perfstat_filepath,gpugraph_enable_print_op_debug,auto_free_cudagraph_allocations_on_launch,allocator_strategy,prim_skip_dynamic,pinned_memory_as_cpu_backend,use_autotune,accuracy_check_rtol_bf16,gpugraph_debug_gpu_memory,cudnn_exhaustive_search,pir_apply_inplace_pass,prim_backward,cache_inference_while_scope,mkl_dir,new_executor_use_local_scope,get_host_by_name_time,async_trace_count,enable_record_memory,enable_cinn_accuracy_check,local_exe_sub_scope_limit,fraction_of_gpu_memory_to_use,trt_ibuilder_cache,gpu_allocator_retry_time,disable_dyshape_in_train,enable_pir_with_pt_in_dy2st,check_infer_symbolic,save_static_runtime_data,use_xqa_optim,enable_adjust_op_order,enable_collect_shape,enable_neighbor_list_use_uva,new_executor_serial_run,enable_pir_api,allow_cinn_ops,cusolver_dir,new_executor_use_cuda_graph,check_nan_inf,accuracy_check_atol_fp16,fuse_parameter_memory_size,benchmark_nccl,tensor_operants_mode,enable_gpu_memory_usage_log_mb,new_executor_sequential_run,gpugraph_merge_grads_segment_size,print_ir,eager_delete_scope,prim_forward_blacklist,cuda_dir,cinn_compile_thread_num,tracer_onednn_ops_on,use_fast_math,ir_inplace_kernel_blacklist,gpugraph_enable_gpu_direct_access,all_blocks_convert_trt,gpugraph_slot_feasign_max_num,pir_broadcast_tree_limit,accuracy_check_rtol_fp32,enable_cinn_compile_cache,apply_pass_to_program,check_kernel_launch,query_dest_rank_by_multi_node,use_mkldnn,multi_node_sample_use_gpu_table,log_memory_stats,cublaslt_exhaustive_search_times,gpugraph_parallel_stream_num,fast_eager_deletion_mode,enable_blaslt_global_search,gpugraph_enable_segment_merge_grads,max_inplace_grad_add,reallocate_gpu_memory_in_mb,selected_gpus 
1874: I0814 04:51:38.315392  1693 init.cc:108] After Parse: argc is 2
1874: I0814 04:51:44.956900  1693 mmap_allocator.cc:348] PID: 1693, MemoryMapFdSet: set size - 0
1874: I0814 04:51:44.967442  1693 mmap_allocator.cc:348] PID: 1693, MemoryMapFdSet: set size - 0
1874: I0814 04:51:45.248441  1693 mmap_allocator.cc:348] PID: 1693, MemoryMapFdSet: set size - 0
1/1 Test #1874: test_moe_op ......................   Passed   10.54 sec

The following tests passed:
	test_moe_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  10.72 sec
>>>>>>> 98c7d712b32df19e084a656e0a0b1831e23ba4a4
