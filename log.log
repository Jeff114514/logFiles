UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 02:52:26.921590  6152 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 02:52:28.051187  6152 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=jit_engine_type,sort_sum_gradient,tracer_onednn_ops_off,custom_device_mem_record,init_allocated_mem,low_precision_op_list,use_auto_growth_v2,use_virtual_memory_auto_growth,free_idle_chunk,tracer_onednn_ops_on,add_dependency_for_communication_op,dynamic_static_unified_comm,mkl_dir,prim_check_ops,cusolver_dir,enable_pir_with_pt_in_dy2st,get_host_by_name_time,logging_pir_py_code_dump_symbolic_dims,gpugraph_debug_gpu_memory,static_executor_perfstat_filepath,gpugraph_offload_param_extends,enable_cinn_compile_cache,enable_api_kernel_fallback,new_executor_use_local_scope,fuse_parameter_memory_size,npu_storage_format,alloc_fill_value,local_exe_sub_scope_limit,nccl_dir,check_nan_inf_level,executor_log_deps_every_microseconds,enable_auto_rdma_trans,prim_backward,print_sub_graph_dir,op_dir,run_kp_kernel,conv_workspace_size_limit,cusparselt_dir,enable_graph_multi_node_sampling,cudnn_batchnorm_spatial_persistent,pir_broadcast_tree_limit,allow_cinn_ops,call_stack_level,gpugraph_enable_gpu_direct_access,apply_pass_to_program,gpugraph_enable_hbm_table_collision_stat,accuracy_check_atol_fp16,cse_max_count,enable_unused_var_check,cinn_compile_thread_num,use_pinned_memory,nccl_blocking_wait,enable_adjust_op_order,graph_get_neighbor_id,prim_all,cuda_memory_async_pool_realease_threshold,graph_neighbor_size_percent,fleet_executor_with_standalone,gpu_allocator_retry_time,sync_after_alloc,use_xqa_optim,gpu_memory_limit_mb,prim_enable_dynamic,gpugraph_parallel_stream_num,query_dest_rank_by_multi_node,print_allocator_trace_info,enable_gpu_memory_usage_log_mb,free_when_no_cache_hit,paddle_num_threads,check_kernel_launch,pir_apply_shape_optimization_pass,tracer_profile_fname,check_nan_inf,memory_fraction_of_eager_deletion,disable_dyshape_in_train,eager_delete_scope,trt_ibuilder_cache,cudnn_exhaustive_search,ir_inplace_kernel_blacklist,win_cuda_bin_dir,enable_exit_when_partial_worker,enable_all2all_use_fp16,embedding_deterministic,fraction_of_gpu_memory_to_use,set_to_1d,prim_enabled,logging_trunc_pir_py_code,save_static_runtime_data,dygraph_debug,use_system_allocator,enable_pir_api,new_executor_use_inplace,accuracy_check_atol_bf16,all_blocks_convert_trt,initial_gpu_memory_in_mb,enable_cinn_auto_tune,gpugraph_merge_grads_segment_size,enable_fusion_fallback,gpugraph_force_device_batch_num_equal,accuracy_check_rtol_bf16,gpugraph_slot_feasign_max_num,cuda_dir,log_memory_stats,graph_embedding_split_infer_mode,fast_eager_deletion_mode,prim_forward,dist_threadpool_size,enable_fuse_parallel_matmul_pass,sync_nccl_allreduce,new_executor_sequential_run,tensorrt_dir,allocator_strategy,tensor_operants_mode,enable_pir_in_executor_trace_run,gpugraph_offload_param_stat,gpugraph_parallel_copyer_split_maxsize,gpugraph_hbm_table_load_factor,gpugraph_sparse_table_storage_mode,new_executor_serial_run,gpugraph_enable_segment_merge_grads,cusparse_dir,enable_cublas_tensor_op_math,enable_opt_get_features,fuse_parameter_groups_size,logging_pir_py_code_dir,enable_record_memory,multi_node_sample_use_gpu_table,prim_skip_dynamic,benchmark_nccl,gpugraph_offload_gather_copy_maxsize,use_auto_growth_pinned_allocator,eager_delete_tensor_gb,cublas_dir,gemm_use_half_precision_compute_type,multiple_of_cupti_buffer_size,use_fast_math,cudnn_deterministic,benchmark,enable_neighbor_list_use_uva,new_executor_use_cuda_graph,einsum_opt,enable_dump_main_program,graph_load_in_parallel,graph_metapath_split_opt,inner_op_parallelism,fraction_of_cpu_memory_to_use,use_cuda_managed_memory,static_runtime_data_save_path,pir_subgraph_saving_dir,accuracy_check_rtol_fp32,enable_pir_in_executor,accuracy_check_atol_fp32,cache_inference_while_scope,curand_dir,use_autotune,use_cuda_malloc_async_allocator,cublaslt_exhaustive_search_times,max_inplace_grad_add,reader_queue_speed_test_mode,use_stride_kernel,prim_forward_blacklist,enable_blaslt_global_search,enable_dependency_builder_debug_info,initial_cpu_memory_in_mb,search_cache_max_number,enable_gpu_memory_usage_log,enable_cse_in_dy2st,use_cinn,gpugraph_load_node_list_into_hbm,accuracy_check_rtol_fp16,pir_apply_inplace_pass,async_trace_count,host_trace_level,manually_trans_conv_filter,pinned_memory_as_cpu_backend,use_mkldnn,enable_async_trace,deny_cinn_ops,dataloader_use_file_descriptor,enable_cinn_accuracy_check,enable_sparse_inner_gather,print_ir,enable_collect_shape,enable_interpretercore_launch_cinn,nvidia_package_dir,cinn_subgraph_graphviz_dir,cuda_malloc_async_pool_memory_throttle_ratio,cublaslt_device_best_config,lapack_dir,cudnn_dir,enable_auto_detect_gpu_topo,convert_all_blocks,use_stream_safe_cuda_allocator,check_infer_symbolic,conv2d_disable_cudnn,auto_growth_chunk_size_in_mb,new_executor_static_build,allreduce_record_one_event,gpugraph_dedup_pull_push_mode,reallocate_gpu_memory_in_mb,cudnn_exhaustive_search_times,enable_tracker_all2all,cupti_dir,gpugraph_storage_mode,logging_pir_py_code_int_tensor_element_limit,pir_debug,auto_free_cudagraph_allocations_on_launch,dump_chunk_info,fraction_of_cuda_pinned_memory_to_use,gpugraph_enable_print_op_debug,use_shm_cache,selected_gpus,mklml_dir 
1884: I0815 02:52:28.051307  6152 init.cc:108] After Parse: argc is 2
1884: I0815 02:52:35.839237  6152 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:52:35.839274  6152 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 02:52:35.839951  6152 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 02:52:35.840396  6152 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 02:52:35.841238  6152 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 02:52:35.841337  6152 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 02:52:35.841430  6152 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 02:52:35.842096  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f60f4000000), and remaining 0
1884: I0815 02:52:35.842361  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.842420  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.842507  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f60f4000200), and remaining 0
1884: I0815 02:52:35.842531  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f60f4000400), and remaining 0
1884: I0815 02:52:35.846273  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f60f4000600), and remaining 0
1884: I0815 02:52:35.846434  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f60f4000800), and remaining 0
1884: I0815 02:52:35.846506  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f60f4000a00), and remaining 0
1884: I0815 02:52:35.846596  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.846616  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.846681  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.846693  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.847617  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adab5a0 for it.
1884: I0815 02:52:35.847767  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.847788  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.847846  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f60f4000e00), and remaining 0
1884: I0815 02:52:35.847918  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f60f40c4400), and remaining 0
1884: I0815 02:52:35.971210  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adab5a0 for it.
1884: I0815 02:52:35.973531  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.973589  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.974210  6152 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f60f4200000), and remaining 0
1884: I0815 02:52:35.987288  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adab5a0 for it.
1884: I0815 02:52:35.987450  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:35.987491  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.987538  6152 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:35.987725  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:35.988782  6152 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 02:52:35.988797  6152 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 02:52:35.988850  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:35.988925  6152 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 02:52:35.988950  6152 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.989006  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:35.989073  6152 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 02:52:35.989087  6152 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.989120  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:35.989274  6152 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 02:52:35.989289  6152 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.989475  6152 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 02:52:35.989503  6152 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:35.989570  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:35.995947  6152 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 02:52:35.996086  6152 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 02:52:35.996119  6152 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 02:52:35.996191  6152 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 02:52:37.481217  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.481276  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.481563  6152 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 02:52:37.481586  6152 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.486259  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.486315  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.487265  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.487287  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.487308  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.488035  6152 program_interpreter.cc:243] New Executor is Running.
1884: I0815 02:52:37.488051  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.488068  6152 scope.cc:202] Create variable feed
1884: I0815 02:52:37.488075  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.488085  6152 scope.cc:202] Create variable fetch
1884: I0815 02:52:37.488090  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.488101  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.488107  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.488111  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.488113  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.490612  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.490986  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.491001  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.491005  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.492766  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.492827  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.492838  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.492846  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.492853  6152 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:52:37.492862  6152 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61c8ed20 type is 7
1884: I0815 02:52:37.492867  6152 scope.cc:202] Create variable x
1884: I0815 02:52:37.492870  6152 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x61c8ee70 type is 7
1884: I0815 02:52:37.492938  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.492946  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.492950  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.492954  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.493075  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.493099  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.493216  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.493227  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.493243  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.493436  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.493467  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.493485  6152 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.493490  6152 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61c96ec0Variable Type 7
1884: I0815 02:52:37.493510  6152 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.493530  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.493583  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.493603  6152 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.494848  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.494905  6152 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.495314  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.500222  6152 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:52:37.500252  6152 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:52:37.500389  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.500425  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.500916  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: I0815 02:52:37.501010  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.501030  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.501456  6152 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: I0815 02:52:37.501550  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.501574  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.501598  6152 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.501901  6152 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:52:37.501909  6152 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:52:37.502043  6152 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 02:52:37.502064  6152 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.502480  6152 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:52:37.502496  6152 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:52:37.502543  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.502564  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.502760  6152 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:52:37.502771  6152 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:52:37.502808  6152 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:52:37.502826  6152 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:52:37.502844  6152 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.505870  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.505901  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.505961  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.505971  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.508194  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.508642  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.508662  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.508668  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.510576  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.510651  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.510659  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:37.510666  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61cc20f0 type is 7
1884: I0815 02:52:37.510674  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.510679  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61cc2460 type is 7
1884: I0815 02:52:37.510684  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.510689  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.510751  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.510757  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.510761  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.510766  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.510821  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.510838  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.510905  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.510912  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.510931  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.511250  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.511270  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.511289  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.511308  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61cc8c70Variable Type 7
1884: I0815 02:52:37.511333  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.511355  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.511382  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.511400  6152 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.512154  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.512185  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.512398  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.525493  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: I0815 02:52:37.525785  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 02:52:37.532167  6152 pir_interpreter.cc:161] PirInterpreter(): 0x61e85220 on Place(gpu:0)
1884: I0815 02:52:37.532214  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.532243  6152 scope.cc:202] Create variable 0x61e852201723690357532198102_inner_var_1
1884: I0815 02:52:37.532253  6152 scope.cc:202] Create variable 0x61e852201723690357532198102_inner_var_2
1884: I0815 02:52:37.532260  6152 scope.cc:202] Create variable 0x61e852201723690357532198102_inner_var_3
1884: I0815 02:52:37.532269  6152 scope.cc:202] Create variable 0x61e852201723690357532198102_inner_var_4
1884: I0815 02:52:37.532277  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:37.532783  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:37.532799  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.532802  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 02:52:37.532852  6152 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61e85180
1884: 1 -> 0x61e852201723690357532198102_inner_var_1 -> 0x61e85200
1884: 2 -> 0x61e852201723690357532198102_inner_var_2 -> 0x61e85a90
1884: 3 -> 0x61e852201723690357532198102_inner_var_3 -> 0x61e84960
1884: 4 -> 0x61e852201723690357532198102_inner_var_4 -> 0x61e85e40
1884: 5 -> fetch0@fetch -> 0x61e86650
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:37.533636  6152 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 02:52:37.547386  6190 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:37.549340  6192 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:37.549427  6192 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61e852201723690357532198102_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.549541  6192 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61e852201723690357532198102_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:37.550339  6191 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:37.550351  6195 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:37.550458  6195 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.550539  6195 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 02:52:37.550604  6195 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61e852201723690357532198102_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61e852201723690357532198102_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.550951  6195 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61e852201723690357532198102_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61e852201723690357532198102_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 02:52:37.551342  6194 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:37.551438  6194 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61e852201723690357532198102_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.551496  6194 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.552784  6194 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61e852201723690357532198102_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61e852201723690357532198102_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:52:37.552842  6194 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61e852201723690357532198102_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.552873  6194 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.549341  6193 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:37.553470  6194 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61e852201723690357532198102_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:52:37.555415  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x61e85390) got event_name: TaskCompletion
1884: I0815 02:52:37.555456  6152 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.669581  6190 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 7088756523201647667 to 10111628474051507475 , after update, data is {current : 0, peak : 800768}.
1884: I0815 02:52:37.669615  6190 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7088756523201647667 to 10506705655010804235 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 02:52:37.669621  6190 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7088756523201647667 to 10506705655010804235 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 02:52:37.670377  6192 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 14538991260395951885 to 10506705655010804235 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 02:52:37.670400  6192 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 14538991260395951885 to 10506705655010804235 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 02:52:37.681500  6194 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 10506705655010804235 to 14697222372681333668 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 02:52:37.681537  6194 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 10506705655010804235 to 14697222372681333668 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 02:52:37.682395  6195 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 02:52:37.682433  6195 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:37.689988  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.690131  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.690212  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.690232  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.692132  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.692589  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.692641  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.692660  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.694273  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.694437  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.694464  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:37.694480  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3dd04a0 type is 7
1884: I0815 02:52:37.694499  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.694511  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x574bf20 type is 7
1884: I0815 02:52:37.694525  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.694538  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.694610  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.694626  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.694639  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.694653  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.694715  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.694754  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.694828  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.694849  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.694876  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.695055  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.695080  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.695106  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.695122  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x57659b0Variable Type 7
1884: I0815 02:52:37.695149  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.695175  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.695205  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.695230  6152 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.696898  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.697012  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.697252  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.706348  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: I0815 02:52:37.706750  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 02:52:37.710296  6152 pir_interpreter.cc:161] PirInterpreter(): 0x61cb2f50 on Place(gpu:0)
1884: I0815 02:52:37.710364  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.710402  6152 scope.cc:202] Create variable 0x61cb2f501723690357710353023_inner_var_1
1884: I0815 02:52:37.710426  6152 scope.cc:202] Create variable 0x61cb2f501723690357710353023_inner_var_2
1884: I0815 02:52:37.710450  6152 scope.cc:202] Create variable 0x61cb2f501723690357710353023_inner_var_3
1884: I0815 02:52:37.710474  6152 scope.cc:202] Create variable 0x61cb2f501723690357710353023_inner_var_4
1884: I0815 02:52:37.710497  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:37.710903  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:37.710944  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.710961  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x3ee4ef0
1884: 1 -> 0x61cb2f501723690357710353023_inner_var_1 -> 0x577e850
1884: 2 -> 0x61cb2f501723690357710353023_inner_var_2 -> 0x5793990
1884: 3 -> 0x61cb2f501723690357710353023_inner_var_3 -> 0x61cafef0
1884: 4 -> 0x61cb2f501723690357710353023_inner_var_4 -> 0x61c92ec0
1884: 5 -> fetch0@fetch -> 0x45a557d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:37.712338  6199 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:37.712426  6199 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61cb2f501723690357710353023_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.712515  6199 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61cb2f501723690357710353023_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:37.715356  6198 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:37.712338  6197 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:37.716327  6201 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:37.716372  6201 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.716420  6201 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:52:37.716468  6201 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61cb2f501723690357710353023_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61cb2f501723690357710353023_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.716598  6201 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61cb2f501723690357710353023_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61cb2f501723690357710353023_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 02:52:37.712342  6196 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:37.712345  6200 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:37.718358  6200 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61cb2f501723690357710353023_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.718395  6200 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.721288  6200 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61cb2f501723690357710353023_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x61cb2f501723690357710353023_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:52:37.721357  6200 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61cb2f501723690357710353023_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.721378  6200 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.727403  6200 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61cb2f501723690357710353023_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:52:37.727563  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x61cb30c0) got event_name: TaskCompletion
1884: I0815 02:52:37.727597  6152 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.792371  6196 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 10111628474051507475 to 4346257494382934393 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 02:52:37.792405  6196 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10111628474051507475 to 14538991260395951885 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 02:52:37.792411  6196 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10111628474051507475 to 14538991260395951885 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 02:52:37.795475  6200 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 14538991260395951885 to 2218075940985657683 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 02:52:37.795496  6200 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 14538991260395951885 to 2218075940985657683 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 02:52:37.796367  6199 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 2218075940985657683 to 14697222372681333668 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 02:52:37.796391  6199 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 2218075940985657683 to 14697222372681333668 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 02:52:37.804370  6201 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:37.812086  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.812173  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.812249  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.812269  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.814124  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.814558  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.814597  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.814615  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.820446  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.820632  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.820660  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:37.820677  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x575b210 type is 7
1884: I0815 02:52:37.820696  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.820709  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3e33d70 type is 7
1884: I0815 02:52:37.820724  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.820737  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.820817  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.820833  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.820848  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.820863  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.820925  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.820952  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.821027  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.821058  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.821085  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.821142  6152 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.821345  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:37.821435  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.821460  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.821487  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.821502  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61c74d80Variable Type 7
1884: I0815 02:52:37.821534  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.821563  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.821595  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.821619  6152 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.821774  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.821810  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.822026  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.822968  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: I0815 02:52:37.823238  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 02:52:37.830904  6152 pir_interpreter.cc:161] PirInterpreter(): 0x61cb1030 on Place(gpu:0)
1884: I0815 02:52:37.830965  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.831005  6152 scope.cc:202] Create variable 0x61cb10301723690357830955272_inner_var_1
1884: I0815 02:52:37.831030  6152 scope.cc:202] Create variable 0x61cb10301723690357830955272_inner_var_2
1884: I0815 02:52:37.831054  6152 scope.cc:202] Create variable 0x61cb10301723690357830955272_inner_var_3
1884: I0815 02:52:37.831079  6152 scope.cc:202] Create variable 0x61cb10301723690357830955272_inner_var_4
1884: I0815 02:52:37.831101  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:37.831569  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:37.831619  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.831636  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5794260
1884: 1 -> 0x61cb10301723690357830955272_inner_var_1 -> 0x61c781f0
1884: 2 -> 0x61cb10301723690357830955272_inner_var_2 -> 0x642f2c80
1884: 3 -> 0x61cb10301723690357830955272_inner_var_3 -> 0x4756fc00
1884: 4 -> 0x61cb10301723690357830955272_inner_var_4 -> 0x61cabe10
1884: 5 -> fetch0@fetch -> 0x61e84e40
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:37.833336  6205 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:37.833357  6207 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:37.833381  6205 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61cb10301723690357830955272_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.833405  6207 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.833446  6207 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 02:52:37.834329  6203 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:37.834335  6204 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:37.833446  6205 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61cb10301723690357830955272_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:37.838333  6207 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61cb10301723690357830955272_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61cb10301723690357830955272_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.838430  6207 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.838583  6207 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:37.838611  6207 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61cb10301723690357830955272_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61cb10301723690357830955272_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 02:52:37.833338  6202 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:37.841351  6205 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61cb10301723690357830955272_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.841398  6205 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.841477  6205 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61cb10301723690357830955272_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61cb10301723690357830955272_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:52:37.841511  6205 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61cb10301723690357830955272_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.841528  6205 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.841542  6205 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61cb10301723690357830955272_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:52:37.841706  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x61cb11a0) got event_name: TaskCompletion
1884: I0815 02:52:37.841750  6152 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.833336  6206 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:37.892374  6202 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 4346257494382934393 to 10111628474051507475 , after update, data is {current : 0, peak : 3328}.
1884: I0815 02:52:37.892410  6202 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4346257494382934393 to 10111628474051507475 , after update, data is {current : -804, peak : 2000}.
1884: I0815 02:52:37.892416  6202 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4346257494382934393 to 10111628474051507475 , after update, data is {current : -804, peak : 2000}.
1884: I0815 02:52:37.900393  6205 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12505802569202608563 to 10111628474051507475 , after update, data is {current : 800, peak : 2000}.
1884: I0815 02:52:37.900429  6205 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 12505802569202608563 to 10111628474051507475 , after update, data is {current : 800, peak : 2000}.
1884: I0815 02:52:37.905412  6207 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 02:52:37.905444  6207 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 02:52:37.905449  6207 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:37.910737  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.910768  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.910826  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.910831  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.912672  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.913062  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.913074  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.913077  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.916971  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.917107  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.917116  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:37.917124  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5742e10 type is 7
1884: I0815 02:52:37.917131  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.917135  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61c82b70 type is 7
1884: I0815 02:52:37.917138  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.917143  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.917212  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.917217  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.917222  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.917227  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.917280  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.917294  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.917366  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.917374  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.917388  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.917665  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.917676  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.917692  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.917696  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x645bbf50Variable Type 7
1884: I0815 02:52:37.917713  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.917728  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.917749  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.917762  6152 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.918509  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.918543  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.918768  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.927898  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: I0815 02:52:37.928175  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 02:52:37.935890  6152 pir_interpreter.cc:161] PirInterpreter(): 0x6431aa90 on Place(gpu:0)
1884: I0815 02:52:37.935930  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.935950  6152 scope.cc:202] Create variable 0x6431aa901723690357935918842_inner_var_1
1884: I0815 02:52:37.935959  6152 scope.cc:202] Create variable 0x6431aa901723690357935918842_inner_var_2
1884: I0815 02:52:37.935968  6152 scope.cc:202] Create variable 0x6431aa901723690357935918842_inner_var_3
1884: I0815 02:52:37.935976  6152 scope.cc:202] Create variable 0x6431aa901723690357935918842_inner_var_4
1884: I0815 02:52:37.935986  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:37.936426  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:37.936440  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.936443  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61cb3e30
1884: 1 -> 0x6431aa901723690357935918842_inner_var_1 -> 0x61cb3e50
1884: 2 -> 0x6431aa901723690357935918842_inner_var_2 -> 0x63f3ba80
1884: 3 -> 0x6431aa901723690357935918842_inner_var_3 -> 0x642f2c60
1884: 4 -> 0x6431aa901723690357935918842_inner_var_4 -> 0x63f3b970
1884: 5 -> fetch0@fetch -> 0x644bb380
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:37.937340  6209 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:37.937340  6208 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:37.937402  6209 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6431aa901723690357935918842_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.937467  6209 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6431aa901723690357935918842_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:37.938333  6213 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:37.938382  6213 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.938428  6213 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 02:52:37.938464  6213 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6431aa901723690357935918842_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6431aa901723690357935918842_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.938712  6213 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6431aa901723690357935918842_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6431aa901723690357935918842_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 02:52:37.939347  6211 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:37.939394  6211 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6431aa901723690357935918842_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.939427  6211 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.940685  6211 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6431aa901723690357935918842_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6431aa901723690357935918842_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:52:37.940724  6211 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6431aa901723690357935918842_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:37.940744  6211 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.937340  6210 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:37.941326  6211 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6431aa901723690357935918842_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:52:37.938335  6212 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:37.946331  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x6431ac00) got event_name: TaskCompletion
1884: I0815 02:52:37.946381  6152 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:37.950240  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.950277  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.955514  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:37.955559  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.957913  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:37.958369  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.958381  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.958386  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.960389  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:37.960536  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:37.960546  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:37.960556  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x645c2290 type is 7
1884: I0815 02:52:37.960564  6152 scope.cc:202] Create variable X
1884: I0815 02:52:37.960567  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x645c2270 type is 7
1884: I0815 02:52:37.960572  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:37.960578  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:37.960646  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:37.960652  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:37.960656  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:37.960660  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:37.960716  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.960738  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.960809  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.960819  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.960837  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:37.961094  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.961107  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:37.961126  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:37.961131  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61e88670Variable Type 7
1884: I0815 02:52:37.961149  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:37.961169  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:37.961192  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:37.961207  6152 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:37.965550  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:37.965607  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:37.965875  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.044380  6208 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 10111628474051507475 to 4346257494382934393 , after update, data is {current : 0, peak : 800768}.
1884: I0815 02:52:38.044418  6208 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10111628474051507475 to 2218075940985657683 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 02:52:38.044425  6208 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10111628474051507475 to 2218075940985657683 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 02:52:38.049377  6211 thread_data_registry.h:135] Add data {current : 799996, peak : 1600000} from thread 2218075940985657683 to 10506705655010804235 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 02:52:38.049412  6211 thread_data_registry.h:135] Add data {current : 799996, peak : 1600000} from thread 2218075940985657683 to 10506705655010804235 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 02:52:38.053364  6209 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 10506705655010804235 to 14697222372681333668 , after update, data is {current : 4000800, peak : 4800000}.
1884: I0815 02:52:38.053396  6209 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 10506705655010804235 to 14697222372681333668 , after update, data is {current : 4000800, peak : 4800800}.
1884: I0815 02:52:38.057401  6213 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 02:52:38.057430  6213 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:38.068678  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.068713  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.068771  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.068776  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.070641  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.071030  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.071041  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.071045  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.072715  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:38.072834  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.072841  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:38.072849  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63f3bce0 type is 7
1884: I0815 02:52:38.072857  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.072861  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x645d56e0 type is 7
1884: I0815 02:52:38.072865  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.072870  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.072932  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.072937  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.072942  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.072945  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.072995  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.073009  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.073071  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.073077  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.073091  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.073247  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.073256  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.073271  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.073274  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63f35f70Variable Type 7
1884: I0815 02:52:38.073290  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.073316  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.073335  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.073348  6152 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.075021  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.075064  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:38.075307  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.091261  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: I0815 02:52:38.091564  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 02:52:38.094985  6152 pir_interpreter.cc:161] PirInterpreter(): 0x63e28300 on Place(gpu:0)
1884: I0815 02:52:38.095024  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.095045  6152 scope.cc:202] Create variable 0x63e283001723690358095014313_inner_var_1
1884: I0815 02:52:38.095054  6152 scope.cc:202] Create variable 0x63e283001723690358095014313_inner_var_2
1884: I0815 02:52:38.095064  6152 scope.cc:202] Create variable 0x63e283001723690358095014313_inner_var_3
1884: I0815 02:52:38.095072  6152 scope.cc:202] Create variable 0x63e283001723690358095014313_inner_var_4
1884: I0815 02:52:38.095082  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:38.095515  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:38.095530  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.095533  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61cb1d40
1884: 1 -> 0x63e283001723690358095014313_inner_var_1 -> 0x61ca9cc0
1884: 2 -> 0x63e283001723690358095014313_inner_var_2 -> 0x574c390
1884: 3 -> 0x63e283001723690358095014313_inner_var_3 -> 0x645c29c0
1884: 4 -> 0x63e283001723690358095014313_inner_var_4 -> 0x63f3ba60
1884: 5 -> fetch0@fetch -> 0x45a5d500
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:38.096340  6214 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.100345  6215 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.100394  6215 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63e283001723690358095014313_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.100469  6215 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63e283001723690358095014313_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.103348  6217 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.100342  6218 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.104331  6216 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.100340  6219 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:38.105365  6219 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.105423  6219 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:52:38.105468  6219 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63e283001723690358095014313_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63e283001723690358095014313_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.105616  6219 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63e283001723690358095014313_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63e283001723690358095014313_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 02:52:38.108356  6216 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63e283001723690358095014313_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.108422  6216 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.111194  6216 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63e283001723690358095014313_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x63e283001723690358095014313_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:52:38.111265  6216 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63e283001723690358095014313_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.111289  6216 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.113307  6216 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63e283001723690358095014313_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:52:38.114418  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x63e28470) got event_name: TaskCompletion
1884: I0815 02:52:38.114454  6152 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.123092  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.123131  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.123195  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.123203  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.129490  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.130018  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.130030  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.130036  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.132032  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:38.132181  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.132192  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:38.132200  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61c8a6c0 type is 7
1884: I0815 02:52:38.132207  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.132211  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61c65100 type is 7
1884: I0815 02:52:38.132216  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.132221  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.132293  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.132298  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.132313  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.132318  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.132375  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.132392  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.132459  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.132473  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.132491  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.132651  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.132663  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.132680  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.132686  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63d07a80Variable Type 7
1884: I0815 02:52:38.132704  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.132723  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.132745  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.132759  6152 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.134456  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.134497  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:38.134723  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.183372  6214 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 4346257494382934393 to 10111628474051507475 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 02:52:38.183409  6214 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4346257494382934393 to 2218075940985657683 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 02:52:38.183414  6214 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4346257494382934393 to 2218075940985657683 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 02:52:38.190349  6216 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 2218075940985657683 to 14538991260395951885 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 02:52:38.190382  6216 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 2218075940985657683 to 14538991260395951885 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 02:52:38.191393  6215 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 14538991260395951885 to 14697222372681333668 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 02:52:38.191412  6215 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 14538991260395951885 to 14697222372681333668 , after update, data is {current : 6400800, peak : 8800800}.
1884: I0815 02:52:38.197508  6219 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 10111628474051507475 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:38.204721  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.204813  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.204888  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.204907  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.207593  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.208061  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.208101  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.208117  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.209795  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:38.209960  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.209987  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:38.210005  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61c8fdd0 type is 7
1884: I0815 02:52:38.210023  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.210036  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x64813b20 type is 7
1884: I0815 02:52:38.210050  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.210063  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.210141  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.210157  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.210171  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.210184  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.210248  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.210274  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.210373  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.210397  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.210426  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.210495  6152 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.210655  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.210726  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.210747  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.210774  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.210789  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61c67920Variable Type 7
1884: I0815 02:52:38.210817  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.210844  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.210876  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.210901  6152 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.210956  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.210990  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:38.211211  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.212141  6152 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9cd0 for it.
1884: I0815 02:52:38.212411  6152 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae93610 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 02:52:38.215981  6152 pir_interpreter.cc:161] PirInterpreter(): 0x64917dd0 on Place(gpu:0)
1884: I0815 02:52:38.216038  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.216075  6152 scope.cc:202] Create variable 0x64917dd01723690358216028643_inner_var_1
1884: I0815 02:52:38.216100  6152 scope.cc:202] Create variable 0x64917dd01723690358216028643_inner_var_2
1884: I0815 02:52:38.216125  6152 scope.cc:202] Create variable 0x64917dd01723690358216028643_inner_var_3
1884: I0815 02:52:38.216149  6152 scope.cc:202] Create variable 0x64917dd01723690358216028643_inner_var_4
1884: I0815 02:52:38.216173  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:38.216600  6152 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:52:38.216647  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.216666  6152 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6433d050
1884: 1 -> 0x64917dd01723690358216028643_inner_var_1 -> 0x5781be0
1884: 2 -> 0x64917dd01723690358216028643_inner_var_2 -> 0x38c1f50
1884: 3 -> 0x64917dd01723690358216028643_inner_var_3 -> 0x61cace40
1884: 4 -> 0x64917dd01723690358216028643_inner_var_4 -> 0x61e88350
1884: 5 -> fetch0@fetch -> 0x61fb73a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:52:38.217890  6223 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.217943  6223 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64917dd01723690358216028643_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.218024  6223 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64917dd01723690358216028643_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.218127  6220 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.218211  6222 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.218286  6225 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:38.218327  6225 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.218384  6225 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 02:52:38.218503  6225 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64917dd01723690358216028643_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64917dd01723690358216028643_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.218575  6225 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.218742  6225 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.218798  6225 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64917dd01723690358216028643_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64917dd01723690358216028643_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 02:52:38.219148  6222 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64917dd01723690358216028643_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.219267  6222 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.219357  6222 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64917dd01723690358216028643_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x64917dd01723690358216028643_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:52:38.219493  6222 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x64917dd01723690358216028643_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.219537  6222 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.219566  6222 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x64917dd01723690358216028643_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:52:38.219664  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x64917f40) got event_name: TaskCompletion
1884: I0815 02:52:38.219681  6152 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.221418  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.221443  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.221500  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.221506  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.218343  6221 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.218343  6224 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.223779  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.224222  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.224234  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.224239  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.226155  6152 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:52:38.226277  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.226286  6152 scope.cc:202] Create variable Out
1884: I0815 02:52:38.226292  6152 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63bfd2d0 type is 7
1884: I0815 02:52:38.226472  6152 scope.cc:202] Create variable X
1884: I0815 02:52:38.226497  6152 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6480aab0 type is 7
1884: I0815 02:52:38.226516  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.226533  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.226621  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.226641  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.226657  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.226675  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.226743  6152 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.226773  6152 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.226859  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.226897  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.226930  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.226995  6152 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.227142  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.227218  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.227250  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.227283  6152 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.227309  6152 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6480d9c0Variable Type 7
1884: I0815 02:52:38.227345  6152 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.227377  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.227420  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.227452  6152 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.227514  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.227557  6152 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:38.227810  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.264569  6222 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 12505802569202608563 to 4346257494382934393 , after update, data is {current : 0, peak : 10240}.
1884: I0815 02:52:38.264595  6222 thread_data_registry.h:135] Add data {current : 800, peak : 1600} from thread 12505802569202608563 to 4346257494382934393 , after update, data is {current : 796, peak : 8000}.
1884: I0815 02:52:38.264601  6222 thread_data_registry.h:135] Add data {current : 800, peak : 1600} from thread 12505802569202608563 to 4346257494382934393 , after update, data is {current : 796, peak : 8000}.
1884: I0815 02:52:38.264743  6223 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2218075940985657683 to 4346257494382934393 , after update, data is {current : 800, peak : 8000}.
1884: I0815 02:52:38.264751  6223 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2218075940985657683 to 4346257494382934393 , after update, data is {current : 800, peak : 8000}.
1884: I0815 02:52:38.265372  6225 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 02:52:38.265401  6225 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 6401600, peak : 8800800}.
1884: I0815 02:52:38.265406  6225 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 4346257494382934393 to 14697222372681333668 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:52:38.272913  6152 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 02:52:38.273034  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 02:52:38.274344  6152 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:52:38.275279  6152 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 02:52:38.275349  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 02:52:38.276765  6152 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 02:52:38.276819  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:52:38.277616  6152 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 02:52:38.278640  6152 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 02:52:38.278666  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:52:38.280045  6152 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 02:52:38.280068  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:52:38.280684  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.280710  6152 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:52:38.280716  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.280723  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.282748  6152 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 02:52:38.282774  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:52:38.283752  6152 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 02:52:38.283782  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 02:52:38.284765  6152 pybind.cc:1827] need skip: 0
1884: I0815 02:52:38.285068  6152 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:52:38.286885  6152 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:52:38.290699  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.290721  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.290725  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.292749  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.292819  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.292840  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.292855  6152 scope.cc:202] Create variable learning_rate_0
1884: I0815 02:52:38.292871  6152 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63d07060 type is 7
1884: I0815 02:52:38.292886  6152 scope.cc:202] Create variable linear_0.b_0
1884: I0815 02:52:38.292897  6152 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d06f40 type is 7
1884: I0815 02:52:38.292922  6152 scope.cc:202] Create variable linear_0.w_0
1884: I0815 02:52:38.292935  6152 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d06fa0 type is 7
1884: I0815 02:52:38.293018  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.293035  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.293047  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.293061  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.293128  6152 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293146  6152 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293167  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 02:52:38.293321  6152 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293334  6152 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293398  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.293443  6152 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293452  6152 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.293478  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.294608  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.296110  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.296591  6152 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:52:38.296831  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.297150  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.297390  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.297408  6152 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:52:38.297478  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.297485  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.297489  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.297590  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.297602  6152 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:52:38.299172  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.300549  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.301676  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.301889  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.301903  6152 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 02:52:38.301910  6152 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x6382b430 type is 7
1884: I0815 02:52:38.301926  6152 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:52:38.301929  6152 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x6382b1b0 type is 7
1884: I0815 02:52:38.301934  6152 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 02:52:38.301936  6152 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x6382b2a0 type is 7
1884: I0815 02:52:38.301940  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.301945  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.301949  6152 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 02:52:38.301952  6152 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x6382c820 type is 7
1884: I0815 02:52:38.301956  6152 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x63d07060 type is 7
1884: I0815 02:52:38.301960  6152 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d06f40 type is 7
1884: I0815 02:52:38.301964  6152 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 02:52:38.301967  6152 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x6382c800 type is 7
1884: I0815 02:52:38.301971  6152 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 02:52:38.301975  6152 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x6382cd60 type is 7
1884: I0815 02:52:38.301977  6152 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d06fa0 type is 7
1884: I0815 02:52:38.301981  6152 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 02:52:38.301985  6152 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x6382cfd0 type is 7
1884: I0815 02:52:38.301987  6152 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 02:52:38.301990  6152 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x6382d210 type is 7
1884: I0815 02:52:38.301995  6152 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:52:38.301997  6152 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6382d470 type is 7
1884: I0815 02:52:38.302094  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.302110  6152 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:52:38.302174  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.302181  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.302186  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.302191  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.302253  6152 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.302268  6152 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.302285  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 02:52:38.302448  6152 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.302459  6152 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.302479  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:52:38.302552  6152 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:52:38.302634  6152 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 02:52:38.303831  6152 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.303853  6152 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.303920  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.303988  6152 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.303997  6152 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304011  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:52:38.304039  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.304082  6152 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304092  6152 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304104  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:52:38.304198  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304208  6152 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304219  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.304343  6152 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.304428  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.304487  6152 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304497  6152 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304512  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:52:38.304545  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.304594  6152 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304603  6152 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304617  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 02:52:38.304720  6152 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304730  6152 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304752  6152 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.304800  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.304808  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.304824  6152 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.304831  6152 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6384c590Variable Type 7
1884: I0815 02:52:38.304849  6152 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.304867  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.304886  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.304900  6152 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.304944  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.304965  6152 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:52:38.304992  6152 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.305001  6152 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.305013  6152 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:52:38.305019  6152 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6382de70Variable Type 7
1884: I0815 02:52:38.305032  6152 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:52:38.305043  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.305056  6152 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.305068  6152 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.305101  6152 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:52:38.305114  6152 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 02:52:38.305567  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:52:38.305604  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:52:38.305621  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:52:38.305655  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:52:38.305688  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.305752  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:52:38.310477  6152 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 02:52:38.310521  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:52:38.311379  6152 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 02:52:38.311404  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:52:38.311825  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.313628  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.314482  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.314605  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.315130  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.316072  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.318398  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.319538  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.321449  6152 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 02:52:38.322412  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.322464  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.322481  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.323721  6152 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:52:38.323772  6152 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3fbd8a0 type is 9
1884: I0815 02:52:38.323792  6152 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x574b870 type is 10
1884: I0815 02:52:38.323807  6152 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d06f40 type is 7
1884: I0815 02:52:38.323822  6152 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d06fa0 type is 7
1884: I0815 02:52:38.323834  6152 scope.cc:202] Create variable saved_params
1884: I0815 02:52:38.323848  6152 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x64891690 type is 17
1884: I0815 02:52:38.323889  6152 interpreter_util.cc:594] Static build: 0
1884: I0815 02:52:38.323902  6152 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:52:38.323915  6152 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:52:38.323928  6152 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:52:38.323997  6152 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.324025  6152 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:52:38.324831  6152 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 02:52:38.324899  6152 analysis_predictor.cc:433] Predictor::init()
1884: I0815 02:52:38.324965  6152 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 02:52:38.326162  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.326247  6152 scope.cc:202] Create variable feed
1884: I0815 02:52:38.326267  6152 naive_executor.cc:189] 0x63e1a220 Create persistable variable feed, which pointer is 0x63d0b980
1884: I0815 02:52:38.326282  6152 scope.cc:202] Create variable fetch
1884: I0815 02:52:38.326294  6152 naive_executor.cc:189] 0x63e1a220 Create persistable variable fetch, which pointer is 0x63d0b820
1884: I0815 02:52:38.326315  6152 scope.cc:202] Create variable linear_0.b_0
1884: I0815 02:52:38.326329  6152 naive_executor.cc:189] 0x63e1a220 Create persistable variable linear_0.b_0, which pointer is 0x63810970
1884: I0815 02:52:38.326345  6152 scope.cc:202] Create variable linear_0.w_0
1884: I0815 02:52:38.326359  6152 naive_executor.cc:189] 0x63e1a220 Create persistable variable linear_0.w_0, which pointer is 0x64350140
1884: I0815 02:52:38.326385  6152 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 02:52:38.326761  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.326859  6152 program_converter.cc:296] is_legacy_program : 0
1884: I0815 02:52:38.326917  6152 executor.cc:183] Old Executor is Running.
1884: I0815 02:52:38.327000  6152 executor.cc:92] Creating Variables for block 0
1884: I0815 02:52:38.327018  6152 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 02:52:38.327032  6152 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x63810970 type is 7
1884: I0815 02:52:38.327045  6152 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 02:52:38.327056  6152 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x64350140 type is 7
1884: I0815 02:52:38.327102  6152 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.327195  6152 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 02:52:38.327251  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.327266  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:38.327425  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.327550  6152 graph.cc:149] create OpNode by feed
1884: I0815 02:52:38.327596  6152 graph.cc:149] create OpNode by matmul_v2
1884: I0815 02:52:38.327621  6152 graph.cc:149] create OpNode by elementwise_add
1884: I0815 02:52:38.327646  6152 graph.cc:149] create OpNode by abs
1884: I0815 02:52:38.327667  6152 graph.cc:149] create OpNode by assign_value
1884: I0815 02:52:38.327694  6152 graph.cc:149] create OpNode by multinomial
1884: I0815 02:52:38.327714  6152 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:52:38.327739  6152 graph.cc:149] create OpNode by scale
1884: I0815 02:52:38.327761  6152 graph.cc:149] create OpNode by scale
1884: I0815 02:52:38.327782  6152 graph.cc:149] create OpNode by fetch
1884: I0815 02:52:38.327807  6152 graph.cc:149] create OpNode by fetch
1884: I0815 02:52:38.327838  6152 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 02:52:38.329097  6152 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 02:52:38.329113  6152 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 02:52:38.329195  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.329210  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 02:52:38.329358  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.329619  6152 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 02:52:38.329689  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.329702  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 02:52:38.329769  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.329783  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 02:52:38.329854  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.329924  6152 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:52:38.329967  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.329980  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 02:52:38.330030  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330051  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.330085  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.330098  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 02:52:38.330170  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330201  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.330235  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.330247  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 02:52:38.330332  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330420  6152 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:52:38.330461  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.330474  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 02:52:38.330538  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330566  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.330598  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.330612  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 02:52:38.330672  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330832  6152 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 02:52:38.330871  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.330886  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 02:52:38.330950  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.330974  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.331007  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.331022  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 02:52:38.331074  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.331104  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.331136  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.331149  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 02:52:38.331203  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.331225  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.331257  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.331272  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 02:52:38.331337  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.331416  6152 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 02:52:38.331460  6152 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:52:38.331483  6152 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:52:38.331506  6152 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 02:52:38.331538  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.331552  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 02:52:38.331606  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.331656  6152 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 02:52:38.331686  6152 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:52:38.331707  6152 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:52:38.331727  6152 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:52:38.331766  6152 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 02:52:38.331786  6152 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 02:52:38.332988  6152 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:52:38.333068  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.333087  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 02:52:38.333148  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.333177  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.333214  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.333227  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 02:52:38.333285  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.333351  6152 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:52:38.333395  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.333410  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 02:52:38.333460  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.333484  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.333515  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.333528  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 02:52:38.333595  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.333707  6152 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 02:52:38.333752  6152 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:52:38.333778  6152 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:52:38.333806  6152 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:52:38.333832  6152 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 02:52:38.333866  6152 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 02:52:38.333894  6152 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 02:52:38.333928  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.334024  6152 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 02:52:38.334061  6152 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:52:38.334085  6152 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:52:38.334110  6152 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:52:38.334134  6152 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 02:52:38.334159  6152 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 02:52:38.334184  6152 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 02:52:38.334244  6152 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 02:52:38.334547  6152 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:52:38.334599  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.334614  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 02:52:38.334694  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.334764  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.334808  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.334863  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.334900  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.334952  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.334986  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335033  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335064  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335107  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335134  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335175  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335199  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335235  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335258  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335289  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335317  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335347  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335384  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.335399  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 02:52:38.335460  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335511  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335547  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.335562  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 02:52:38.335603  6152 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 02:52:38.335614  6152 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:52:38.335675  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335705  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335742  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.335755  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 02:52:38.335795  6152 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 02:52:38.335814  6152 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:52:38.335865  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.335896  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.335932  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.335944  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 02:52:38.335985  6152 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 02:52:38.335996  6152 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:52:38.336038  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.336066  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.336098  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.336112  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 02:52:38.336151  6152 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 02:52:38.336162  6152 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:52:38.336210  6152 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:52:38.336239  6152 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:52:38.336272  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.336285  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 02:52:38.336336  6152 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 02:52:38.336393  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.336408  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 02:52:38.336525  6152 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:52:38.336566  6152 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.336601  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:52:38.336687  6152 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 02:52:38.336719  6152 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:52:38.336760  6152 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:52:38.336795  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.336808  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 02:52:38.337940  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.337973  6152 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 02:52:38.338040  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.338055  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:38.338688  6152 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 02:52:38.338924  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.339025  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.339042  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:38.339476  6152 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:52:38.339741  6152 graph.h:183] deleting __fuse_statis__
1884: I0815 02:52:38.339763  6152 graph.h:183] deleting pass_recorder
1884: I0815 02:52:38.339779  6152 graph.h:183] deleting stale_program_op_descs
1884: I0815 02:52:38.339884  6152 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 02:52:38.339905  6152 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 02:52:38.339918  6152 naive_executor.cc:195] 0x63e1a220 Create variable abs_0.tmp_0, which pointer is 0x63c32010
1884: I0815 02:52:38.339934  6152 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 02:52:38.339946  6152 naive_executor.cc:195] 0x63e1a220 Create variable gaussian_0.tmp_0, which pointer is 0x6491b870
1884: I0815 02:52:38.339969  6152 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 02:52:38.339982  6152 naive_executor.cc:195] 0x63e1a220 Create variable linear_0.tmp_1, which pointer is 0x64343280
1884: I0815 02:52:38.339994  6152 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:52:38.340006  6152 naive_executor.cc:195] 0x63e1a220 Create variable multinomial_0.tmp_0, which pointer is 0x64342d20
1884: I0815 02:52:38.340019  6152 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 02:52:38.340031  6152 naive_executor.cc:195] 0x63e1a220 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x64343020
1884: I0815 02:52:38.340044  6152 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 02:52:38.340056  6152 naive_executor.cc:195] 0x63e1a220 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x64341620
1884: I0815 02:52:38.340071  6152 scope.cc:202] Create variable feed
1884: I0815 02:52:38.340085  6152 scope.cc:202] Create variable fetch
1884: I0815 02:52:38.340116  6152 naive_executor.cc:46] NaiveExecutor init with scope 0x63e1a220
1884: I0815 02:52:38.340129  6152 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 02:52:38.340375  6152 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:52:38.340404  6152 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:52:38.340446  6152 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 02:52:38.340461  6152 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 02:52:38.340479  6152 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.340526  6152 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 02:52:38.340795  6152 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.340829  6152 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 02:52:38.340904  6152 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.340945  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 02:52:38.533625  6152 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:52:38.533774  6152 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.533805  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:52:38.533967  6152 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:52:38.534020  6152 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.534065  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:52:38.534160  6152 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 02:52:38.534226  6152 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.534258  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:52:38.534335  6152 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:52:38.534387  6152 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:52:38.534420  6152 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:52:38.534473  6152 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 02:52:38.534505  6152 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.534556  6152 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 02:52:38.534597  6152 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 02:52:38.535125  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.535159  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 02:52:38.693809  6152 pir_interpreter.cc:161] PirInterpreter(): 0x6384cf60 on Place(gpu:0)
1884: I0815 02:52:38.693857  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_0
1884: I0815 02:52:38.693876  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_1
1884: I0815 02:52:38.693882  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_2
1884: I0815 02:52:38.693893  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_3
1884: I0815 02:52:38.693924  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_4
1884: I0815 02:52:38.693936  6152 scope.cc:202] Create variable 0x6384cf601723690358693843467_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 1 )  ( 2 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x6384cf601723690358693843467_inner_var_0 -> 0x638e02f0
1884: 1 -> 0x6384cf601723690358693843467_inner_var_1 -> 0x639259e0
1884: 2 -> 0x6384cf601723690358693843467_inner_var_2 -> 0x63c02200
1884: 3 -> linear_1.w_0 -> 0x63e05030
1884: 4 -> linear_1.b_0 -> 0x6488c960
1884: 5 -> learning_rate_1 -> 0x6482ca50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 02:52:38.695354  6230 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:38.695436  6230 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.695534  6230 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.695578  6230 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 02:52:38.695606  6230 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.695623  6230 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.695634  6230 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:52:38.695354  6226 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.695354  6227 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.699368  6227 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.695361  6228 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.699433  6228 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.699456  6227 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:52:38.699497  6228 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.695354  6229 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.699362  6226 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.703377  6226 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6384cf601723690358693843467_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.703451  6230 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6384cf601723690358693843467_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6384cf601723690358693843467_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6384cf601723690358693843467_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.703572  6230 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6384cf601723690358693843467_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6384cf601723690358693843467_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6384cf601723690358693843467_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 02:52:38.704335  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x6384d0d0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 02:52:38.707353  6152 pir_interpreter.cc:161] PirInterpreter(): 0x61ca35d0 on Place(gpu:0)
1884: I0815 02:52:38.707412  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_1
1884: I0815 02:52:38.707428  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_4
1884: I0815 02:52:38.707435  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_5
1884: I0815 02:52:38.707442  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_6
1884: I0815 02:52:38.707465  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_7
1884: I0815 02:52:38.707474  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_8
1884: I0815 02:52:38.707481  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_9
1884: I0815 02:52:38.707510  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_10
1884: I0815 02:52:38.707518  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_11
1884: I0815 02:52:38.707525  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_12
1884: I0815 02:52:38.707533  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_13
1884: I0815 02:52:38.707542  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_14
1884: I0815 02:52:38.707551  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_15
1884: I0815 02:52:38.707558  6152 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:52:38.707567  6152 scope.cc:202] Create variable 0x61ca35d01723690358707386086_inner_var_17
1884: I0815 02:52:38.707584  6152 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x6482ca50
1884: 1 -> 0x61ca35d01723690358707386086_inner_var_1 -> 0x63f20c50
1884: 2 -> linear_1.b_0 -> 0x6488c960
1884: 3 -> linear_1.w_0 -> 0x63e05030
1884: 4 -> 0x61ca35d01723690358707386086_inner_var_4 -> 0x63804860
1884: 5 -> 0x61ca35d01723690358707386086_inner_var_5 -> 0x61c8d7c0
1884: 6 -> 0x61ca35d01723690358707386086_inner_var_6 -> 0x63d0a320
1884: 7 -> 0x61ca35d01723690358707386086_inner_var_7 -> 0x63e00610
1884: 8 -> 0x61ca35d01723690358707386086_inner_var_8 -> 0x64923f30
1884: 9 -> 0x61ca35d01723690358707386086_inner_var_9 -> 0x644498d0
1884: 10 -> 0x61ca35d01723690358707386086_inner_var_10 -> 0x63d0a200
1884: 11 -> 0x61ca35d01723690358707386086_inner_var_11 -> 0x644cb570
1884: 12 -> 0x61ca35d01723690358707386086_inner_var_12 -> 0x6392c720
1884: 13 -> 0x61ca35d01723690358707386086_inner_var_13 -> 0x63c2ac00
1884: 14 -> 0x61ca35d01723690358707386086_inner_var_14 -> 0x63c02120
1884: 15 -> 0x61ca35d01723690358707386086_inner_var_15 -> 0x638b6120
1884: 16 -> fetch0@fetch -> 0x63d0cca0
1884: 17 -> 0x61ca35d01723690358707386086_inner_var_17 -> 0x63d0cc60
1884: 18 -> fetch1@fetch -> 0x638dfe10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 02:52:38.714349  6235 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:52:38.714447  6232 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.714417  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714483  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:52:38.714502  6232 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714515  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714558  6232 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:52:38.714576  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.714579  6232 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714608  6232 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.714623  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:52:38.714638  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:52:38.714705  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:52:38.714721  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61ca35d01723690358707386086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714762  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61ca35d01723690358707386086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:52:38.714797  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61ca35d01723690358707386086_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714833  6235 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:52:38.714870  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61ca35d01723690358707386086_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.714900  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61ca35d01723690358707386086_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61ca35d01723690358707386086_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.714936  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.714947  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61ca35d01723690358707386086_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61ca35d01723690358707386086_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.714980  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61ca35d01723690358707386086_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.715001  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.715013  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61ca35d01723690358707386086_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.715009  6232 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ca35d01723690358707386086_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.715029  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61ca35d01723690358707386086_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.715039  6232 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.715063  6235 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.715126  6232 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ca35d01723690358707386086_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.715157  6232 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61ca35d01723690358707386086_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.715174  6232 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.715188  6232 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61ca35d01723690358707386086_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.714447  6234 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.718362  6231 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.718372  6233 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.718431  6235 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.718521  6235 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.718618  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.718638  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61ca35d01723690358707386086_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61ca35d01723690358707386086_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.718708  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.718739  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.718729  6233 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.718756  6233 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:52:38.718752  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.718770  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61ca35d01723690358707386086_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.718822  6233 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ca35d01723690358707386086_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.718847  6233 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61ca35d01723690358707386086_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.718847  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61ca35d01723690358707386086_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:52:38.718863  6233 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.718868  6235 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61ca35d01723690358707386086_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ca35d01723690358707386086_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.718878  6233 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61ca35d01723690358707386086_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.718896  6235 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:52:38.718907  6235 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61ca35d01723690358707386086_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ca35d01723690358707386086_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61ca35d01723690358707386086_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:52:38.719327  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x61ca3740) got event_name: TaskCompletion
1884: I0815 02:52:38.719380  6152 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.719427  6152 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 02:52:38.731015  6152 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 02:52:38.731076  6152 analysis_predictor.cc:433] Predictor::init()
1884: I0815 02:52:38.731877  6152 scope.cc:202] Create variable linear_1.b_0
1884: I0815 02:52:38.731940  6152 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 02:52:38.732470  6152 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587325281890"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587325281890"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 02:52:38.732733  6152 pir_interpreter.cc:161] PirInterpreter(): 0x64465eb0 on Place(cpu)
1884: I0815 02:52:38.732764  6152 scope.cc:202] Create variable 0x64465eb01723690358732752825_inner_var_0
1884: I0815 02:52:38.732796  6152 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587325281890"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236903587325281890 -> 0x63f1c240
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 02:52:38.732978  6152 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 02:52:38.733444  6236 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.733448  6237 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.733505  6237 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236903587325281890:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.733578  6237 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236903587325281890:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:52:38.736346  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x64466020) got event_name: TaskCompletion
1884: I0815 02:52:38.737360  6238 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.737360  6240 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.737360  6239 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.741525  6237 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : 88, peak : 144}.
1884: I0815 02:52:38.741544  6237 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : 88, peak : 144}.
1884: I0815 02:52:38.746431  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.746464  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587466436521"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587466436521"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 02:52:38.746932  6152 pir_interpreter.cc:161] PirInterpreter(): 0x6380d590 on Place(cpu)
1884: I0815 02:52:38.746968  6152 scope.cc:202] Create variable 0x6380d5901723690358746952789_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587466436521"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236903587466436521 -> 0x6491eba0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 02:52:38.747370  6241 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.748340  6242 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.748343  6243 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.748389  6242 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236903587466436521:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.748492  6242 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236903587466436521:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.751333  6245 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.748347  6244 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.751329  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x6380d700) got event_name: TaskCompletion
1884: I0815 02:52:38.760390  6242 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 7126788430861792400 to 4991363412201691562 , after update, data is {current : 96, peak : 144}.
1884: I0815 02:52:38.760422  6242 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 7126788430861792400 to 4991363412201691562 , after update, data is {current : 96, peak : 144}.
1884: I0815 02:52:38.767414  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.767448  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236903587466436521",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236903587676571622"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236903587466436521",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236903587676571622"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 02:52:38.767951  6152 pir_interpreter.cc:161] PirInterpreter(): 0x61c8b7c0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236903587466436521",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236903587676571622"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236903587676571622 -> 0x6491eba0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 02:52:38.769347  6247 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.769354  6249 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.769416  6247 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.769485  6247 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.772326  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x61c8b930) got event_name: TaskCompletion
1884: I0815 02:52:38.769352  6250 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.769347  6246 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.769347  6248 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.780499  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.780529  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587807346153"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587807346153"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 02:52:38.781013  6152 pir_interpreter.cc:161] PirInterpreter(): 0x64465eb0 on Place(cpu)
1884: I0815 02:52:38.781046  6152 scope.cc:202] Create variable 0x64465eb01723690358781032607_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236903587807346153"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236903587807346153 -> 0x638195d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 02:52:38.782342  6255 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:52:38.782351  6252 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:52:38.782407  6255 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236903587807346153:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.782503  6255 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236903587807346153:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:52:38.785326  6152 pir_interpreter.cc:1766] main_thread_blocker_(0x64466020) got event_name: TaskCompletion
1884: I0815 02:52:38.782349  6254 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:52:38.782342  6253 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:52:38.782342  6251 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.790365  6255 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : 100, peak : 144}.
1884: I0815 02:52:38.790398  6255 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : 100, peak : 144}.
1884: I0815 02:52:38.793401  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.793426  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:38.793604  6152 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 02:52:38.793709  6152 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 02:52:38.793748  6152 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587807346153"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587676571622"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587807346153"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587676571622"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 02:52:38.794643  6152 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 02:52:38.794665  6152 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 02:52:38.794718  6152 pir_interpreter.cc:161] PirInterpreter(): 0x64465eb0 on Place(cpu)
1884: I0815 02:52:38.794766  6152 scope.cc:202] Create variable feed_name_0
1884: I0815 02:52:38.794785  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_5
1884: I0815 02:52:38.794808  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_6
1884: I0815 02:52:38.794818  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_7
1884: I0815 02:52:38.794826  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_8
1884: I0815 02:52:38.794844  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_9
1884: I0815 02:52:38.794853  6152 scope.cc:202] Create variable 0x64465eb01723690358794739196_inner_var_10
1884: I0815 02:52:38.794880  6152 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.794906  6152 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 02:52:38.795136  6152 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.795150  6152 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587807346153"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236903587676571622"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236903587807346153 -> 0x638195d0
1884: 1 -> constant_folding@_17236903587676571622 -> 0x6491eba0
1884: 2 -> linear_1.b_0 -> 0x61c76620
1884: 3 -> linear_1.w_0 -> 0x642f06a0
1884: 4 -> feed_name_0 -> 0x63f3a6e0
1884: 5 -> 0x64465eb01723690358794739196_inner_var_5 -> 0x63f1c0b0
1884: 6 -> 0x64465eb01723690358794739196_inner_var_6 -> 0x6393b540
1884: 7 -> 0x64465eb01723690358794739196_inner_var_7 -> 0x642f0510
1884: 8 -> 0x64465eb01723690358794739196_inner_var_8 -> 0x638dccd0
1884: 9 -> fetch_name_0 -> 0x6491eb80
1884: 10 -> fetch_name_1 -> 0x6480ff00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 02:52:38.795840  6152 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 02:52:38.795936  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796021  6152 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:52:38.796047  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.796080  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x64465eb01723690358794739196_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x64465eb01723690358794739196_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796124  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x64465eb01723690358794739196_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.796155  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796176  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.796190  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796223  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236903587676571622:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.796241  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236903587807346153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796272  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236903587807346153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x64465eb01723690358794739196_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:52:38.796293  6152 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236903587807346153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:52:38.796352  6152 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236903587807346153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x64465eb01723690358794739196_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:52:38.796360  6256 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:52:38.796378  6152 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 02:52:38.796403  6152 helper.h:475] after run : [cpu current allocated memory: 6.10591MB], [cpu current reserved memory: 6.10591MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 02:52:38.796432  6152 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 02:52:38.796599  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.796604  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:38.800371  6256 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : -92, peak : 144}.
1884: I0815 02:52:38.800403  6256 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 3540291087396145173 to 4991363412201691562 , after update, data is {current : -92, peak : 144}.
1884: I0815 02:52:38.801398  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:38.801417  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......FFF..
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 60641 / 100000 (60.6%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([2, 0, 0, ..., 3, 3, 1], dtype=int64)
1884:  y: array([0, 0, 0, ..., 0, 0, 0])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 209964 / 300000 (70%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[3, 3, 0, ..., 3, 3, 3],
1884:        [2, 2, 2, ..., 0, 3, 0],
1884:        [0, 3, 2, ..., 3, 1, 0]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 99 / 100 (99%)
1884: Max absolute difference: 988
1884: Max relative difference: inf
1884:  x: array([749, 172, 517, 125, 602, 402, 280, 597, 659, 459, 573, 582, 776,
1884:        971, 576, 439, 542, 379, 519, 884, 740, 936, 287, 295,  83, 988,
1884:        488, 158,  32, 222, 585, 816, 588, 381, 471, 770, 174, 291, 463,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 5.671s
1884: 
1884: FAILED (failures=3, skipped=3)
1884: 
1884: I0815 02:52:38.804049  6152 mmap_allocator.cc:348] PID: 6152, MemoryMapFdSet: set size - 0
1884: I0815 02:52:38.825294  6152 mmap_allocator.cc:348] PID: 6152, MemoryMapFdSet: set size - 0
1884: I0815 02:52:38.970229  6228 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2218075940985657683 to 4991363412201691562 , after update, data is {current : -88, peak : 144}.
1884: I0815 02:52:38.970268  6228 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2218075940985657683 to 4991363412201691562 , after update, data is {current : -88, peak : 144}.
1884: I0815 02:52:38.970388  6227 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14538991260395951885 to 4991363412201691562 , after update, data is {current : -72, peak : 144}.
1884: I0815 02:52:38.970422  6227 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14538991260395951885 to 4991363412201691562 , after update, data is {current : -72, peak : 144}.
1884: I0815 02:52:38.974362  6226 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4346257494382934393 to 4991363412201691562 , after update, data is {current : -68, peak : 144}.
1884: I0815 02:52:38.974395  6226 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4346257494382934393 to 4991363412201691562 , after update, data is {current : -68, peak : 144}.
1884: I0815 02:52:38.975502  6230 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 3100099212397260293 to 4991363412201691562 , after update, data is {current : -92, peak : 144}.
1884: I0815 02:52:38.975528  6230 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 3100099212397260293 to 4991363412201691562 , after update, data is {current : -92, peak : 144}.
1884: I0815 02:52:38.975534  6230 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 3100099212397260293 to 4991363412201691562 , after update, data is {current : 512, peak : 768}.
1884: I0815 02:52:38.975865  6232 thread_data_registry.h:135] Add data {current : -256, peak : 0} from thread 11065749050063902401 to 4991363412201691562 , after update, data is {current : 256, peak : 768}.
1884: I0815 02:52:38.975872  6232 thread_data_registry.h:135] Add data {current : 140, peak : 260} from thread 11065749050063902401 to 4991363412201691562 , after update, data is {current : 48, peak : 260}.
1884: I0815 02:52:38.975878  6232 thread_data_registry.h:135] Add data {current : 140, peak : 260} from thread 11065749050063902401 to 4991363412201691562 , after update, data is {current : 48, peak : 260}.
1884: I0815 02:52:38.975953  6233 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 4991363412201691562 to 18118767963559639514 , after update, data is {current : 768, peak : 1536}.
1884: I0815 02:52:38.975960  6233 thread_data_registry.h:135] Add data {current : 48, peak : 260} from thread 4991363412201691562 to 18118767963559639514 , after update, data is {current : 28, peak : 260}.
1884: I0815 02:52:38.975965  6233 thread_data_registry.h:135] Add data {current : 48, peak : 260} from thread 4991363412201691562 to 18118767963559639514 , after update, data is {current : 28, peak : 260}.
1884: I0815 02:52:38.980377  6235 thread_data_registry.h:135] Add data {current : 28, peak : 260} from thread 18118767963559639514 to 14697222372681333668 , after update, data is {current : 6401792, peak : 8800800}.
1884: I0815 02:52:38.980412  6235 thread_data_registry.h:135] Add data {current : 28, peak : 260} from thread 18118767963559639514 to 14697222372681333668 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 02:52:38.980418  6235 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 18118767963559639514 to 14697222372681333668 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 02:52:39.329646  6152 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:52:39.329686  6152 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:52:39.329747  6152 mmap_allocator.cc:348] PID: 6152, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   13.85 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  14.04 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
