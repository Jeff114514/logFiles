UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 03:48:18.148522  6509 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 03:48:19.130064  6509 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=enable_opt_get_features,gpugraph_enable_gpu_direct_access,multi_node_sample_use_gpu_table,print_allocator_trace_info,enable_graph_multi_node_sampling,get_host_by_name_time,enable_cse_in_dy2st,graph_load_in_parallel,gpugraph_offload_param_stat,conv2d_disable_cudnn,enable_dependency_builder_debug_info,cinn_subgraph_graphviz_dir,run_kp_kernel,async_trace_count,prim_check_ops,einsum_opt,gpugraph_parallel_stream_num,prim_forward,cusparse_dir,gpugraph_offload_param_extends,free_when_no_cache_hit,static_runtime_data_save_path,cublaslt_device_best_config,accuracy_check_atol_fp32,mkl_dir,prim_skip_dynamic,enable_pir_with_pt_in_dy2st,tensorrt_dir,enable_cublas_tensor_op_math,dynamic_static_unified_comm,gpugraph_merge_grads_segment_size,enable_cinn_auto_tune,check_nan_inf,win_cuda_bin_dir,save_static_runtime_data,initial_gpu_memory_in_mb,op_dir,enable_auto_rdma_trans,sort_sum_gradient,graph_metapath_split_opt,log_memory_stats,search_cache_max_number,prim_forward_blacklist,enable_neighbor_list_use_uva,gpu_memory_limit_mb,gpugraph_hbm_table_load_factor,enable_all2all_use_fp16,cusolver_dir,accuracy_check_atol_bf16,pinned_memory_as_cpu_backend,local_exe_sub_scope_limit,auto_growth_chunk_size_in_mb,cudnn_exhaustive_search_times,enable_cinn_accuracy_check,tracer_onednn_ops_on,enable_interpretercore_launch_cinn,cse_max_count,cache_inference_while_scope,gpugraph_enable_hbm_table_collision_stat,check_kernel_launch,free_idle_chunk,graph_embedding_split_infer_mode,lapack_dir,enable_record_memory,use_shm_cache,nccl_dir,print_ir,enable_blaslt_global_search,pir_apply_inplace_pass,query_dest_rank_by_multi_node,gpu_allocator_retry_time,graph_neighbor_size_percent,alloc_fill_value,allow_cinn_ops,auto_free_cudagraph_allocations_on_launch,enable_gpu_memory_usage_log_mb,graph_get_neighbor_id,gemm_use_half_precision_compute_type,gpugraph_slot_feasign_max_num,fraction_of_cpu_memory_to_use,dygraph_debug,accuracy_check_rtol_bf16,manually_trans_conv_filter,reader_queue_speed_test_mode,enable_pir_in_executor,use_cinn,nvidia_package_dir,max_inplace_grad_add,accuracy_check_atol_fp16,pir_apply_shape_optimization_pass,accuracy_check_rtol_fp16,enable_dump_main_program,dataloader_use_file_descriptor,prim_backward,cupti_dir,nccl_blocking_wait,gpugraph_enable_segment_merge_grads,cuda_malloc_async_pool_memory_throttle_ratio,call_stack_level,set_to_1d,use_mkldnn,custom_device_mem_record,tracer_profile_fname,deny_cinn_ops,new_executor_use_local_scope,enable_fusion_fallback,use_stream_safe_cuda_allocator,gpugraph_storage_mode,memory_fraction_of_eager_deletion,prim_enabled,tracer_onednn_ops_off,benchmark,use_autotune,npu_storage_format,enable_fuse_parallel_matmul_pass,new_executor_use_inplace,gpugraph_debug_gpu_memory,allocator_strategy,sync_after_alloc,gpugraph_dedup_pull_push_mode,cudnn_deterministic,enable_adjust_op_order,embedding_deterministic,all_blocks_convert_trt,use_stride_kernel,curand_dir,pir_broadcast_tree_limit,gpugraph_sparse_table_storage_mode,cublas_dir,add_dependency_for_communication_op,fuse_parameter_memory_size,enable_pir_api,logging_pir_py_code_dump_symbolic_dims,cudnn_batchnorm_spatial_persistent,logging_trunc_pir_py_code,gpugraph_parallel_copyer_split_maxsize,use_xqa_optim,gpugraph_offload_gather_copy_maxsize,tensor_operants_mode,selected_gpus,cuda_memory_async_pool_realease_threshold,new_executor_static_build,fast_eager_deletion_mode,new_executor_serial_run,cublaslt_exhaustive_search_times,prim_all,enable_unused_var_check,prim_enable_dynamic,logging_pir_py_code_dir,inner_op_parallelism,trt_ibuilder_cache,gpugraph_force_device_batch_num_equal,use_virtual_memory_auto_growth,enable_sparse_inner_gather,logging_pir_py_code_int_tensor_element_limit,gpugraph_load_node_list_into_hbm,cuda_dir,use_auto_growth_pinned_allocator,use_cuda_malloc_async_allocator,benchmark_nccl,eager_delete_scope,new_executor_sequential_run,conv_workspace_size_limit,use_pinned_memory,paddle_num_threads,print_sub_graph_dir,enable_exit_when_partial_worker,sync_nccl_allreduce,cinn_compile_thread_num,fraction_of_gpu_memory_to_use,enable_async_trace,enable_gpu_memory_usage_log,fleet_executor_with_standalone,multiple_of_cupti_buffer_size,reallocate_gpu_memory_in_mb,check_nan_inf_level,dump_chunk_info,enable_collect_shape,ir_inplace_kernel_blacklist,pir_debug,low_precision_op_list,cudnn_dir,enable_cinn_compile_cache,convert_all_blocks,new_executor_use_cuda_graph,accuracy_check_rtol_fp32,allreduce_record_one_event,fraction_of_cuda_pinned_memory_to_use,init_allocated_mem,fuse_parameter_groups_size,cusparselt_dir,host_trace_level,static_executor_perfstat_filepath,use_auto_growth_v2,enable_tracker_all2all,dist_threadpool_size,use_system_allocator,use_fast_math,jit_engine_type,use_cuda_managed_memory,eager_delete_tensor_gb,apply_pass_to_program,enable_pir_in_executor_trace_run,gpugraph_enable_print_op_debug,check_infer_symbolic,disable_dyshape_in_train,enable_api_kernel_fallback,enable_auto_detect_gpu_topo,cudnn_exhaustive_search,executor_log_deps_every_microseconds,mklml_dir,pir_subgraph_saving_dir,initial_cpu_memory_in_mb 
1884: I0815 03:48:19.130182  6509 init.cc:108] After Parse: argc is 2
1884: I0815 03:48:26.813241  6509 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:48:26.813279  6509 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 03:48:26.814002  6509 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 03:48:26.814463  6509 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 03:48:26.815322  6509 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 03:48:26.815423  6509 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 03:48:26.815521  6509 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 03:48:26.816255  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5152000000), and remaining 0
1884: I0815 03:48:26.816566  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.816632  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.816743  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5152000200), and remaining 0
1884: I0815 03:48:26.816779  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5152000400), and remaining 0
1884: I0815 03:48:26.820544  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5152000600), and remaining 0
1884: I0815 03:48:26.820703  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f5152000800), and remaining 0
1884: I0815 03:48:26.820796  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f5152000a00), and remaining 0
1884: I0815 03:48:26.820902  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.820927  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.820998  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.821012  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.821962  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195b09c0 for it.
1884: I0815 03:48:26.822124  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.822150  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.822211  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f5152000e00), and remaining 0
1884: I0815 03:48:26.822289  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f51520c4400), and remaining 0
1884: I0815 03:48:26.945576  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195b09c0 for it.
1884: I0815 03:48:26.945793  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.945835  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.946445  6509 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f5152200000), and remaining 0
1884: I0815 03:48:26.956542  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195b09c0 for it.
1884: I0815 03:48:26.956693  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:26.956735  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.956785  6509 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:26.956988  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:26.957979  6509 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 03:48:26.957999  6509 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 03:48:26.958057  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:26.958145  6509 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 03:48:26.958173  6509 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.958238  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:26.958329  6509 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 03:48:26.958350  6509 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.958387  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:26.958552  6509 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 03:48:26.958571  6509 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.958735  6509 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 03:48:26.958761  6509 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:26.958837  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:26.962270  6509 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 03:48:26.962394  6509 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 03:48:26.962425  6509 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 03:48:26.962495  6509 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 03:48:28.457471  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.457526  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.457796  6509 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 03:48:28.457815  6509 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.462385  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.462430  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.463395  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.463418  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.463433  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.464164  6509 program_interpreter.cc:243] New Executor is Running.
1884: I0815 03:48:28.464179  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.464202  6509 scope.cc:202] Create variable feed
1884: I0815 03:48:28.464210  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.464219  6509 scope.cc:202] Create variable fetch
1884: I0815 03:48:28.464222  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.464233  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.464242  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.464246  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.464248  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.466696  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.467065  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.467079  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.467084  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.468832  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.468891  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.468902  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.468910  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.468919  6509 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:48:28.468927  6509 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x5fd13c90 type is 7
1884: I0815 03:48:28.468932  6509 scope.cc:202] Create variable x
1884: I0815 03:48:28.468935  6509 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5fd12750 type is 7
1884: I0815 03:48:28.468997  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.469004  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.469009  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.469013  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.469135  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.469161  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.469283  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.469292  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.469317  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.469509  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.469538  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.469559  6509 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.469564  6509 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fd1a7b0Variable Type 7
1884: I0815 03:48:28.469584  6509 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.469609  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.469659  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.469679  6509 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.470893  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.470952  6509 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.471374  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.476395  6509 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:48:28.476423  6509 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:48:28.476547  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.476580  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.477077  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: I0815 03:48:28.477171  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.477195  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.477625  6509 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: I0815 03:48:28.477694  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.477715  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.477742  6509 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.478019  6509 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:48:28.478031  6509 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:48:28.478140  6509 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 03:48:28.478163  6509 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.478557  6509 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:48:28.478571  6509 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:48:28.478615  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.478636  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.478823  6509 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:48:28.478833  6509 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:48:28.478875  6509 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:48:28.478893  6509 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:48:28.478910  6509 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.481843  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.481873  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.481936  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.481946  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.484045  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.484439  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.484457  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.484463  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.486321  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.486390  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.486402  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.486409  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fd46250 type is 7
1884: I0815 03:48:28.486418  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.486425  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fd465c0 type is 7
1884: I0815 03:48:28.486433  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.486438  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.486500  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.486508  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.486513  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.486518  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.486577  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.486594  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.486662  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.486673  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.486692  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.487007  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.487025  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.487043  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.487054  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fd4cc70Variable Type 7
1884: I0815 03:48:28.487073  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.487093  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.487119  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.487136  6509 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.487891  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.487923  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.488128  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.500531  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: I0815 03:48:28.500824  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:48:28.507509  6509 pir_interpreter.cc:161] PirInterpreter(): 0x5ff09440 on Place(gpu:0)
1884: I0815 03:48:28.507562  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.507596  6509 scope.cc:202] Create variable 0x5ff094401723693708507544290_inner_var_1
1884: I0815 03:48:28.507608  6509 scope.cc:202] Create variable 0x5ff094401723693708507544290_inner_var_2
1884: I0815 03:48:28.507622  6509 scope.cc:202] Create variable 0x5ff094401723693708507544290_inner_var_3
1884: I0815 03:48:28.507632  6509 scope.cc:202] Create variable 0x5ff094401723693708507544290_inner_var_4
1884: I0815 03:48:28.507643  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:28.508169  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:28.508188  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.508193  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 03:48:28.508244  6509 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5ff08b80
1884: 1 -> 0x5ff094401723693708507544290_inner_var_1 -> 0x5ff08850
1884: 2 -> 0x5ff094401723693708507544290_inner_var_2 -> 0x5ff09cd0
1884: 3 -> 0x5ff094401723693708507544290_inner_var_3 -> 0x5ff09420
1884: 4 -> 0x5ff094401723693708507544290_inner_var_4 -> 0x5ff0a080
1884: 5 -> fetch0@fetch -> 0x5ff0a890
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:28.509027  6509 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 03:48:28.509423  6547 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:28.525357  6548 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:28.525460  6548 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff094401723693708507544290_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.525621  6548 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff094401723693708507544290_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:28.526343  6551 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:28.526463  6549 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:28.527413  6552 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:28.527506  6552 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.527588  6552 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:48:28.527657  6552 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff094401723693708507544290_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ff094401723693708507544290_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.527982  6552 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff094401723693708507544290_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ff094401723693708507544290_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:48:28.525363  6550 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:28.529415  6550 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff094401723693708507544290_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.529469  6550 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.530766  6550 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff094401723693708507544290_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5ff094401723693708507544290_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:48:28.530822  6550 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff094401723693708507544290_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.530856  6550 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.531471  6550 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff094401723693708507544290_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:48:28.535490  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x5ff095b0) got event_name: TaskCompletion
1884: I0815 03:48:28.535555  6509 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.630386  6547 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 1783357461655525252 to 3067046922234382951 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:48:28.630424  6547 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 1783357461655525252 to 11749734598755656615 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 03:48:28.630430  6547 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 1783357461655525252 to 11749734598755656615 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 03:48:28.635361  6548 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5593954173007686561 to 11749734598755656615 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 03:48:28.635393  6548 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5593954173007686561 to 11749734598755656615 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 03:48:28.637379  6550 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 11749734598755656615 to 9145187466738323728 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:48:28.637413  6550 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 11749734598755656615 to 9145187466738323728 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:48:28.642398  6552 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 3067046922234382951 to 9145187466738323728 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 03:48:28.642438  6552 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 3067046922234382951 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:28.651870  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.651906  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.651966  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.651973  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.653810  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.654186  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.654196  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.654201  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.655788  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.655917  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.655926  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.655932  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x2596840 type is 7
1884: I0815 03:48:28.655941  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.655943  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62377960 type is 7
1884: I0815 03:48:28.655948  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.655952  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.656011  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.656015  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.656019  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.656023  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.656076  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.656091  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.656152  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.656159  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.656173  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.656352  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.656363  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.656378  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.656381  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3b3d030Variable Type 7
1884: I0815 03:48:28.656397  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.656414  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.656433  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.656445  6509 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.658099  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.658138  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.658380  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.665611  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: I0815 03:48:28.665894  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:48:28.669248  6509 pir_interpreter.cc:161] PirInterpreter(): 0x5fd33010 on Place(gpu:0)
1884: I0815 03:48:28.669287  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.669318  6509 scope.cc:202] Create variable 0x5fd330101723693708669276216_inner_var_1
1884: I0815 03:48:28.669332  6509 scope.cc:202] Create variable 0x5fd330101723693708669276216_inner_var_2
1884: I0815 03:48:28.669342  6509 scope.cc:202] Create variable 0x5fd330101723693708669276216_inner_var_3
1884: I0815 03:48:28.669349  6509 scope.cc:202] Create variable 0x5fd330101723693708669276216_inner_var_4
1884: I0815 03:48:28.669359  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:28.669759  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:28.669777  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.669781  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fede820
1884: 1 -> 0x5fd330101723693708669276216_inner_var_1 -> 0x3b23040
1884: 2 -> 0x5fd330101723693708669276216_inner_var_2 -> 0x5fd14960
1884: 3 -> 0x5fd330101723693708669276216_inner_var_3 -> 0x4c462f60
1884: 4 -> 0x5fd330101723693708669276216_inner_var_4 -> 0x5ff11a00
1884: 5 -> fetch0@fetch -> 0x3b5da70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:28.671335  6555 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:28.671339  6553 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:28.671382  6555 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fd330101723693708669276216_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.671450  6555 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fd330101723693708669276216_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:28.671487  6554 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:28.672398  6556 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:28.672459  6557 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:28.672514  6558 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:28.672545  6558 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.672580  6558 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:48:28.672675  6558 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fd330101723693708669276216_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fd330101723693708669276216_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.672797  6558 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fd330101723693708669276216_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fd330101723693708669276216_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.672938  6557 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fd330101723693708669276216_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.672956  6557 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.675734  6557 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fd330101723693708669276216_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fd330101723693708669276216_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.675793  6557 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fd330101723693708669276216_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.675817  6557 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.677860  6557 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fd330101723693708669276216_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.677915  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x5fd33180) got event_name: TaskCompletion
1884: I0815 03:48:28.677932  6509 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.718504  6553 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 3067046922234382951 to 2231328903312545631 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:48:28.718537  6553 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 3067046922234382951 to 2231328903312545631 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:48:28.718544  6553 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 3067046922234382951 to 213944256656944574 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:48:28.718677  6557 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 2231328903312545631 to 213944256656944574 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:48:28.718683  6557 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 2231328903312545631 to 213944256656944574 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:48:28.719367  6555 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 11749734598755656615 to 213944256656944574 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:48:28.719512  6555 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 11749734598755656615 to 213944256656944574 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:48:28.720975  6558 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:48:28.720990  6558 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:48:28.720995  6558 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:28.728226  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.728260  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.728327  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.728333  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.730109  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.730535  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.730551  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.730556  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.732098  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.732225  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.732234  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.732240  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x1ee4350 type is 7
1884: I0815 03:48:28.732247  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.732250  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4a636220 type is 7
1884: I0815 03:48:28.732255  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.732259  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.732324  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.732329  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.732332  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.732336  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.732388  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.732401  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.732460  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.732466  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.732481  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.732522  6509 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.732668  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:28.732729  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.732738  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.732753  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.732758  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fcfb830Variable Type 7
1884: I0815 03:48:28.732774  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.732789  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.732810  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.732822  6509 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.733065  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.733086  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.733282  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.734135  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: I0815 03:48:28.734351  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:48:28.737496  6509 pir_interpreter.cc:161] PirInterpreter(): 0x62539470 on Place(gpu:0)
1884: I0815 03:48:28.737529  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.737550  6509 scope.cc:202] Create variable 0x625394701723693708737520179_inner_var_1
1884: I0815 03:48:28.737558  6509 scope.cc:202] Create variable 0x625394701723693708737520179_inner_var_2
1884: I0815 03:48:28.737567  6509 scope.cc:202] Create variable 0x625394701723693708737520179_inner_var_3
1884: I0815 03:48:28.737576  6509 scope.cc:202] Create variable 0x625394701723693708737520179_inner_var_4
1884: I0815 03:48:28.737584  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:28.737957  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:28.737968  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.737972  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fcf88b0
1884: 1 -> 0x625394701723693708737520179_inner_var_1 -> 0x3b23fd0
1884: 2 -> 0x625394701723693708737520179_inner_var_2 -> 0x625391f0
1884: 3 -> 0x625394701723693708737520179_inner_var_3 -> 0x3b3d610
1884: 4 -> 0x625394701723693708737520179_inner_var_4 -> 0x5fd35d30
1884: 5 -> fetch0@fetch -> 0x5fd15a10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:28.739334  6564 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:28.739334  6562 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:28.739385  6563 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:28.739389  6562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625394701723693708737520179_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.739389  6564 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.739439  6564 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:48:28.739451  6562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625394701723693708737520179_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:28.739472  6564 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625394701723693708737520179_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625394701723693708737520179_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.739526  6564 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.739655  6564 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:28.739694  6564 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625394701723693708737520179_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625394701723693708737520179_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:48:28.739805  6562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625394701723693708737520179_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.739842  6562 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.740049  6562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625394701723693708737520179_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x625394701723693708737520179_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:48:28.740078  6562 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x625394701723693708737520179_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.740097  6562 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.740113  6562 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x625394701723693708737520179_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:48:28.740181  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x625395e0) got event_name: TaskCompletion
1884: I0815 03:48:28.740196  6509 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.740336  6561 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:28.741439  6560 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:28.753338  6559 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:28.774169  6559 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 213944256656944574 to 17915269595889428849 , after update, data is {current : 0, peak : 3328}.
1884: I0815 03:48:28.774201  6559 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 213944256656944574 to 17915269595889428849 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:48:28.774206  6559 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 213944256656944574 to 17915269595889428849 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:48:28.776907  6562 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11749734598755656615 to 17915269595889428849 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:48:28.776932  6562 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11749734598755656615 to 17915269595889428849 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:48:28.777362  6564 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 17915269595889428849 to 9145187466738323728 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:48:28.777381  6564 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 17915269595889428849 to 9145187466738323728 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:48:28.777386  6564 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 17915269595889428849 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:28.785279  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.785321  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.785380  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.785387  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.787207  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.787590  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.787601  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.787606  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.789163  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.789281  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.789289  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.789295  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3b5dbe0 type is 7
1884: I0815 03:48:28.789316  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.789320  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fd164f0 type is 7
1884: I0815 03:48:28.789324  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.789330  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.789387  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.789392  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.789397  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.789400  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.789451  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.789465  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.789523  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.789530  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.789544  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.789817  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.789829  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.789844  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.789848  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6255b440Variable Type 7
1884: I0815 03:48:28.789865  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.789881  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.789901  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.789913  6509 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.790618  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.790654  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.790863  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.795395  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: I0815 03:48:28.795646  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:48:28.799204  6509 pir_interpreter.cc:161] PirInterpreter(): 0x6255bbd0 on Place(gpu:0)
1884: I0815 03:48:28.799240  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.799260  6509 scope.cc:202] Create variable 0x6255bbd01723693708799229848_inner_var_1
1884: I0815 03:48:28.799268  6509 scope.cc:202] Create variable 0x6255bbd01723693708799229848_inner_var_2
1884: I0815 03:48:28.799278  6509 scope.cc:202] Create variable 0x6255bbd01723693708799229848_inner_var_3
1884: I0815 03:48:28.799286  6509 scope.cc:202] Create variable 0x6255bbd01723693708799229848_inner_var_4
1884: I0815 03:48:28.799295  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:28.799710  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:28.799722  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.799726  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fcfa1f0
1884: 1 -> 0x6255bbd01723693708799229848_inner_var_1 -> 0x5fcfa270
1884: 2 -> 0x6255bbd01723693708799229848_inner_var_2 -> 0x5fd46210
1884: 3 -> 0x6255bbd01723693708799229848_inner_var_3 -> 0x5fd36a50
1884: 4 -> 0x6255bbd01723693708799229848_inner_var_4 -> 0x43ad1bd0
1884: 5 -> fetch0@fetch -> 0x625391c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:28.801357  6569 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:28.801394  6569 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6255bbd01723693708799229848_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.801461  6569 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6255bbd01723693708799229848_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:28.802330  6567 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:28.803328  6566 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:28.803347  6565 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:28.803365  6570 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:28.803421  6570 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.803473  6570 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:48:28.803521  6570 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6255bbd01723693708799229848_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6255bbd01723693708799229848_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.803793  6570 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6255bbd01723693708799229848_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6255bbd01723693708799229848_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:48:28.804330  6568 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:28.804378  6568 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6255bbd01723693708799229848_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.804414  6568 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.805789  6568 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6255bbd01723693708799229848_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6255bbd01723693708799229848_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:48:28.805836  6568 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6255bbd01723693708799229848_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.805860  6568 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.806537  6568 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6255bbd01723693708799229848_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:48:28.806591  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x6255bd40) got event_name: TaskCompletion
1884: I0815 03:48:28.806608  6509 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.810966  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.811002  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.811064  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.811070  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.813179  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.813611  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.813623  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.813628  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.815629  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.815778  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.815788  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.815795  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61fc09d0 type is 7
1884: I0815 03:48:28.815804  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.815807  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fd09fc0 type is 7
1884: I0815 03:48:28.815812  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.815819  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.815886  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.815891  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.815896  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.815901  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.815958  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.815979  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.816047  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.816057  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.816074  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.816349  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.816363  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.816382  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.816387  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62644a30Variable Type 7
1884: I0815 03:48:28.816407  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.816426  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.816450  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.816465  6509 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.817219  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.817250  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.817468  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.872411  6565 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 17915269595889428849 to 213944256656944574 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:48:28.872452  6565 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 17915269595889428849 to 5593954173007686561 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 03:48:28.872458  6565 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 17915269595889428849 to 5593954173007686561 , after update, data is {current : 799996, peak : 1600000}.
1884: I0815 03:48:28.873401  6568 thread_data_registry.h:135] Add data {current : 799996, peak : 1600000} from thread 5593954173007686561 to 2231328903312545631 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 03:48:28.873421  6568 thread_data_registry.h:135] Add data {current : 799996, peak : 1600000} from thread 5593954173007686561 to 2231328903312545631 , after update, data is {current : 800000, peak : 1600000}.
1884: I0815 03:48:28.874373  6569 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 2231328903312545631 to 9145187466738323728 , after update, data is {current : 4000800, peak : 4800000}.
1884: I0815 03:48:28.874389  6569 thread_data_registry.h:135] Add data {current : 800000, peak : 1600000} from thread 2231328903312545631 to 9145187466738323728 , after update, data is {current : 4000800, peak : 4800800}.
1884: I0815 03:48:28.883387  6570 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 03:48:28.883426  6570 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:28.891956  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.891992  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.892050  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.892055  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.895131  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.895553  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.895565  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.895568  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.897164  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.897296  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.901340  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.901351  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3b4e250 type is 7
1884: I0815 03:48:28.901362  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.901365  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fd33f20 type is 7
1884: I0815 03:48:28.901371  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.901376  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.901458  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.901463  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.901468  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.901472  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.901528  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.901542  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.901609  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.901615  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.901633  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.901799  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.901808  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.901823  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.901829  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61ee1120Variable Type 7
1884: I0815 03:48:28.901845  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.901861  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.901880  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.901892  6509 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.903610  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.903654  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.903883  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.911866  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: I0815 03:48:28.912140  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:48:28.915484  6509 pir_interpreter.cc:161] PirInterpreter(): 0x5fd35320 on Place(gpu:0)
1884: I0815 03:48:28.915524  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.915544  6509 scope.cc:202] Create variable 0x5fd353201723693708915513264_inner_var_1
1884: I0815 03:48:28.915553  6509 scope.cc:202] Create variable 0x5fd353201723693708915513264_inner_var_2
1884: I0815 03:48:28.915562  6509 scope.cc:202] Create variable 0x5fd353201723693708915513264_inner_var_3
1884: I0815 03:48:28.915570  6509 scope.cc:202] Create variable 0x5fd353201723693708915513264_inner_var_4
1884: I0815 03:48:28.915580  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:28.915972  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:28.915985  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.915988  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fd15170
1884: 1 -> 0x5fd353201723693708915513264_inner_var_1 -> 0x239d8a0
1884: 2 -> 0x5fd353201723693708915513264_inner_var_2 -> 0x4c463340
1884: 3 -> 0x5fd353201723693708915513264_inner_var_3 -> 0x3b660d0
1884: 4 -> 0x5fd353201723693708915513264_inner_var_4 -> 0x5fd41a60
1884: 5 -> fetch0@fetch -> 0x3b53840
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:28.921335  6571 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:28.921343  6575 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:28.921386  6575 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fd353201723693708915513264_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.921460  6575 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fd353201723693708915513264_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:28.922338  6576 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:28.922384  6576 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.922425  6576 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:48:28.922456  6576 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fd353201723693708915513264_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fd353201723693708915513264_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.922596  6576 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fd353201723693708915513264_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fd353201723693708915513264_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.924100  6575 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fd353201723693708915513264_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.924145  6575 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.921337  6573 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:28.925329  6574 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:28.926940  6575 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fd353201723693708915513264_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5fd353201723693708915513264_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.927006  6575 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fd353201723693708915513264_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:28.927028  6575 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.921339  6572 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:28.929085  6575 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fd353201723693708915513264_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:48:28.929319  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x5fd35490) got event_name: TaskCompletion
1884: I0815 03:48:28.929350  6509 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:48:28.939786  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.939826  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.939890  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:28.939896  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.942018  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:28.942454  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.942474  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.942481  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.944490  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:28.944634  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:28.944644  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:28.944651  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62646070 type is 7
1884: I0815 03:48:28.944660  6509 scope.cc:202] Create variable X
1884: I0815 03:48:28.944664  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x620c8fd0 type is 7
1884: I0815 03:48:28.944669  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:28.944674  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:28.944742  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:28.944748  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:28.944753  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:28.944757  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:28.944815  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.944831  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.944898  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.944906  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.944924  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:28.945075  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.945086  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:28.945103  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:28.945108  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61ed87f0Variable Type 7
1884: I0815 03:48:28.945127  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:28.945145  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:28.945168  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:28.945183  6509 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:28.946857  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:28.946905  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:28.947134  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.012373  6571 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 213944256656944574 to 17915269595889428849 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:48:29.012409  6571 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 213944256656944574 to 3067046922234382951 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:48:29.012415  6571 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 213944256656944574 to 3067046922234382951 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:48:29.024349  6575 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 3067046922234382951 to 9145187466738323728 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 03:48:29.024390  6575 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 3067046922234382951 to 9145187466738323728 , after update, data is {current : 6400800, peak : 8800800}.
1884: I0815 03:48:29.029378  6576 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 17915269595889428849 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:29.040020  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:29.040053  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.040115  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:29.040120  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.041985  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:29.042387  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.042400  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.042405  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.043998  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:29.044116  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:29.044126  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:29.044131  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61ed3bf0 type is 7
1884: I0815 03:48:29.044138  6509 scope.cc:202] Create variable X
1884: I0815 03:48:29.044142  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61ed3440 type is 7
1884: I0815 03:48:29.044147  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:29.044150  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:29.044209  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:29.044214  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.044217  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.044220  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.044271  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.044284  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.044353  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.044360  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.044375  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.044420  6509 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.044564  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.044622  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.044631  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.044646  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:29.044651  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3b47230Variable Type 7
1884: I0815 03:48:29.044667  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:29.044682  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.044703  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.044714  6509 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.044826  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:29.044845  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:29.045048  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.045904  6509 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x195f8f60 for it.
1884: I0815 03:48:29.046120  6509 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19695ea0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:48:29.049269  6509 pir_interpreter.cc:161] PirInterpreter(): 0x6255bbb0 on Place(gpu:0)
1884: I0815 03:48:29.049312  6509 scope.cc:202] Create variable X
1884: I0815 03:48:29.049333  6509 scope.cc:202] Create variable 0x6255bbb01723693709049293631_inner_var_1
1884: I0815 03:48:29.049342  6509 scope.cc:202] Create variable 0x6255bbb01723693709049293631_inner_var_2
1884: I0815 03:48:29.049352  6509 scope.cc:202] Create variable 0x6255bbb01723693709049293631_inner_var_3
1884: I0815 03:48:29.049360  6509 scope.cc:202] Create variable 0x6255bbb01723693709049293631_inner_var_4
1884: I0815 03:48:29.049368  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:29.049729  6509 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:48:29.049741  6509 scope.cc:202] Create variable X
1884: I0815 03:48:29.049744  6509 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x43ad53f0
1884: 1 -> 0x6255bbb01723693709049293631_inner_var_1 -> 0x6201df40
1884: 2 -> 0x6255bbb01723693709049293631_inner_var_2 -> 0x5fd2d300
1884: 3 -> 0x6255bbb01723693709049293631_inner_var_3 -> 0x3b5cc40
1884: 4 -> 0x6255bbb01723693709049293631_inner_var_4 -> 0x601c36d0
1884: 5 -> fetch0@fetch -> 0x61f945e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:48:29.051349  6578 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.051344  6579 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.051391  6578 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6255bbb01723693709049293631_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.051448  6578 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6255bbb01723693709049293631_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.052357  6580 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.051348  6577 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.055330  6582 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:29.055369  6582 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.055411  6582 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:48:29.055454  6582 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6255bbb01723693709049293631_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6255bbb01723693709049293631_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.055503  6582 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.055646  6582 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.055675  6582 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6255bbb01723693709049293631_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6255bbb01723693709049293631_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:48:29.056367  6580 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6255bbb01723693709049293631_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.056416  6580 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.056497  6580 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6255bbb01723693709049293631_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6255bbb01723693709049293631_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:48:29.056524  6580 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6255bbb01723693709049293631_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.056541  6580 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.056552  6580 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6255bbb01723693709049293631_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:48:29.057318  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x6255bd20) got event_name: TaskCompletion
1884: I0815 03:48:29.057348  6509 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.059123  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:29.059144  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.059202  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:29.059208  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.051344  6581 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.061282  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:29.061724  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.061735  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.061741  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.063647  6509 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:48:29.063774  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:29.063784  6509 scope.cc:202] Create variable Out
1884: I0815 03:48:29.063791  6509 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x629b2020 type is 7
1884: I0815 03:48:29.063799  6509 scope.cc:202] Create variable X
1884: I0815 03:48:29.063803  6509 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x628925b0 type is 7
1884: I0815 03:48:29.063808  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:29.063813  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:29.063882  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:29.063887  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.063892  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.063896  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.063954  6509 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.063970  6509 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.064040  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.064049  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.064066  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.064112  6509 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.064239  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.064291  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.064321  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.064342  6509 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:29.064348  6509 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x620b91a0Variable Type 7
1884: I0815 03:48:29.064366  6509 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:29.064385  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.064409  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.064424  6509 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.064484  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:29.064517  6509 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:29.064749  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.102378  6577 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 17915269595889428849 to 213944256656944574 , after update, data is {current : 0, peak : 10240}.
1884: I0815 03:48:29.102416  6577 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17915269595889428849 to 5593954173007686561 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:48:29.102422  6577 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 17915269595889428849 to 5593954173007686561 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:48:29.104359  6578 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3067046922234382951 to 5593954173007686561 , after update, data is {current : 800, peak : 1600}.
1884: I0815 03:48:29.104382  6578 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3067046922234382951 to 5593954173007686561 , after update, data is {current : 800, peak : 1600}.
1884: I0815 03:48:29.107359  6580 thread_data_registry.h:135] Add data {current : 800, peak : 1600} from thread 5593954173007686561 to 213944256656944574 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:48:29.107381  6580 thread_data_registry.h:135] Add data {current : 800, peak : 1600} from thread 5593954173007686561 to 213944256656944574 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:48:29.111372  6582 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 03:48:29.111397  6582 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 6401600, peak : 8800800}.
1884: I0815 03:48:29.111402  6582 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 213944256656944574 to 9145187466738323728 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:48:29.120111  6509 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 03:48:29.120174  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:48:29.121384  6509 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:48:29.122246  6509 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 03:48:29.122273  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:48:29.123692  6509 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 03:48:29.123713  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:48:29.124408  6509 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 03:48:29.125393  6509 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 03:48:29.125422  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:48:29.126829  6509 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 03:48:29.126848  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:48:29.127449  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.127473  6509 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:48:29.127477  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.127483  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.129554  6509 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 03:48:29.129582  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:48:29.130548  6509 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 03:48:29.130573  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:48:29.131538  6509 pybind.cc:1827] need skip: 0
1884: I0815 03:48:29.131839  6509 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:48:29.133715  6509 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:48:29.137566  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.137588  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.137593  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.139627  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:29.139650  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:29.139658  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:29.139662  6509 scope.cc:202] Create variable learning_rate_0
1884: I0815 03:48:29.139669  6509 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61b17730 type is 7
1884: I0815 03:48:29.139673  6509 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:48:29.139676  6509 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61b179c0 type is 7
1884: I0815 03:48:29.139680  6509 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:48:29.139684  6509 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61b17a40 type is 7
1884: I0815 03:48:29.139753  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:29.139757  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.139761  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.139765  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.139825  6509 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.139839  6509 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.139860  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:48:29.140005  6509 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.140012  6509 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.140076  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.140113  6509 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.140120  6509 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.140146  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.141297  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.142802  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:29.143258  6509 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:48:29.143503  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.143816  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.144037  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.144049  6509 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:48:29.144119  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.144124  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.144127  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.144232  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.144240  6509 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:48:29.145819  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.147233  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.148391  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.148608  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:29.148620  6509 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:48:29.148628  6509 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x61fa6d00 type is 7
1884: I0815 03:48:29.148636  6509 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:48:29.148639  6509 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x61fa6a80 type is 7
1884: I0815 03:48:29.148643  6509 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 03:48:29.148646  6509 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x61fa6b70 type is 7
1884: I0815 03:48:29.148650  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:29.148655  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:29.148660  6509 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:48:29.148663  6509 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x61fa8140 type is 7
1884: I0815 03:48:29.148667  6509 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61b17730 type is 7
1884: I0815 03:48:29.148671  6509 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61b179c0 type is 7
1884: I0815 03:48:29.148675  6509 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 03:48:29.148679  6509 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x61fa8120 type is 7
1884: I0815 03:48:29.148681  6509 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:48:29.148684  6509 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x61fa85f0 type is 7
1884: I0815 03:48:29.148687  6509 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61b17a40 type is 7
1884: I0815 03:48:29.148691  6509 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 03:48:29.148694  6509 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x61fa8860 type is 7
1884: I0815 03:48:29.148698  6509 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 03:48:29.148701  6509 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x61fa8aa0 type is 7
1884: I0815 03:48:29.148705  6509 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:48:29.148708  6509 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61fa8d00 type is 7
1884: I0815 03:48:29.148809  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.148824  6509 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:48:29.148892  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:29.148897  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.148901  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.148905  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.148968  6509 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.148983  6509 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.149000  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:48:29.149147  6509 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.149155  6509 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.149175  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:48:29.149250  6509 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:48:29.149343  6509 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 03:48:29.150524  6509 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150542  6509 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150609  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.150674  6509 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150681  6509 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150694  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:48:29.150717  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.150753  6509 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150759  6509 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150770  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:48:29.150859  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150867  6509 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.150880  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.150990  6509 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.151072  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.151126  6509 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151134  6509 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151147  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:48:29.151178  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.151226  6509 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151237  6509 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151252  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:48:29.151373  6509 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151383  6509 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151405  6509 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.151453  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.151460  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.151476  6509 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:29.151481  6509 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x628b0200Variable Type 7
1884: I0815 03:48:29.151499  6509 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:29.151515  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.151532  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151544  6509 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.151583  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:29.151610  6509 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:48:29.151635  6509 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.151641  6509 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.151654  6509 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:48:29.151657  6509 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x624369a0Variable Type 7
1884: I0815 03:48:29.151671  6509 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:48:29.151680  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.151693  6509 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.151703  6509 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.151734  6509 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:48:29.151746  6509 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 03:48:29.152181  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:48:29.152213  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:48:29.152230  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:48:29.152263  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:48:29.152295  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.152320  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:48:29.157032  6509 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:48:29.157078  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:48:29.157898  6509 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:48:29.157918  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:48:29.158344  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.160122  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.160989  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.161114  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.161638  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.162566  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.164863  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.165931  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.167809  6509 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 03:48:29.168711  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.168725  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.168730  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.169970  6509 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:48:29.169986  6509 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3b236f0 type is 9
1884: I0815 03:48:29.169996  6509 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3b22e10 type is 10
1884: I0815 03:48:29.170001  6509 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61b179c0 type is 7
1884: I0815 03:48:29.170006  6509 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61b17a40 type is 7
1884: I0815 03:48:29.170010  6509 scope.cc:202] Create variable saved_params
1884: I0815 03:48:29.170013  6509 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x624da7d0 type is 17
1884: I0815 03:48:29.170043  6509 interpreter_util.cc:594] Static build: 0
1884: I0815 03:48:29.170046  6509 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:48:29.170049  6509 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:48:29.170053  6509 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:48:29.170105  6509 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.170120  6509 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:48:29.170939  6509 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:48:29.170976  6509 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:48:29.171036  6509 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 03:48:29.172298  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.172370  6509 scope.cc:202] Create variable feed
1884: I0815 03:48:29.172376  6509 naive_executor.cc:189] 0x62533e00 Create persistable variable feed, which pointer is 0x61dd7200
1884: I0815 03:48:29.172382  6509 scope.cc:202] Create variable fetch
1884: I0815 03:48:29.172384  6509 naive_executor.cc:189] 0x62533e00 Create persistable variable fetch, which pointer is 0x61dd70a0
1884: I0815 03:48:29.172389  6509 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:48:29.172391  6509 naive_executor.cc:189] 0x62533e00 Create persistable variable linear_0.b_0, which pointer is 0x619f2470
1884: I0815 03:48:29.172397  6509 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:48:29.172399  6509 naive_executor.cc:189] 0x62533e00 Create persistable variable linear_0.w_0, which pointer is 0x61ad7340
1884: I0815 03:48:29.172415  6509 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 03:48:29.172749  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.172834  6509 program_converter.cc:296] is_legacy_program : 0
1884: I0815 03:48:29.172880  6509 executor.cc:183] Old Executor is Running.
1884: I0815 03:48:29.172961  6509 executor.cc:92] Creating Variables for block 0
1884: I0815 03:48:29.172966  6509 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 03:48:29.172969  6509 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x619f2470 type is 7
1884: I0815 03:48:29.172973  6509 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 03:48:29.172976  6509 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x61ad7340 type is 7
1884: I0815 03:48:29.173013  6509 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.173095  6509 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 03:48:29.173136  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.173141  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:29.173283  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.173408  6509 graph.cc:149] create OpNode by feed
1884: I0815 03:48:29.173444  6509 graph.cc:149] create OpNode by matmul_v2
1884: I0815 03:48:29.173457  6509 graph.cc:149] create OpNode by elementwise_add
1884: I0815 03:48:29.173470  6509 graph.cc:149] create OpNode by abs
1884: I0815 03:48:29.173478  6509 graph.cc:149] create OpNode by assign_value
1884: I0815 03:48:29.173493  6509 graph.cc:149] create OpNode by multinomial
1884: I0815 03:48:29.173502  6509 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:48:29.173516  6509 graph.cc:149] create OpNode by scale
1884: I0815 03:48:29.173527  6509 graph.cc:149] create OpNode by scale
1884: I0815 03:48:29.173537  6509 graph.cc:149] create OpNode by fetch
1884: I0815 03:48:29.173552  6509 graph.cc:149] create OpNode by fetch
1884: I0815 03:48:29.173570  6509 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 03:48:29.174789  6509 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 03:48:29.174795  6509 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 03:48:29.174865  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.174870  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 03:48:29.174978  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175225  6509 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:48:29.175282  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175285  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 03:48:29.175325  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175330  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 03:48:29.175369  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175432  6509 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:48:29.175462  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175465  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 03:48:29.175482  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175494  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.175514  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175523  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 03:48:29.175560  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175580  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.175601  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175604  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 03:48:29.175645  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175719  6509 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:48:29.175745  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175747  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 03:48:29.175778  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175796  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.175814  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.175817  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 03:48:29.175844  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.175992  6509 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:48:29.176016  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.176019  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 03:48:29.176048  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.176062  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.176081  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.176084  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 03:48:29.176105  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.176116  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.176136  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.176138  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 03:48:29.176159  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.176172  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.176190  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.176193  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 03:48:29.176216  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.176280  6509 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:48:29.176316  6509 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:48:29.176328  6509 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:48:29.176340  6509 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 03:48:29.176362  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.176365  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 03:48:29.176386  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.176424  6509 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:48:29.176442  6509 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:48:29.176452  6509 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:48:29.176462  6509 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:48:29.176491  6509 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:48:29.176501  6509 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 03:48:29.177680  6509 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:48:29.177723  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.177727  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 03:48:29.177753  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.177772  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.177796  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.177799  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 03:48:29.177821  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.177870  6509 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:48:29.177897  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.177901  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 03:48:29.177918  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.177932  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.177951  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.177954  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 03:48:29.177987  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.178071  6509 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 03:48:29.178098  6509 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:48:29.178112  6509 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:48:29.178124  6509 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:48:29.178138  6509 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:48:29.178149  6509 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:48:29.178164  6509 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 03:48:29.178184  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.178254  6509 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 03:48:29.178275  6509 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:48:29.178287  6509 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:48:29.178297  6509 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:48:29.178349  6509 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:48:29.178362  6509 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:48:29.178375  6509 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 03:48:29.178422  6509 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:48:29.178697  6509 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:48:29.178725  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.178727  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 03:48:29.178774  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.178833  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.178864  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.178908  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.178933  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.178972  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.178994  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179028  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179047  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179078  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179093  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179121  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179136  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179159  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179172  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179191  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179200  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179217  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179240  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179242  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 03:48:29.179266  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179311  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179335  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179339  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 03:48:29.179349  6509 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:48:29.179352  6509 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:48:29.179401  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179420  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179445  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179447  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:48:29.179456  6509 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:48:29.179458  6509 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:48:29.179499  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179519  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179541  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179544  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 03:48:29.179553  6509 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:48:29.179554  6509 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:48:29.179586  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179602  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179622  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179625  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:48:29.179633  6509 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:48:29.179636  6509 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:48:29.179673  6509 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:48:29.179692  6509 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:48:29.179713  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179726  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 03:48:29.179738  6509 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 03:48:29.179780  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.179783  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 03:48:29.179860  6509 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:48:29.179886  6509 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.179908  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:48:29.179978  6509 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 03:48:29.179996  6509 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:48:29.180024  6509 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:48:29.180047  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.180049  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 03:48:29.180995  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.181007  6509 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 03:48:29.181058  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.181062  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:29.181675  6509 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 03:48:29.181885  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.181953  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.181957  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:29.182374  6509 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:48:29.182612  6509 graph.h:183] deleting __fuse_statis__
1884: I0815 03:48:29.182618  6509 graph.h:183] deleting pass_recorder
1884: I0815 03:48:29.182623  6509 graph.h:183] deleting stale_program_op_descs
1884: I0815 03:48:29.182713  6509 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 03:48:29.182722  6509 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:48:29.182726  6509 naive_executor.cc:195] 0x62533e00 Create variable abs_0.tmp_0, which pointer is 0x61ee9370
1884: I0815 03:48:29.182734  6509 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:48:29.182736  6509 naive_executor.cc:195] 0x62533e00 Create variable gaussian_0.tmp_0, which pointer is 0x61edc480
1884: I0815 03:48:29.182749  6509 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:48:29.182751  6509 naive_executor.cc:195] 0x62533e00 Create variable linear_0.tmp_1, which pointer is 0x623dd100
1884: I0815 03:48:29.182755  6509 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:48:29.182758  6509 naive_executor.cc:195] 0x62533e00 Create variable multinomial_0.tmp_0, which pointer is 0x623dcba0
1884: I0815 03:48:29.182761  6509 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 03:48:29.182765  6509 naive_executor.cc:195] 0x62533e00 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x623dcea0
1884: I0815 03:48:29.182767  6509 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 03:48:29.182770  6509 naive_executor.cc:195] 0x62533e00 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x623db4a0
1884: I0815 03:48:29.182777  6509 scope.cc:202] Create variable feed
1884: I0815 03:48:29.182780  6509 scope.cc:202] Create variable fetch
1884: I0815 03:48:29.182801  6509 naive_executor.cc:46] NaiveExecutor init with scope 0x62533e00
1884: I0815 03:48:29.182806  6509 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 03:48:29.183015  6509 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:48:29.183028  6509 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:48:29.183055  6509 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 03:48:29.183058  6509 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 03:48:29.183066  6509 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.183102  6509 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:48:29.183383  6509 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.183408  6509 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:48:29.183468  6509 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.183496  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 03:48:29.414235  6509 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:48:29.414394  6509 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.414426  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:48:29.414515  6509 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:48:29.414548  6509 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.414574  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:48:29.414644  6509 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:48:29.414687  6509 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.414702  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:48:29.414762  6509 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:48:29.414789  6509 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:48:29.414804  6509 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:48:29.414841  6509 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:48:29.414858  6509 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.414894  6509 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:48:29.414922  6509 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:48:29.415449  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.415464  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 03:48:29.625809  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61fa9020 on Place(gpu:0)
1884: I0815 03:48:29.625861  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_0
1884: I0815 03:48:29.625880  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_1
1884: I0815 03:48:29.625887  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_2
1884: I0815 03:48:29.625896  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_3
1884: I0815 03:48:29.625936  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_4
1884: I0815 03:48:29.625948  6509 scope.cc:202] Create variable 0x61fa90201723693709625845275_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x61fa90201723693709625845275_inner_var_0 -> 0x6289a100
1884: 1 -> 0x61fa90201723693709625845275_inner_var_1 -> 0x619e7320
1884: 2 -> 0x61fa90201723693709625845275_inner_var_2 -> 0x61fb9cf0
1884: 3 -> linear_1.w_0 -> 0x620bc9a0
1884: 4 -> linear_1.b_0 -> 0x5fd01020
1884: 5 -> learning_rate_1 -> 0x61fae650
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:48:29.627359  6584 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.627430  6584 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.627516  6584 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:48:29.627540  6584 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.627578  6584 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.627589  6584 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.627602  6584 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61fa90201723693709625845275_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.627359  6587 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:29.629549  6587 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.629623  6587 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.629668  6587 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 03:48:29.629693  6587 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.629709  6587 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.629719  6587 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:48:29.629731  6587 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61fa90201723693709625845275_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61fa90201723693709625845275_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61fa90201723693709625845275_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.629793  6587 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61fa90201723693709625845275_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61fa90201723693709625845275_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61fa90201723693709625845275_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 03:48:29.630335  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61fa9190) got event_name: TaskCompletion
1884: I0815 03:48:29.627359  6583 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.627359  6586 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.631332  6585 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 03:48:29.637049  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61b18a90 on Place(gpu:0)
1884: I0815 03:48:29.637105  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_1
1884: I0815 03:48:29.637121  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_4
1884: I0815 03:48:29.637130  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_5
1884: I0815 03:48:29.637135  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_6
1884: I0815 03:48:29.637157  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_7
1884: I0815 03:48:29.637166  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_8
1884: I0815 03:48:29.637173  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_9
1884: I0815 03:48:29.637200  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_10
1884: I0815 03:48:29.637207  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_11
1884: I0815 03:48:29.637215  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_12
1884: I0815 03:48:29.637224  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_13
1884: I0815 03:48:29.637231  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_14
1884: I0815 03:48:29.637239  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_15
1884: I0815 03:48:29.637246  6509 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:48:29.637255  6509 scope.cc:202] Create variable 0x61b18a901723693709637080655_inner_var_17
1884: I0815 03:48:29.637262  6509 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x61fae650
1884: 1 -> 0x61b18a901723693709637080655_inner_var_1 -> 0x61fadd40
1884: 2 -> linear_1.b_0 -> 0x5fd01020
1884: 3 -> linear_1.w_0 -> 0x620bc9a0
1884: 4 -> 0x61b18a901723693709637080655_inner_var_4 -> 0x5ff22c30
1884: 5 -> 0x61b18a901723693709637080655_inner_var_5 -> 0x6289a540
1884: 6 -> 0x61b18a901723693709637080655_inner_var_6 -> 0x61a20ff0
1884: 7 -> 0x61b18a901723693709637080655_inner_var_7 -> 0x61b18fb0
1884: 8 -> 0x61b18a901723693709637080655_inner_var_8 -> 0x61dc8320
1884: 9 -> 0x61b18a901723693709637080655_inner_var_9 -> 0x5fd043d0
1884: 10 -> 0x61b18a901723693709637080655_inner_var_10 -> 0x61a0c630
1884: 11 -> 0x61b18a901723693709637080655_inner_var_11 -> 0x5fd04330
1884: 12 -> 0x61b18a901723693709637080655_inner_var_12 -> 0x61fc1ef0
1884: 13 -> 0x61b18a901723693709637080655_inner_var_13 -> 0x62538300
1884: 14 -> 0x61b18a901723693709637080655_inner_var_14 -> 0x61fb9a70
1884: 15 -> 0x61b18a901723693709637080655_inner_var_15 -> 0x61f8c200
1884: 16 -> fetch0@fetch -> 0x619d7330
1884: 17 -> 0x61b18a901723693709637080655_inner_var_17 -> 0x628b15a0
1884: 18 -> fetch1@fetch -> 0x5fd266c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 03:48:29.659209  6592 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:48:29.659281  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659351  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:48:29.659385  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659402  6590 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.659454  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.659461  6590 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659514  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:48:29.659529  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:48:29.659531  6590 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.659552  6590 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659572  6590 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61b18a901723693709637080655_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:48:29.659591  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:48:29.659607  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61b18a901723693709637080655_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659649  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61b18a901723693709637080655_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:48:29.659678  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61b18a901723693709637080655_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659714  6592 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:48:29.659747  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61b18a901723693709637080655_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.659773  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61b18a901723693709637080655_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61b18a901723693709637080655_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659812  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.659824  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61b18a901723693709637080655_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61b18a901723693709637080655_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.659858  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61b18a901723693709637080655_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659879  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.659889  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61b18a901723693709637080655_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.659905  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61b18a901723693709637080655_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.659933  6592 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.660015  6592 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.660053  6592 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.660110  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.660126  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61b18a901723693709637080655_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61b18a901723693709637080655_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.660154  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.660179  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.660190  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.660205  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61b18a901723693709637080655_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.660269  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61b18a901723693709637080655_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:48:29.660288  6592 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61b18a901723693709637080655_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61b18a901723693709637080655_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.660323  6592 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:48:29.660334  6592 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61b18a901723693709637080655_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61b18a901723693709637080655_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:48:29.661428  6591 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.661496  6591 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.661535  6591 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.661618  6591 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b18a901723693709637080655_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.661649  6591 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61b18a901723693709637080655_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.661669  6591 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.661682  6591 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61b18a901723693709637080655_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.661702  6591 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b18a901723693709637080655_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.661717  6591 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:48:29.661747  6591 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b18a901723693709637080655_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b18a901723693709637080655_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.661763  6591 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61b18a901723693709637080655_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.661777  6591 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.661789  6591 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61b18a901723693709637080655_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.666335  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61b18c00) got event_name: TaskCompletion
1884: I0815 03:48:29.666405  6509 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.666460  6509 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:48:29.673002  6588 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.675340  6589 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.691541  6509 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:48:29.691602  6509 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:48:29.702522  6509 scope.cc:202] Create variable linear_1.b_0
1884: I0815 03:48:29.702654  6509 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 03:48:29.703262  6509 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097033529270"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097033529270"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 03:48:29.703584  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61ed8d20 on Place(cpu)
1884: I0815 03:48:29.703617  6509 scope.cc:202] Create variable 0x61ed8d201723693709703603756_inner_var_0
1884: I0815 03:48:29.703650  6509 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097033529270"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236937097033529270 -> 0x6287b0b0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 03:48:29.703840  6509 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 03:48:29.727383  6593 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.728335  6595 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.728401  6595 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236937097033529270:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.728488  6595 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236937097033529270:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:48:29.729334  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61ed8e90) got event_name: TaskCompletion
1884: I0815 03:48:29.729355  6596 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.730336  6594 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.728334  6597 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.734371  6595 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 8174807831224338348 to 8252089699174413958 , after update, data is {current : 208, peak : 312}.
1884: I0815 03:48:29.734393  6595 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 8174807831224338348 to 8252089699174413958 , after update, data is {current : 208, peak : 312}.
1884: I0815 03:48:29.748446  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.748483  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097486630591"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097486630591"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:48:29.748960  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61ed8d20 on Place(cpu)
1884: I0815 03:48:29.748993  6509 scope.cc:202] Create variable 0x61ed8d201723693709748979695_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097486630591"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236937097486630591 -> 0x61a0bbc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:48:29.750348  6599 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.750414  6599 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236937097486630591:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.750526  6599 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236937097486630591:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.750348  6602 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.754333  6598 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.750348  6601 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.754333  6600 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.754329  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61ed8e90) got event_name: TaskCompletion
1884: I0815 03:48:29.768378  6599 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 18040376018129179811 to 8252089699174413958 , after update, data is {current : 216, peak : 312}.
1884: I0815 03:48:29.768411  6599 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 18040376018129179811 to 8252089699174413958 , after update, data is {current : 216, peak : 312}.
1884: I0815 03:48:29.778416  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.778452  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236937097486630591",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236937097786600662"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236937097486630591",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236937097786600662"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:48:29.778959  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61ed8d20 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236937097486630591",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236937097786600662"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236937097786600662 -> 0x61a0bbc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 03:48:29.780352  6605 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.780411  6605 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.780478  6605 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.780349  6603 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.780349  6604 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.784335  6606 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.784336  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61ed8e90) got event_name: TaskCompletion
1884: I0815 03:48:29.788355  6607 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.796434  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.796468  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097966861683"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097966861683"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 03:48:29.796965  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61ed8d20 on Place(cpu)
1884: I0815 03:48:29.797000  6509 scope.cc:202] Create variable 0x61ed8d201723693709796986143_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236937097966861683"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236937097966861683 -> 0x61ed6800
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:48:29.798339  6610 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:48:29.798396  6610 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236937097966861683:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.798485  6610 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236937097966861683:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:48:29.798533  6509 pir_interpreter.cc:1766] main_thread_blocker_(0x61ed8e90) got event_name: TaskCompletion
1884: I0815 03:48:29.799350  6609 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:48:29.798342  6608 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.798341  6612 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:48:29.798342  6611 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:48:29.811578  6610 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8174807831224338348 to 8252089699174413958 , after update, data is {current : 220, peak : 312}.
1884: I0815 03:48:29.811609  6610 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8174807831224338348 to 8252089699174413958 , after update, data is {current : 220, peak : 312}.
1884: I0815 03:48:29.815433  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.815464  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:29.815654  6509 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 03:48:29.815757  6509 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 03:48:29.815798  6509 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097966861683"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097786600662"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097966861683"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097786600662"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 03:48:29.816707  6509 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 03:48:29.816722  6509 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 03:48:29.816776  6509 pir_interpreter.cc:161] PirInterpreter(): 0x61ed8d20 on Place(cpu)
1884: I0815 03:48:29.816823  6509 scope.cc:202] Create variable feed_name_0
1884: I0815 03:48:29.816839  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_5
1884: I0815 03:48:29.816862  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_6
1884: I0815 03:48:29.816872  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_7
1884: I0815 03:48:29.816879  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_8
1884: I0815 03:48:29.816898  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_9
1884: I0815 03:48:29.816908  6509 scope.cc:202] Create variable 0x61ed8d201723693709816795378_inner_var_10
1884: I0815 03:48:29.816936  6509 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.816962  6509 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:48:29.817180  6509 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.817193  6509 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097966861683"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236937097786600662"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236937097966861683 -> 0x61ed6800
1884: 1 -> constant_folding@_17236937097786600662 -> 0x61a0bbc0
1884: 2 -> linear_1.b_0 -> 0x62911cd0
1884: 3 -> linear_1.w_0 -> 0x61ab4f20
1884: 4 -> feed_name_0 -> 0x62891250
1884: 5 -> 0x61ed8d201723693709816795378_inner_var_5 -> 0x6252d600
1884: 6 -> 0x61ed8d201723693709816795378_inner_var_6 -> 0x619e8990
1884: 7 -> 0x61ed8d201723693709816795378_inner_var_7 -> 0x61b1a680
1884: 8 -> 0x61ed8d201723693709816795378_inner_var_8 -> 0x6299aa90
1884: 9 -> fetch_name_0 -> 0x61a0bdf0
1884: 10 -> fetch_name_1 -> 0x61ab53e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:48:29.817886  6509 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 03:48:29.817996  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818071  6509 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:48:29.818097  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.818130  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x61ed8d201723693709816795378_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x61ed8d201723693709816795378_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818173  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x61ed8d201723693709816795378_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.818203  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818225  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.818239  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818276  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236937097786600662:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.818295  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236937097966861683:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818334  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236937097966861683:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x61ed8d201723693709816795378_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:48:29.818356  6509 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236937097966861683:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:48:29.818393  6509 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236937097966861683:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61ed8d201723693709816795378_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:48:29.818418  6509 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:48:29.818439  6509 helper.h:475] after run : [cpu current allocated memory: 6.10602MB], [cpu current reserved memory: 6.10602MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:48:29.818461  6509 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:48:29.818620  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.818626  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:29.820370  6613 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:48:29.820483  6613 thread_data_registry.h:135] Add data {current : -312, peak : 0} from thread 17049885404204193386 to 8252089699174413958 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:29.820492  6613 thread_data_registry.h:135] Add data {current : -312, peak : 0} from thread 17049885404204193386 to 8252089699174413958 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:29.821367  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:29.821383  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......FFF..
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 60501 / 100000 (60.5%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([3, 2, 2, ..., 3, 3, 0], dtype=int64)
1884:  y: array([0, 0, 0, ..., 0, 0, 0])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 210407 / 300000 (70.1%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[0, 1, 3, ..., 3, 0, 1],
1884:        [2, 1, 0, ..., 0, 2, 3],
1884:        [3, 1, 0, ..., 1, 1, 2]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 100 / 100 (100%)
1884: Max absolute difference: 990
1884: Max relative difference: inf
1884:  x: array([ 29,  20, 556, 799, 857, 807, 822, 428,  44, 979, 577, 669, 769,
1884:        223, 976, 889,   3, 301, 412, 168,  72, 534, 623, 605, 738, 858,
1884:        398, 134, 690, 113, 808, 370, 558, 119, 846, 787, 413, 600, 375,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 5.603s
1884: 
1884: FAILED (failures=3, skipped=3)
1884: 
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39392 0.16297 0.1303  0.31281]
1884: [array([3, 0, 0, ..., 3, 1, 3], dtype=int64)]
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39436 0.16301 0.12998 0.31265]
1884: [array([2, 0, 2, ..., 1, 3, 3], dtype=int64)]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39684 0.16104 0.12731 0.31481]
1884:  [0.25381 0.15133 0.35168 0.24318]
1884:  [0.24583 0.20314 0.17722 0.37381]]
1884: [array([[3, 3, 0, ..., 0, 3, 0],
1884:        [2, 3, 2, ..., 1, 0, 2],
1884:        [2, 0, 0, ..., 3, 1, 3]], dtype=int64)]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39359 0.16334 0.12757 0.3155 ]
1884:  [0.25396 0.15125 0.3496  0.24519]
1884:  [0.24878 0.20206 0.17788 0.37128]]
1884: [array([[3, 0, 2, ..., 3, 3, 3],
1884:        [0, 1, 2, ..., 2, 2, 0],
1884:        [0, 2, 3, ..., 3, 0, 0]], dtype=int64)]
1884: I0815 03:48:29.823585  6509 mmap_allocator.cc:348] PID: 6509, MemoryMapFdSet: set size - 0
1884: I0815 03:48:29.856729  6509 mmap_allocator.cc:348] PID: 6509, MemoryMapFdSet: set size - 0
1884: I0815 03:48:30.072386  6584 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 2231328903312545631 to 8252089699174413958 , after update, data is {current : -68, peak : 312}.
1884: I0815 03:48:30.072424  6584 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 2231328903312545631 to 8252089699174413958 , after update, data is {current : -68, peak : 312}.
1884: I0815 03:48:30.081382  6587 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 7916615516623474849 to 8252089699174413958 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:30.081418  6587 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 7916615516623474849 to 8252089699174413958 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:30.081423  6587 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 7916615516623474849 to 8252089699174413958 , after update, data is {current : 256, peak : 768}.
1884: I0815 03:48:30.083366  6591 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 8252089699174413958 to 960101410995847736 , after update, data is {current : 768, peak : 1536}.
1884: I0815 03:48:30.083384  6591 thread_data_registry.h:135] Add data {current : -92, peak : 312} from thread 8252089699174413958 to 960101410995847736 , after update, data is {current : -112, peak : 312}.
1884: I0815 03:48:30.083388  6591 thread_data_registry.h:135] Add data {current : -92, peak : 312} from thread 8252089699174413958 to 960101410995847736 , after update, data is {current : -112, peak : 312}.
1884: I0815 03:48:30.085371  6590 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 6813862619827832556 to 960101410995847736 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:30.085392  6590 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 6813862619827832556 to 960101410995847736 , after update, data is {current : -92, peak : 312}.
1884: I0815 03:48:30.086542  6592 thread_data_registry.h:135] Add data {current : -92, peak : 312} from thread 960101410995847736 to 9145187466738323728 , after update, data is {current : 6401792, peak : 8800800}.
1884: I0815 03:48:30.086557  6592 thread_data_registry.h:135] Add data {current : -92, peak : 312} from thread 960101410995847736 to 9145187466738323728 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 03:48:30.086562  6592 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 960101410995847736 to 9145187466738323728 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 03:48:30.441459  6509 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:48:30.441495  6509 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:48:30.441545  6509 mmap_allocator.cc:348] PID: 6509, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   13.67 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  13.86 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
