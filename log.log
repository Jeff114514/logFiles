UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0813 09:06:29.280215 25950 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0813 09:06:30.230762 25950 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=inner_op_parallelism,use_fast_math,enable_opt_get_features,sync_nccl_allreduce,enable_exit_when_partial_worker,allow_cinn_ops,gpu_memory_limit_mb,apply_pass_to_program,selected_gpus,gpugraph_offload_gather_copy_maxsize,cupti_dir,enable_api_kernel_fallback,new_executor_use_inplace,fast_eager_deletion_mode,cublaslt_exhaustive_search_times,enable_adjust_op_order,fraction_of_cuda_pinned_memory_to_use,reader_queue_speed_test_mode,enable_cinn_accuracy_check,enable_sparse_inner_gather,cuda_dir,dynamic_static_unified_comm,accuracy_check_rtol_fp32,ir_inplace_kernel_blacklist,sort_sum_gradient,prim_skip_dynamic,auto_free_cudagraph_allocations_on_launch,enable_blaslt_global_search,sync_after_alloc,cudnn_deterministic,nvidia_package_dir,enable_dependency_builder_debug_info,log_memory_stats,all_blocks_convert_trt,gpugraph_dedup_pull_push_mode,use_pinned_memory,logging_pir_py_code_dump_symbolic_dims,mklml_dir,check_nan_inf_level,custom_device_mem_record,new_executor_static_build,dygraph_debug,dist_threadpool_size,enable_record_memory,low_precision_op_list,query_dest_rank_by_multi_node,cudnn_batchnorm_spatial_persistent,cublaslt_device_best_config,use_shm_cache,enable_fusion_fallback,cusparse_dir,cudnn_exhaustive_search_times,pir_broadcast_tree_limit,free_idle_chunk,einsum_opt,enable_pir_api,cse_max_count,deny_cinn_ops,gpugraph_storage_mode,alloc_fill_value,enable_pir_in_executor_trace_run,cuda_malloc_async_pool_memory_throttle_ratio,use_cinn,new_executor_sequential_run,enable_tracker_all2all,use_stride_kernel,check_infer_symbolic,gpugraph_merge_grads_segment_size,static_executor_perfstat_filepath,enable_pir_in_executor,executor_log_deps_every_microseconds,accuracy_check_rtol_fp16,cusparselt_dir,gpugraph_enable_print_op_debug,enable_auto_rdma_trans,cudnn_exhaustive_search,host_trace_level,accuracy_check_atol_fp16,call_stack_level,use_autotune,async_trace_count,check_nan_inf,gpugraph_offload_param_stat,npu_storage_format,enable_neighbor_list_use_uva,tensor_operants_mode,enable_cse_in_dy2st,gpugraph_hbm_table_load_factor,get_host_by_name_time,win_cuda_bin_dir,lapack_dir,gpu_allocator_retry_time,new_executor_serial_run,embedding_deterministic,pir_apply_inplace_pass,pir_debug,cinn_compile_thread_num,enable_gpu_memory_usage_log,gpugraph_parallel_stream_num,manually_trans_conv_filter,gpugraph_parallel_copyer_split_maxsize,pir_apply_shape_optimization_pass,enable_all2all_use_fp16,nccl_blocking_wait,mkl_dir,print_allocator_trace_info,accuracy_check_atol_bf16,enable_async_trace,conv_workspace_size_limit,enable_cinn_auto_tune,enable_fuse_parallel_matmul_pass,enable_unused_var_check,prim_enable_dynamic,enable_cinn_compile_cache,local_exe_sub_scope_limit,nccl_dir,prim_forward_blacklist,run_kp_kernel,gpugraph_enable_gpu_direct_access,add_dependency_for_communication_op,fraction_of_gpu_memory_to_use,cudnn_dir,gpugraph_sparse_table_storage_mode,search_cache_max_number,prim_enabled,use_cuda_managed_memory,new_executor_use_local_scope,eager_delete_scope,pir_subgraph_saving_dir,enable_dump_main_program,logging_pir_py_code_dir,use_auto_growth_v2,use_cuda_malloc_async_allocator,use_xqa_optim,use_mkldnn,pinned_memory_as_cpu_backend,accuracy_check_atol_fp32,jit_engine_type,tracer_onednn_ops_off,multi_node_sample_use_gpu_table,gpugraph_enable_segment_merge_grads,gpugraph_force_device_batch_num_equal,cusolver_dir,set_to_1d,enable_pir_with_pt_in_dy2st,graph_load_in_parallel,op_dir,check_kernel_launch,use_system_allocator,new_executor_use_cuda_graph,graph_neighbor_size_percent,prim_all,fleet_executor_with_standalone,dataloader_use_file_descriptor,memory_fraction_of_eager_deletion,conv2d_disable_cudnn,graph_metapath_split_opt,enable_interpretercore_launch_cinn,initial_gpu_memory_in_mb,graph_get_neighbor_id,initial_cpu_memory_in_mb,gpugraph_debug_gpu_memory,graph_embedding_split_infer_mode,convert_all_blocks,reallocate_gpu_memory_in_mb,accuracy_check_rtol_bf16,curand_dir,logging_trunc_pir_py_code,cache_inference_while_scope,gpugraph_offload_param_extends,gemm_use_half_precision_compute_type,cublas_dir,print_sub_graph_dir,static_runtime_data_save_path,enable_gpu_memory_usage_log_mb,fuse_parameter_groups_size,trt_ibuilder_cache,logging_pir_py_code_int_tensor_element_limit,use_virtual_memory_auto_growth,allreduce_record_one_event,tensorrt_dir,prim_forward,enable_cublas_tensor_op_math,benchmark_nccl,fuse_parameter_memory_size,eager_delete_tensor_gb,gpugraph_slot_feasign_max_num,tracer_profile_fname,cuda_memory_async_pool_realease_threshold,disable_dyshape_in_train,use_auto_growth_pinned_allocator,gpugraph_enable_hbm_table_collision_stat,enable_collect_shape,auto_growth_chunk_size_in_mb,free_when_no_cache_hit,allocator_strategy,paddle_num_threads,prim_backward,benchmark,print_ir,gpugraph_load_node_list_into_hbm,enable_graph_multi_node_sampling,multiple_of_cupti_buffer_size,fraction_of_cpu_memory_to_use,dump_chunk_info,prim_check_ops,use_stream_safe_cuda_allocator,init_allocated_mem,save_static_runtime_data,tracer_onednn_ops_on,cinn_subgraph_graphviz_dir,enable_auto_detect_gpu_topo,max_inplace_grad_add 
1884: I0813 09:06:30.230875 25950 init.cc:108] After Parse: argc is 2
1884: I0813 09:06:38.184526 25950 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 09:06:38.184568 25950 dygraph_functions.cc:77659] { Input: []} 
1884: W0813 09:06:38.185313 25950 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0813 09:06:38.185802 25950 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0813 09:06:38.186636 25950 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0813 09:06:38.186731 25950 allocator_facade.cc:212] selected allocator strategy:1
1884: I0813 09:06:38.186837 25950 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0813 09:06:38.187958 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6aec000000), and remaining 0
1884: I0813 09:06:38.188217 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.188277 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.188385 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6aec000200), and remaining 0
1884: I0813 09:06:38.188421 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6aec000400), and remaining 0
1884: I0813 09:06:38.192479 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6aec000600), and remaining 0
1884: I0813 09:06:38.192621 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f6aec000800), and remaining 0
1884: I0813 09:06:38.192690 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f6aec000a00), and remaining 0
1884: I0813 09:06:38.192781 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.192801 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.192868 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.192880 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.194064 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x196d9e00 for it.
1884: I0813 09:06:38.194207 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.194231 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.194288 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f6aec000e00), and remaining 0
1884: I0813 09:06:38.194401 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f6aec0c4400), and remaining 0
1884: I0813 09:06:38.317699 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x196d9e00 for it.
1884: I0813 09:06:38.317904 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.317950 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.318603 25950 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f6aec200000), and remaining 0
1884: I0813 09:06:38.326535 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x196d9e00 for it.
1884: I0813 09:06:38.326664 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:38.326702 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.326750 25950 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:38.326938 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:38.327896 25950 dygraph_functions.cc:33459] Running AD API: full
1884: I0813 09:06:38.327916 25950 dygraph_functions.cc:33480] { Input: []} 
1884: I0813 09:06:38.327966 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:38.328037 25950 dygraph_functions.cc:64553] Running AD API: scale
1884: I0813 09:06:38.328063 25950 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.328125 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:38.328207 25950 dygraph_functions.cc:26170] Running AD API: exp
1884: I0813 09:06:38.328223 25950 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.328260 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:38.328444 25950 dygraph_functions.cc:72508] Running AD API: sum
1884: I0813 09:06:38.328464 25950 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.328631 25950 dygraph_functions.cc:83176] Running AD API: divide
1884: I0813 09:06:38.328656 25950 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:38.328727 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:38.332077 25950 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0813 09:06:38.332183 25950 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0813 09:06:38.332211 25950 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0813 09:06:38.332274 25950 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0813 09:06:39.799361 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.799419 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.799654 25950 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0813 09:06:39.799675 25950 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.804162 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:39.804204 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.805191 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.805210 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.805222 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.805965 25950 program_interpreter.cc:243] New Executor is Running.
1884: I0813 09:06:39.805981 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:39.805999 25950 scope.cc:202] Create variable feed
1884: I0813 09:06:39.806010 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:39.806023 25950 scope.cc:202] Create variable fetch
1884: I0813 09:06:39.806028 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:39.806038 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:39.806044 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.806047 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.806051 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.808465 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:39.808809 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.808822 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.808826 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.810482 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:39.810535 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:39.810544 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:39.810552 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:39.810561 25950 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 09:06:39.810570 25950 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x60028a30 type is 7
1884: I0813 09:06:39.810575 25950 scope.cc:202] Create variable x
1884: I0813 09:06:39.810577 25950 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x600274f0 type is 7
1884: I0813 09:06:39.810642 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:39.810647 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.810652 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.810655 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.810778 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.810801 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.810925 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.810937 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.810956 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.811136 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.811167 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.811185 25950 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:39.811193 25950 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6002f550Variable Type 7
1884: I0813 09:06:39.811218 25950 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:39.811239 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.811292 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.811319 25950 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.812561 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:39.812615 25950 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:39.813010 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.816674 25950 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 09:06:39.816694 25950 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 09:06:39.816802 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.816831 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.817312 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: I0813 09:06:39.817389 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.817411 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.817831 25950 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: I0813 09:06:39.817891 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.817914 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.817938 25950 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.818198 25950 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 09:06:39.818209 25950 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 09:06:39.818324 25950 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0813 09:06:39.818348 25950 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.818738 25950 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 09:06:39.818749 25950 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 09:06:39.818792 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.818811 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.818989 25950 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 09:06:39.818996 25950 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 09:06:39.819032 25950 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 09:06:39.819049 25950 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 09:06:39.819067 25950 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.821806 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:39.821831 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.821887 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:39.821897 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.823859 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:39.824229 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.824244 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.824249 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.826050 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:39.826109 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:39.826120 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:39.826126 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6005a980 type is 7
1884: I0813 09:06:39.826138 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.826143 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6005acf0 type is 7
1884: I0813 09:06:39.826148 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:39.826154 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:39.826210 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:39.826218 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.826222 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.826227 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.826282 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.826297 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.826382 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.826392 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.826411 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.826673 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.826687 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.826706 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:39.826714 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60061440Variable Type 7
1884: I0813 09:06:39.826732 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:39.826751 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.826777 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.826794 25950 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.827531 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:39.827560 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:39.827746 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.843719 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: I0813 09:06:39.844014 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0813 09:06:39.850531 25950 pir_interpreter.cc:161] PirInterpreter(): 0x5ffffeb0 on Place(gpu:0)
1884: I0813 09:06:39.850574 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.850606 25950 scope.cc:202] Create variable 0x5ffffeb01723539999850559250_inner_var_1
1884: I0813 09:06:39.850615 25950 scope.cc:202] Create variable 0x5ffffeb01723539999850559250_inner_var_2
1884: I0813 09:06:39.850622 25950 scope.cc:202] Create variable 0x5ffffeb01723539999850559250_inner_var_3
1884: I0813 09:06:39.850631 25950 scope.cc:202] Create variable 0x5ffffeb01723539999850559250_inner_var_4
1884: I0813 09:06:39.850638 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:39.851043 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:39.851058 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.851061 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0813 09:06:39.851107 25950 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5ffffd80
1884: 1 -> 0x5ffffeb01723539999850559250_inner_var_1 -> 0x5ffffe90
1884: 2 -> 0x5ffffeb01723539999850559250_inner_var_2 -> 0x60000630
1884: 3 -> 0x5ffffeb01723539999850559250_inner_var_3 -> 0x5ffff720
1884: 4 -> 0x5ffffeb01723539999850559250_inner_var_4 -> 0x600008b0
1884: 5 -> fetch0@fetch -> 0x60008a10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:39.851778 25950 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0813 09:06:39.852031 25988 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:39.852161 25989 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:39.852169 25990 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:39.852383 25992 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:39.852392 25993 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:39.852391 25991 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:39.852389 25990 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ffffeb01723539999850559250_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.852476 25993 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.852545 25993 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0813 09:06:39.852581 25990 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ffffeb01723539999850559250_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:39.852609 25993 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ffffeb01723539999850559250_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ffffeb01723539999850559250_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.853019 25993 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ffffeb01723539999850559250_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ffffeb01723539999850559250_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0813 09:06:39.853130 25990 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ffffeb01723539999850559250_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.853164 25990 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.854434 25990 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ffffeb01723539999850559250_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5ffffeb01723539999850559250_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 09:06:39.854491 25990 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5ffffeb01723539999850559250_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.854523 25990 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:39.855132 25990 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5ffffeb01723539999850559250_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 09:06:39.855185 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60000020) got event_name: TaskCompletion
1884: I0813 09:06:39.855219 25950 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:39.931828 25988 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 6488762446806076819 to 10905079535235314701 , after update, data is {current : 0, peak : 800768}.
1884: I0813 09:06:39.931864 25988 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 6488762446806076819 to 4298919162646634860 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 09:06:39.931870 25988 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 6488762446806076819 to 4298919162646634860 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 09:06:39.932111 25990 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 800000, peak : 2400000}.
1884: I0813 09:06:39.932139 25990 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 800000, peak : 2400000}.
1884: I0813 09:06:39.932325 25993 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0813 09:06:39.932343 25993 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:39.937358 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:39.937389 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.937448 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:39.937455 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.939226 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:39.939591 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.939605 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.939610 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.941120 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:39.941237 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:39.941247 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:39.941254 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6270b900 type is 7
1884: I0813 09:06:39.941262 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.941265 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6270bc70 type is 7
1884: I0813 09:06:39.941270 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:39.941274 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:39.941339 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:39.941345 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:39.941349 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:39.941352 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:39.941404 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.941421 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.941489 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.941498 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.941512 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:39.941674 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.941685 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:39.941701 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:39.941707 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x600518b0Variable Type 7
1884: I0813 09:06:39.941722 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:39.941740 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.941761 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:39.941777 25950 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.943442 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:39.943480 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:39.943691 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:39.948740 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: I0813 09:06:39.948969 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0813 09:06:39.952207 25950 pir_interpreter.cc:161] PirInterpreter(): 0x627b8890 on Place(gpu:0)
1884: I0813 09:06:39.952243 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.952268 25950 scope.cc:202] Create variable 0x627b88901723539999952234771_inner_var_1
1884: I0813 09:06:39.952278 25950 scope.cc:202] Create variable 0x627b88901723539999952234771_inner_var_2
1884: I0813 09:06:39.952287 25950 scope.cc:202] Create variable 0x627b88901723539999952234771_inner_var_3
1884: I0813 09:06:39.952297 25950 scope.cc:202] Create variable 0x627b88901723539999952234771_inner_var_4
1884: I0813 09:06:39.952320 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:39.952693 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:39.952708 25950 scope.cc:202] Create variable X
1884: I0813 09:06:39.952713 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60031990
1884: 1 -> 0x627b88901723539999952234771_inner_var_1 -> 0x62672ad0
1884: 2 -> 0x627b88901723539999952234771_inner_var_2 -> 0x626a39a0
1884: 3 -> 0x627b88901723539999952234771_inner_var_3 -> 0x626a3740
1884: 4 -> 0x627b88901723539999952234771_inner_var_4 -> 0x626722d0
1884: 5 -> fetch0@fetch -> 0x627a8640
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:39.953436 25994 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:39.953512 25995 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:39.953536 25996 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:39.953593 25997 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:39.953603 25998 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:39.953649 25999 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:39.953649 25998 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x627b88901723539999952234771_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.953676 25999 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.953723 25998 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x627b88901723539999952234771_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:39.953716 25999 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 09:06:39.953774 25999 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x627b88901723539999952234771_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x627b88901723539999952234771_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.953908 25999 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x627b88901723539999952234771_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x627b88901723539999952234771_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0813 09:06:39.953965 25998 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x627b88901723539999952234771_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.953992 25998 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:39.956764 25998 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x627b88901723539999952234771_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x627b88901723539999952234771_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 09:06:39.956815 25998 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x627b88901723539999952234771_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:39.956837 25998 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:39.958833 25998 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x627b88901723539999952234771_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 09:06:39.958880 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x627b8a00) got event_name: TaskCompletion
1884: I0813 09:06:39.958904 25950 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:39.998570 25994 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 10905079535235314701 to 4298919162646634860 , after update, data is {current : 0, peak : 2400768}.
1884: I0813 09:06:39.998601 25994 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10905079535235314701 to 18183931933480510821 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 09:06:39.998606 25994 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 10905079535235314701 to 18183931933480510821 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 09:06:39.998847 25998 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 18183931933480510821 to 10650572563105818707 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0813 09:06:39.998865 25998 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 18183931933480510821 to 10650572563105818707 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0813 09:06:39.998986 25999 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:40.003180 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.003206 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.003263 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.003271 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.005013 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.005358 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.005373 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.005378 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.006922 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:40.007037 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.007048 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:40.007053 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x627c2170 type is 7
1884: I0813 09:06:40.007061 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.007073 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x627c24c0 type is 7
1884: I0813 09:06:40.007077 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.007082 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.007135 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.007141 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.007145 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.007148 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.007198 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.007215 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.007278 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.007287 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.007309 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.007354 25950 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.007500 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.007568 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.007579 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.007594 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.007601 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6294ec10Variable Type 7
1884: I0813 09:06:40.007618 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.007635 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.007658 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.007671 25950 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.007886 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.007908 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:40.008095 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.008898 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: I0813 09:06:40.009097 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0813 09:06:40.012166 25950 pir_interpreter.cc:161] PirInterpreter(): 0x629624d0 on Place(gpu:0)
1884: I0813 09:06:40.012200 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.012223 25950 scope.cc:202] Create variable 0x629624d01723540000012191426_inner_var_1
1884: I0813 09:06:40.012234 25950 scope.cc:202] Create variable 0x629624d01723540000012191426_inner_var_2
1884: I0813 09:06:40.012246 25950 scope.cc:202] Create variable 0x629624d01723540000012191426_inner_var_3
1884: I0813 09:06:40.012259 25950 scope.cc:202] Create variable 0x629624d01723540000012191426_inner_var_4
1884: I0813 09:06:40.012270 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:40.012635 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:40.012652 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.012656 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62848050
1884: 1 -> 0x629624d01723540000012191426_inner_var_1 -> 0x629620b0
1884: 2 -> 0x629624d01723540000012191426_inner_var_2 -> 0x6279ec30
1884: 3 -> 0x629624d01723540000012191426_inner_var_3 -> 0x62957f90
1884: 4 -> 0x629624d01723540000012191426_inner_var_4 -> 0x627c2620
1884: 5 -> fetch0@fetch -> 0x6264b1f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:40.013350 26000 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.013460 26002 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.013465 26001 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.013522 26003 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.013586 26005 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.013589 26004 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.013600 26003 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629624d01723540000012191426_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.013635 26005 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.013672 26003 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629624d01723540000012191426_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.013697 26005 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0813 09:06:40.013744 26005 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x629624d01723540000012191426_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x629624d01723540000012191426_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.013813 26005 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.013972 26005 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.014005 26005 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x629624d01723540000012191426_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x629624d01723540000012191426_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0813 09:06:40.014075 26003 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x629624d01723540000012191426_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.014103 26003 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.014349 26003 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x629624d01723540000012191426_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x629624d01723540000012191426_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 09:06:40.014386 26003 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x629624d01723540000012191426_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.014406 26003 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.014420 26003 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x629624d01723540000012191426_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 09:06:40.014449 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x62962640) got event_name: TaskCompletion
1884: I0813 09:06:40.014470 25950 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.046749 26000 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 4298919162646634860 to 10905079535235314701 , after update, data is {current : 0, peak : 3328}.
1884: I0813 09:06:40.046779 26000 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4298919162646634860 to 10905079535235314701 , after update, data is {current : -804, peak : 2000}.
1884: I0813 09:06:40.046785 26000 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4298919162646634860 to 10905079535235314701 , after update, data is {current : -804, peak : 2000}.
1884: I0813 09:06:40.047089 26003 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 3880290343404252688 to 10905079535235314701 , after update, data is {current : 800, peak : 2000}.
1884: I0813 09:06:40.047104 26003 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 3880290343404252688 to 10905079535235314701 , after update, data is {current : 800, peak : 2000}.
1884: I0813 09:06:40.047250 26005 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0813 09:06:40.047277 26005 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0813 09:06:40.047283 26005 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:40.051888 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.051921 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.051977 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.051985 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.053735 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.054093 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.054106 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.054111 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.055677 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:40.055799 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.055811 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:40.055819 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62972230 type is 7
1884: I0813 09:06:40.055825 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.055830 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62271b00 type is 7
1884: I0813 09:06:40.055833 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.055837 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.055893 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.055900 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.055904 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.055907 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.055961 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.055977 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.056037 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.056046 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.056061 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.056324 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.056339 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.056355 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.056362 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6270b7f0Variable Type 7
1884: I0813 09:06:40.056380 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.056397 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.056419 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.056437 25950 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.057174 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.057209 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:40.057431 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.062322 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: I0813 09:06:40.062582 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0813 09:06:40.065807 25950 pir_interpreter.cc:161] PirInterpreter(): 0x600226c0 on Place(gpu:0)
1884: I0813 09:06:40.065847 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.065871 25950 scope.cc:202] Create variable 0x600226c01723540000065837453_inner_var_1
1884: I0813 09:06:40.065881 25950 scope.cc:202] Create variable 0x600226c01723540000065837453_inner_var_2
1884: I0813 09:06:40.065891 25950 scope.cc:202] Create variable 0x600226c01723540000065837453_inner_var_3
1884: I0813 09:06:40.065902 25950 scope.cc:202] Create variable 0x600226c01723540000065837453_inner_var_4
1884: I0813 09:06:40.065913 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:40.066293 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:40.066318 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.066323 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6270b6b0
1884: 1 -> 0x600226c01723540000065837453_inner_var_1 -> 0x62303160
1884: 2 -> 0x600226c01723540000065837453_inner_var_2 -> 0x622f3720
1884: 3 -> 0x600226c01723540000065837453_inner_var_3 -> 0x62390170
1884: 4 -> 0x600226c01723540000065837453_inner_var_4 -> 0x198fd2e0
1884: 5 -> fetch0@fetch -> 0x62302e30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:40.067059 26006 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.067129 26007 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.067205 26009 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.067211 26008 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.067224 26010 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.067243 26008 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x600226c01723540000065837453_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.067314 26011 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.067324 26008 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x600226c01723540000065837453_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.067350 26011 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.067394 26011 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0813 09:06:40.067430 26011 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x600226c01723540000065837453_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x600226c01723540000065837453_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.067653 26011 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x600226c01723540000065837453_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x600226c01723540000065837453_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0813 09:06:40.067730 26008 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x600226c01723540000065837453_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.067766 26008 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.069005 26008 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x600226c01723540000065837453_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x600226c01723540000065837453_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 09:06:40.069046 26008 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x600226c01723540000065837453_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.069067 26008 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.069669 26008 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x600226c01723540000065837453_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 09:06:40.069710 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60022830) got event_name: TaskCompletion
1884: I0813 09:06:40.069732 25950 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.103752 26006 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 10905079535235314701 to 4298919162646634860 , after update, data is {current : 0, peak : 800768}.
1884: I0813 09:06:40.103780 26006 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10905079535235314701 to 3880290343404252688 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 09:06:40.103785 26006 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 10905079535235314701 to 3880290343404252688 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 09:06:40.104058 26008 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 3880290343404252688 to 10650572563105818707 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0813 09:06:40.104075 26008 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 3880290343404252688 to 10650572563105818707 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0813 09:06:40.104214 26011 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0813 09:06:40.104229 26011 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:40.109546 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.109580 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.109640 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.109647 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.111454 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.111824 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.111838 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.111843 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.113391 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:40.113508 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.113519 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:40.113525 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x600517c0 type is 7
1884: I0813 09:06:40.113533 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.113538 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x627abc40 type is 7
1884: I0813 09:06:40.113543 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.113548 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.113602 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.113608 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.113612 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.113615 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.113667 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.113682 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.113749 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.113756 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.113771 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.113921 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.113934 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.113948 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.113955 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6209c230Variable Type 7
1884: I0813 09:06:40.113969 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.113986 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.114004 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.114018 25950 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.115701 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.115737 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:40.115957 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.120472 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: I0813 09:06:40.120697 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0813 09:06:40.123909 25950 pir_interpreter.cc:161] PirInterpreter(): 0x6207d8e0 on Place(gpu:0)
1884: I0813 09:06:40.123947 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.123970 25950 scope.cc:202] Create variable 0x6207d8e01723540000123936580_inner_var_1
1884: I0813 09:06:40.123981 25950 scope.cc:202] Create variable 0x6207d8e01723540000123936580_inner_var_2
1884: I0813 09:06:40.123993 25950 scope.cc:202] Create variable 0x6207d8e01723540000123936580_inner_var_3
1884: I0813 09:06:40.124004 25950 scope.cc:202] Create variable 0x6207d8e01723540000123936580_inner_var_4
1884: I0813 09:06:40.124013 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:40.124379 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:40.124395 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.124399 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6207de00
1884: 1 -> 0x6207d8e01723540000123936580_inner_var_1 -> 0x6207f7a0
1884: 2 -> 0x6207d8e01723540000123936580_inner_var_2 -> 0x6207f660
1884: 3 -> 0x6207d8e01723540000123936580_inner_var_3 -> 0x62672b70
1884: 4 -> 0x6207d8e01723540000123936580_inner_var_4 -> 0x600341b0
1884: 5 -> fetch0@fetch -> 0x620841e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:40.125106 26012 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.125183 26013 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.125205 26014 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.125258 26015 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.125285 26016 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.125341 26017 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.125349 26016 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6207d8e01723540000123936580_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.125367 26017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.125404 26017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 09:06:40.125409 26016 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6207d8e01723540000123936580_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.125440 26017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6207d8e01723540000123936580_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6207d8e01723540000123936580_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.125571 26017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6207d8e01723540000123936580_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6207d8e01723540000123936580_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0813 09:06:40.125620 26016 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6207d8e01723540000123936580_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.125643 26016 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.128404 26016 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6207d8e01723540000123936580_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6207d8e01723540000123936580_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 09:06:40.128455 26016 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6207d8e01723540000123936580_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.128479 26016 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.130502 26016 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6207d8e01723540000123936580_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 09:06:40.130550 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x6207da50) got event_name: TaskCompletion
1884: I0813 09:06:40.130571 25950 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.170156 26012 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 4298919162646634860 to 10905079535235314701 , after update, data is {current : 0, peak : 2400768}.
1884: I0813 09:06:40.170182 26012 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4298919162646634860 to 584047281285333926 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 09:06:40.170187 26012 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4298919162646634860 to 584047281285333926 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 09:06:40.170452 26016 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 584047281285333926 to 10650572563105818707 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0813 09:06:40.170466 26016 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 584047281285333926 to 10650572563105818707 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0813 09:06:40.170581 26017 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 10905079535235314701 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:40.174922 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.174952 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.175009 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.175016 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.176759 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.177115 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.177129 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.177134 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.178686 25950 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 09:06:40.178799 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.178809 25950 scope.cc:202] Create variable Out
1884: I0813 09:06:40.178817 25950 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6209bd00 type is 7
1884: I0813 09:06:40.178831 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.178834 25950 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6294ec80 type is 7
1884: I0813 09:06:40.178838 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.178843 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.178897 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.178903 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.178907 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.178911 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.178961 25950 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.178977 25950 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.179040 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.179049 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.179062 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.179106 25950 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.179241 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.179297 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.179316 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.179332 25950 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.179339 25950 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61f33570Variable Type 7
1884: I0813 09:06:40.179354 25950 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.179373 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.179394 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.179409 25950 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.179524 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.179546 25950 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:40.179741 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.180567 25950 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x198dd600 for it.
1884: I0813 09:06:40.180778 25950 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x197a3cf0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0813 09:06:40.183871 25950 pir_interpreter.cc:161] PirInterpreter(): 0x6294f3e0 on Place(gpu:0)
1884: I0813 09:06:40.183907 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.183930 25950 scope.cc:202] Create variable 0x6294f3e01723540000183897519_inner_var_1
1884: I0813 09:06:40.183941 25950 scope.cc:202] Create variable 0x6294f3e01723540000183897519_inner_var_2
1884: I0813 09:06:40.183951 25950 scope.cc:202] Create variable 0x6294f3e01723540000183897519_inner_var_3
1884: I0813 09:06:40.183961 25950 scope.cc:202] Create variable 0x6294f3e01723540000183897519_inner_var_4
1884: I0813 09:06:40.183970 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:40.184334 25950 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 09:06:40.184350 25950 scope.cc:202] Create variable X
1884: I0813 09:06:40.184353 25950 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61f34030
1884: 1 -> 0x6294f3e01723540000183897519_inner_var_1 -> 0x61f34050
1884: 2 -> 0x6294f3e01723540000183897519_inner_var_2 -> 0x627c2810
1884: 3 -> 0x6294f3e01723540000183897519_inner_var_3 -> 0x62083180
1884: 4 -> 0x6294f3e01723540000183897519_inner_var_4 -> 0x61f341c0
1884: 5 -> fetch0@fetch -> 0x6270bb10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 09:06:40.185030 26018 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.185117 26020 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.185178 26021 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.185238 26023 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.185268 26022 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.185262 26023 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.185253 26021 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6294f3e01723540000183897519_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.185297 26023 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0813 09:06:40.185345 26019 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.185349 26021 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6294f3e01723540000183897519_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.185369 26023 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6294f3e01723540000183897519_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6294f3e01723540000183897519_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.185417 26023 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.185551 26023 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.185580 26023 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6294f3e01723540000183897519_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6294f3e01723540000183897519_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0813 09:06:40.185635 26019 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6294f3e01723540000183897519_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.185658 26019 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.185801 26019 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6294f3e01723540000183897519_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6294f3e01723540000183897519_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 09:06:40.185827 26019 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6294f3e01723540000183897519_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.185844 26019 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.185859 26019 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6294f3e01723540000183897519_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 09:06:40.185897 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x6294f550) got event_name: TaskCompletion
1884: I0813 09:06:40.185915 25950 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.217959 26018 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 10905079535235314701 to 4298919162646634860 , after update, data is {current : 0, peak : 10240}.
1884: I0813 09:06:40.217989 26018 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 10905079535235314701 to 584047281285333926 , after update, data is {current : 796, peak : 1600}.
1884: I0813 09:06:40.217995 26018 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 10905079535235314701 to 584047281285333926 , after update, data is {current : 796, peak : 1600}.
1884: I0813 09:06:40.218236 26019 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 584047281285333926 to 4298919162646634860 , after update, data is {current : 796, peak : 8000}.
1884: I0813 09:06:40.218258 26019 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 584047281285333926 to 4298919162646634860 , after update, data is {current : 796, peak : 8000}.
1884: I0813 09:06:40.218271 26021 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16278705021704809697 to 4298919162646634860 , after update, data is {current : 800, peak : 8000}.
1884: I0813 09:06:40.218287 26021 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16278705021704809697 to 4298919162646634860 , after update, data is {current : 800, peak : 8000}.
1884: I0813 09:06:40.218442 26023 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0813 09:06:40.218454 26023 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0813 09:06:40.218458 26023 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 4298919162646634860 to 10650572563105818707 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 09:06:40.225044 25950 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0813 09:06:40.225098 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0813 09:06:40.226235 25950 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 09:06:40.227068 25950 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0813 09:06:40.227098 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0813 09:06:40.228428 25950 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0813 09:06:40.228453 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 09:06:40.229132 25950 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0813 09:06:40.230103 25950 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0813 09:06:40.230126 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 09:06:40.231741 25950 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0813 09:06:40.231765 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 09:06:40.232357 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.232385 25950 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 09:06:40.232391 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.232398 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.234419 25950 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0813 09:06:40.234444 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 09:06:40.235396 25950 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0813 09:06:40.235424 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0813 09:06:40.236361 25950 pybind.cc:1827] need skip: 0
1884: I0813 09:06:40.236665 25950 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 09:06:40.238508 25950 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 09:06:40.242167 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.242189 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.242194 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.244201 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.244223 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.244235 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.244241 25950 scope.cc:202] Create variable learning_rate_0
1884: I0813 09:06:40.244248 25950 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6238d5c0 type is 7
1884: I0813 09:06:40.244252 25950 scope.cc:202] Create variable linear_0.b_0
1884: I0813 09:06:40.244256 25950 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6238e510 type is 7
1884: I0813 09:06:40.244259 25950 scope.cc:202] Create variable linear_0.w_0
1884: I0813 09:06:40.244262 25950 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6238e5c0 type is 7
1884: I0813 09:06:40.244334 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.244341 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.244345 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.244349 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.244408 25950 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.244424 25950 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.244446 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0813 09:06:40.244588 25950 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.244598 25950 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.246263 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.246340 25950 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.246349 25950 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.246379 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.247453 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.248855 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.249313 25950 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 09:06:40.249548 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.249852 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.250077 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.250092 25950 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 09:06:40.250164 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.250170 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.250174 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.250276 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.250288 25950 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 09:06:40.251848 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.253171 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.254262 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.254472 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.254485 25950 scope.cc:202] Create variable abs_0.tmp_0
1884: I0813 09:06:40.254494 25950 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x62662340 type is 7
1884: I0813 09:06:40.254501 25950 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 09:06:40.254505 25950 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x626620c0 type is 7
1884: I0813 09:06:40.254509 25950 scope.cc:202] Create variable cast_0.tmp_0
1884: I0813 09:06:40.254513 25950 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x626621b0 type is 7
1884: I0813 09:06:40.254516 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.254521 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.254525 25950 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0813 09:06:40.254528 25950 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x62663730 type is 7
1884: I0813 09:06:40.254534 25950 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6238d5c0 type is 7
1884: I0813 09:06:40.254537 25950 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6238e510 type is 7
1884: I0813 09:06:40.254541 25950 scope.cc:202] Create variable linear_0.tmp_0
1884: I0813 09:06:40.254545 25950 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x62663710 type is 7
1884: I0813 09:06:40.254549 25950 scope.cc:202] Create variable linear_0.tmp_1
1884: I0813 09:06:40.254552 25950 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x62663c70 type is 7
1884: I0813 09:06:40.254557 25950 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6238e5c0 type is 7
1884: I0813 09:06:40.254560 25950 scope.cc:202] Create variable mean_0.tmp_0
1884: I0813 09:06:40.254565 25950 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x62663ee0 type is 7
1884: I0813 09:06:40.254568 25950 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0813 09:06:40.254571 25950 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x62664120 type is 7
1884: I0813 09:06:40.254575 25950 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 09:06:40.254580 25950 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x62664380 type is 7
1884: I0813 09:06:40.254665 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.254681 25950 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 09:06:40.254742 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.254750 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.254753 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.254757 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.254812 25950 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.254827 25950 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.254842 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0813 09:06:40.254969 25950 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.254979 25950 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.254999 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 09:06:40.255080 25950 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 09:06:40.255165 25950 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0813 09:06:40.257755 25950 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.257781 25950 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.257853 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.257916 25950 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.257928 25950 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.257941 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 09:06:40.257967 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.258008 25950 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258016 25950 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258028 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 09:06:40.258128 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258137 25950 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258150 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.258266 25950 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.258363 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.258423 25950 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258433 25950 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258447 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 09:06:40.258483 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.258535 25950 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258544 25950 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258558 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0813 09:06:40.258668 25950 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258679 25950 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258700 25950 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.258746 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.258756 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.258771 25950 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.258780 25950 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6279b120Variable Type 7
1884: I0813 09:06:40.258797 25950 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.258814 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.258833 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.258846 25950 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.258888 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.258909 25950 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 09:06:40.258936 25950 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.258944 25950 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.258957 25950 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0813 09:06:40.258963 25950 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6266d050Variable Type 7
1884: I0813 09:06:40.258976 25950 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 09:06:40.258987 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.259003 25950 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.259016 25950 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.259049 25950 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 09:06:40.259063 25950 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0813 09:06:40.259506 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 09:06:40.259543 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 09:06:40.259560 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 09:06:40.259593 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 09:06:40.259624 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.259642 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 09:06:40.264218 25950 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0813 09:06:40.264261 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 09:06:40.265038 25950 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0813 09:06:40.265061 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 09:06:40.265451 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.267163 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.267997 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.268122 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.268630 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.269541 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.271700 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.272748 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.274570 25950 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0813 09:06:40.275413 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.275429 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.275434 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.276636 25950 interpreter_util.cc:1169] Creating Variables
1884: I0813 09:06:40.276656 25950 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3c62260 type is 9
1884: I0813 09:06:40.276665 25950 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x2149180 type is 10
1884: I0813 09:06:40.276672 25950 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6238e510 type is 7
1884: I0813 09:06:40.276676 25950 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6238e5c0 type is 7
1884: I0813 09:06:40.276681 25950 scope.cc:202] Create variable saved_params
1884: I0813 09:06:40.276684 25950 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x61a5f1d0 type is 17
1884: I0813 09:06:40.276713 25950 interpreter_util.cc:594] Static build: 0
1884: I0813 09:06:40.276719 25950 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 09:06:40.276723 25950 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 09:06:40.276726 25950 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 09:06:40.276777 25950 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.276793 25950 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 09:06:40.278107 25950 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0813 09:06:40.278147 25950 analysis_predictor.cc:433] Predictor::init()
1884: I0813 09:06:40.278213 25950 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0813 09:06:40.279541 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.279603 25950 scope.cc:202] Create variable feed
1884: I0813 09:06:40.279609 25950 naive_executor.cc:189] 0x61ab49f0 Create persistable variable feed, which pointer is 0x626b2e90
1884: I0813 09:06:40.279614 25950 scope.cc:202] Create variable fetch
1884: I0813 09:06:40.279618 25950 naive_executor.cc:189] 0x61ab49f0 Create persistable variable fetch, which pointer is 0x626b2d30
1884: I0813 09:06:40.279620 25950 scope.cc:202] Create variable linear_0.b_0
1884: I0813 09:06:40.279623 25950 naive_executor.cc:189] 0x61ab49f0 Create persistable variable linear_0.b_0, which pointer is 0x61ab4d20
1884: I0813 09:06:40.279629 25950 scope.cc:202] Create variable linear_0.w_0
1884: I0813 09:06:40.279631 25950 naive_executor.cc:189] 0x61ab49f0 Create persistable variable linear_0.w_0, which pointer is 0x61ab43f0
1884: I0813 09:06:40.279646 25950 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0813 09:06:40.279994 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.280076 25950 program_converter.cc:296] is_legacy_program : 0
1884: I0813 09:06:40.280119 25950 executor.cc:183] Old Executor is Running.
1884: I0813 09:06:40.280195 25950 executor.cc:92] Creating Variables for block 0
1884: I0813 09:06:40.280205 25950 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0813 09:06:40.280207 25950 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x61ab4d20 type is 7
1884: I0813 09:06:40.280210 25950 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0813 09:06:40.280212 25950 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x61ab43f0 type is 7
1884: I0813 09:06:40.280251 25950 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.280349 25950 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0813 09:06:40.280393 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.280400 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.280534 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.280642 25950 graph.cc:149] create OpNode by feed
1884: I0813 09:06:40.280678 25950 graph.cc:149] create OpNode by matmul_v2
1884: I0813 09:06:40.280694 25950 graph.cc:149] create OpNode by elementwise_add
1884: I0813 09:06:40.280707 25950 graph.cc:149] create OpNode by abs
1884: I0813 09:06:40.280718 25950 graph.cc:149] create OpNode by assign_value
1884: I0813 09:06:40.280736 25950 graph.cc:149] create OpNode by multinomial
1884: I0813 09:06:40.280746 25950 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 09:06:40.280761 25950 graph.cc:149] create OpNode by scale
1884: I0813 09:06:40.280772 25950 graph.cc:149] create OpNode by scale
1884: I0813 09:06:40.280786 25950 graph.cc:149] create OpNode by fetch
1884: I0813 09:06:40.280802 25950 graph.cc:149] create OpNode by fetch
1884: I0813 09:06:40.280822 25950 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0813 09:06:40.281993 25950 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0813 09:06:40.282001 25950 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0813 09:06:40.282073 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282080 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0813 09:06:40.282187 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.282444 25950 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0813 09:06:40.282507 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282513 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0813 09:06:40.282545 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282550 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0813 09:06:40.282588 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.282649 25950 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 09:06:40.282680 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282684 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0813 09:06:40.282699 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.282712 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.282734 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282739 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0813 09:06:40.282778 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.282799 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.282822 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282827 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0813 09:06:40.282868 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.282943 25950 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 09:06:40.282972 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.282977 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0813 09:06:40.283007 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283027 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.283051 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283056 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0813 09:06:40.283083 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283236 25950 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0813 09:06:40.283263 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283268 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0813 09:06:40.283298 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283334 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.283357 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283362 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0813 09:06:40.283385 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283399 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.283421 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283425 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0813 09:06:40.283447 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283461 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.283483 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283488 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0813 09:06:40.283514 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283581 25950 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0813 09:06:40.283615 25950 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 09:06:40.283629 25950 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 09:06:40.283644 25950 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0813 09:06:40.283668 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.283674 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0813 09:06:40.283694 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.283735 25950 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0813 09:06:40.283754 25950 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 09:06:40.283766 25950 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 09:06:40.283778 25950 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 09:06:40.283808 25950 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0813 09:06:40.283820 25950 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0813 09:06:40.285018 25950 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 09:06:40.285064 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.285070 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0813 09:06:40.285097 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.285120 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.285145 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.285151 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0813 09:06:40.285173 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.285223 25950 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 09:06:40.285254 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.285259 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0813 09:06:40.285279 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.285295 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.285323 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.285329 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0813 09:06:40.285358 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.285446 25950 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0813 09:06:40.285475 25950 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 09:06:40.285490 25950 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 09:06:40.285506 25950 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 09:06:40.285521 25950 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0813 09:06:40.285535 25950 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0813 09:06:40.285552 25950 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0813 09:06:40.285574 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.285650 25950 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0813 09:06:40.285672 25950 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 09:06:40.285686 25950 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 09:06:40.285699 25950 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 09:06:40.285713 25950 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0813 09:06:40.285728 25950 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0813 09:06:40.285743 25950 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0813 09:06:40.285789 25950 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0813 09:06:40.286053 25950 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 09:06:40.286083 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.286088 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0813 09:06:40.286134 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286195 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286231 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286278 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286314 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286356 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286381 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286419 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286440 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286474 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286492 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286525 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286540 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286567 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286581 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286604 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286615 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286635 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286660 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.286665 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0813 09:06:40.286690 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286733 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286759 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.286764 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0813 09:06:40.286775 25950 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0813 09:06:40.286778 25950 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0813 09:06:40.286820 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286842 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286868 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.286873 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0813 09:06:40.286882 25950 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0813 09:06:40.286885 25950 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0813 09:06:40.286924 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.286947 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.286970 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.286975 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0813 09:06:40.286983 25950 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0813 09:06:40.286986 25950 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0813 09:06:40.287017 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.287036 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.287060 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.287063 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0813 09:06:40.287072 25950 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0813 09:06:40.287075 25950 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0813 09:06:40.287111 25950 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 09:06:40.287132 25950 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 09:06:40.287155 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.287160 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0813 09:06:40.287173 25950 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0813 09:06:40.287216 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.287221 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0813 09:06:40.287290 25950 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 09:06:40.287321 25950 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.287343 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 09:06:40.287417 25950 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0813 09:06:40.287436 25950 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 09:06:40.287464 25950 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 09:06:40.287489 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.287494 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0813 09:06:40.288409 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.288429 25950 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0813 09:06:40.288481 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.288486 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.289094 25950 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0813 09:06:40.289312 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.289386 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.289391 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.289798 25950 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 09:06:40.290017 25950 graph.h:183] deleting __fuse_statis__
1884: I0813 09:06:40.290026 25950 graph.h:183] deleting pass_recorder
1884: I0813 09:06:40.290031 25950 graph.h:183] deleting stale_program_op_descs
1884: I0813 09:06:40.290117 25950 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0813 09:06:40.290127 25950 scope.cc:202] Create variable abs_0.tmp_0
1884: I0813 09:06:40.290130 25950 naive_executor.cc:195] 0x61ab49f0 Create variable abs_0.tmp_0, which pointer is 0x62652fd0
1884: I0813 09:06:40.290136 25950 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0813 09:06:40.290143 25950 naive_executor.cc:195] 0x61ab49f0 Create variable gaussian_0.tmp_0, which pointer is 0x61a6ca50
1884: I0813 09:06:40.290154 25950 scope.cc:202] Create variable linear_0.tmp_1
1884: I0813 09:06:40.290158 25950 naive_executor.cc:195] 0x61ab49f0 Create variable linear_0.tmp_1, which pointer is 0x622762b0
1884: I0813 09:06:40.290163 25950 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 09:06:40.290165 25950 naive_executor.cc:195] 0x61ab49f0 Create variable multinomial_0.tmp_0, which pointer is 0x62275d50
1884: I0813 09:06:40.290169 25950 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0813 09:06:40.290171 25950 naive_executor.cc:195] 0x61ab49f0 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62276050
1884: I0813 09:06:40.290174 25950 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0813 09:06:40.290177 25950 naive_executor.cc:195] 0x61ab49f0 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62274650
1884: I0813 09:06:40.290184 25950 scope.cc:202] Create variable feed
1884: I0813 09:06:40.290187 25950 scope.cc:202] Create variable fetch
1884: I0813 09:06:40.290210 25950 naive_executor.cc:46] NaiveExecutor init with scope 0x61ab49f0
1884: I0813 09:06:40.290215 25950 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0813 09:06:40.290447 25950 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 09:06:40.290464 25950 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 09:06:40.290494 25950 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0813 09:06:40.290499 25950 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0813 09:06:40.290506 25950 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.290542 25950 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 09:06:40.291345 25950 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.291368 25950 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 09:06:40.291421 25950 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.291448 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0813 09:06:40.374374 25950 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0813 09:06:40.374487 25950 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.374513 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 09:06:40.374585 25950 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0813 09:06:40.374619 25950 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.374641 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 09:06:40.374702 25950 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0813 09:06:40.374743 25950 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.374760 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 09:06:40.374814 25950 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0813 09:06:40.374845 25950 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 09:06:40.374859 25950 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 09:06:40.374893 25950 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0813 09:06:40.374912 25950 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.374945 25950 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 09:06:40.374970 25950 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0813 09:06:40.375397 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.375409 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0813 09:06:40.418484 25950 pir_interpreter.cc:161] PirInterpreter(): 0x62854240 on Place(gpu:0)
1884: I0813 09:06:40.418524 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_0
1884: I0813 09:06:40.418540 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_1
1884: I0813 09:06:40.418548 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_2
1884: I0813 09:06:40.418557 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_3
1884: I0813 09:06:40.418586 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_4
1884: I0813 09:06:40.418601 25950 scope.cc:202] Create variable 0x628542401723540000418513637_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x628542401723540000418513637_inner_var_0 -> 0x61f78620
1884: 1 -> 0x628542401723540000418513637_inner_var_1 -> 0x17515e90
1884: 2 -> 0x628542401723540000418513637_inner_var_2 -> 0x60047310
1884: 3 -> linear_1.w_0 -> 0x6268a6c0
1884: 4 -> linear_1.b_0 -> 0x61aa5140
1884: 5 -> learning_rate_1 -> 0x626a3720
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0813 09:06:40.419405 26024 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.419423 26025 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.419504 26027 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.419507 26026 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.419524 26028 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.419533 26027 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419534 26025 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419533 26024 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419572 26028 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419641 26028 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.419637 26027 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.419646 26024 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.419646 26025 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x628542401723540000418513637_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 09:06:40.419684 26028 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0813 09:06:40.419711 26028 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419732 26028 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.419744 26028 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 09:06:40.419759 26028 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x628542401723540000418513637_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x628542401723540000418513637_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x628542401723540000418513637_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.419823 26028 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x628542401723540000418513637_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x628542401723540000418513637_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x628542401723540000418513637_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0813 09:06:40.419889 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x628543b0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0813 09:06:40.421955 25950 pir_interpreter.cc:161] PirInterpreter(): 0x61b3d290 on Place(gpu:0)
1884: I0813 09:06:40.421995 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_1
1884: I0813 09:06:40.422011 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_4
1884: I0813 09:06:40.422020 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_5
1884: I0813 09:06:40.422027 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_6
1884: I0813 09:06:40.422050 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_7
1884: I0813 09:06:40.422060 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_8
1884: I0813 09:06:40.422067 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_9
1884: I0813 09:06:40.422096 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_10
1884: I0813 09:06:40.422104 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_11
1884: I0813 09:06:40.422111 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_12
1884: I0813 09:06:40.422122 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_13
1884: I0813 09:06:40.422129 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_14
1884: I0813 09:06:40.422138 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_15
1884: I0813 09:06:40.422145 25950 scope.cc:202] Create variable fetch0@fetch
1884: I0813 09:06:40.422158 25950 scope.cc:202] Create variable 0x61b3d2901723540000421979087_inner_var_17
1884: I0813 09:06:40.422165 25950 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x626a3720
1884: 1 -> 0x61b3d2901723540000421979087_inner_var_1 -> 0x61f7dd10
1884: 2 -> linear_1.b_0 -> 0x61aa5140
1884: 3 -> linear_1.w_0 -> 0x6268a6c0
1884: 4 -> 0x61b3d2901723540000421979087_inner_var_4 -> 0x62672d90
1884: 5 -> 0x61b3d2901723540000421979087_inner_var_5 -> 0x627a7ad0
1884: 6 -> 0x61b3d2901723540000421979087_inner_var_6 -> 0x60014ce0
1884: 7 -> 0x61b3d2901723540000421979087_inner_var_7 -> 0x61a5d8e0
1884: 8 -> 0x61b3d2901723540000421979087_inner_var_8 -> 0x62960920
1884: 9 -> 0x61b3d2901723540000421979087_inner_var_9 -> 0x3e45330
1884: 10 -> 0x61b3d2901723540000421979087_inner_var_10 -> 0x6002aa50
1884: 11 -> 0x61b3d2901723540000421979087_inner_var_11 -> 0x6001dc10
1884: 12 -> 0x61b3d2901723540000421979087_inner_var_12 -> 0x3e453a0
1884: 13 -> 0x61b3d2901723540000421979087_inner_var_13 -> 0x60029e30
1884: 14 -> 0x61b3d2901723540000421979087_inner_var_14 -> 0x61b9db50
1884: 15 -> 0x61b3d2901723540000421979087_inner_var_15 -> 0x5ffffb60
1884: 16 -> fetch0@fetch -> 0x62672370
1884: 17 -> 0x61b3d2901723540000421979087_inner_var_17 -> 0x6268a810
1884: 18 -> fetch1@fetch -> 0x60014dc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0813 09:06:40.423875 26029 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.424007 26030 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.424015 26031 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.424103 26032 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.424115 26031 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424125 26030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424202 26031 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 09:06:40.424218 26033 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 09:06:40.424203 26030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.424263 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424295 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 09:06:40.424331 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424379 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.424474 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 09:06:40.424492 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 09:06:40.424561 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 09:06:40.424577 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61b3d2901723540000421979087_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424625 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61b3d2901723540000421979087_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 09:06:40.424656 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61b3d2901723540000421979087_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424700 26033 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 09:06:40.424741 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61b3d2901723540000421979087_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.424772 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61b3d2901723540000421979087_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61b3d2901723540000421979087_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424813 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.424830 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61b3d2901723540000421979087_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61b3d2901723540000421979087_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.424862 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61b3d2901723540000421979087_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424886 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.424890 26030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b3d2901723540000421979087_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424901 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61b3d2901723540000421979087_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.424922 26030 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.424919 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61b3d2901723540000421979087_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.424947 26033 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.425010 26030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b3d2901723540000421979087_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.425045 26030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61b3d2901723540000421979087_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425058 26033 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.425067 26030 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.425082 26030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61b3d2901723540000421979087_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.425100 26033 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.425170 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.425192 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61b3d2901723540000421979087_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61b3d2901723540000421979087_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.425228 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425236 26030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425254 26030 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0813 09:06:40.425257 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.425280 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.425305 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61b3d2901723540000421979087_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425304 26030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61b3d2901723540000421979087_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.425340 26030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61b3d2901723540000421979087_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425359 26030 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.425371 26030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61b3d2901723540000421979087_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.425393 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61b3d2901723540000421979087_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 09:06:40.425415 26033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61b3d2901723540000421979087_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61b3d2901723540000421979087_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.425448 26033 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 09:06:40.425460 26033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61b3d2901723540000421979087_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61b3d2901723540000421979087_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61b3d2901723540000421979087_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 09:06:40.425500 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x61b3d400) got event_name: TaskCompletion
1884: I0813 09:06:40.425529 25950 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.425565 25950 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0813 09:06:40.431298 25950 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0813 09:06:40.431360 25950 analysis_predictor.cc:433] Predictor::init()
1884: I0813 09:06:40.432050 25950 scope.cc:202] Create variable linear_1.b_0
1884: I0813 09:06:40.432106 25950 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0813 09:06:40.432559 25950 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004326091060"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004326091060"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0813 09:06:40.432766 25950 pir_interpreter.cc:161] PirInterpreter(): 0x60054be0 on Place(cpu)
1884: I0813 09:06:40.432790 25950 scope.cc:202] Create variable 0x60054be01723540000432782342_inner_var_0
1884: I0813 09:06:40.432818 25950 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004326091060"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235400004326091060 -> 0x61b61140
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0813 09:06:40.432973 25950 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0813 09:06:40.433151 26034 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.433315 26035 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.433326 26037 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.433352 26035 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17235400004326091060:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.433413 26035 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17235400004326091060:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 09:06:40.433446 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60054d50) got event_name: TaskCompletion
1884: I0813 09:06:40.433511 26036 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.433597 26038 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.433750 26035 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 15502579085582410590 to 13177511974603097822 , after update, data is {current : -4, peak : 104}.
1884: I0813 09:06:40.433764 26035 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 15502579085582410590 to 13177511974603097822 , after update, data is {current : -4, peak : 104}.
1884: I0813 09:06:40.433840 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.433848 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004339216351"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004339216351"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0813 09:06:40.434078 25950 pir_interpreter.cc:161] PirInterpreter(): 0x60054be0 on Place(cpu)
1884: I0813 09:06:40.434099 25950 scope.cc:202] Create variable 0x60054be01723540000434093208_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004339216351"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235400004339216351 -> 0x61a5e170
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0813 09:06:40.434352 26039 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.434434 26040 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.434470 26040 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235400004339216351:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.434481 26043 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.434530 26040 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235400004339216351:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.434556 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60054d50) got event_name: TaskCompletion
1884: I0813 09:06:40.434587 26041 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.434638 26042 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.434783 26040 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 2208637585624081016 to 13177511974603097822 , after update, data is {current : 4, peak : 104}.
1884: I0813 09:06:40.434795 26040 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 2208637585624081016 to 13177511974603097822 , after update, data is {current : 4, peak : 104}.
1884: I0813 09:06:40.434856 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.434864 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235400004339216351",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235400004349418362"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235400004339216351",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235400004349418362"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0813 09:06:40.435097 25950 pir_interpreter.cc:161] PirInterpreter(): 0x60054be0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235400004339216351",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235400004349418362"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235400004349418362 -> 0x61a5e170
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0813 09:06:40.435366 26044 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.435428 26045 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.435482 26046 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.435501 26046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.435536 26046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.435562 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60054d50) got event_name: TaskCompletion
1884: I0813 09:06:40.435591 26047 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.435675 26048 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.435896 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.435904 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004359801293"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004359801293"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0813 09:06:40.436110 25950 pir_interpreter.cc:161] PirInterpreter(): 0x60054be0 on Place(cpu)
1884: I0813 09:06:40.436128 25950 scope.cc:202] Create variable 0x60054be01723540000436122723_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235400004359801293"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235400004359801293 -> 0x626b7a50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0813 09:06:40.436328 26049 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.436409 26050 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 09:06:40.436458 26051 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 09:06:40.436470 26051 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235400004359801293:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.436506 26051 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235400004359801293:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 09:06:40.436532 25950 pir_interpreter.cc:1766] main_thread_blocker_(0x60054d50) got event_name: TaskCompletion
1884: I0813 09:06:40.436563 26052 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 09:06:40.436614 26053 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 09:06:40.436781 26051 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6569985384778046703 to 13177511974603097822 , after update, data is {current : 8, peak : 104}.
1884: I0813 09:06:40.436789 26051 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6569985384778046703 to 13177511974603097822 , after update, data is {current : 8, peak : 104}.
1884: I0813 09:06:40.436843 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.436851 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.436915 25950 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0813 09:06:40.436978 25950 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0813 09:06:40.437016 25950 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004359801293"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004349418362"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004359801293"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004349418362"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0813 09:06:40.437705 25950 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0813 09:06:40.437722 25950 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0813 09:06:40.437759 25950 pir_interpreter.cc:161] PirInterpreter(): 0x60054be0 on Place(cpu)
1884: I0813 09:06:40.437788 25950 scope.cc:202] Create variable feed_name_0
1884: I0813 09:06:40.437804 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_5
1884: I0813 09:06:40.437826 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_6
1884: I0813 09:06:40.437837 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_7
1884: I0813 09:06:40.437844 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_8
1884: I0813 09:06:40.437866 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_9
1884: I0813 09:06:40.437877 25950 scope.cc:202] Create variable 0x60054be01723540000437773663_inner_var_10
1884: I0813 09:06:40.437901 25950 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.437924 25950 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 09:06:40.438081 25950 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.438094 25950 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004359801293"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235400004349418362"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235400004359801293 -> 0x626b7a50
1884: 1 -> constant_folding@_17235400004349418362 -> 0x61a5e170
1884: 2 -> linear_1.b_0 -> 0x60020fc0
1884: 3 -> linear_1.w_0 -> 0x62644d20
1884: 4 -> feed_name_0 -> 0x61a75060
1884: 5 -> 0x60054be01723540000437773663_inner_var_5 -> 0x61aa2fe0
1884: 6 -> 0x60054be01723540000437773663_inner_var_6 -> 0x61f75950
1884: 7 -> 0x60054be01723540000437773663_inner_var_7 -> 0x6269cc80
1884: 8 -> 0x60054be01723540000437773663_inner_var_8 -> 0x62716eb0
1884: 9 -> fetch_name_0 -> 0x61bae8a0
1884: 10 -> fetch_name_1 -> 0x61b7ed40
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0813 09:06:40.438671 25950 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0813 09:06:40.438740 26054 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 09:06:40.438733 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.438795 25950 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 09:06:40.438818 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.438850 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x60054be01723540000437773663_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x60054be01723540000437773663_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.438896 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x60054be01723540000437773663_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.438928 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.438951 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.438966 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.438999 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17235400004349418362:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.439024 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17235400004359801293:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.439055 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17235400004359801293:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x60054be01723540000437773663_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 09:06:40.439082 25950 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17235400004359801293:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 09:06:40.439110 25950 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17235400004359801293:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x60054be01723540000437773663_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 09:06:40.439136 25950 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 09:06:40.439157 25950 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 09:06:40.439179 25950 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0813 09:06:40.439307 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.439316 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.439368 26054 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 15502579085582410590 to 13177511974603097822 , after update, data is {current : -184, peak : 104}.
1884: I0813 09:06:40.439379 26054 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 15502579085582410590 to 13177511974603097822 , after update, data is {current : -184, peak : 104}.
1884: I0813 09:06:40.439422 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.439429 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.441325 25950 mmap_allocator.cc:348] PID: 25950, MemoryMapFdSet: set size - 0
1884: I0813 09:06:40.452970 25950 mmap_allocator.cc:348] PID: 25950, MemoryMapFdSet: set size - 0
1884: I0813 09:06:40.519294 26027 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1536160873816225215 to 13177511974603097822 , after update, data is {current : -180, peak : 104}.
1884: I0813 09:06:40.519332 26027 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1536160873816225215 to 13177511974603097822 , after update, data is {current : -180, peak : 104}.
1884: I0813 09:06:40.519351 26025 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18183931933480510821 to 13177511974603097822 , after update, data is {current : -164, peak : 104}.
1884: I0813 09:06:40.519384 26025 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18183931933480510821 to 13177511974603097822 , after update, data is {current : -164, peak : 104}.
1884: I0813 09:06:40.519393 26024 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4298919162646634860 to 13177511974603097822 , after update, data is {current : -160, peak : 104}.
1884: I0813 09:06:40.519418 26024 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 4298919162646634860 to 13177511974603097822 , after update, data is {current : -160, peak : 104}.
1884: I0813 09:06:40.519614 26028 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 14434128632663990648 to 13177511974603097822 , after update, data is {current : -184, peak : 104}.
1884: I0813 09:06:40.519639 26028 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 14434128632663990648 to 13177511974603097822 , after update, data is {current : -184, peak : 104}.
1884: I0813 09:06:40.519644 26028 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 14434128632663990648 to 7260702272558398186 , after update, data is {current : 256, peak : 768}.
1884: I0813 09:06:40.519961 26030 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 7260702272558398186 to 13177511974603097822 , after update, data is {current : 768, peak : 1536}.
1884: I0813 09:06:40.519973 26030 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 7260702272558398186 to 13177511974603097822 , after update, data is {current : 12, peak : 268}.
1884: I0813 09:06:40.519977 26030 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 7260702272558398186 to 13177511974603097822 , after update, data is {current : 12, peak : 268}.
1884: I0813 09:06:40.520047 26031 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16066465179282561404 to 13177511974603097822 , after update, data is {current : 28, peak : 268}.
1884: I0813 09:06:40.520056 26031 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16066465179282561404 to 13177511974603097822 , after update, data is {current : 28, peak : 268}.
1884: I0813 09:06:40.520200 26033 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 13177511974603097822 to 10650572563105818707 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0813 09:06:40.520211 26033 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 13177511974603097822 to 10650572563105818707 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0813 09:06:40.520219 26033 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 13177511974603097822 to 10650572563105818707 , after update, data is {current : 1536, peak : 2401024}.
1884: I0813 09:06:40.684679 25950 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 09:06:40.684707 25950 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 09:06:40.684749 25950 mmap_allocator.cc:348] PID: 25950, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   12.59 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  12.77 sec
