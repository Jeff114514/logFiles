UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 02:51:02.530275  6033 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 02:51:03.329335  6033 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=eager_delete_tensor_gb,local_exe_sub_scope_limit,gpugraph_hbm_table_load_factor,logging_trunc_pir_py_code,use_fast_math,dump_chunk_info,enable_gpu_memory_usage_log_mb,gpugraph_offload_param_stat,static_runtime_data_save_path,gpugraph_force_device_batch_num_equal,add_dependency_for_communication_op,enable_pir_with_pt_in_dy2st,call_stack_level,dist_threadpool_size,sync_nccl_allreduce,enable_collect_shape,disable_dyshape_in_train,multiple_of_cupti_buffer_size,prim_check_ops,new_executor_use_cuda_graph,use_stream_safe_cuda_allocator,enable_cse_in_dy2st,prim_enabled,enable_pir_in_executor_trace_run,graph_get_neighbor_id,logging_pir_py_code_dir,enable_gpu_memory_usage_log,search_cache_max_number,enable_dependency_builder_debug_info,max_inplace_grad_add,paddle_num_threads,enable_exit_when_partial_worker,use_virtual_memory_auto_growth,use_cinn,gpugraph_dedup_pull_push_mode,cublaslt_exhaustive_search_times,gpugraph_load_node_list_into_hbm,inner_op_parallelism,gpugraph_enable_segment_merge_grads,pir_subgraph_saving_dir,logging_pir_py_code_int_tensor_element_limit,prim_all,dynamic_static_unified_comm,gpugraph_merge_grads_segment_size,fraction_of_cuda_pinned_memory_to_use,check_nan_inf,tracer_onednn_ops_off,fleet_executor_with_standalone,gpugraph_parallel_stream_num,free_idle_chunk,accuracy_check_rtol_fp32,cublas_dir,cudnn_exhaustive_search,benchmark,cudnn_batchnorm_spatial_persistent,cudnn_dir,gemm_use_half_precision_compute_type,gpu_memory_limit_mb,pir_debug,gpugraph_offload_gather_copy_maxsize,embedding_deterministic,fuse_parameter_memory_size,benchmark_nccl,cusolver_dir,deny_cinn_ops,allreduce_record_one_event,cupti_dir,win_cuda_bin_dir,graph_neighbor_size_percent,gpugraph_parallel_copyer_split_maxsize,conv_workspace_size_limit,sort_sum_gradient,gpugraph_slot_feasign_max_num,enable_interpretercore_launch_cinn,convert_all_blocks,print_allocator_trace_info,use_shm_cache,allocator_strategy,enable_record_memory,custom_device_mem_record,accuracy_check_rtol_bf16,use_pinned_memory,apply_pass_to_program,enable_tracker_all2all,all_blocks_convert_trt,new_executor_static_build,async_trace_count,lapack_dir,pir_broadcast_tree_limit,accuracy_check_atol_fp32,new_executor_use_local_scope,enable_unused_var_check,static_executor_perfstat_filepath,auto_free_cudagraph_allocations_on_launch,cuda_malloc_async_pool_memory_throttle_ratio,curand_dir,gpugraph_storage_mode,enable_all2all_use_fp16,gpugraph_sparse_table_storage_mode,enable_async_trace,multi_node_sample_use_gpu_table,gpugraph_debug_gpu_memory,enable_cublas_tensor_op_math,use_autotune,graph_embedding_split_infer_mode,gpugraph_enable_print_op_debug,enable_opt_get_features,check_infer_symbolic,use_cuda_managed_memory,host_trace_level,accuracy_check_atol_fp16,sync_after_alloc,enable_fusion_fallback,graph_load_in_parallel,cusparselt_dir,cinn_subgraph_graphviz_dir,pinned_memory_as_cpu_backend,ir_inplace_kernel_blacklist,gpugraph_offload_param_extends,gpugraph_enable_hbm_table_collision_stat,use_cuda_malloc_async_allocator,log_memory_stats,use_stride_kernel,enable_blaslt_global_search,enable_fuse_parallel_matmul_pass,enable_pir_api,allow_cinn_ops,accuracy_check_rtol_fp16,use_system_allocator,gpugraph_enable_gpu_direct_access,check_kernel_launch,enable_neighbor_list_use_uva,selected_gpus,dygraph_debug,manually_trans_conv_filter,enable_cinn_auto_tune,enable_adjust_op_order,cuda_dir,fraction_of_cpu_memory_to_use,print_sub_graph_dir,enable_auto_rdma_trans,fuse_parameter_groups_size,cinn_compile_thread_num,enable_cinn_compile_cache,reader_queue_speed_test_mode,nccl_blocking_wait,jit_engine_type,npu_storage_format,new_executor_sequential_run,prim_skip_dynamic,enable_cinn_accuracy_check,conv2d_disable_cudnn,enable_graph_multi_node_sampling,prim_forward_blacklist,memory_fraction_of_eager_deletion,initial_cpu_memory_in_mb,cusparse_dir,alloc_fill_value,pir_apply_inplace_pass,check_nan_inf_level,use_auto_growth_v2,tensorrt_dir,enable_dump_main_program,low_precision_op_list,op_dir,einsum_opt,use_mkldnn,cublaslt_device_best_config,cudnn_exhaustive_search_times,fast_eager_deletion_mode,fraction_of_gpu_memory_to_use,init_allocated_mem,logging_pir_py_code_dump_symbolic_dims,tensor_operants_mode,mkl_dir,run_kp_kernel,enable_pir_in_executor,trt_ibuilder_cache,cuda_memory_async_pool_realease_threshold,query_dest_rank_by_multi_node,prim_backward,get_host_by_name_time,prim_forward,eager_delete_scope,tracer_profile_fname,cache_inference_while_scope,accuracy_check_atol_bf16,tracer_onednn_ops_on,pir_apply_shape_optimization_pass,cudnn_deterministic,dataloader_use_file_descriptor,use_auto_growth_pinned_allocator,cse_max_count,enable_sparse_inner_gather,prim_enable_dynamic,mklml_dir,nccl_dir,new_executor_serial_run,print_ir,graph_metapath_split_opt,nvidia_package_dir,initial_gpu_memory_in_mb,executor_log_deps_every_microseconds,auto_growth_chunk_size_in_mb,reallocate_gpu_memory_in_mb,save_static_runtime_data,free_when_no_cache_hit,enable_auto_detect_gpu_topo,new_executor_use_inplace,enable_api_kernel_fallback,use_xqa_optim,set_to_1d,gpu_allocator_retry_time 
1884: I0815 02:51:03.329449  6033 init.cc:108] After Parse: argc is 2
1884: I0815 02:51:10.770473  6033 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:51:10.770519  6033 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 02:51:10.771281  6033 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 02:51:10.771741  6033 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 02:51:10.772554  6033 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 02:51:10.772665  6033 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 02:51:10.772758  6033 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 02:51:10.773511  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fb545e00000), and remaining 0
1884: I0815 02:51:10.773833  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.773911  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.774016  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fb545e00200), and remaining 0
1884: I0815 02:51:10.774051  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fb545e00400), and remaining 0
1884: I0815 02:51:10.777801  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fb545e00600), and remaining 0
1884: I0815 02:51:10.777958  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fb545e00800), and remaining 0
1884: I0815 02:51:10.778050  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7fb545e00a00), and remaining 0
1884: I0815 02:51:10.778146  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.778168  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.778237  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.778250  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.779263  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5b5720 for it.
1884: I0815 02:51:10.779428  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.779453  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.779507  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7fb545e00e00), and remaining 0
1884: I0815 02:51:10.779588  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fb545ec4400), and remaining 0
1884: I0815 02:51:10.911458  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5b5720 for it.
1884: I0815 02:51:10.911697  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.911756  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.912459  6033 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7fb53e000000), and remaining 0
1884: I0815 02:51:10.920998  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5b5720 for it.
1884: I0815 02:51:10.921116  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:10.921150  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.921190  6033 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:10.921415  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:10.922410  6033 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 02:51:10.922427  6033 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 02:51:10.922483  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:10.922571  6033 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 02:51:10.922597  6033 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.922659  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:10.922796  6033 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 02:51:10.922816  6033 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.922854  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:10.923105  6033 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 02:51:10.923125  6033 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.923310  6033 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 02:51:10.923336  6033 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:10.923410  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:10.927074  6033 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 02:51:10.927225  6033 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 02:51:10.927253  6033 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 02:51:10.927337  6033 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 02:51:12.305260  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.305331  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.305609  6033 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 02:51:12.305629  6033 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.310595  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.310632  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.311719  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.311738  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.311756  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.312541  6033 program_interpreter.cc:243] New Executor is Running.
1884: I0815 02:51:12.312553  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.312577  6033 scope.cc:202] Create variable feed
1884: I0815 02:51:12.312587  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.312597  6033 scope.cc:202] Create variable fetch
1884: I0815 02:51:12.312602  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.312614  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.312619  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.312623  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.312628  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.315088  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.315444  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.315457  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.315462  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.317135  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.317183  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.317191  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.317198  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.317206  6033 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:51:12.317214  6033 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61ae4320 type is 7
1884: I0815 02:51:12.317219  6033 scope.cc:202] Create variable x
1884: I0815 02:51:12.317222  6033 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x61ae4470 type is 7
1884: I0815 02:51:12.317284  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.317291  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.317296  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.317308  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.317449  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.317476  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.317600  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.317610  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.317625  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.317790  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.317819  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.317839  6033 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.317844  6033 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61aec4c0Variable Type 7
1884: I0815 02:51:12.317865  6033 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.317886  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.317937  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.317956  6033 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.319171  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.319231  6033 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.319635  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.323737  6033 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:51:12.323755  6033 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:51:12.323848  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.323875  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.324384  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: I0815 02:51:12.324456  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.324479  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.324925  6033 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: I0815 02:51:12.324985  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.325006  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.325029  6033 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.325310  6033 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:51:12.325321  6033 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:51:12.325431  6033 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 02:51:12.325455  6033 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.325841  6033 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:51:12.325851  6033 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:51:12.325893  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.325912  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.326094  6033 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 02:51:12.326103  6033 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 02:51:12.326138  6033 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 02:51:12.326155  6033 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 02:51:12.326171  6033 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.328886  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.328907  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.328960  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.328969  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.330873  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.331226  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.331241  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.331245  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.333052  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.333101  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.333110  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.333117  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61b171e0 type is 7
1884: I0815 02:51:12.333124  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.333130  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61b17550 type is 7
1884: I0815 02:51:12.333135  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.333141  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.333197  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.333205  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.333210  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.333214  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.333261  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.333277  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.333343  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.333353  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.333371  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.333606  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.333621  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.333639  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.333647  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61b1dcd0Variable Type 7
1884: I0815 02:51:12.333664  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.333683  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.333706  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.333722  6033 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.334417  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.334447  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.334652  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.345880  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: I0815 02:51:12.346069  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 02:51:12.353494  6033 pir_interpreter.cc:161] PirInterpreter(): 0x61aefaf0 on Place(gpu:0)
1884: I0815 02:51:12.353542  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.353574  6033 scope.cc:202] Create variable 0x61aefaf01723690272353520595_inner_var_1
1884: I0815 02:51:12.353585  6033 scope.cc:202] Create variable 0x61aefaf01723690272353520595_inner_var_2
1884: I0815 02:51:12.353597  6033 scope.cc:202] Create variable 0x61aefaf01723690272353520595_inner_var_3
1884: I0815 02:51:12.353606  6033 scope.cc:202] Create variable 0x61aefaf01723690272353520595_inner_var_4
1884: I0815 02:51:12.353616  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.354028  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.354043  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.354048  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 02:51:12.354091  6033 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61aef9c0
1884: 1 -> 0x61aefaf01723690272353520595_inner_var_1 -> 0x61aefad0
1884: 2 -> 0x61aefaf01723690272353520595_inner_var_2 -> 0x61af0230
1884: 3 -> 0x61aefaf01723690272353520595_inner_var_3 -> 0x61aef360
1884: 4 -> 0x61aefaf01723690272353520595_inner_var_4 -> 0x61abcd40
1884: 5 -> fetch0@fetch -> 0x61abd0c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.354863  6033 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 02:51:12.355131  6071 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.355291  6072 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.355309  6073 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.355445  6075 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.355450  6074 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.355486  6076 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.355475  6073 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61aefaf01723690272353520595_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.355535  6076 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.355584  6076 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 02:51:12.355595  6073 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61aefaf01723690272353520595_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.355633  6076 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61aefaf01723690272353520595_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61aefaf01723690272353520595_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.355825  6076 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61aefaf01723690272353520595_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61aefaf01723690272353520595_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 02:51:12.355897  6073 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61aefaf01723690272353520595_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.355923  6073 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.357139  6073 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61aefaf01723690272353520595_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61aefaf01723690272353520595_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:51:12.357182  6073 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61aefaf01723690272353520595_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.357208  6073 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.357801  6073 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61aefaf01723690272353520595_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:51:12.357841  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x61aefc60) got event_name: TaskCompletion
1884: I0815 02:51:12.357867  6033 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.432341  6071 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 8869556184210671875 to 16320666145216894810 , after update, data is {current : 0, peak : 800768}.
1884: I0815 02:51:12.432379  6071 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8869556184210671875 to 422596952099611063 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 02:51:12.432385  6071 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8869556184210671875 to 422596952099611063 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 02:51:12.432575  6073 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 02:51:12.432590  6073 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 02:51:12.432763  6076 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 02:51:12.432776  6076 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.438141  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.438169  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.438234  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.438241  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.440244  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.440591  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.440605  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.440611  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.442174  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.442266  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.442277  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.442284  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6426e170 type is 7
1884: I0815 02:51:12.442292  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.442296  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x642696a0 type is 7
1884: I0815 02:51:12.442310  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.442317  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.442373  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.442379  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.442384  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.442389  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.442445  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.442461  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.442521  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.442529  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.442544  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.442704  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.442715  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.442729  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.442736  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x642614f0Variable Type 7
1884: I0815 02:51:12.442754  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.442775  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.442795  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.442809  6033 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.444408  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.444442  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.444650  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.451352  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: I0815 02:51:12.451550  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 02:51:12.454955  6033 pir_interpreter.cc:161] PirInterpreter(): 0x641607a0 on Place(gpu:0)
1884: I0815 02:51:12.454990  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.455011  6033 scope.cc:202] Create variable 0x641607a01723690272454981271_inner_var_1
1884: I0815 02:51:12.455022  6033 scope.cc:202] Create variable 0x641607a01723690272454981271_inner_var_2
1884: I0815 02:51:12.455031  6033 scope.cc:202] Create variable 0x641607a01723690272454981271_inner_var_3
1884: I0815 02:51:12.455042  6033 scope.cc:202] Create variable 0x641607a01723690272454981271_inner_var_4
1884: I0815 02:51:12.455050  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.455423  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.455438  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.455442  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61b0d210
1884: 1 -> 0x641607a01723690272454981271_inner_var_1 -> 0x641c8a30
1884: 2 -> 0x641607a01723690272454981271_inner_var_2 -> 0x641c8410
1884: 3 -> 0x641607a01723690272454981271_inner_var_3 -> 0x61b0d420
1884: 4 -> 0x641607a01723690272454981271_inner_var_4 -> 0x64269510
1884: 5 -> fetch0@fetch -> 0x64279040
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.456151  6077 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.456215  6078 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.456246  6079 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.456346  6080 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.456403  6081 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.456455  6082 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.456434  6080 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x641607a01723690272454981271_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.456493  6082 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.456558  6082 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:51:12.456571  6080 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x641607a01723690272454981271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.456614  6082 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x641607a01723690272454981271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x641607a01723690272454981271_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.456755  6082 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x641607a01723690272454981271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x641607a01723690272454981271_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.456813  6080 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x641607a01723690272454981271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.456840  6080 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.459703  6080 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x641607a01723690272454981271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x641607a01723690272454981271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.459753  6080 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x641607a01723690272454981271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.459776  6080 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.461871  6080 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x641607a01723690272454981271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.461922  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x64160910) got event_name: TaskCompletion
1884: I0815 02:51:12.461949  6033 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.499986  6077 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 16320666145216894810 to 422596952099611063 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 02:51:12.500006  6077 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 16320666145216894810 to 8256796732245392036 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 02:51:12.500011  6077 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 16320666145216894810 to 8256796732245392036 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 02:51:12.500177  6080 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 8256796732245392036 to 3797982842794208457 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 02:51:12.500187  6080 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 8256796732245392036 to 3797982842794208457 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 02:51:12.500368  6082 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.504338  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.504364  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.504416  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.504424  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.506096  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.506431  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.506443  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.506448  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.507979  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.508072  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.508083  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.508090  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x642f6d50 type is 7
1884: I0815 02:51:12.508096  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.508101  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61b0f2e0 type is 7
1884: I0815 02:51:12.508106  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.508111  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.508162  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.508169  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.508173  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.508177  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.508226  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.508240  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.508298  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.508318  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.508333  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.508374  6033 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.508510  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.508574  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.508584  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.508599  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.508606  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64100a70Variable Type 7
1884: I0815 02:51:12.508622  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.508641  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.508662  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.508677  6033 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.508947  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.508968  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.509150  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.509922  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: I0815 02:51:12.510097  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 02:51:12.513098  6033 pir_interpreter.cc:161] PirInterpreter(): 0x4d943870 on Place(gpu:0)
1884: I0815 02:51:12.513129  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.513151  6033 scope.cc:202] Create variable 0x4d9438701723690272513121917_inner_var_1
1884: I0815 02:51:12.513161  6033 scope.cc:202] Create variable 0x4d9438701723690272513121917_inner_var_2
1884: I0815 02:51:12.513170  6033 scope.cc:202] Create variable 0x4d9438701723690272513121917_inner_var_3
1884: I0815 02:51:12.513181  6033 scope.cc:202] Create variable 0x4d9438701723690272513121917_inner_var_4
1884: I0815 02:51:12.513193  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.513535  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.513551  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.513556  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6440efa0
1884: 1 -> 0x4d9438701723690272513121917_inner_var_1 -> 0x6440f020
1884: 2 -> 0x4d9438701723690272513121917_inner_var_2 -> 0x6427d1a0
1884: 3 -> 0x4d9438701723690272513121917_inner_var_3 -> 0x6426e8e0
1884: 4 -> 0x4d9438701723690272513121917_inner_var_4 -> 0x6427d6c0
1884: 5 -> fetch0@fetch -> 0x64411120
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.514226  6083 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.514295  6084 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.514328  6085 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.514370  6086 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.514393  6087 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.514433  6088 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.514431  6087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d9438701723690272513121917_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.514456  6088 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.514489  6088 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 02:51:12.514492  6087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d9438701723690272513121917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.514525  6088 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d9438701723690272513121917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4d9438701723690272513121917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.514567  6088 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.514675  6088 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.514700  6088 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d9438701723690272513121917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4d9438701723690272513121917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 02:51:12.514758  6087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d9438701723690272513121917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.514779  6087 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.515090  6087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d9438701723690272513121917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x4d9438701723690272513121917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:51:12.515121  6087 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4d9438701723690272513121917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.515139  6087 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.515152  6087 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4d9438701723690272513121917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:51:12.515180  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x4d9439e0) got event_name: TaskCompletion
1884: I0815 02:51:12.515200  6033 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.547596  6083 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 422596952099611063 to 16320666145216894810 , after update, data is {current : 0, peak : 3328}.
1884: I0815 02:51:12.547619  6083 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 422596952099611063 to 16320666145216894810 , after update, data is {current : -804, peak : 2000}.
1884: I0815 02:51:12.547626  6083 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 422596952099611063 to 16320666145216894810 , after update, data is {current : -804, peak : 2000}.
1884: I0815 02:51:12.547797  6087 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 518810095190034572 to 16320666145216894810 , after update, data is {current : 800, peak : 2000}.
1884: I0815 02:51:12.547807  6087 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 518810095190034572 to 16320666145216894810 , after update, data is {current : 800, peak : 2000}.
1884: I0815 02:51:12.547976  6088 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 02:51:12.547987  6088 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 02:51:12.547992  6088 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.552163  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.552186  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.552235  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.552243  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.553881  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.554206  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.554219  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.554224  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.555742  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.555821  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.555831  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.555837  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61b0b6a0 type is 7
1884: I0815 02:51:12.555845  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.555847  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61b0b970 type is 7
1884: I0815 02:51:12.555851  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.555855  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.555905  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.555912  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.555915  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.555919  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.555965  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.555979  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.556032  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.556041  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.556054  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.556272  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.556285  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.556309  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.556316  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61ad1cc0Variable Type 7
1884: I0815 02:51:12.556332  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.556350  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.556371  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.556385  6033 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.557093  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.557121  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.557317  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.559659  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: I0815 02:51:12.559828  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 02:51:12.562870  6033 pir_interpreter.cc:161] PirInterpreter(): 0x61ac6090 on Place(gpu:0)
1884: I0815 02:51:12.562903  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.562925  6033 scope.cc:202] Create variable 0x61ac60901723690272562895571_inner_var_1
1884: I0815 02:51:12.562937  6033 scope.cc:202] Create variable 0x61ac60901723690272562895571_inner_var_2
1884: I0815 02:51:12.562947  6033 scope.cc:202] Create variable 0x61ac60901723690272562895571_inner_var_3
1884: I0815 02:51:12.562958  6033 scope.cc:202] Create variable 0x61ac60901723690272562895571_inner_var_4
1884: I0815 02:51:12.562968  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.563298  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.563324  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.563328  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x64268ed0
1884: 1 -> 0x61ac60901723690272562895571_inner_var_1 -> 0x61b0b7d0
1884: 2 -> 0x61ac60901723690272562895571_inner_var_2 -> 0x641c7380
1884: 3 -> 0x61ac60901723690272562895571_inner_var_3 -> 0x64268090
1884: 4 -> 0x61ac60901723690272562895571_inner_var_4 -> 0x63992a00
1884: 5 -> fetch0@fetch -> 0x44ea5c80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.563993  6089 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.564078  6090 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.564102  6091 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.564185  6092 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.564183  6093 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.564217  6094 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.564213  6092 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61ac60901723690272562895571_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.564246  6094 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.564291  6092 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61ac60901723690272562895571_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.564294  6094 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 02:51:12.564355  6094 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61ac60901723690272562895571_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61ac60901723690272562895571_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.564612  6094 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61ac60901723690272562895571_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61ac60901723690272562895571_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 02:51:12.564679  6092 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ac60901723690272562895571_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.564705  6092 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.565961  6092 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61ac60901723690272562895571_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61ac60901723690272562895571_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:51:12.566002  6092 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61ac60901723690272562895571_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.566025  6092 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.566653  6092 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61ac60901723690272562895571_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 02:51:12.566699  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x61ac6200) got event_name: TaskCompletion
1884: I0815 02:51:12.566721  6033 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.602385  6089 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 16320666145216894810 to 422596952099611063 , after update, data is {current : 0, peak : 800768}.
1884: I0815 02:51:12.602399  6089 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 16320666145216894810 to 8256796732245392036 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 02:51:12.602406  6089 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 16320666145216894810 to 8256796732245392036 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 02:51:12.602588  6092 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8256796732245392036 to 3797982842794208457 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 02:51:12.602612  6092 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8256796732245392036 to 3797982842794208457 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 02:51:12.602777  6094 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 02:51:12.602788  6094 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.607550  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.607575  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.607626  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.607633  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.609333  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.609658  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.609673  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.609676  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.611193  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.611280  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.611290  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.611296  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63a000e0 type is 7
1884: I0815 02:51:12.611315  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.611317  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6426db10 type is 7
1884: I0815 02:51:12.611322  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.611326  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.611379  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.611385  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.611389  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.611393  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.611440  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.611454  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.611512  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.611521  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.611534  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.611671  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.611682  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.611696  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.611702  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63736ce0Variable Type 7
1884: I0815 02:51:12.611717  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.611735  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.611754  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.611769  6033 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.613433  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.613471  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.613672  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.620406  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: I0815 02:51:12.620594  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 02:51:12.623715  6033 pir_interpreter.cc:161] PirInterpreter(): 0x4d943870 on Place(gpu:0)
1884: I0815 02:51:12.623749  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.623772  6033 scope.cc:202] Create variable 0x4d9438701723690272623741021_inner_var_1
1884: I0815 02:51:12.623783  6033 scope.cc:202] Create variable 0x4d9438701723690272623741021_inner_var_2
1884: I0815 02:51:12.623795  6033 scope.cc:202] Create variable 0x4d9438701723690272623741021_inner_var_3
1884: I0815 02:51:12.623805  6033 scope.cc:202] Create variable 0x4d9438701723690272623741021_inner_var_4
1884: I0815 02:51:12.623814  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.624153  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.624169  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.624172  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6386eea0
1884: 1 -> 0x4d9438701723690272623741021_inner_var_1 -> 0x6386ef20
1884: 2 -> 0x4d9438701723690272623741021_inner_var_2 -> 0x61ac5ce0
1884: 3 -> 0x4d9438701723690272623741021_inner_var_3 -> 0x63736d40
1884: 4 -> 0x4d9438701723690272623741021_inner_var_4 -> 0x63a00c20
1884: 5 -> fetch0@fetch -> 0x64268860
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.624863  6095 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.624943  6096 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.624972  6097 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.625012  6098 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.625049  6099 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.625090  6100 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.625087  6099 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d9438701723690272623741021_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.625113  6100 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.625141  6100 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:51:12.625142  6099 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4d9438701723690272623741021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.625173  6100 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d9438701723690272623741021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4d9438701723690272623741021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.625279  6100 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4d9438701723690272623741021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4d9438701723690272623741021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.625339  6099 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d9438701723690272623741021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.625363  6099 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.628217  6099 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4d9438701723690272623741021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x4d9438701723690272623741021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.628265  6099 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4d9438701723690272623741021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.628288  6099 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.630380  6099 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4d9438701723690272623741021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 02:51:12.630429  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x4d9439e0) got event_name: TaskCompletion
1884: I0815 02:51:12.630451  6033 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.674613  6095 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 422596952099611063 to 16320666145216894810 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 02:51:12.674626  6095 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 422596952099611063 to 518810095190034572 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 02:51:12.674633  6095 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 422596952099611063 to 518810095190034572 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 02:51:12.674785  6099 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 518810095190034572 to 3797982842794208457 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 02:51:12.674796  6099 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 518810095190034572 to 3797982842794208457 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 02:51:12.674970  6100 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 16320666145216894810 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.679087  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.679117  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.679170  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.679178  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.680905  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.681241  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.681254  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.681258  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.682788  6033 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 02:51:12.682878  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.682888  6033 scope.cc:202] Create variable Out
1884: I0815 02:51:12.682893  6033 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6384f4d0 type is 7
1884: I0815 02:51:12.682904  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.682906  6033 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x642770f0 type is 7
1884: I0815 02:51:12.682910  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.682915  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.682966  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.682973  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.682977  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.682981  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.683029  6033 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.683043  6033 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.683099  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.683107  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.683121  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.683161  6033 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.683279  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.683341  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.683352  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.683367  6033 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.683374  6033 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6473dcc0Variable Type 7
1884: I0815 02:51:12.683389  6033 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.683408  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.683429  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.683444  6033 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.683568  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.683590  6033 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.683777  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.684568  6033 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1a5f9350 for it.
1884: I0815 02:51:12.684746  6033 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1a59eb30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 02:51:12.687796  6033 pir_interpreter.cc:161] PirInterpreter(): 0x64614080 on Place(gpu:0)
1884: I0815 02:51:12.687829  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.687852  6033 scope.cc:202] Create variable 0x646140801723690272687821864_inner_var_1
1884: I0815 02:51:12.687863  6033 scope.cc:202] Create variable 0x646140801723690272687821864_inner_var_2
1884: I0815 02:51:12.687875  6033 scope.cc:202] Create variable 0x646140801723690272687821864_inner_var_3
1884: I0815 02:51:12.687885  6033 scope.cc:202] Create variable 0x646140801723690272687821864_inner_var_4
1884: I0815 02:51:12.687896  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.688220  6033 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 02:51:12.688234  6033 scope.cc:202] Create variable X
1884: I0815 02:51:12.688238  6033 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x64735c00
1884: 1 -> 0x646140801723690272687821864_inner_var_1 -> 0x64735c80
1884: 2 -> 0x646140801723690272687821864_inner_var_2 -> 0x64276e90
1884: 3 -> 0x646140801723690272687821864_inner_var_3 -> 0x64277040
1884: 4 -> 0x646140801723690272687821864_inner_var_4 -> 0x4d943d20
1884: 5 -> fetch0@fetch -> 0x61adae70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 02:51:12.688923  6101 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.689004  6102 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.689028  6103 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.689081  6104 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.689105  6105 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.689150  6106 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.689144  6105 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x646140801723690272687821864_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.689172  6106 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.689186  6105 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x646140801723690272687821864_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.689203  6106 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 02:51:12.689235  6106 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x646140801723690272687821864_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x646140801723690272687821864_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.689278  6106 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.689409  6106 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.689438  6106 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x646140801723690272687821864_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x646140801723690272687821864_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 02:51:12.689491  6105 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x646140801723690272687821864_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.689512  6105 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.689697  6105 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x646140801723690272687821864_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x646140801723690272687821864_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:51:12.689718  6105 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x646140801723690272687821864_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.689738  6105 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.689749  6105 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x646140801723690272687821864_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 02:51:12.689785  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x646141f0) got event_name: TaskCompletion
1884: I0815 02:51:12.689810  6033 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.722723  6101 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 16320666145216894810 to 422596952099611063 , after update, data is {current : 0, peak : 10240}.
1884: I0815 02:51:12.722736  6101 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 16320666145216894810 to 422596952099611063 , after update, data is {current : -804, peak : 8000}.
1884: I0815 02:51:12.722741  6101 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 16320666145216894810 to 422596952099611063 , after update, data is {current : -804, peak : 8000}.
1884: I0815 02:51:12.722925  6105 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 8004638495335938813 to 422596952099611063 , after update, data is {current : 800, peak : 8000}.
1884: I0815 02:51:12.722934  6105 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 8004638495335938813 to 422596952099611063 , after update, data is {current : 800, peak : 8000}.
1884: I0815 02:51:12.723125  6106 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 02:51:12.723142  6106 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 02:51:12.723147  6106 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 422596952099611063 to 3797982842794208457 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 02:51:12.729666  6033 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 02:51:12.729722  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 02:51:12.730829  6033 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:51:12.731698  6033 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 02:51:12.731729  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 02:51:12.733050  6033 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 02:51:12.733075  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:51:12.733774  6033 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 02:51:12.734791  6033 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 02:51:12.734818  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:51:12.736176  6033 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 02:51:12.736199  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:51:12.736804  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.736832  6033 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 02:51:12.736838  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.736845  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.738912  6033 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 02:51:12.738940  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:51:12.739939  6033 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 02:51:12.739969  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 02:51:12.740967  6033 pybind.cc:1827] need skip: 0
1884: I0815 02:51:12.741274  6033 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:51:12.743095  6033 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 02:51:12.746830  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.746850  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.746855  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.748888  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.748909  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.748920  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.748925  6033 scope.cc:202] Create variable learning_rate_0
1884: I0815 02:51:12.748934  6033 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x64262ac0 type is 7
1884: I0815 02:51:12.748937  6033 scope.cc:202] Create variable linear_0.b_0
1884: I0815 02:51:12.748940  6033 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x633ca570 type is 7
1884: I0815 02:51:12.748945  6033 scope.cc:202] Create variable linear_0.w_0
1884: I0815 02:51:12.748948  6033 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x633ca620 type is 7
1884: I0815 02:51:12.749011  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.749017  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.749022  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.749024  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.749078  6033 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749091  6033 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749114  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 02:51:12.749259  6033 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749270  6033 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749354  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.749400  6033 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749409  6033 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.749436  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.750448  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.751798  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.752247  6033 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 02:51:12.752486  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.752781  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.752996  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.753012  6033 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:51:12.753077  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.753083  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.753087  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.753191  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.753203  6033 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:51:12.754789  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.756151  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.757261  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.757463  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.757476  6033 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 02:51:12.757484  6033 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x64148f20 type is 7
1884: I0815 02:51:12.757491  6033 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:51:12.757496  6033 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x64148ca0 type is 7
1884: I0815 02:51:12.757501  6033 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 02:51:12.757505  6033 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x64148d90 type is 7
1884: I0815 02:51:12.757509  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.757514  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.757519  6033 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 02:51:12.757521  6033 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x6414a360 type is 7
1884: I0815 02:51:12.757525  6033 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x64262ac0 type is 7
1884: I0815 02:51:12.757529  6033 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x633ca570 type is 7
1884: I0815 02:51:12.757534  6033 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 02:51:12.757536  6033 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x6414a340 type is 7
1884: I0815 02:51:12.757540  6033 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 02:51:12.757544  6033 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x6414a8a0 type is 7
1884: I0815 02:51:12.757546  6033 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x633ca620 type is 7
1884: I0815 02:51:12.757550  6033 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 02:51:12.757553  6033 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x6414ab10 type is 7
1884: I0815 02:51:12.757557  6033 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 02:51:12.757560  6033 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x6414ad50 type is 7
1884: I0815 02:51:12.757565  6033 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:51:12.757568  6033 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6414afb0 type is 7
1884: I0815 02:51:12.757651  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.757666  6033 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:51:12.757726  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.757732  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.757737  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.757740  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.757790  6033 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.757802  6033 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.757819  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 02:51:12.757951  6033 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.757961  6033 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.757982  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:51:12.758059  6033 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:51:12.758193  6033 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 02:51:12.759325  6033 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759346  6033 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759409  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.759471  6033 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759482  6033 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759500  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:51:12.759524  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.759567  6033 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759575  6033 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759588  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:51:12.759675  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759685  6033 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759697  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.759809  6033 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.759888  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.759946  6033 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759956  6033 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.759971  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:51:12.760007  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.760061  6033 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760071  6033 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760083  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 02:51:12.760192  6033 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760202  6033 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760221  6033 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.760265  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.760273  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.760288  6033 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.760294  6033 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x633a10e0Variable Type 7
1884: I0815 02:51:12.760320  6033 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.760339  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.760357  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760370  6033 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.760409  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.760432  6033 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 02:51:12.760459  6033 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.760468  6033 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.760480  6033 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 02:51:12.760486  6033 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x633a1ba0Variable Type 7
1884: I0815 02:51:12.760499  6033 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 02:51:12.760510  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.760525  6033 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.760537  6033 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.760569  6033 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 02:51:12.760583  6033 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 02:51:12.761003  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 02:51:12.761037  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:51:12.761055  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:51:12.761088  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 02:51:12.761121  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.761138  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 02:51:12.765228  6033 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 02:51:12.765264  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:51:12.765974  6033 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 02:51:12.765998  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:51:12.766326  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.768018  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.768859  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.768980  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.769516  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.770416  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.772540  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.773602  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.775475  6033 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 02:51:12.776209  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.776225  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.776229  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.777434  6033 interpreter_util.cc:1169] Creating Variables
1884: I0815 02:51:12.777452  6033 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4f39e60 type is 9
1884: I0815 02:51:12.777459  6033 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4f50100 type is 10
1884: I0815 02:51:12.777465  6033 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x633ca570 type is 7
1884: I0815 02:51:12.777469  6033 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x633ca620 type is 7
1884: I0815 02:51:12.777474  6033 scope.cc:202] Create variable saved_params
1884: I0815 02:51:12.777477  6033 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x633dfc40 type is 17
1884: I0815 02:51:12.777506  6033 interpreter_util.cc:594] Static build: 0
1884: I0815 02:51:12.777513  6033 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 02:51:12.777516  6033 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 02:51:12.777519  6033 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 02:51:12.777558  6033 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.777570  6033 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 02:51:12.778319  6033 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 02:51:12.778362  6033 analysis_predictor.cc:433] Predictor::init()
1884: I0815 02:51:12.778419  6033 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 02:51:12.779659  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.779716  6033 scope.cc:202] Create variable feed
1884: I0815 02:51:12.779724  6033 naive_executor.cc:189] 0x64635c60 Create persistable variable feed, which pointer is 0x6430c250
1884: I0815 02:51:12.779729  6033 scope.cc:202] Create variable fetch
1884: I0815 02:51:12.779732  6033 naive_executor.cc:189] 0x64635c60 Create persistable variable fetch, which pointer is 0x6430c0f0
1884: I0815 02:51:12.779735  6033 scope.cc:202] Create variable linear_0.b_0
1884: I0815 02:51:12.779739  6033 naive_executor.cc:189] 0x64635c60 Create persistable variable linear_0.b_0, which pointer is 0x6430adb0
1884: I0815 02:51:12.779743  6033 scope.cc:202] Create variable linear_0.w_0
1884: I0815 02:51:12.779747  6033 naive_executor.cc:189] 0x64635c60 Create persistable variable linear_0.w_0, which pointer is 0x63389240
1884: I0815 02:51:12.779762  6033 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 02:51:12.780107  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.780205  6033 program_converter.cc:296] is_legacy_program : 0
1884: I0815 02:51:12.780258  6033 executor.cc:183] Old Executor is Running.
1884: I0815 02:51:12.780345  6033 executor.cc:92] Creating Variables for block 0
1884: I0815 02:51:12.780354  6033 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 02:51:12.780357  6033 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x6430adb0 type is 7
1884: I0815 02:51:12.780361  6033 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 02:51:12.780364  6033 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x63389240 type is 7
1884: I0815 02:51:12.780397  6033 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.780473  6033 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 02:51:12.780517  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.780524  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:12.780658  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.780766  6033 graph.cc:149] create OpNode by feed
1884: I0815 02:51:12.780803  6033 graph.cc:149] create OpNode by matmul_v2
1884: I0815 02:51:12.780818  6033 graph.cc:149] create OpNode by elementwise_add
1884: I0815 02:51:12.780833  6033 graph.cc:149] create OpNode by abs
1884: I0815 02:51:12.780843  6033 graph.cc:149] create OpNode by assign_value
1884: I0815 02:51:12.780862  6033 graph.cc:149] create OpNode by multinomial
1884: I0815 02:51:12.780872  6033 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 02:51:12.780887  6033 graph.cc:149] create OpNode by scale
1884: I0815 02:51:12.780900  6033 graph.cc:149] create OpNode by scale
1884: I0815 02:51:12.780911  6033 graph.cc:149] create OpNode by fetch
1884: I0815 02:51:12.780927  6033 graph.cc:149] create OpNode by fetch
1884: I0815 02:51:12.780948  6033 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 02:51:12.782298  6033 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 02:51:12.782337  6033 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 02:51:12.782413  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.782420  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 02:51:12.782531  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.782783  6033 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 02:51:12.782845  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.782851  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 02:51:12.782886  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.782891  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 02:51:12.782929  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.782989  6033 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:51:12.783020  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783025  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 02:51:12.783041  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783054  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783077  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783082  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 02:51:12.783120  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783141  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783165  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783170  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 02:51:12.783212  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783289  6033 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:51:12.783326  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783331  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 02:51:12.783363  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783382  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783404  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783409  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 02:51:12.783439  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783593  6033 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 02:51:12.783622  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783627  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 02:51:12.783655  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783671  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783694  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783699  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 02:51:12.783720  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783733  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783756  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783761  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 02:51:12.783782  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783795  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.783816  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.783821  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 02:51:12.783845  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.783913  6033 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 02:51:12.783943  6033 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:51:12.783958  6033 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:51:12.783972  6033 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 02:51:12.783995  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.784000  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 02:51:12.784022  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.784063  6033 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 02:51:12.784083  6033 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:51:12.784096  6033 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:51:12.784107  6033 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:51:12.784138  6033 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 02:51:12.784150  6033 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 02:51:12.785400  6033 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:51:12.785449  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.785455  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 02:51:12.785483  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.785503  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.785530  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.785535  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 02:51:12.785558  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.785607  6033 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 02:51:12.785638  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.785643  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 02:51:12.785660  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.785676  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.785697  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.785702  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 02:51:12.785734  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.785820  6033 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 02:51:12.785840  6033 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:51:12.785856  6033 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:51:12.785869  6033 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:51:12.785885  6033 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 02:51:12.785900  6033 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 02:51:12.785915  6033 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 02:51:12.785938  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786007  6033 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 02:51:12.786027  6033 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 02:51:12.786041  6033 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 02:51:12.786054  6033 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 02:51:12.786067  6033 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 02:51:12.786082  6033 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 02:51:12.786095  6033 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 02:51:12.786139  6033 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 02:51:12.786429  6033 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:51:12.786461  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.786466  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 02:51:12.786513  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786572  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786607  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786651  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786679  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786720  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786744  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786782  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786801  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786834  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786852  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786882  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786898  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786924  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786937  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786960  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.786970  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.786989  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787014  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787019  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 02:51:12.787045  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.787086  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787112  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787117  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 02:51:12.787127  6033 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 02:51:12.787129  6033 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:51:12.787178  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.787199  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787225  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787230  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 02:51:12.787238  6033 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 02:51:12.787241  6033 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:51:12.787281  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.787323  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787349  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787354  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 02:51:12.787364  6033 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 02:51:12.787365  6033 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:51:12.787398  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.787416  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787438  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787443  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 02:51:12.787451  6033 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 02:51:12.787454  6033 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 02:51:12.787492  6033 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 02:51:12.787511  6033 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 02:51:12.787534  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787539  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 02:51:12.787551  6033 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 02:51:12.787595  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787600  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 02:51:12.787671  6033 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:51:12.787693  6033 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.787709  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 02:51:12.787765  6033 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 02:51:12.787781  6033 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 02:51:12.787809  6033 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 02:51:12.787832  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.787837  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 02:51:12.788753  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.788767  6033 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 02:51:12.788820  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.788825  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:12.789438  6033 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 02:51:12.789647  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.789718  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.789724  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:12.790129  6033 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 02:51:12.790351  6033 graph.h:183] deleting __fuse_statis__
1884: I0815 02:51:12.790360  6033 graph.h:183] deleting pass_recorder
1884: I0815 02:51:12.790365  6033 graph.h:183] deleting stale_program_op_descs
1884: I0815 02:51:12.790449  6033 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 02:51:12.790459  6033 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 02:51:12.790465  6033 naive_executor.cc:195] 0x64635c60 Create variable abs_0.tmp_0, which pointer is 0x64109310
1884: I0815 02:51:12.790472  6033 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 02:51:12.790474  6033 naive_executor.cc:195] 0x64635c60 Create variable gaussian_0.tmp_0, which pointer is 0x633a9890
1884: I0815 02:51:12.790486  6033 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 02:51:12.790491  6033 naive_executor.cc:195] 0x64635c60 Create variable linear_0.tmp_1, which pointer is 0x639bb560
1884: I0815 02:51:12.790495  6033 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 02:51:12.790498  6033 naive_executor.cc:195] 0x64635c60 Create variable multinomial_0.tmp_0, which pointer is 0x639bb000
1884: I0815 02:51:12.790501  6033 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 02:51:12.790504  6033 naive_executor.cc:195] 0x64635c60 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x639bb300
1884: I0815 02:51:12.790508  6033 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 02:51:12.790510  6033 naive_executor.cc:195] 0x64635c60 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x639b9900
1884: I0815 02:51:12.790515  6033 scope.cc:202] Create variable feed
1884: I0815 02:51:12.790519  6033 scope.cc:202] Create variable fetch
1884: I0815 02:51:12.790539  6033 naive_executor.cc:46] NaiveExecutor init with scope 0x64635c60
1884: I0815 02:51:12.790545  6033 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 02:51:12.790737  6033 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 02:51:12.790752  6033 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 02:51:12.790781  6033 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 02:51:12.790786  6033 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 02:51:12.790793  6033 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.790827  6033 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 02:51:12.791036  6033 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.791052  6033 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 02:51:12.791100  6033 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.791126  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 02:51:12.833103  6033 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:51:12.833211  6033 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.833245  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 02:51:12.833320  6033 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:51:12.833353  6033 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.833381  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 02:51:12.833462  6033 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 02:51:12.833508  6033 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.833524  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:51:12.833580  6033 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 02:51:12.833611  6033 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 02:51:12.833626  6033 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 02:51:12.833662  6033 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 02:51:12.833679  6033 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.833712  6033 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 02:51:12.833737  6033 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 02:51:12.834228  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.834239  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 02:51:12.882339  6033 pir_interpreter.cc:161] PirInterpreter(): 0x6425f440 on Place(gpu:0)
1884: I0815 02:51:12.882385  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_0
1884: I0815 02:51:12.882404  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_1
1884: I0815 02:51:12.882416  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_2
1884: I0815 02:51:12.882426  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_3
1884: I0815 02:51:12.882458  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_4
1884: I0815 02:51:12.882472  6033 scope.cc:202] Create variable 0x6425f4401723690272882371107_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x6425f4401723690272882371107_inner_var_0 -> 0x2f1e530
1884: 1 -> 0x6425f4401723690272882371107_inner_var_1 -> 0x646190c0
1884: 2 -> 0x6425f4401723690272882371107_inner_var_2 -> 0x61ae5c20
1884: 3 -> linear_1.w_0 -> 0x64616d00
1884: 4 -> linear_1.b_0 -> 0x45892230
1884: 5 -> learning_rate_1 -> 0x633981a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 02:51:12.883297  6107 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.883314  6108 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.883373  6109 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.883375  6110 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.883416  6110 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883410  6109 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883445  6111 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.883425  6108 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883474  6111 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883493  6110 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.883499  6108 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.883494  6109 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6425f4401723690272882371107_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:51:12.883527  6111 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.883569  6111 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 02:51:12.883590  6111 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883606  6111 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.883618  6111 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:51:12.883632  6111 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6425f4401723690272882371107_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6425f4401723690272882371107_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6425f4401723690272882371107_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.883695  6111 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6425f4401723690272882371107_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6425f4401723690272882371107_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6425f4401723690272882371107_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 02:51:12.883749  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x6425f5b0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 02:51:12.885793  6033 pir_interpreter.cc:161] PirInterpreter(): 0x63ab5910 on Place(gpu:0)
1884: I0815 02:51:12.885828  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_1
1884: I0815 02:51:12.885844  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_4
1884: I0815 02:51:12.885854  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_5
1884: I0815 02:51:12.885860  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_6
1884: I0815 02:51:12.885885  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_7
1884: I0815 02:51:12.885895  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_8
1884: I0815 02:51:12.885905  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_9
1884: I0815 02:51:12.885931  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_10
1884: I0815 02:51:12.885941  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_11
1884: I0815 02:51:12.885948  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_12
1884: I0815 02:51:12.885957  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_13
1884: I0815 02:51:12.885965  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_14
1884: I0815 02:51:12.885975  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_15
1884: I0815 02:51:12.885982  6033 scope.cc:202] Create variable fetch0@fetch
1884: I0815 02:51:12.885993  6033 scope.cc:202] Create variable 0x63ab59101723690272885815431_inner_var_17
1884: I0815 02:51:12.886003  6033 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x633981a0
1884: 1 -> 0x63ab59101723690272885815431_inner_var_1 -> 0x61ad68c0
1884: 2 -> linear_1.b_0 -> 0x45892230
1884: 3 -> linear_1.w_0 -> 0x64616d00
1884: 4 -> 0x63ab59101723690272885815431_inner_var_4 -> 0x44d0d060
1884: 5 -> 0x63ab59101723690272885815431_inner_var_5 -> 0x641018b0
1884: 6 -> 0x63ab59101723690272885815431_inner_var_6 -> 0x61affe10
1884: 7 -> 0x63ab59101723690272885815431_inner_var_7 -> 0x6442dcb0
1884: 8 -> 0x63ab59101723690272885815431_inner_var_8 -> 0x6442dcf0
1884: 9 -> 0x63ab59101723690272885815431_inner_var_9 -> 0x637489b0
1884: 10 -> 0x63ab59101723690272885815431_inner_var_10 -> 0x63747720
1884: 11 -> 0x63ab59101723690272885815431_inner_var_11 -> 0x64279300
1884: 12 -> 0x63ab59101723690272885815431_inner_var_12 -> 0x646195e0
1884: 13 -> 0x63ab59101723690272885815431_inner_var_13 -> 0x61ae42c0
1884: 14 -> 0x63ab59101723690272885815431_inner_var_14 -> 0x61ac67d0
1884: 15 -> 0x63ab59101723690272885815431_inner_var_15 -> 0x61adf3b0
1884: 16 -> fetch0@fetch -> 0x44ee1b60
1884: 17 -> 0x63ab59101723690272885815431_inner_var_17 -> 0x639b3690
1884: 18 -> fetch1@fetch -> 0x641c80d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 02:51:12.887660  6112 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.887779  6113 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.887795  6114 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.887888  6115 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.887905  6114 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.887910  6113 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.887919  6116 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 02:51:12.887938  6114 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.887951  6113 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:51:12.887970  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.887998  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:51:12.888025  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888067  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.888154  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:51:12.888172  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:51:12.888234  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 02:51:12.888252  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63ab59101723690272885815431_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888296  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x63ab59101723690272885815431_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 02:51:12.888329  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63ab59101723690272885815431_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888365  6116 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:51:12.888402  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x63ab59101723690272885815431_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.888430  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63ab59101723690272885815431_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63ab59101723690272885815431_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888468  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.888484  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x63ab59101723690272885815431_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x63ab59101723690272885815431_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.888513  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x63ab59101723690272885815431_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888533  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.888535  6113 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ab59101723690272885815431_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888545  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x63ab59101723690272885815431_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.888561  6113 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.888561  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63ab59101723690272885815431_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888583  6116 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.888638  6113 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ab59101723690272885815431_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.888667  6113 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63ab59101723690272885815431_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888682  6116 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.888686  6113 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.888701  6113 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63ab59101723690272885815431_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.888736  6116 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.888801  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.888821  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63ab59101723690272885815431_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x63ab59101723690272885815431_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.888854  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888859  6113 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888873  6113 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 02:51:12.888880  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.888896  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.888906  6113 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63ab59101723690272885815431_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.888914  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x63ab59101723690272885815431_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888923  6113 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63ab59101723690272885815431_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.888936  6113 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.888948  6113 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63ab59101723690272885815431_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.888988  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x63ab59101723690272885815431_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:51:12.889007  6116 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63ab59101723690272885815431_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab59101723690272885815431_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.889034  6116 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 02:51:12.889046  6116 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x63ab59101723690272885815431_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63ab59101723690272885815431_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x63ab59101723690272885815431_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 02:51:12.889081  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x63ab5a80) got event_name: TaskCompletion
1884: I0815 02:51:12.889108  6033 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.889139  6033 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 02:51:12.894795  6033 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 02:51:12.894842  6033 analysis_predictor.cc:433] Predictor::init()
1884: I0815 02:51:12.895519  6033 scope.cc:202] Create variable linear_1.b_0
1884: I0815 02:51:12.895568  6033 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 02:51:12.896016  6033 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728960671270"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728960671270"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 02:51:12.896195  6033 pir_interpreter.cc:161] PirInterpreter(): 0x634c4030 on Place(cpu)
1884: I0815 02:51:12.896216  6033 scope.cc:202] Create variable 0x634c40301723690272896209665_inner_var_0
1884: I0815 02:51:12.896243  6033 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728960671270"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236902728960671270 -> 0x633de440
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 02:51:12.896391  6033 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 02:51:12.896631  6118 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.896652  6119 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.896703  6120 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.896703  6118 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236902728960671270:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.896745  6118 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236902728960671270:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 02:51:12.896770  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x634c41a0) got event_name: TaskCompletion
1884: I0815 02:51:12.896811  6121 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.897678  6117 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.897950  6118 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 882913286771405229 to 5906900807252820571 , after update, data is {current : -4, peak : 104}.
1884: I0815 02:51:12.897960  6118 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 882913286771405229 to 5906900807252820571 , after update, data is {current : -4, peak : 104}.
1884: I0815 02:51:12.898020  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.898027  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728980974011"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728980974011"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 02:51:12.898237  6033 pir_interpreter.cc:161] PirInterpreter(): 0x634c4030 on Place(cpu)
1884: I0815 02:51:12.898258  6033 scope.cc:202] Create variable 0x634c40301723690272898251457_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902728980974011"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236902728980974011 -> 0x5c9ec390
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 02:51:12.898478  6122 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.898546  6123 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.898593  6124 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.898597  6125 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.898622  6124 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236902728980974011:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.898689  6124 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236902728980974011:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.898715  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x634c41a0) got event_name: TaskCompletion
1884: I0815 02:51:12.898746  6126 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.898965  6124 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1138612386297659002 to 5906900807252820571 , after update, data is {current : 4, peak : 104}.
1884: I0815 02:51:12.898975  6124 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1138612386297659002 to 5906900807252820571 , after update, data is {current : 4, peak : 104}.
1884: I0815 02:51:12.899027  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.899034  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236902728980974011",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236902728991098372"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236902728980974011",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236902728991098372"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 02:51:12.899259  6033 pir_interpreter.cc:161] PirInterpreter(): 0x634c4030 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236902728980974011",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236902728991098372"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236902728991098372 -> 0x5c9ec390
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 02:51:12.899534  6127 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.899603  6128 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.899608  6129 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.899650  6130 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.899663  6129 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.899698  6131 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.899693  6129 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.899718  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x634c41a0) got event_name: TaskCompletion
1884: I0815 02:51:12.899991  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.899998  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902729000751323"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902729000751323"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 02:51:12.900203  6033 pir_interpreter.cc:161] PirInterpreter(): 0x634c4030 on Place(cpu)
1884: I0815 02:51:12.900223  6033 scope.cc:202] Create variable 0x634c40301723690272900216611_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236902729000751323"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236902729000751323 -> 0x633e1480
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 02:51:12.900409  6132 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.900477  6133 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 02:51:12.900524  6134 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 02:51:12.900529  6135 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 02:51:12.900550  6135 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236902729000751323:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.900573  6136 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 02:51:12.900584  6135 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236902729000751323:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 02:51:12.900605  6033 pir_interpreter.cc:1766] main_thread_blocker_(0x634c41a0) got event_name: TaskCompletion
1884: I0815 02:51:12.900771  6135 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12061824291199564791 to 5906900807252820571 , after update, data is {current : 8, peak : 104}.
1884: I0815 02:51:12.900779  6135 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12061824291199564791 to 5906900807252820571 , after update, data is {current : 8, peak : 104}.
1884: I0815 02:51:12.900871  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.900877  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:12.900938  6033 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 02:51:12.900995  6033 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 02:51:12.901031  6033 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902729000751323"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902728991098372"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902729000751323"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902728991098372"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 02:51:12.901688  6033 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 02:51:12.901706  6033 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 02:51:12.901741  6033 pir_interpreter.cc:161] PirInterpreter(): 0x634c4030 on Place(cpu)
1884: I0815 02:51:12.901772  6033 scope.cc:202] Create variable feed_name_0
1884: I0815 02:51:12.901787  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_5
1884: I0815 02:51:12.901809  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_6
1884: I0815 02:51:12.901821  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_7
1884: I0815 02:51:12.901830  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_8
1884: I0815 02:51:12.901849  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_9
1884: I0815 02:51:12.901861  6033 scope.cc:202] Create variable 0x634c40301723690272901756675_inner_var_10
1884: I0815 02:51:12.901885  6033 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.901907  6033 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 02:51:12.902045  6033 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.902060  6033 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902729000751323"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236902728991098372"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236902729000751323 -> 0x633e1480
1884: 1 -> constant_folding@_17236902728991098372 -> 0x5c9ec390
1884: 2 -> linear_1.b_0 -> 0x5cbf1aa0
1884: 3 -> linear_1.w_0 -> 0x640abd20
1884: 4 -> feed_name_0 -> 0x634d6a00
1884: 5 -> 0x634c40301723690272901756675_inner_var_5 -> 0x63748bb0
1884: 6 -> 0x634c40301723690272901756675_inner_var_6 -> 0x64267650
1884: 7 -> 0x634c40301723690272901756675_inner_var_7 -> 0x6339f950
1884: 8 -> 0x634c40301723690272901756675_inner_var_8 -> 0x641498c0
1884: 9 -> fetch_name_0 -> 0x6469cc50
1884: 10 -> fetch_name_1 -> 0x64635230
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 02:51:12.902628  6033 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 02:51:12.902685  6137 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 02:51:12.902681  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.902733  6033 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 02:51:12.902756  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.902787  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x634c40301723690272901756675_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x634c40301723690272901756675_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.902828  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x634c40301723690272901756675_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.902861  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.902882  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.902899  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.902928  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236902728991098372:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.902953  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236902729000751323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.902982  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236902729000751323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x634c40301723690272901756675_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 02:51:12.903009  6033 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236902729000751323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 02:51:12.903036  6033 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236902729000751323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x634c40301723690272901756675_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 02:51:12.903064  6033 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0815 02:51:12.903085  6033 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0815 02:51:12.903108  6033 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 02:51:12.903224  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.903231  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:12.903280  6137 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 882913286771405229 to 5906900807252820571 , after update, data is {current : -184, peak : 104}.
1884: I0815 02:51:12.903290  6137 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 882913286771405229 to 5906900807252820571 , after update, data is {current : -184, peak : 104}.
1884: I0815 02:51:12.903345  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:12.903353  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EEE..
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_optput(check_pir=True)
1884: AttributeError: 'TestMultinomialOp' object has no attribute 'check_optput'
1884: 
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_optput(check_pir=True)
1884: AttributeError: 'TestMultinomialOp2' object has no attribute 'check_optput'
1884: 
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_optput(check_pir=True)
1884: AttributeError: 'TestMultinomialOp3' object has no attribute 'check_optput'
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.608s
1884: 
1884: FAILED (errors=3, skipped=3)
1884: 
1884: I0815 02:51:12.905838  6033 mmap_allocator.cc:348] PID: 6033, MemoryMapFdSet: set size - 0
1884: I0815 02:51:12.918573  6033 mmap_allocator.cc:348] PID: 6033, MemoryMapFdSet: set size - 0
1884: I0815 02:51:13.026870  6109 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 8256796732245392036 to 5906900807252820571 , after update, data is {current : -168, peak : 104}.
1884: I0815 02:51:13.026906  6109 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 8256796732245392036 to 5906900807252820571 , after update, data is {current : -168, peak : 104}.
1884: I0815 02:51:13.026923  6110 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16274797212322662021 to 5906900807252820571 , after update, data is {current : -164, peak : 104}.
1884: I0815 02:51:13.026959  6110 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16274797212322662021 to 5906900807252820571 , after update, data is {current : -164, peak : 104}.
1884: I0815 02:51:13.026968  6108 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8004638495335938813 to 5906900807252820571 , after update, data is {current : -160, peak : 104}.
1884: I0815 02:51:13.026993  6108 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8004638495335938813 to 5906900807252820571 , after update, data is {current : -160, peak : 104}.
1884: I0815 02:51:13.027148  6111 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 10080992789879735256 to 5906900807252820571 , after update, data is {current : -184, peak : 104}.
1884: I0815 02:51:13.027161  6111 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 10080992789879735256 to 5906900807252820571 , after update, data is {current : -184, peak : 104}.
1884: I0815 02:51:13.027168  6111 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 10080992789879735256 to 5906900807252820571 , after update, data is {current : 1280, peak : 1536}.
1884: I0815 02:51:13.027460  6113 thread_data_registry.h:135] Add data {current : -512, peak : 0} from thread 12389014779693969598 to 5906900807252820571 , after update, data is {current : 768, peak : 1536}.
1884: I0815 02:51:13.027470  6113 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 12389014779693969598 to 5906900807252820571 , after update, data is {current : 24, peak : 280}.
1884: I0815 02:51:13.027474  6113 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 12389014779693969598 to 5906900807252820571 , after update, data is {current : 24, peak : 280}.
1884: I0815 02:51:13.027530  6114 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 692737695738107097 to 5906900807252820571 , after update, data is {current : 28, peak : 280}.
1884: I0815 02:51:13.027540  6114 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 692737695738107097 to 5906900807252820571 , after update, data is {current : 28, peak : 280}.
1884: I0815 02:51:13.027689  6116 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 5906900807252820571 to 3797982842794208457 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 02:51:13.027701  6116 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 5906900807252820571 to 3797982842794208457 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 02:51:13.027709  6116 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 5906900807252820571 to 3797982842794208457 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 02:51:13.186355  6033 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 02:51:13.186381  6033 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 02:51:13.186441  6033 mmap_allocator.cc:348] PID: 6033, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   11.78 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  11.96 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
