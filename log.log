UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 03:51:54.232347  6633 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 03:51:55.047000  6633 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=gpugraph_sparse_table_storage_mode,enable_pir_api,new_executor_use_inplace,mklml_dir,use_cuda_malloc_async_allocator,enable_dependency_builder_debug_info,new_executor_use_local_scope,enable_pir_in_executor_trace_run,accuracy_check_atol_fp32,gpugraph_hbm_table_load_factor,check_kernel_launch,enable_cse_in_dy2st,prim_forward,op_dir,embedding_deterministic,save_static_runtime_data,add_dependency_for_communication_op,print_allocator_trace_info,gpugraph_parallel_stream_num,fuse_parameter_memory_size,einsum_opt,run_kp_kernel,tensor_operants_mode,gpugraph_offload_gather_copy_maxsize,use_autotune,tracer_onednn_ops_off,call_stack_level,prim_backward,use_mkldnn,enable_gpu_memory_usage_log_mb,graph_get_neighbor_id,gpu_allocator_retry_time,fraction_of_cuda_pinned_memory_to_use,check_infer_symbolic,max_inplace_grad_add,nccl_dir,pir_debug,enable_graph_multi_node_sampling,enable_adjust_op_order,accuracy_check_rtol_fp16,allreduce_record_one_event,reallocate_gpu_memory_in_mb,search_cache_max_number,cuda_dir,gpugraph_dedup_pull_push_mode,executor_log_deps_every_microseconds,get_host_by_name_time,static_executor_perfstat_filepath,use_virtual_memory_auto_growth,initial_gpu_memory_in_mb,prim_forward_blacklist,memory_fraction_of_eager_deletion,npu_storage_format,benchmark,prim_check_ops,nccl_blocking_wait,use_auto_growth_v2,gpugraph_storage_mode,init_allocated_mem,accuracy_check_atol_bf16,enable_async_trace,dump_chunk_info,enable_cinn_auto_tune,gpugraph_load_node_list_into_hbm,custom_device_mem_record,async_trace_count,cinn_compile_thread_num,fleet_executor_with_standalone,graph_load_in_parallel,initial_cpu_memory_in_mb,new_executor_sequential_run,graph_neighbor_size_percent,cudnn_dir,logging_trunc_pir_py_code,enable_collect_shape,cusparselt_dir,use_shm_cache,paddle_num_threads,static_runtime_data_save_path,manually_trans_conv_filter,gpugraph_enable_gpu_direct_access,log_memory_stats,use_cinn,gpugraph_parallel_copyer_split_maxsize,cublaslt_device_best_config,allow_cinn_ops,conv2d_disable_cudnn,graph_embedding_split_infer_mode,dygraph_debug,pir_apply_inplace_pass,tracer_profile_fname,enable_cublas_tensor_op_math,enable_cinn_compile_cache,enable_pir_in_executor,pir_apply_shape_optimization_pass,prim_enable_dynamic,low_precision_op_list,new_executor_use_cuda_graph,print_ir,cinn_subgraph_graphviz_dir,pir_broadcast_tree_limit,enable_pir_with_pt_in_dy2st,convert_all_blocks,ir_inplace_kernel_blacklist,multi_node_sample_use_gpu_table,auto_free_cudagraph_allocations_on_launch,use_xqa_optim,tensorrt_dir,gpugraph_debug_gpu_memory,cudnn_deterministic,logging_pir_py_code_int_tensor_element_limit,gpugraph_enable_hbm_table_collision_stat,enable_dump_main_program,new_executor_static_build,allocator_strategy,sort_sum_gradient,host_trace_level,graph_metapath_split_opt,enable_blaslt_global_search,curand_dir,enable_sparse_inner_gather,pinned_memory_as_cpu_backend,prim_enabled,accuracy_check_atol_fp16,gpugraph_enable_segment_merge_grads,inner_op_parallelism,prim_skip_dynamic,gemm_use_half_precision_compute_type,fraction_of_cpu_memory_to_use,use_stride_kernel,fast_eager_deletion_mode,cudnn_exhaustive_search,sync_nccl_allreduce,disable_dyshape_in_train,use_system_allocator,multiple_of_cupti_buffer_size,gpugraph_slot_feasign_max_num,auto_growth_chunk_size_in_mb,lapack_dir,gpugraph_enable_print_op_debug,enable_opt_get_features,fraction_of_gpu_memory_to_use,prim_all,new_executor_serial_run,cublaslt_exhaustive_search_times,logging_pir_py_code_dump_symbolic_dims,jit_engine_type,enable_auto_detect_gpu_topo,use_pinned_memory,fuse_parameter_groups_size,gpugraph_force_device_batch_num_equal,enable_interpretercore_launch_cinn,alloc_fill_value,local_exe_sub_scope_limit,sync_after_alloc,tracer_onednn_ops_on,gpugraph_offload_param_stat,all_blocks_convert_trt,eager_delete_tensor_gb,enable_fuse_parallel_matmul_pass,enable_api_kernel_fallback,benchmark_nccl,enable_auto_rdma_trans,enable_neighbor_list_use_uva,cuda_memory_async_pool_realease_threshold,win_cuda_bin_dir,enable_unused_var_check,enable_tracker_all2all,free_when_no_cache_hit,eager_delete_scope,gpugraph_offload_param_extends,query_dest_rank_by_multi_node,trt_ibuilder_cache,use_stream_safe_cuda_allocator,check_nan_inf,check_nan_inf_level,cupti_dir,cusparse_dir,cuda_malloc_async_pool_memory_throttle_ratio,use_cuda_managed_memory,cudnn_exhaustive_search_times,cse_max_count,set_to_1d,enable_cinn_accuracy_check,logging_pir_py_code_dir,dist_threadpool_size,gpu_memory_limit_mb,conv_workspace_size_limit,use_auto_growth_pinned_allocator,gpugraph_merge_grads_segment_size,enable_all2all_use_fp16,free_idle_chunk,accuracy_check_rtol_fp32,apply_pass_to_program,nvidia_package_dir,print_sub_graph_dir,cudnn_batchnorm_spatial_persistent,cublas_dir,cusolver_dir,pir_subgraph_saving_dir,reader_queue_speed_test_mode,accuracy_check_rtol_bf16,enable_gpu_memory_usage_log,cache_inference_while_scope,enable_exit_when_partial_worker,use_fast_math,dynamic_static_unified_comm,enable_fusion_fallback,enable_record_memory,deny_cinn_ops,mkl_dir,dataloader_use_file_descriptor,selected_gpus 
1884: I0815 03:51:55.047112  6633 init.cc:108] After Parse: argc is 2
1884: I0815 03:52:02.849009  6633 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:52:02.849056  6633 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 03:52:02.849874  6633 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 03:52:02.850342  6633 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 03:52:02.851204  6633 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 03:52:02.851328  6633 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 03:52:02.851424  6633 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 03:52:02.852207  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fc111000000), and remaining 0
1884: I0815 03:52:02.852530  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:02.852619  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:02.852732  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fc111000200), and remaining 0
1884: I0815 03:52:02.852768  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fc111000400), and remaining 0
1884: I0815 03:52:02.856853  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fc111000600), and remaining 0
1884: I0815 03:52:02.857015  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fc111000800), and remaining 0
1884: I0815 03:52:02.857111  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7fc111000a00), and remaining 0
1884: I0815 03:52:02.857221  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:02.857249  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:02.857324  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:02.857338  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:02.858330  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x193cf300 for it.
1884: I0815 03:52:02.858491  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:02.858517  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:02.858572  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7fc111000e00), and remaining 0
1884: I0815 03:52:02.858660  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fc1110c4400), and remaining 0
1884: I0815 03:52:02.991549  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x193cf300 for it.
1884: I0815 03:52:02.991787  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:02.991835  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:02.992502  6633 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7fc111200000), and remaining 0
1884: I0815 03:52:03.003793  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x193cf300 for it.
1884: I0815 03:52:03.003952  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:03.003994  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:03.004045  6633 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:03.004283  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:03.005359  6633 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 03:52:03.005378  6633 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 03:52:03.005429  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:03.005515  6633 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 03:52:03.005542  6633 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:03.005602  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:03.005707  6633 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 03:52:03.005726  6633 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:03.005764  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:03.005959  6633 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 03:52:03.005976  6633 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:03.006137  6633 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 03:52:03.006161  6633 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:03.006232  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:03.009953  6633 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 03:52:03.010084  6633 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 03:52:03.010113  6633 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 03:52:03.010182  6633 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 03:52:04.513664  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.513757  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.514163  6633 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 03:52:04.514183  6633 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.520444  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.520488  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.521649  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.521668  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.521687  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.523846  6633 program_interpreter.cc:243] New Executor is Running.
1884: I0815 03:52:04.523861  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.523885  6633 scope.cc:202] Create variable feed
1884: I0815 03:52:04.523895  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.523903  6633 scope.cc:202] Create variable fetch
1884: I0815 03:52:04.523909  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.523921  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.523926  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.523931  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.523933  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.526417  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.526774  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.526787  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.526791  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.528558  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.528610  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.528617  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.528625  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.528631  6633 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:52:04.528640  6633 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x5ff338b0 type is 7
1884: I0815 03:52:04.528645  6633 scope.cc:202] Create variable x
1884: I0815 03:52:04.528647  6633 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5ff32370 type is 7
1884: I0815 03:52:04.528708  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.528714  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.528718  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.528723  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.528877  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.528906  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.529038  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.529047  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.529063  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.529274  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.529314  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.529336  6633 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.529342  6633 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5ff3a3d0Variable Type 7
1884: I0815 03:52:04.529371  6633 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.529395  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.529448  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.529470  6633 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.530731  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.530784  6633 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.531206  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.535539  6633 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:52:04.535559  6633 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:52:04.535661  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.535696  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.536242  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: I0815 03:52:04.536327  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.536350  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.536809  6633 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: I0815 03:52:04.536872  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.536895  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.536919  6633 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.537189  6633 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:52:04.537200  6633 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:52:04.537331  6633 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 03:52:04.537355  6633 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.537730  6633 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:52:04.537742  6633 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:52:04.537788  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.537808  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.537993  6633 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:52:04.538002  6633 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:52:04.538038  6633 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:52:04.538055  6633 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:52:04.538071  6633 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.540884  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.540908  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.540963  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.540972  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.543008  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.543385  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.543401  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.543406  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.545202  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.545255  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.545266  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.545276  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5ff64f60 type is 7
1884: I0815 03:52:04.545284  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.545290  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5ff652d0 type is 7
1884: I0815 03:52:04.545295  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.545310  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.545368  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.545377  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.545382  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.545385  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.545441  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.545462  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.545529  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.545539  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.545557  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.545828  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.545845  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.545862  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.545869  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5ff6ba50Variable Type 7
1884: I0815 03:52:04.545887  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.545907  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.545933  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.545950  6633 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.546654  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.546694  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.546881  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.558009  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: I0815 03:52:04.558248  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:52:04.565070  6633 pir_interpreter.cc:161] PirInterpreter(): 0x60128370 on Place(gpu:0)
1884: I0815 03:52:04.565122  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.565152  6633 scope.cc:202] Create variable 0x601283701723693924565099941_inner_var_1
1884: I0815 03:52:04.565163  6633 scope.cc:202] Create variable 0x601283701723693924565099941_inner_var_2
1884: I0815 03:52:04.565174  6633 scope.cc:202] Create variable 0x601283701723693924565099941_inner_var_3
1884: I0815 03:52:04.565184  6633 scope.cc:202] Create variable 0x601283701723693924565099941_inner_var_4
1884: I0815 03:52:04.565196  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.565649  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.565666  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.565670  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 03:52:04.565719  6633 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x601282d0
1884: 1 -> 0x601283701723693924565099941_inner_var_1 -> 0x60128350
1884: 2 -> 0x601283701723693924565099941_inner_var_2 -> 0x60128be0
1884: 3 -> 0x601283701723693924565099941_inner_var_3 -> 0x60127ab0
1884: 4 -> 0x601283701723693924565099941_inner_var_4 -> 0x60128f90
1884: 5 -> fetch0@fetch -> 0x601297a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.566494  6633 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 03:52:04.566735  6671 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.566884  6672 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.566977  6673 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.567003  6674 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.567085  6675 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.567109  6674 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x601283701723693924565099941_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.567147  6676 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.567214  6676 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.567238  6674 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x601283701723693924565099941_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.567265  6676 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:52:04.567322  6676 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x601283701723693924565099941_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x601283701723693924565099941_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.567551  6676 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x601283701723693924565099941_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x601283701723693924565099941_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:52:04.567623  6674 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x601283701723693924565099941_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.567649  6674 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.568897  6674 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x601283701723693924565099941_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x601283701723693924565099941_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:52:04.568938  6674 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x601283701723693924565099941_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.568965  6674 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.569562  6674 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x601283701723693924565099941_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:52:04.569602  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x601284e0) got event_name: TaskCompletion
1884: I0815 03:52:04.569629  6633 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.645548  6671 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 14226227902810773705 to 7714433183376124983 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:52:04.645576  6671 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14226227902810773705 to 371173175500456571 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:52:04.645581  6671 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 14226227902810773705 to 371173175500456571 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:52:04.645773  6674 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 371173175500456571 to 8242500734260742476 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:52:04.645789  6674 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 371173175500456571 to 8242500734260742476 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:52:04.645963  6676 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 03:52:04.645982  6676 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.652055  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.652082  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.652148  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.652156  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.653934  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.654291  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.654312  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.654320  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.655862  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.655969  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.655980  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.655987  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x397c270 type is 7
1884: I0815 03:52:04.655995  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.655998  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5ff6d7f0 type is 7
1884: I0815 03:52:04.656002  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.656006  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.656062  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.656069  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.656072  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.656076  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.656134  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.656152  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.656217  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.656226  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.656240  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.656399  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.656411  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.656426  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.656432  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x625dfd30Variable Type 7
1884: I0815 03:52:04.656448  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.656466  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.656489  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.656504  6633 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.658176  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.658212  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.658435  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.662835  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: I0815 03:52:04.663026  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:52:04.666160  6633 pir_interpreter.cc:161] PirInterpreter(): 0x5ff3dc50 on Place(gpu:0)
1884: I0815 03:52:04.666198  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.666221  6633 scope.cc:202] Create variable 0x5ff3dc501723693924666187672_inner_var_1
1884: I0815 03:52:04.666232  6633 scope.cc:202] Create variable 0x5ff3dc501723693924666187672_inner_var_2
1884: I0815 03:52:04.666244  6633 scope.cc:202] Create variable 0x5ff3dc501723693924666187672_inner_var_3
1884: I0815 03:52:04.666253  6633 scope.cc:202] Create variable 0x5ff3dc501723693924666187672_inner_var_4
1884: I0815 03:52:04.666265  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.666599  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.666615  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.666618  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x43cfd8e0
1884: 1 -> 0x5ff3dc501723693924666187672_inner_var_1 -> 0x3970840
1884: 2 -> 0x5ff3dc501723693924666187672_inner_var_2 -> 0x39413e0
1884: 3 -> 0x5ff3dc501723693924666187672_inner_var_3 -> 0x5ff6b530
1884: 4 -> 0x5ff3dc501723693924666187672_inner_var_4 -> 0x60129d10
1884: 5 -> fetch0@fetch -> 0x43cf38f0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.667367  6677 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.667479  6678 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.667539  6680 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.667575  6680 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff3dc501723693924666187672_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.667660  6679 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.667676  6680 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff3dc501723693924666187672_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.667745  6681 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.667745  6682 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.667794  6682 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.667826  6682 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:52:04.667872  6682 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff3dc501723693924666187672_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ff3dc501723693924666187672_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.668011  6682 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff3dc501723693924666187672_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5ff3dc501723693924666187672_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.668100  6681 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff3dc501723693924666187672_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.668156  6681 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.671046  6681 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff3dc501723693924666187672_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5ff3dc501723693924666187672_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.671121  6681 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff3dc501723693924666187672_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.671150  6681 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.673213  6681 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff3dc501723693924666187672_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.673277  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x5ff3ddc0) got event_name: TaskCompletion
1884: I0815 03:52:04.673312  6633 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.714898  6677 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 7714433183376124983 to 4823167728561319615 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:52:04.714933  6677 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 7714433183376124983 to 8924888205254337880 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:52:04.714939  6677 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 7714433183376124983 to 8924888205254337880 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:52:04.715147  6681 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 8924888205254337880 to 155935143093453406 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:52:04.715171  6681 thread_data_registry.h:135] Add data {current : 2399996, peak : 4800000} from thread 8924888205254337880 to 155935143093453406 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:52:04.715214  6680 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 155935143093453406 to 8242500734260742476 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:52:04.715229  6680 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 155935143093453406 to 8242500734260742476 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:52:04.715382  6682 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.719543  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.719576  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.719642  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.719650  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.721385  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.721730  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.721745  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.721750  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.723266  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.723398  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.723412  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.723418  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x39783e0 type is 7
1884: I0815 03:52:04.723428  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.723434  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x39842c0 type is 7
1884: I0815 03:52:04.723439  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.723444  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.723498  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.723505  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.723508  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.723513  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.723569  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.723584  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.723650  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.723659  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.723673  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.723723  6633 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.723878  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:04.723948  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.723958  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.723973  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.723979  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3948370Variable Type 7
1884: I0815 03:52:04.723997  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.724016  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.724038  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.724053  6633 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.724323  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.724347  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.724547  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.725363  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: I0815 03:52:04.725557  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:52:04.728659  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62655d30 on Place(gpu:0)
1884: I0815 03:52:04.728695  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.728718  6633 scope.cc:202] Create variable 0x62655d301723693924728685917_inner_var_1
1884: I0815 03:52:04.728730  6633 scope.cc:202] Create variable 0x62655d301723693924728685917_inner_var_2
1884: I0815 03:52:04.728741  6633 scope.cc:202] Create variable 0x62655d301723693924728685917_inner_var_3
1884: I0815 03:52:04.728752  6633 scope.cc:202] Create variable 0x62655d301723693924728685917_inner_var_4
1884: I0815 03:52:04.728763  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.729090  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.729105  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.729110  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x43cf1d60
1884: 1 -> 0x62655d301723693924728685917_inner_var_1 -> 0x601287a0
1884: 2 -> 0x62655d301723693924728685917_inner_var_2 -> 0x5ff1a820
1884: 3 -> 0x62655d301723693924728685917_inner_var_3 -> 0x3974fc0
1884: 4 -> 0x62655d301723693924728685917_inner_var_4 -> 0x5ff3d410
1884: 5 -> fetch0@fetch -> 0x6024ac90
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.729828  6683 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.729908  6684 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.729924  6685 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.729962  6686 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.730010  6687 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.730038  6688 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.730046  6687 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62655d301723693924728685917_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.730063  6688 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.730103  6688 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:52:04.730113  6687 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62655d301723693924728685917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.730144  6688 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62655d301723693924728685917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62655d301723693924728685917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.730195  6688 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.730324  6688 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:04.730353  6688 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62655d301723693924728685917_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62655d301723693924728685917_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:52:04.730412  6687 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62655d301723693924728685917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.730434  6687 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.730695  6687 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62655d301723693924728685917_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62655d301723693924728685917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:52:04.730720  6687 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62655d301723693924728685917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.730739  6687 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.730753  6687 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62655d301723693924728685917_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:52:04.730785  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x62655ea0) got event_name: TaskCompletion
1884: I0815 03:52:04.730808  6633 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.763347  6683 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 4823167728561319615 to 7714433183376124983 , after update, data is {current : 0, peak : 3328}.
1884: I0815 03:52:04.763375  6683 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4823167728561319615 to 7714433183376124983 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:52:04.763381  6683 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4823167728561319615 to 7714433183376124983 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:52:04.763697  6687 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13142502398131216068 to 7714433183376124983 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:52:04.763711  6687 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 13142502398131216068 to 7714433183376124983 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:52:04.763826  6688 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:52:04.763837  6688 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:52:04.763841  6688 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.770143  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.770170  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.770236  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.770242  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.772024  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.772390  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.772405  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.772410  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.773957  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.774065  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.774076  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.774083  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6012a1d0 type is 7
1884: I0815 03:52:04.774096  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.774106  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5ff1f160 type is 7
1884: I0815 03:52:04.774109  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.774114  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.774170  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.774176  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.774180  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.774184  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.774240  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.774256  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.774334  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.774344  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.774358  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.774618  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.774631  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.774647  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.774653  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5ff55ef0Variable Type 7
1884: I0815 03:52:04.774670  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.774690  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.774713  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.774729  6633 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.775422  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.775452  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.775661  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.781216  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: I0815 03:52:04.781414  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:52:04.784595  6633 pir_interpreter.cc:161] PirInterpreter(): 0x621ce0c0 on Place(gpu:0)
1884: I0815 03:52:04.784631  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.784655  6633 scope.cc:202] Create variable 0x621ce0c01723693924784621056_inner_var_1
1884: I0815 03:52:04.784667  6633 scope.cc:202] Create variable 0x621ce0c01723693924784621056_inner_var_2
1884: I0815 03:52:04.784678  6633 scope.cc:202] Create variable 0x621ce0c01723693924784621056_inner_var_3
1884: I0815 03:52:04.784688  6633 scope.cc:202] Create variable 0x621ce0c01723693924784621056_inner_var_4
1884: I0815 03:52:04.784700  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.785044  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.785060  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.785064  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60127fd0
1884: 1 -> 0x621ce0c01723693924784621056_inner_var_1 -> 0x3997760
1884: 2 -> 0x621ce0c01723693924784621056_inner_var_2 -> 0x5ff0b5d0
1884: 3 -> 0x621ce0c01723693924784621056_inner_var_3 -> 0x5ff1a710
1884: 4 -> 0x621ce0c01723693924784621056_inner_var_4 -> 0x5ff18f30
1884: 5 -> fetch0@fetch -> 0x5ff03730
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.785784  6689 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.785866  6690 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.785881  6691 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.785933  6692 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.785959  6693 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.786001  6694 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.786002  6693 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x621ce0c01723693924784621056_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.786026  6694 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.786072  6693 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x621ce0c01723693924784621056_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.786077  6694 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:52:04.786114  6694 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x621ce0c01723693924784621056_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x621ce0c01723693924784621056_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.786334  6694 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x621ce0c01723693924784621056_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x621ce0c01723693924784621056_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:52:04.786398  6693 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x621ce0c01723693924784621056_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.786422  6693 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.787650  6693 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x621ce0c01723693924784621056_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x621ce0c01723693924784621056_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:52:04.787690  6693 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x621ce0c01723693924784621056_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.787710  6693 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.788277  6693 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x621ce0c01723693924784621056_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:52:04.788321  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x621ce230) got event_name: TaskCompletion
1884: I0815 03:52:04.788342  6633 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.792321  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.792346  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.792405  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.792414  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.794338  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.794750  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.794766  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.794772  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.796658  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.796754  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.796766  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.796775  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x621e0600 type is 7
1884: I0815 03:52:04.796783  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.796789  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5ff23330 type is 7
1884: I0815 03:52:04.796795  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.796800  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.796870  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.796877  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.796882  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.796886  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.796947  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.796967  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.797041  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.797055  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.797075  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.797312  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.797327  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.797346  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.797354  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60101cc0Variable Type 7
1884: I0815 03:52:04.797374  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.797396  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.797422  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.797439  6633 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.798178  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.798209  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.798424  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.836270  6689 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 7714433183376124983 to 4823167728561319615 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:52:04.836308  6689 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7714433183376124983 to 8924888205254337880 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:52:04.836313  6689 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 7714433183376124983 to 8924888205254337880 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:52:04.836491  6693 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8924888205254337880 to 8242500734260742476 , after update, data is {current : 4000800, peak : 4800000}.
1884: I0815 03:52:04.836505  6693 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8924888205254337880 to 8242500734260742476 , after update, data is {current : 4000800, peak : 4800800}.
1884: I0815 03:52:04.836674  6694 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 03:52:04.836686  6694 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.842393  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.842420  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.842485  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.842492  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.844317  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.844678  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.844692  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.844698  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.846235  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.846349  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.846360  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.846366  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x622e1250 type is 7
1884: I0815 03:52:04.846376  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.846382  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x625a2140 type is 7
1884: I0815 03:52:04.846387  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.846392  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.846449  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.846457  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.846459  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.846464  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.846519  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.846535  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.846601  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.846609  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.846623  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.846800  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.846812  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.846827  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.846834  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5ff21370Variable Type 7
1884: I0815 03:52:04.846850  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.846869  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.846892  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.846907  6633 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.848578  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.848616  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.848834  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.854290  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: I0815 03:52:04.854496  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:52:04.857673  6633 pir_interpreter.cc:161] PirInterpreter(): 0x60fde2f0 on Place(gpu:0)
1884: I0815 03:52:04.857710  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.857734  6633 scope.cc:202] Create variable 0x60fde2f01723693924857699806_inner_var_1
1884: I0815 03:52:04.857745  6633 scope.cc:202] Create variable 0x60fde2f01723693924857699806_inner_var_2
1884: I0815 03:52:04.857756  6633 scope.cc:202] Create variable 0x60fde2f01723693924857699806_inner_var_3
1884: I0815 03:52:04.857767  6633 scope.cc:202] Create variable 0x60fde2f01723693924857699806_inner_var_4
1884: I0815 03:52:04.857779  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.858117  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.858132  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.858136  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x625f72e0
1884: 1 -> 0x60fde2f01723693924857699806_inner_var_1 -> 0x43cfcc40
1884: 2 -> 0x60fde2f01723693924857699806_inner_var_2 -> 0x5ff35680
1884: 3 -> 0x60fde2f01723693924857699806_inner_var_3 -> 0x5ff23cc0
1884: 4 -> 0x60fde2f01723693924857699806_inner_var_4 -> 0x5ff4f270
1884: 5 -> fetch0@fetch -> 0x6287cf30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.858878  6695 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.858947  6696 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.858986  6697 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.859040  6699 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.859053  6698 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.859086  6698 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60fde2f01723693924857699806_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.859174  6698 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60fde2f01723693924857699806_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.859246  6700 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.859277  6700 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.859313  6700 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:52:04.859351  6700 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60fde2f01723693924857699806_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60fde2f01723693924857699806_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.859493  6700 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60fde2f01723693924857699806_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60fde2f01723693924857699806_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.859573  6698 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60fde2f01723693924857699806_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.859632  6698 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.862493  6698 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60fde2f01723693924857699806_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60fde2f01723693924857699806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.862565  6698 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60fde2f01723693924857699806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.862593  6698 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.864652  6698 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60fde2f01723693924857699806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:52:04.864710  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x60fde460) got event_name: TaskCompletion
1884: I0815 03:52:04.864733  6633 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.877365  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.877393  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.877454  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.877463  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.879411  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.879824  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.879840  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.879846  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.881743  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.881840  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.881853  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.881862  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x622e8740 type is 7
1884: I0815 03:52:04.881871  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.881877  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x622e7a80 type is 7
1884: I0815 03:52:04.881883  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.881888  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.881959  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.881968  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.881971  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.881976  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.882035  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.882051  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.882122  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.882131  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.882149  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.882316  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.882330  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.882349  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.882357  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x622e25b0Variable Type 7
1884: I0815 03:52:04.882375  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.882396  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.882422  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.882440  6633 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.884102  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.884137  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.884353  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.930318  6695 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 4823167728561319615 to 7714433183376124983 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:52:04.930342  6695 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4823167728561319615 to 371173175500456571 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:52:04.930348  6695 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4823167728561319615 to 371173175500456571 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:52:04.930567  6698 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 371173175500456571 to 8242500734260742476 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 03:52:04.930591  6698 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 371173175500456571 to 8242500734260742476 , after update, data is {current : 6400800, peak : 8800800}.
1884: I0815 03:52:04.930789  6700 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 7714433183376124983 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.935591  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.935624  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.935688  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.935695  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.937426  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.937788  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.937801  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.937806  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.939327  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.939431  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.939442  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.939450  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5ff1a5c0 type is 7
1884: I0815 03:52:04.939460  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.939465  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x622e9480 type is 7
1884: I0815 03:52:04.939471  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.939474  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.939530  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.939536  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.939540  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.939544  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.939601  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.939616  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.939685  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.939693  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.939707  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.939750  6633 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.939904  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:04.939965  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.939975  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.939992  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.939998  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62bd06b0Variable Type 7
1884: I0815 03:52:04.940016  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.940035  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.940059  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.940074  6633 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.940189  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.940213  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.940423  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.941242  6633 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19417c60 for it.
1884: I0815 03:52:04.941468  6633 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x194b4db0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:52:04.944574  6633 pir_interpreter.cc:161] PirInterpreter(): 0x601314a0 on Place(gpu:0)
1884: I0815 03:52:04.944608  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.944631  6633 scope.cc:202] Create variable 0x601314a01723693924944599659_inner_var_1
1884: I0815 03:52:04.944643  6633 scope.cc:202] Create variable 0x601314a01723693924944599659_inner_var_2
1884: I0815 03:52:04.944654  6633 scope.cc:202] Create variable 0x601314a01723693924944599659_inner_var_3
1884: I0815 03:52:04.944665  6633 scope.cc:202] Create variable 0x601314a01723693924944599659_inner_var_4
1884: I0815 03:52:04.944677  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:04.945003  6633 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:52:04.945019  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.945022  6633 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5ff23760
1884: 1 -> 0x601314a01723693924944599659_inner_var_1 -> 0x626f3450
1884: 2 -> 0x601314a01723693924944599659_inner_var_2 -> 0x62bd03d0
1884: 3 -> 0x601314a01723693924944599659_inner_var_3 -> 0x206b800
1884: 4 -> 0x601314a01723693924944599659_inner_var_4 -> 0x39517e0
1884: 5 -> fetch0@fetch -> 0x3941be0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:52:04.945724  6701 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:04.945806  6702 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:04.945819  6703 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:04.945873  6704 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:04.945899  6705 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:04.945943  6706 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:04.945940  6705 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x601314a01723693924944599659_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.945966  6706 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.945997  6705 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x601314a01723693924944599659_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:04.945999  6706 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:52:04.946035  6706 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x601314a01723693924944599659_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x601314a01723693924944599659_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.946079  6706 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.946192  6706 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:04.946218  6706 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x601314a01723693924944599659_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x601314a01723693924944599659_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:52:04.946269  6705 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x601314a01723693924944599659_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.946290  6705 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.946465  6705 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x601314a01723693924944599659_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x601314a01723693924944599659_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:52:04.946489  6705 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x601314a01723693924944599659_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:04.946508  6705 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.946519  6705 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x601314a01723693924944599659_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:52:04.946545  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x60131610) got event_name: TaskCompletion
1884: I0815 03:52:04.946565  6633 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:52:04.947942  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.947965  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.948016  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:04.948025  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.949882  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:04.950290  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.950313  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.950325  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.952148  6633 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:52:04.952235  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:04.952247  6633 scope.cc:202] Create variable Out
1884: I0815 03:52:04.952253  6633 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62a9ce10 type is 7
1884: I0815 03:52:04.952263  6633 scope.cc:202] Create variable X
1884: I0815 03:52:04.952267  6633 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62a93a10 type is 7
1884: I0815 03:52:04.952272  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:04.952277  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:04.952353  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:04.952360  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:04.952365  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:04.952369  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:04.952422  6633 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.952437  6633 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.952500  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.952509  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.952527  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:04.952564  6633 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.952661  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:04.952709  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.952721  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:04.952739  6633 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:04.952746  6633 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62aae8d0Variable Type 7
1884: I0815 03:52:04.952764  6633 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:04.952783  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.952808  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:04.952823  6633 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:04.952931  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:04.952956  6633 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:04.953166  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:04.988639  6701 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 7714433183376124983 to 4823167728561319615 , after update, data is {current : 0, peak : 10240}.
1884: I0815 03:52:04.988667  6701 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 7714433183376124983 to 4823167728561319615 , after update, data is {current : -804, peak : 8000}.
1884: I0815 03:52:04.988672  6701 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 7714433183376124983 to 4823167728561319615 , after update, data is {current : -804, peak : 8000}.
1884: I0815 03:52:04.988850  6705 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 8924888205254337880 to 4823167728561319615 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:52:04.988862  6705 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 8924888205254337880 to 4823167728561319615 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:52:04.989037  6706 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 03:52:04.989048  6706 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 6401600, peak : 8800800}.
1884: I0815 03:52:04.989053  6706 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 4823167728561319615 to 8242500734260742476 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:52:04.996419  6633 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 03:52:04.996479  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:52:04.997610  6633 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:52:04.998472  6633 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 03:52:04.998502  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:52:04.999805  6633 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 03:52:04.999830  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:52:05.000519  6633 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 03:52:05.001523  6633 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 03:52:05.001550  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:52:05.002907  6633 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 03:52:05.002930  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:52:05.003523  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.003551  6633 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:52:05.003558  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.003566  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:05.005649  6633 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 03:52:05.005676  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:52:05.006639  6633 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 03:52:05.006667  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:52:05.007659  6633 pybind.cc:1827] need skip: 0
1884: I0815 03:52:05.007972  6633 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:52:05.009774  6633 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:52:05.013954  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.013973  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.013978  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.015992  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:05.016012  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:05.016023  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:05.016028  6633 scope.cc:202] Create variable learning_rate_0
1884: I0815 03:52:05.016037  6633 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x5ff27d50 type is 7
1884: I0815 03:52:05.016041  6633 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:52:05.016044  6633 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x5ff28e00 type is 7
1884: I0815 03:52:05.016049  6633 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:52:05.016053  6633 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x5ff27c80 type is 7
1884: I0815 03:52:05.016117  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:05.016124  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.016129  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.016131  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.016191  6633 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016207  6633 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016232  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:52:05.016387  6633 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016399  6633 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016472  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.016518  6633 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016527  6633 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.016553  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.017571  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.018906  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:05.019356  6633 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:52:05.019577  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.019876  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.020095  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.020112  6633 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:52:05.020179  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.020186  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.020190  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.020293  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.020311  6633 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:52:05.021843  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.023169  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.024262  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.024468  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:05.024482  6633 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:52:05.024492  6633 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x60c49750 type is 7
1884: I0815 03:52:05.024498  6633 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:52:05.024503  6633 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x60c494d0 type is 7
1884: I0815 03:52:05.024508  6633 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 03:52:05.024511  6633 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x60c495c0 type is 7
1884: I0815 03:52:05.024515  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:05.024520  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:05.024524  6633 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:52:05.024528  6633 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x60c4ab40 type is 7
1884: I0815 03:52:05.024533  6633 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x5ff27d50 type is 7
1884: I0815 03:52:05.024536  6633 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x5ff28e00 type is 7
1884: I0815 03:52:05.024540  6633 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 03:52:05.024544  6633 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x60c4ab20 type is 7
1884: I0815 03:52:05.024549  6633 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:52:05.024551  6633 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x60c4b080 type is 7
1884: I0815 03:52:05.024555  6633 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x5ff27c80 type is 7
1884: I0815 03:52:05.024559  6633 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 03:52:05.024562  6633 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x60c4b2f0 type is 7
1884: I0815 03:52:05.024566  6633 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 03:52:05.024569  6633 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x60c4b530 type is 7
1884: I0815 03:52:05.024573  6633 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:52:05.024576  6633 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x60c4b790 type is 7
1884: I0815 03:52:05.024660  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.024675  6633 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:52:05.024736  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:05.024742  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.024747  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.024750  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.024803  6633 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.024818  6633 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.024834  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:52:05.024971  6633 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.024981  6633 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.025002  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:52:05.025085  6633 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:52:05.025197  6633 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 03:52:05.026466  6633 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026485  6633 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026552  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.026618  6633 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026628  6633 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026641  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:52:05.026667  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.026712  6633 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026721  6633 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026733  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:52:05.026825  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026835  6633 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.026849  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:05.026962  6633 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.027050  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.027110  6633 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027120  6633 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027134  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:52:05.027169  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.027220  6633 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027228  6633 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027242  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:52:05.027365  6633 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027376  6633 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027397  6633 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.027442  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.027451  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.027467  6633 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:05.027474  6633 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x626e6f70Variable Type 7
1884: I0815 03:52:05.027491  6633 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:05.027508  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:05.027529  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027541  6633 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.027581  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:05.027606  6633 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:52:05.027632  6633 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.027640  6633 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.027653  6633 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:52:05.027659  6633 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x626e8d10Variable Type 7
1884: I0815 03:52:05.027671  6633 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:52:05.027683  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:05.027698  6633 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.027710  6633 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.027742  6633 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:52:05.027756  6633 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 03:52:05.028190  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:52:05.028223  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:52:05.028240  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:52:05.028273  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:52:05.028321  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:05.028338  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:52:05.032413  6633 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:52:05.032449  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:52:05.033144  6633 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:52:05.033166  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:52:05.033517  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.035182  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.036023  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.036146  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.036656  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.037545  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.039615  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.040660  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.042534  6633 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 03:52:05.043352  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.043370  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.043375  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.044572  6633 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:52:05.044591  6633 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x397ed40 type is 9
1884: I0815 03:52:05.044598  6633 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x395bc80 type is 10
1884: I0815 03:52:05.044603  6633 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x5ff28e00 type is 7
1884: I0815 03:52:05.044608  6633 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x5ff27c80 type is 7
1884: I0815 03:52:05.044613  6633 scope.cc:202] Create variable saved_params
1884: I0815 03:52:05.044616  6633 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x60fce780 type is 17
1884: I0815 03:52:05.044646  6633 interpreter_util.cc:594] Static build: 0
1884: I0815 03:52:05.044651  6633 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:52:05.044656  6633 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:52:05.044658  6633 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:52:05.044705  6633 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.044719  6633 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:52:05.045534  6633 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:52:05.045576  6633 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:52:05.045637  6633 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 03:52:05.046903  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.046962  6633 scope.cc:202] Create variable feed
1884: I0815 03:52:05.046970  6633 naive_executor.cc:189] 0x621b9070 Create persistable variable feed, which pointer is 0x60c56f20
1884: I0815 03:52:05.046975  6633 scope.cc:202] Create variable fetch
1884: I0815 03:52:05.046978  6633 naive_executor.cc:189] 0x621b9070 Create persistable variable fetch, which pointer is 0x60c5d100
1884: I0815 03:52:05.046983  6633 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:52:05.046985  6633 naive_executor.cc:189] 0x621b9070 Create persistable variable linear_0.b_0, which pointer is 0x61067130
1884: I0815 03:52:05.046990  6633 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:52:05.046993  6633 naive_executor.cc:189] 0x621b9070 Create persistable variable linear_0.w_0, which pointer is 0x621b8de0
1884: I0815 03:52:05.047009  6633 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 03:52:05.047360  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.047462  6633 program_converter.cc:296] is_legacy_program : 0
1884: I0815 03:52:05.047523  6633 executor.cc:183] Old Executor is Running.
1884: I0815 03:52:05.047602  6633 executor.cc:92] Creating Variables for block 0
1884: I0815 03:52:05.047610  6633 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 03:52:05.047613  6633 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x61067130 type is 7
1884: I0815 03:52:05.047616  6633 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 03:52:05.047619  6633 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x621b8de0 type is 7
1884: I0815 03:52:05.047653  6633 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.047736  6633 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 03:52:05.047780  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.047786  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.047921  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.048028  6633 graph.cc:149] create OpNode by feed
1884: I0815 03:52:05.048065  6633 graph.cc:149] create OpNode by matmul_v2
1884: I0815 03:52:05.048080  6633 graph.cc:149] create OpNode by elementwise_add
1884: I0815 03:52:05.048095  6633 graph.cc:149] create OpNode by abs
1884: I0815 03:52:05.048106  6633 graph.cc:149] create OpNode by assign_value
1884: I0815 03:52:05.048123  6633 graph.cc:149] create OpNode by multinomial
1884: I0815 03:52:05.048133  6633 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:52:05.048151  6633 graph.cc:149] create OpNode by scale
1884: I0815 03:52:05.048166  6633 graph.cc:149] create OpNode by scale
1884: I0815 03:52:05.048177  6633 graph.cc:149] create OpNode by fetch
1884: I0815 03:52:05.048193  6633 graph.cc:149] create OpNode by fetch
1884: I0815 03:52:05.048214  6633 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 03:52:05.049573  6633 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 03:52:05.049582  6633 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 03:52:05.049655  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.049661  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 03:52:05.049775  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050024  6633 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:52:05.050083  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050089  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 03:52:05.050122  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050127  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 03:52:05.050166  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050227  6633 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:52:05.050259  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050266  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 03:52:05.050282  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050295  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.050326  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050333  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 03:52:05.050370  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050391  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.050415  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050420  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 03:52:05.050464  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050539  6633 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:52:05.050568  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050573  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 03:52:05.050604  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050624  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.050647  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050652  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 03:52:05.050679  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050828  6633 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:52:05.050858  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050863  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 03:52:05.050894  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050911  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.050935  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.050940  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 03:52:05.050961  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.050974  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.050997  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.051002  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 03:52:05.051023  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.051038  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.051059  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.051064  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 03:52:05.051086  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.051153  6633 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:52:05.051187  6633 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:52:05.051201  6633 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:52:05.051215  6633 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 03:52:05.051239  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.051244  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 03:52:05.051267  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.051313  6633 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:52:05.051333  6633 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:52:05.051344  6633 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:52:05.051357  6633 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:52:05.051388  6633 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:52:05.051398  6633 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 03:52:05.052690  6633 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:52:05.052735  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.052742  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 03:52:05.052768  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.052789  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.052816  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.052821  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 03:52:05.052845  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.052893  6633 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:52:05.052922  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.052928  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 03:52:05.052945  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.052960  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.052982  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.052987  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 03:52:05.053021  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.053107  6633 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 03:52:05.053135  6633 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:52:05.053150  6633 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:52:05.053166  6633 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:52:05.053181  6633 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:52:05.053195  6633 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:52:05.053211  6633 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 03:52:05.053234  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.053313  6633 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 03:52:05.053336  6633 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:52:05.053349  6633 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:52:05.053364  6633 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:52:05.053377  6633 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:52:05.053392  6633 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:52:05.053407  6633 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 03:52:05.053452  6633 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:52:05.053740  6633 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:52:05.053771  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.053776  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 03:52:05.053823  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.053882  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.053917  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.053963  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.053992  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054033  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054056  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054093  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054114  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054147  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054167  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054196  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054212  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054239  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054251  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054275  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054286  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054311  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054337  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054342  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 03:52:05.054368  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054409  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054433  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054438  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 03:52:05.054448  6633 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:52:05.054451  6633 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:52:05.054500  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054522  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054548  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054554  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:52:05.054563  6633 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:52:05.054566  6633 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:52:05.054606  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054628  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054654  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054659  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 03:52:05.054667  6633 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:52:05.054669  6633 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:52:05.054702  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054720  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054744  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054749  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:52:05.054756  6633 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:52:05.054759  6633 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:52:05.054797  6633 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:52:05.054817  6633 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:52:05.054841  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054847  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 03:52:05.054858  6633 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 03:52:05.054904  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.054909  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 03:52:05.054983  6633 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:52:05.055007  6633 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.055025  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:52:05.055092  6633 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 03:52:05.055110  6633 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:52:05.055137  6633 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:52:05.055161  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.055166  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 03:52:05.056058  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.056073  6633 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 03:52:05.056124  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.056131  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.056736  6633 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 03:52:05.056946  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.057017  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.057024  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.057437  6633 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:52:05.057657  6633 graph.h:183] deleting __fuse_statis__
1884: I0815 03:52:05.057665  6633 graph.h:183] deleting pass_recorder
1884: I0815 03:52:05.057670  6633 graph.h:183] deleting stale_program_op_descs
1884: I0815 03:52:05.057842  6633 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 03:52:05.057852  6633 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:52:05.057855  6633 naive_executor.cc:195] 0x621b9070 Create variable abs_0.tmp_0, which pointer is 0x621d9a00
1884: I0815 03:52:05.057861  6633 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:52:05.057864  6633 naive_executor.cc:195] 0x621b9070 Create variable gaussian_0.tmp_0, which pointer is 0x626fb190
1884: I0815 03:52:05.057868  6633 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:52:05.057870  6633 naive_executor.cc:195] 0x621b9070 Create variable linear_0.tmp_1, which pointer is 0x62bc8190
1884: I0815 03:52:05.057883  6633 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:52:05.057888  6633 naive_executor.cc:195] 0x621b9070 Create variable multinomial_0.tmp_0, which pointer is 0x62bc7c30
1884: I0815 03:52:05.057893  6633 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 03:52:05.057895  6633 naive_executor.cc:195] 0x621b9070 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62bc7f30
1884: I0815 03:52:05.057898  6633 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 03:52:05.057901  6633 naive_executor.cc:195] 0x621b9070 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62bc6530
1884: I0815 03:52:05.057907  6633 scope.cc:202] Create variable feed
1884: I0815 03:52:05.057910  6633 scope.cc:202] Create variable fetch
1884: I0815 03:52:05.057932  6633 naive_executor.cc:46] NaiveExecutor init with scope 0x621b9070
1884: I0815 03:52:05.057938  6633 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 03:52:05.058029  6633 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:52:05.058044  6633 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:52:05.058073  6633 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 03:52:05.058079  6633 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 03:52:05.058086  6633 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.058122  6633 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:52:05.058379  6633 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.058396  6633 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:52:05.058449  6633 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.058475  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 03:52:05.138177  6633 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:52:05.138298  6633 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.138334  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:52:05.138406  6633 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:52:05.138437  6633 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.138460  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:52:05.138531  6633 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:52:05.138574  6633 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.138592  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:52:05.138645  6633 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:52:05.138672  6633 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:52:05.138687  6633 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:52:05.138722  6633 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:52:05.138742  6633 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.138775  6633 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:52:05.138803  6633 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:52:05.139233  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.139245  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 03:52:05.177484  6633 pir_interpreter.cc:161] PirInterpreter(): 0x6285b870 on Place(gpu:0)
1884: I0815 03:52:05.177528  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_0
1884: I0815 03:52:05.177547  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_1
1884: I0815 03:52:05.177556  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_2
1884: I0815 03:52:05.177565  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_3
1884: I0815 03:52:05.177603  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_4
1884: I0815 03:52:05.177618  6633 scope.cc:202] Create variable 0x6285b8701723693925177516342_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 1 )  ( 2 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x6285b8701723693925177516342_inner_var_0 -> 0x61068300
1884: 1 -> 0x6285b8701723693925177516342_inner_var_1 -> 0x622ecb40
1884: 2 -> 0x6285b8701723693925177516342_inner_var_2 -> 0x60fe2a40
1884: 3 -> linear_1.w_0 -> 0x60126760
1884: 4 -> linear_1.b_0 -> 0x622eae70
1884: 5 -> learning_rate_1 -> 0x5ff17100
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:52:05.178457  6707 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.178471  6708 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.178530  6710 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.178541  6709 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.178570  6711 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:05.178568  6709 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178575  6708 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178591  6711 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178572  6710 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178651  6711 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.178650  6709 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:52:05.178651  6708 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.178659  6710 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6285b8701723693925177516342_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.178694  6711 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 03:52:05.178714  6711 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178731  6711 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.178741  6711 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:52:05.178755  6711 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6285b8701723693925177516342_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6285b8701723693925177516342_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6285b8701723693925177516342_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.178815  6711 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x6285b8701723693925177516342_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6285b8701723693925177516342_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6285b8701723693925177516342_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 03:52:05.178869  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x6285b9e0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 03:52:05.180917  6633 pir_interpreter.cc:161] PirInterpreter(): 0x5ff1b320 on Place(gpu:0)
1884: I0815 03:52:05.180955  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_1
1884: I0815 03:52:05.180971  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_4
1884: I0815 03:52:05.180981  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_5
1884: I0815 03:52:05.180989  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_6
1884: I0815 03:52:05.181011  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_7
1884: I0815 03:52:05.181022  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_8
1884: I0815 03:52:05.181030  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_9
1884: I0815 03:52:05.181058  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_10
1884: I0815 03:52:05.181068  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_11
1884: I0815 03:52:05.181074  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_12
1884: I0815 03:52:05.181083  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_13
1884: I0815 03:52:05.181092  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_14
1884: I0815 03:52:05.181100  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_15
1884: I0815 03:52:05.181108  6633 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:52:05.181118  6633 scope.cc:202] Create variable 0x5ff1b3201723693925180941806_inner_var_17
1884: I0815 03:52:05.181125  6633 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x5ff17100
1884: 1 -> 0x5ff1b3201723693925180941806_inner_var_1 -> 0x6286ae40
1884: 2 -> linear_1.b_0 -> 0x622eae70
1884: 3 -> linear_1.w_0 -> 0x60126760
1884: 4 -> 0x5ff1b3201723693925180941806_inner_var_4 -> 0x60fc5a20
1884: 5 -> 0x5ff1b3201723693925180941806_inner_var_5 -> 0x6266d710
1884: 6 -> 0x5ff1b3201723693925180941806_inner_var_6 -> 0x60136e80
1884: 7 -> 0x5ff1b3201723693925180941806_inner_var_7 -> 0x62a98270
1884: 8 -> 0x5ff1b3201723693925180941806_inner_var_8 -> 0x60c20b40
1884: 9 -> 0x5ff1b3201723693925180941806_inner_var_9 -> 0x5ff1ed60
1884: 10 -> 0x5ff1b3201723693925180941806_inner_var_10 -> 0x610684b0
1884: 11 -> 0x5ff1b3201723693925180941806_inner_var_11 -> 0x61068650
1884: 12 -> 0x5ff1b3201723693925180941806_inner_var_12 -> 0x5ff0a8d0
1884: 13 -> 0x5ff1b3201723693925180941806_inner_var_13 -> 0x60c5d2c0
1884: 14 -> 0x5ff1b3201723693925180941806_inner_var_14 -> 0x626e8780
1884: 15 -> 0x5ff1b3201723693925180941806_inner_var_15 -> 0x610684d0
1884: 16 -> fetch0@fetch -> 0x61062c60
1884: 17 -> 0x5ff1b3201723693925180941806_inner_var_17 -> 0x62bbbe80
1884: 18 -> fetch1@fetch -> 0x6227c7c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 03:52:05.182778  6712 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.182891  6713 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.182905  6714 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.182977  6715 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.182983  6714 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183012  6716 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:52:05.183005  6712 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183019  6714 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:52:05.183053  6712 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.183050  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183091  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:52:05.183120  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183163  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.183210  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:52:05.183225  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:52:05.183285  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:52:05.183307  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x5ff1b3201723693925180941806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183352  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x5ff1b3201723693925180941806_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:52:05.183379  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x5ff1b3201723693925180941806_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183416  6716 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:52:05.183450  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x5ff1b3201723693925180941806_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.183480  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x5ff1b3201723693925180941806_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x5ff1b3201723693925180941806_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183519  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.183534  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x5ff1b3201723693925180941806_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x5ff1b3201723693925180941806_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.183563  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x5ff1b3201723693925180941806_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183584  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.183583  6712 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff1b3201723693925180941806_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183597  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x5ff1b3201723693925180941806_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.183609  6712 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.183614  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x5ff1b3201723693925180941806_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183637  6716 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.183686  6712 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff1b3201723693925180941806_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.183717  6712 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff1b3201723693925180941806_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183732  6716 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.183735  6712 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:52:05.183748  6712 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff1b3201723693925180941806_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.183780  6716 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.183846  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.183864  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5ff1b3201723693925180941806_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x5ff1b3201723693925180941806_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.183898  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183905  6712 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183921  6712 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:52:05.183924  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.183938  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.183954  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x5ff1b3201723693925180941806_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183957  6712 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5ff1b3201723693925180941806_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.183975  6712 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff1b3201723693925180941806_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.183990  6712 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:52:05.184003  6712 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5ff1b3201723693925180941806_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.184028  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x5ff1b3201723693925180941806_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:52:05.184048  6716 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x5ff1b3201723693925180941806_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5ff1b3201723693925180941806_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.184073  6716 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:52:05.184087  6716 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x5ff1b3201723693925180941806_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5ff1b3201723693925180941806_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5ff1b3201723693925180941806_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:52:05.184121  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x5ff1b490) got event_name: TaskCompletion
1884: I0815 03:52:05.184147  6633 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:52:05.184176  6633 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:52:05.189815  6633 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:52:05.189863  6633 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:52:05.190550  6633 scope.cc:202] Create variable linear_1.b_0
1884: I0815 03:52:05.190601  6633 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 03:52:05.191067  6633 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251911190450"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251911190450"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 03:52:05.191272  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62acbb50 on Place(cpu)
1884: I0815 03:52:05.191295  6633 scope.cc:202] Create variable 0x62acbb501723693925191287884_inner_var_0
1884: I0815 03:52:05.191334  6633 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251911190450"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236939251911190450 -> 0x43cf0010
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 03:52:05.191493  6633 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 03:52:05.191622  6717 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:05.191756  6718 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.191763  6719 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.191826  6720 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.191826  6719 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236939251911190450:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.191857  6719 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236939251911190450:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:52:05.191867  6721 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.191880  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x62acbcc0) got event_name: TaskCompletion
1884: I0815 03:52:05.192133  6719 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17662595104946424704 to 7008603906678534296 , after update, data is {current : 212, peak : 268}.
1884: I0815 03:52:05.192142  6719 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 17662595104946424704 to 7008603906678534296 , after update, data is {current : 212, peak : 268}.
1884: I0815 03:52:05.192193  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.192200  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251922697761"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251922697761"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:52:05.192427  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62acbb50 on Place(cpu)
1884: I0815 03:52:05.192446  6633 scope.cc:202] Create variable 0x62acbb501723693925192441219_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251922697761"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236939251922697761 -> 0x62654c00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:52:05.192667  6722 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:05.192719  6723 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.192740  6724 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.192770  6725 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.192795  6726 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.192790  6725 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236939251922697761:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.192842  6725 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236939251922697761:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.192864  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x62acbcc0) got event_name: TaskCompletion
1884: I0815 03:52:05.193037  6725 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1296615298700030421 to 7008603906678534296 , after update, data is {current : 220, peak : 268}.
1884: I0815 03:52:05.193044  6725 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 1296615298700030421 to 7008603906678534296 , after update, data is {current : 220, peak : 268}.
1884: I0815 03:52:05.193137  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.193143  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236939251922697761",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236939251932211352"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236939251922697761",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236939251932211352"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:52:05.193384  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62acbb50 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236939251922697761",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236939251932211352"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236939251932211352 -> 0x62654c00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 03:52:05.193642  6727 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:05.193702  6728 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.193728  6729 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.193756  6730 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.193789  6731 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.193785  6730 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.193821  6730 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.193846  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x62acbcc0) got event_name: TaskCompletion
1884: I0815 03:52:05.194106  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.194113  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251941900153"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251941900153"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 03:52:05.194327  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62acbb50 on Place(cpu)
1884: I0815 03:52:05.194346  6633 scope.cc:202] Create variable 0x62acbb501723693925194341366_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236939251941900153"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236939251941900153 -> 0x625df570
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:52:05.194535  6732 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:05.194594  6733 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:52:05.194607  6734 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:52:05.194643  6735 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:52:05.194662  6736 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:52:05.194660  6735 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236939251941900153:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.194703  6735 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236939251941900153:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:52:05.194728  6633 pir_interpreter.cc:1766] main_thread_blocker_(0x62acbcc0) got event_name: TaskCompletion
1884: I0815 03:52:05.194892  6735 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1296615298700030421 to 7008603906678534296 , after update, data is {current : 224, peak : 268}.
1884: I0815 03:52:05.194901  6735 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1296615298700030421 to 7008603906678534296 , after update, data is {current : 224, peak : 268}.
1884: I0815 03:52:05.194994  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.195003  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.195068  6633 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 03:52:05.195132  6633 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 03:52:05.195171  6633 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251941900153"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251932211352"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251941900153"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251932211352"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 03:52:05.195874  6633 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 03:52:05.195892  6633 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 03:52:05.195930  6633 pir_interpreter.cc:161] PirInterpreter(): 0x62acbb50 on Place(cpu)
1884: I0815 03:52:05.195961  6633 scope.cc:202] Create variable feed_name_0
1884: I0815 03:52:05.195976  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_5
1884: I0815 03:52:05.195999  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_6
1884: I0815 03:52:05.196012  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_7
1884: I0815 03:52:05.196020  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_8
1884: I0815 03:52:05.196039  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_9
1884: I0815 03:52:05.196053  6633 scope.cc:202] Create variable 0x62acbb501723693925195944712_inner_var_10
1884: I0815 03:52:05.196074  6633 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.196097  6633 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:52:05.196254  6633 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.196270  6633 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251941900153"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236939251932211352"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236939251941900153 -> 0x625df570
1884: 1 -> constant_folding@_17236939251932211352 -> 0x62654c00
1884: 2 -> linear_1.b_0 -> 0x62a97f80
1884: 3 -> linear_1.w_0 -> 0x60fdd640
1884: 4 -> feed_name_0 -> 0x60c52bc0
1884: 5 -> 0x62acbb501723693925195944712_inner_var_5 -> 0x60fc63d0
1884: 6 -> 0x62acbb501723693925195944712_inner_var_6 -> 0x60cdf030
1884: 7 -> 0x62acbb501723693925195944712_inner_var_7 -> 0x61066340
1884: 8 -> 0x62acbb501723693925195944712_inner_var_8 -> 0x62654d40
1884: 9 -> fetch_name_0 -> 0x5ff27790
1884: 10 -> fetch_name_1 -> 0x625df510
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:52:05.196843  6633 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 03:52:05.196908  6737 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:52:05.196902  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.196970  6633 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:52:05.196997  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.197027  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62acbb501723693925195944712_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62acbb501723693925195944712_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.197067  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62acbb501723693925195944712_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.197099  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.197119  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.197134  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.197162  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236939251932211352:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.197187  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236939251941900153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.197218  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236939251941900153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62acbb501723693925195944712_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:52:05.197245  6633 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236939251941900153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:52:05.197274  6633 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236939251941900153:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62acbb501723693925195944712_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:52:05.197310  6633 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:52:05.197333  6633 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:52:05.197357  6633 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:52:05.197484  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.197491  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.197541  6737 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 249760691866723513 to 7008603906678534296 , after update, data is {current : 32, peak : 268}.
1884: I0815 03:52:05.197551  6737 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 249760691866723513 to 7008603906678534296 , after update, data is {current : 32, peak : 268}.
1884: I0815 03:52:05.197588  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.197595  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......FFF..
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 60219 / 100000 (60.2%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([3, 2, 3, ..., 0, 0, 3], dtype=int64)
1884:  y: array([0, 0, 0, ..., 0, 0, 0])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 210305 / 300000 (70.1%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[0, 0, 0, ..., 0, 3, 2],
1884:        [0, 1, 0, ..., 3, 0, 3],
1884:        [1, 1, 0, ..., 2, 0, 3]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 100 / 100 (100%)
1884: Max absolute difference: 987
1884: Max relative difference: inf
1884:  x: array([487, 911, 577, 539,  91, 105,  73, 792, 682,   3, 959, 810, 454,
1884:        180, 306,  23, 572, 164, 855, 809, 143, 904, 198, 811, 600, 191,
1884:        325, 948, 833, 412, 596, 482, 516, 786,  28, 395, 735, 700, 365,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.913s
1884: 
1884: FAILED (failures=3, skipped=3)
1884: 
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39505 0.1633  0.12963 0.31202]
1884: [array([0, 3, 1, ..., 1, 3, 1], dtype=int64)]
1884: {'X': array([0.69646919, 0.28613933, 0.22685145, 0.55131477])}
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39361 0.16254 0.12863 0.31522]
1884: [array([0, 0, 0, ..., 0, 0, 3], dtype=int64)]
1884: {'X': array([0.69646919, 0.28613933, 0.22685145, 0.55131477])}
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39485 0.16364 0.12939 0.31212]
1884:  [0.25614 0.15062 0.3479  0.24534]
1884:  [0.24613 0.19889 0.1771  0.37788]]
1884: [array([[3, 3, 2, ..., 3, 3, 2],
1884:        [3, 0, 3, ..., 1, 2, 0],
1884:        [3, 3, 2, ..., 0, 0, 0]], dtype=int64)]
1884: {'X': array([[0.69646919, 0.28613933, 0.22685145, 0.55131477],
1884:        [0.71946897, 0.42310646, 0.9807642 , 0.68482974],
1884:        [0.4809319 , 0.39211752, 0.34317802, 0.72904971]])}
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39538 0.16376 0.12948 0.31138]
1884:  [0.25537 0.15257 0.34891 0.24315]
1884:  [0.24712 0.20083 0.1757  0.37635]]
1884: [array([[2, 3, 3, ..., 0, 3, 0],
1884:        [3, 2, 0, ..., 2, 0, 0],
1884:        [2, 3, 1, ..., 3, 1, 2]], dtype=int64)]
1884: {'X': array([[0.69646919, 0.28613933, 0.22685145, 0.55131477],
1884:        [0.71946897, 0.42310646, 0.9807642 , 0.68482974],
1884:        [0.4809319 , 0.39211752, 0.34317802, 0.72904971]])}
1884: I0815 03:52:05.199784  6633 mmap_allocator.cc:348] PID: 6633, MemoryMapFdSet: set size - 0
1884: I0815 03:52:05.211633  6633 mmap_allocator.cc:348] PID: 6633, MemoryMapFdSet: set size - 0
1884: I0815 03:52:05.287019  6709 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 155935143093453406 to 7008603906678534296 , after update, data is {current : 48, peak : 268}.
1884: I0815 03:52:05.287053  6709 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 155935143093453406 to 7008603906678534296 , after update, data is {current : 48, peak : 268}.
1884: I0815 03:52:05.287061  6710 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6594130148410606453 to 7008603906678534296 , after update, data is {current : 52, peak : 268}.
1884: I0815 03:52:05.287083  6710 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6594130148410606453 to 7008603906678534296 , after update, data is {current : 52, peak : 268}.
1884: I0815 03:52:05.287122  6708 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8924888205254337880 to 7008603906678534296 , after update, data is {current : 56, peak : 268}.
1884: I0815 03:52:05.287137  6708 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8924888205254337880 to 7008603906678534296 , after update, data is {current : 56, peak : 268}.
1884: I0815 03:52:05.287279  6711 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 386003630154949096 to 7008603906678534296 , after update, data is {current : 32, peak : 268}.
1884: I0815 03:52:05.287297  6711 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 386003630154949096 to 7008603906678534296 , after update, data is {current : 32, peak : 268}.
1884: I0815 03:52:05.287319  6711 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 386003630154949096 to 7008603906678534296 , after update, data is {current : 256, peak : 768}.
1884: I0815 03:52:05.287529  6712 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 7008603906678534296 to 5026948830793405923 , after update, data is {current : 768, peak : 1536}.
1884: I0815 03:52:05.287545  6712 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 7008603906678534296 to 5026948830793405923 , after update, data is {current : 12, peak : 268}.
1884: I0815 03:52:05.287550  6712 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 7008603906678534296 to 5026948830793405923 , after update, data is {current : 12, peak : 268}.
1884: I0815 03:52:05.287719  6714 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13905414744617885431 to 5026948830793405923 , after update, data is {current : 28, peak : 268}.
1884: I0815 03:52:05.287729  6714 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13905414744617885431 to 5026948830793405923 , after update, data is {current : 28, peak : 268}.
1884: I0815 03:52:05.287859  6716 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 5026948830793405923 to 8242500734260742476 , after update, data is {current : 6401792, peak : 8800800}.
1884: I0815 03:52:05.287868  6716 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 5026948830793405923 to 8242500734260742476 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 03:52:05.287873  6716 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 5026948830793405923 to 8242500734260742476 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 03:52:05.453744  6633 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:52:05.453780  6633 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:52:05.453840  6633 mmap_allocator.cc:348] PID: 6633, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   12.36 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  12.54 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
