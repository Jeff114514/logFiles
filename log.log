UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 03:07:16.316401  6270 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 03:07:17.531586  6270 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=benchmark,gpugraph_offload_param_stat,cublaslt_device_best_config,prim_forward,dump_chunk_info,cusparse_dir,use_auto_growth_pinned_allocator,accuracy_check_rtol_fp32,fuse_parameter_memory_size,enable_async_trace,dygraph_debug,memory_fraction_of_eager_deletion,new_executor_static_build,conv2d_disable_cudnn,use_fast_math,enable_gpu_memory_usage_log,dataloader_use_file_descriptor,eager_delete_tensor_gb,graph_embedding_split_infer_mode,npu_storage_format,gpugraph_offload_param_extends,cublas_dir,logging_pir_py_code_dump_symbolic_dims,enable_opt_get_features,auto_free_cudagraph_allocations_on_launch,use_autotune,use_cinn,enable_gpu_memory_usage_log_mb,pir_broadcast_tree_limit,benchmark_nccl,use_stream_safe_cuda_allocator,cudnn_deterministic,enable_graph_multi_node_sampling,max_inplace_grad_add,graph_get_neighbor_id,custom_device_mem_record,enable_pir_with_pt_in_dy2st,fleet_executor_with_standalone,tracer_onednn_ops_off,cinn_subgraph_graphviz_dir,enable_interpretercore_launch_cinn,cuda_memory_async_pool_realease_threshold,use_virtual_memory_auto_growth,enable_exit_when_partial_worker,gpugraph_sparse_table_storage_mode,gpugraph_slot_feasign_max_num,print_sub_graph_dir,reallocate_gpu_memory_in_mb,gpugraph_hbm_table_load_factor,manually_trans_conv_filter,gpugraph_parallel_copyer_split_maxsize,gpugraph_force_device_batch_num_equal,print_ir,enable_cinn_compile_cache,init_allocated_mem,gpugraph_dedup_pull_push_mode,low_precision_op_list,nvidia_package_dir,enable_unused_var_check,fraction_of_gpu_memory_to_use,enable_auto_rdma_trans,sync_nccl_allreduce,all_blocks_convert_trt,enable_collect_shape,cuda_malloc_async_pool_memory_throttle_ratio,search_cache_max_number,allocator_strategy,enable_fusion_fallback,free_idle_chunk,nccl_blocking_wait,accuracy_check_atol_bf16,allow_cinn_ops,nccl_dir,accuracy_check_atol_fp32,use_xqa_optim,cudnn_exhaustive_search_times,gpugraph_enable_print_op_debug,gpugraph_parallel_stream_num,graph_metapath_split_opt,multi_node_sample_use_gpu_table,pir_apply_inplace_pass,gpugraph_merge_grads_segment_size,enable_cse_in_dy2st,enable_api_kernel_fallback,new_executor_use_inplace,trt_ibuilder_cache,gpugraph_offload_gather_copy_maxsize,enable_cublas_tensor_op_math,use_cuda_malloc_async_allocator,tracer_profile_fname,accuracy_check_rtol_bf16,multiple_of_cupti_buffer_size,sync_after_alloc,logging_pir_py_code_dir,enable_dependency_builder_debug_info,cudnn_batchnorm_spatial_persistent,query_dest_rank_by_multi_node,cudnn_exhaustive_search,gpugraph_debug_gpu_memory,cuda_dir,logging_trunc_pir_py_code,alloc_fill_value,prim_enable_dynamic,log_memory_stats,enable_all2all_use_fp16,prim_backward,new_executor_use_local_scope,host_trace_level,enable_record_memory,ir_inplace_kernel_blacklist,fraction_of_cpu_memory_to_use,conv_workspace_size_limit,new_executor_use_cuda_graph,enable_adjust_op_order,gpugraph_enable_gpu_direct_access,gpugraph_enable_segment_merge_grads,graph_load_in_parallel,use_system_allocator,fast_eager_deletion_mode,deny_cinn_ops,mkl_dir,save_static_runtime_data,call_stack_level,set_to_1d,initial_gpu_memory_in_mb,cache_inference_while_scope,use_cuda_managed_memory,op_dir,use_auto_growth_v2,async_trace_count,gpugraph_enable_hbm_table_collision_stat,convert_all_blocks,enable_neighbor_list_use_uva,pir_apply_shape_optimization_pass,add_dependency_for_communication_op,enable_tracker_all2all,executor_log_deps_every_microseconds,enable_blaslt_global_search,jit_engine_type,allreduce_record_one_event,prim_forward_blacklist,cse_max_count,run_kp_kernel,check_infer_symbolic,cublaslt_exhaustive_search_times,use_shm_cache,curand_dir,free_when_no_cache_hit,fraction_of_cuda_pinned_memory_to_use,gpu_memory_limit_mb,enable_fuse_parallel_matmul_pass,static_executor_perfstat_filepath,selected_gpus,mklml_dir,auto_growth_chunk_size_in_mb,sort_sum_gradient,initial_cpu_memory_in_mb,enable_sparse_inner_gather,cudnn_dir,accuracy_check_atol_fp16,reader_queue_speed_test_mode,einsum_opt,prim_enabled,dynamic_static_unified_comm,get_host_by_name_time,inner_op_parallelism,cinn_compile_thread_num,tracer_onednn_ops_on,cupti_dir,local_exe_sub_scope_limit,tensor_operants_mode,gpu_allocator_retry_time,pir_debug,gpugraph_storage_mode,static_runtime_data_save_path,win_cuda_bin_dir,use_mkldnn,check_nan_inf_level,gemm_use_half_precision_compute_type,logging_pir_py_code_int_tensor_element_limit,new_executor_serial_run,prim_skip_dynamic,enable_auto_detect_gpu_topo,embedding_deterministic,check_kernel_launch,fuse_parameter_groups_size,use_pinned_memory,dist_threadpool_size,accuracy_check_rtol_fp16,enable_pir_in_executor_trace_run,cusolver_dir,prim_check_ops,lapack_dir,enable_cinn_accuracy_check,enable_cinn_auto_tune,enable_dump_main_program,use_stride_kernel,prim_all,paddle_num_threads,cusparselt_dir,print_allocator_trace_info,new_executor_sequential_run,gpugraph_load_node_list_into_hbm,graph_neighbor_size_percent,apply_pass_to_program,disable_dyshape_in_train,eager_delete_scope,enable_pir_in_executor,tensorrt_dir,pir_subgraph_saving_dir,pinned_memory_as_cpu_backend,enable_pir_api,check_nan_inf 
1884: I0815 03:07:17.531710  6270 init.cc:108] After Parse: argc is 2
1884: I0815 03:07:25.425506  6270 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:07:25.425568  6270 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 03:07:25.426329  6270 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 03:07:25.426725  6270 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 03:07:25.427907  6270 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 03:07:25.428020  6270 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 03:07:25.428124  6270 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 03:07:25.428906  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f90a6000000), and remaining 0
1884: I0815 03:07:25.429226  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.429322  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.429441  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f90a6000200), and remaining 0
1884: I0815 03:07:25.429481  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f90a6000400), and remaining 0
1884: I0815 03:07:25.433274  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f90a6000600), and remaining 0
1884: I0815 03:07:25.433441  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f90a6000800), and remaining 0
1884: I0815 03:07:25.433542  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f90a6000a00), and remaining 0
1884: I0815 03:07:25.433657  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.433686  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.433759  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.433773  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.434975  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19173110 for it.
1884: I0815 03:07:25.435149  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.435177  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.435242  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f90a6000e00), and remaining 0
1884: I0815 03:07:25.435329  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f90a60c4400), and remaining 0
1884: I0815 03:07:25.550024  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19173110 for it.
1884: I0815 03:07:25.550246  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.550288  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.550916  6270 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f90a6200000), and remaining 0
1884: I0815 03:07:25.561668  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19173110 for it.
1884: I0815 03:07:25.561832  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:25.561868  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.561918  6270 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:25.562116  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:25.563148  6270 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 03:07:25.563169  6270 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 03:07:25.563225  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:25.563315  6270 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 03:07:25.563344  6270 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.563409  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:25.563496  6270 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 03:07:25.563513  6270 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.563540  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:25.563719  6270 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 03:07:25.563735  6270 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.563908  6270 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 03:07:25.563931  6270 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:25.564003  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:25.565973  6270 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 03:07:25.566092  6270 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 03:07:25.566118  6270 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 03:07:25.566177  6270 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 03:07:27.055883  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.055941  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.056226  6270 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 03:07:27.056248  6270 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.061579  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.061626  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.062773  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.062798  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.062815  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.063617  6270 program_interpreter.cc:243] New Executor is Running.
1884: I0815 03:07:27.063633  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.063657  6270 scope.cc:202] Create variable feed
1884: I0815 03:07:27.063668  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.063680  6270 scope.cc:202] Create variable fetch
1884: I0815 03:07:27.063683  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.063694  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.063701  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.063704  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.063709  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.066298  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.066689  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.066704  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.066709  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.068540  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.068601  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.068612  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.068621  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.068631  6270 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:07:27.068641  6270 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6024aef0 type is 7
1884: I0815 03:07:27.068645  6270 scope.cc:202] Create variable x
1884: I0815 03:07:27.068650  6270 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x602499b0 type is 7
1884: I0815 03:07:27.068717  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.068723  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.068727  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.068732  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.068878  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.068907  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.069033  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.069046  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.069067  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.069264  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.069294  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.069324  6270 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.069332  6270 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60251a10Variable Type 7
1884: I0815 03:07:27.069361  6270 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.069388  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.069442  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.069464  6270 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.070739  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.070796  6270 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.071203  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.075204  6270 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:07:27.075232  6270 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:07:27.075361  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.075397  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.075927  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: I0815 03:07:27.076021  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.076047  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.076508  6270 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: I0815 03:07:27.076577  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.076602  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.076630  6270 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.076921  6270 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:07:27.076936  6270 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:07:27.077057  6270 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 03:07:27.077082  6270 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.077493  6270 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:07:27.077508  6270 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:07:27.077553  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.077574  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.077764  6270 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:07:27.077775  6270 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:07:27.077811  6270 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:07:27.077831  6270 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:07:27.077847  6270 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.080891  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.080924  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.080989  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.081000  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.083177  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.083582  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.083601  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.083607  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.085512  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.085584  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.085597  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.085608  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6027ca70 type is 7
1884: I0815 03:07:27.085620  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.085628  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6027cde0 type is 7
1884: I0815 03:07:27.085633  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.085639  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.085705  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.085713  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.085718  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.085723  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.085783  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.085805  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.085873  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.085884  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.085904  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.086213  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.086231  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.086251  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.086261  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x602817c0Variable Type 7
1884: I0815 03:07:27.086280  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.086311  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.086337  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.086354  6270 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.087124  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.087159  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.087379  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.098702  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: I0815 03:07:27.099007  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:07:27.106139  6270 pir_interpreter.cc:161] PirInterpreter(): 0x6043f950 on Place(gpu:0)
1884: I0815 03:07:27.106199  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.106235  6270 scope.cc:202] Create variable 0x6043f9501723691247106175478_inner_var_1
1884: I0815 03:07:27.106248  6270 scope.cc:202] Create variable 0x6043f9501723691247106175478_inner_var_2
1884: I0815 03:07:27.106261  6270 scope.cc:202] Create variable 0x6043f9501723691247106175478_inner_var_3
1884: I0815 03:07:27.106271  6270 scope.cc:202] Create variable 0x6043f9501723691247106175478_inner_var_4
1884: I0815 03:07:27.106282  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.106845  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.106866  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.106871  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 03:07:27.106922  6270 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6043f8b0
1884: 1 -> 0x6043f9501723691247106175478_inner_var_1 -> 0x6043f930
1884: 2 -> 0x6043f9501723691247106175478_inner_var_2 -> 0x60440200
1884: 3 -> 0x6043f9501723691247106175478_inner_var_3 -> 0x6043f090
1884: 4 -> 0x6043f9501723691247106175478_inner_var_4 -> 0x604405b0
1884: 5 -> fetch0@fetch -> 0x60440dc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.107717  6270 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 03:07:27.116359  6308 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.116400  6313 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.116501  6311 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.116487  6313 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.116533  6311 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6043f9501723691247106175478_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.116562  6313 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:07:27.116657  6311 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6043f9501723691247106175478_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.116731  6313 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6043f9501723691247106175478_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6043f9501723691247106175478_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.116997  6313 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6043f9501723691247106175478_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6043f9501723691247106175478_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:07:27.117106  6311 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6043f9501723691247106175478_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.117130  6311 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.120357  6309 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.121354  6312 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.121681  6311 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6043f9501723691247106175478_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6043f9501723691247106175478_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:07:27.121776  6311 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6043f9501723691247106175478_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.121816  6311 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.125963  6311 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6043f9501723691247106175478_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:07:27.126098  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x6043fac0) got event_name: TaskCompletion
1884: I0815 03:07:27.126127  6270 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.144345  6310 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.223390  6308 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 5213701416252841917 to 8710450716130284383 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:07:27.223426  6308 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 5213701416252841917 to 4880498836285001438 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:07:27.223433  6308 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 5213701416252841917 to 4880498836285001438 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:07:27.223647  6311 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4880498836285001438 to 6227015612790772271 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:07:27.223654  6311 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4880498836285001438 to 6227015612790772271 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:07:27.225379  6313 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 03:07:27.225409  6313 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.232158  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.232194  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.232254  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.232259  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.234217  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.234639  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.234651  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.234656  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.236281  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.236449  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.236459  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.236466  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60232170 type is 7
1884: I0815 03:07:27.236474  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.236477  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x602667b0 type is 7
1884: I0815 03:07:27.236481  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.236486  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.236547  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.236552  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.236557  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.236560  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.236616  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.236630  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.236692  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.236699  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.236714  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.236871  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.236881  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.236896  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.236899  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3978270Variable Type 7
1884: I0815 03:07:27.236915  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.236932  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.236951  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.236964  6270 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.238665  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.238710  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.238941  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.243921  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: I0815 03:07:27.244203  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:07:27.247684  6270 pir_interpreter.cc:161] PirInterpreter(): 0x60266b00 on Place(gpu:0)
1884: I0815 03:07:27.247723  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.247745  6270 scope.cc:202] Create variable 0x60266b001723691247247711733_inner_var_1
1884: I0815 03:07:27.247753  6270 scope.cc:202] Create variable 0x60266b001723691247247711733_inner_var_2
1884: I0815 03:07:27.247764  6270 scope.cc:202] Create variable 0x60266b001723691247247711733_inner_var_3
1884: I0815 03:07:27.247772  6270 scope.cc:202] Create variable 0x60266b001723691247247711733_inner_var_4
1884: I0815 03:07:27.247783  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.248188  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.248200  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.248204  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6024ee80
1884: 1 -> 0x60266b001723691247247711733_inner_var_1 -> 0x395ebe0
1884: 2 -> 0x60266b001723691247247711733_inner_var_2 -> 0x44023600
1884: 3 -> 0x60266b001723691247247711733_inner_var_3 -> 0x1e64780
1884: 4 -> 0x60266b001723691247247711733_inner_var_4 -> 0x6043eb40
1884: 5 -> fetch0@fetch -> 0x603dba00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.249338  6318 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.249338  6315 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.249397  6318 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60266b001723691247247711733_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.249467  6318 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60266b001723691247247711733_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.249344  6316 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.249338  6314 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.253335  6319 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.253394  6319 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.253450  6319 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:07:27.253499  6319 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60266b001723691247247711733_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60266b001723691247247711733_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.253641  6319 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60266b001723691247247711733_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60266b001723691247247711733_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.249347  6317 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.257355  6315 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60266b001723691247247711733_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.257418  6315 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.260293  6315 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60266b001723691247247711733_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60266b001723691247247711733_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.260380  6315 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60266b001723691247247711733_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.260406  6315 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.262460  6315 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60266b001723691247247711733_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.263324  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x60266c70) got event_name: TaskCompletion
1884: I0815 03:07:27.263363  6270 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.330376  6314 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8710450716130284383 to 13774760641286790190 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:07:27.330415  6314 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8710450716130284383 to 16896230401215005821 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:07:27.330420  6314 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8710450716130284383 to 16896230401215005821 , after update, data is {current : 2399996, peak : 4800000}.
1884: I0815 03:07:27.335364  6318 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1456120502615146076 to 16896230401215005821 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:07:27.335402  6318 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1456120502615146076 to 16896230401215005821 , after update, data is {current : 2400000, peak : 4800000}.
1884: I0815 03:07:27.340360  6315 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 16896230401215005821 to 6227015612790772271 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:07:27.340390  6315 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800000} from thread 16896230401215005821 to 6227015612790772271 , after update, data is {current : 3200000, peak : 4800000}.
1884: I0815 03:07:27.341447  6319 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.350418  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.350455  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.350520  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.350526  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.352444  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.352833  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.352842  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.352847  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.354450  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.354590  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.354599  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.354606  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x3968bc0 type is 7
1884: I0815 03:07:27.354614  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.354617  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x35f2e20 type is 7
1884: I0815 03:07:27.354621  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.354626  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.354686  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.354691  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.354694  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.354699  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.354755  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.354769  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.354831  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.354838  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.354853  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.354900  6270 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.355052  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.355113  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.355122  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.355137  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.355142  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x397fa50Variable Type 7
1884: I0815 03:07:27.355159  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.355176  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.355195  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.355208  6270 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.359361  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.359423  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.359666  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.360678  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: I0815 03:07:27.360922  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:07:27.364236  6270 pir_interpreter.cc:161] PirInterpreter(): 0x62b75260 on Place(gpu:0)
1884: I0815 03:07:27.364272  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.364295  6270 scope.cc:202] Create variable 0x62b752601723691247364263178_inner_var_1
1884: I0815 03:07:27.364315  6270 scope.cc:202] Create variable 0x62b752601723691247364263178_inner_var_2
1884: I0815 03:07:27.364324  6270 scope.cc:202] Create variable 0x62b752601723691247364263178_inner_var_3
1884: I0815 03:07:27.364337  6270 scope.cc:202] Create variable 0x62b752601723691247364263178_inner_var_4
1884: I0815 03:07:27.364348  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.364756  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.364775  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.364779  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60284780
1884: 1 -> 0x62b752601723691247364263178_inner_var_1 -> 0x6021a180
1884: 2 -> 0x62b752601723691247364263178_inner_var_2 -> 0x62b90480
1884: 3 -> 0x62b752601723691247364263178_inner_var_3 -> 0x43433300
1884: 4 -> 0x62b752601723691247364263178_inner_var_4 -> 0x398f260
1884: 5 -> fetch0@fetch -> 0x393e040
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.366333  6322 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.366336  6320 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.366382  6322 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b752601723691247364263178_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.366448  6322 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b752601723691247364263178_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.366762  6325 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.366802  6325 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.366843  6325 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:07:27.366876  6325 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62b752601723691247364263178_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62b752601723691247364263178_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.366930  6325 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.367072  6325 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.367101  6325 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62b752601723691247364263178_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62b752601723691247364263178_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:07:27.368327  6324 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.368391  6324 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62b752601723691247364263178_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.368429  6324 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.368513  6324 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62b752601723691247364263178_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62b752601723691247364263178_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:07:27.368541  6324 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62b752601723691247364263178_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.368558  6324 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.368568  6324 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62b752601723691247364263178_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:07:27.369323  6323 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.366334  6321 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.377532  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x62b753d0) got event_name: TaskCompletion
1884: I0815 03:07:27.377601  6270 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.427594  6320 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 13774760641286790190 to 8710450716130284383 , after update, data is {current : 0, peak : 3328}.
1884: I0815 03:07:27.427628  6320 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 13774760641286790190 to 16896230401215005821 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:07:27.427634  6320 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 13774760641286790190 to 16896230401215005821 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:07:27.432402  6324 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 16896230401215005821 to 8710450716130284383 , after update, data is {current : 796, peak : 2000}.
1884: I0815 03:07:27.432433  6324 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 16896230401215005821 to 8710450716130284383 , after update, data is {current : 796, peak : 2000}.
1884: I0815 03:07:27.436352  6322 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13273578191364386087 to 8710450716130284383 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:07:27.436383  6322 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13273578191364386087 to 8710450716130284383 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:07:27.437362  6325 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:07:27.437391  6325 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 3200800, peak : 4800000}.
1884: I0815 03:07:27.437397  6325 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.442613  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.442644  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.442704  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.442710  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.444594  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.444969  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.444979  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.444984  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.449672  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.449826  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.449837  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.449846  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x624ca0b0 type is 7
1884: I0815 03:07:27.449853  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.449857  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6024ae10 type is 7
1884: I0815 03:07:27.449862  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.449867  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.449932  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.449937  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.449942  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.449946  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.450006  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.450021  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.450083  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.450089  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.450104  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.450409  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.450421  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.450438  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.450444  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6043edb0Variable Type 7
1884: I0815 03:07:27.450464  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.450480  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.450500  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.450515  6270 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.451270  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.451313  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.451534  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.454748  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: I0815 03:07:27.454991  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:07:27.458401  6270 pir_interpreter.cc:161] PirInterpreter(): 0x624c9b80 on Place(gpu:0)
1884: I0815 03:07:27.458446  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.458467  6270 scope.cc:202] Create variable 0x624c9b801723691247458435713_inner_var_1
1884: I0815 03:07:27.458477  6270 scope.cc:202] Create variable 0x624c9b801723691247458435713_inner_var_2
1884: I0815 03:07:27.458487  6270 scope.cc:202] Create variable 0x624c9b801723691247458435713_inner_var_3
1884: I0815 03:07:27.458495  6270 scope.cc:202] Create variable 0x624c9b801723691247458435713_inner_var_4
1884: I0815 03:07:27.458504  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.458892  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.458904  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.458909  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6043eb20
1884: 1 -> 0x624c9b801723691247458435713_inner_var_1 -> 0x6296d470
1884: 2 -> 0x624c9b801723691247458435713_inner_var_2 -> 0x6026c160
1884: 3 -> 0x624c9b801723691247458435713_inner_var_3 -> 0x3984a70
1884: 4 -> 0x624c9b801723691247458435713_inner_var_4 -> 0x625c68e0
1884: 5 -> fetch0@fetch -> 0x60261360
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.460331  6328 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.460332  6331 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.460376  6328 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x624c9b801723691247458435713_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.460397  6330 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.460395  6331 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.460444  6328 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x624c9b801723691247458435713_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.460451  6331 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:07:27.460486  6331 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x624c9b801723691247458435713_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x624c9b801723691247458435713_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.460736  6331 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x624c9b801723691247458435713_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x624c9b801723691247458435713_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:07:27.461331  6327 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.461349  6328 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x624c9b801723691247458435713_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.461375  6328 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.462633  6328 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x624c9b801723691247458435713_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x624c9b801723691247458435713_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:07:27.462684  6328 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x624c9b801723691247458435713_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.462707  6328 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.463307  6328 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x624c9b801723691247458435713_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:07:27.463346  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x624c9cf0) got event_name: TaskCompletion
1884: I0815 03:07:27.463364  6270 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.460333  6329 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.460335  6326 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.470311  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.470345  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.470413  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.470419  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.472572  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.472990  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.473001  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.473007  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.474980  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.475121  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.475131  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.475140  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62b78280 type is 7
1884: I0815 03:07:27.475148  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.475152  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x60249420 type is 7
1884: I0815 03:07:27.475157  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.475163  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.475230  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.475236  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.475241  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.475246  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.475314  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.475338  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.475416  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.475426  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.475446  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.475718  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.475730  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.475749  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.475756  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6023b900Variable Type 7
1884: I0815 03:07:27.475776  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.475796  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.475821  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.475836  6270 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.476616  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.476655  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.476868  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.517376  6326 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 8710450716130284383 to 13774760641286790190 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:07:27.517417  6326 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8710450716130284383 to 4880498836285001438 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:07:27.517424  6326 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 8710450716130284383 to 4880498836285001438 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:07:27.529349  6328 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4880498836285001438 to 6227015612790772271 , after update, data is {current : 4000800, peak : 4800000}.
1884: I0815 03:07:27.529378  6328 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 4880498836285001438 to 6227015612790772271 , after update, data is {current : 4000800, peak : 4800800}.
1884: I0815 03:07:27.530455  6331 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 03:07:27.530481  6331 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.539001  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.539036  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.539096  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.539103  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.540979  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.541376  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.541386  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.541391  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.542984  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.543109  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.543118  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.543124  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62a6fdd0 type is 7
1884: I0815 03:07:27.543133  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.543136  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6249a1d0 type is 7
1884: I0815 03:07:27.543140  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.543145  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.543206  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.543211  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.543215  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.543219  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.543273  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.543288  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.543371  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.543380  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.543393  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.543552  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.543568  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.543586  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.543589  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60441980Variable Type 7
1884: I0815 03:07:27.543605  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.543622  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.543641  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.543653  6270 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.545385  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.545428  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.545660  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.554317  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: I0815 03:07:27.554616  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:07:27.558148  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623ccb20 on Place(gpu:0)
1884: I0815 03:07:27.558189  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.558211  6270 scope.cc:202] Create variable 0x623ccb201723691247558177635_inner_var_1
1884: I0815 03:07:27.558220  6270 scope.cc:202] Create variable 0x623ccb201723691247558177635_inner_var_2
1884: I0815 03:07:27.558230  6270 scope.cc:202] Create variable 0x623ccb201723691247558177635_inner_var_3
1884: I0815 03:07:27.558239  6270 scope.cc:202] Create variable 0x623ccb201723691247558177635_inner_var_4
1884: I0815 03:07:27.558248  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.558682  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.558696  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.558699  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60269bf0
1884: 1 -> 0x623ccb201723691247558177635_inner_var_1 -> 0x6024bac0
1884: 2 -> 0x623ccb201723691247558177635_inner_var_2 -> 0x602553b0
1884: 3 -> 0x623ccb201723691247558177635_inner_var_3 -> 0x622afa70
1884: 4 -> 0x623ccb201723691247558177635_inner_var_4 -> 0x623cd1e0
1884: 5 -> fetch0@fetch -> 0x6043ff20
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.559690  6333 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.559720  6333 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x623ccb201723691247558177635_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.559783  6333 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x623ccb201723691247558177635_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.559863  6337 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.559882  6337 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.559911  6337 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:07:27.559942  6337 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x623ccb201723691247558177635_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x623ccb201723691247558177635_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.560074  6337 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x623ccb201723691247558177635_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x623ccb201723691247558177635_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.560129  6333 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x623ccb201723691247558177635_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.560150  6333 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.560335  6334 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.560335  6335 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.562970  6333 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x623ccb201723691247558177635_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x623ccb201723691247558177635_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.563027  6333 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x623ccb201723691247558177635_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.563050  6333 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.560335  6336 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.560335  6332 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.565106  6333 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x623ccb201723691247558177635_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:07:27.565162  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623ccc90) got event_name: TaskCompletion
1884: I0815 03:07:27.565178  6270 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.573956  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.573995  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.574060  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.574066  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.576247  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.576705  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.576719  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.576725  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.578768  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.578917  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.578928  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.578936  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x623b1a30 type is 7
1884: I0815 03:07:27.578945  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.578948  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x623cc270 type is 7
1884: I0815 03:07:27.578953  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.578959  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.579028  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.579033  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.579037  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.579042  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.579104  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.579119  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.579190  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.579197  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.579216  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.579383  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.579396  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.579414  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.579419  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62b91bc0Variable Type 7
1884: I0815 03:07:27.579438  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.579456  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.579480  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.579495  6270 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.581198  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.581238  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.581513  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.629377  6332 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 13774760641286790190 to 8710450716130284383 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:07:27.629415  6332 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 13774760641286790190 to 1456120502615146076 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:07:27.629421  6332 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 13774760641286790190 to 1456120502615146076 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:07:27.631428  6333 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1456120502615146076 to 6227015612790772271 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 03:07:27.631456  6333 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1456120502615146076 to 6227015612790772271 , after update, data is {current : 6400800, peak : 8800800}.
1884: I0815 03:07:27.638451  6337 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 8710450716130284383 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.644126  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.644165  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.644227  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.644232  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.646138  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.646559  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.646570  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.646575  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.648190  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.648334  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.648344  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.648351  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60441960 type is 7
1884: I0815 03:07:27.648358  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.648362  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x628f7b60 type is 7
1884: I0815 03:07:27.648366  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.648371  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.648443  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.648453  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.648458  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.648461  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.648519  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.648535  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.648597  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.648603  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.648618  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.648665  6270 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.648819  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.648874  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.648883  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.648900  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.648905  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x623b2760Variable Type 7
1884: I0815 03:07:27.648922  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.648939  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.648958  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.648970  6270 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.649076  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.649097  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.653347  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.654484  6270 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19190b60 for it.
1884: I0815 03:07:27.654757  6270 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19258800 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:07:27.658102  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623bd8a0 on Place(gpu:0)
1884: I0815 03:07:27.658138  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.658159  6270 scope.cc:202] Create variable 0x623bd8a01723691247658127590_inner_var_1
1884: I0815 03:07:27.658169  6270 scope.cc:202] Create variable 0x623bd8a01723691247658127590_inner_var_2
1884: I0815 03:07:27.658177  6270 scope.cc:202] Create variable 0x623bd8a01723691247658127590_inner_var_3
1884: I0815 03:07:27.658186  6270 scope.cc:202] Create variable 0x623bd8a01723691247658127590_inner_var_4
1884: I0815 03:07:27.658195  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:27.658638  6270 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:07:27.658653  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.658656  6270 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6249a340
1884: 1 -> 0x623bd8a01723691247658127590_inner_var_1 -> 0x6026ba70
1884: 2 -> 0x623bd8a01723691247658127590_inner_var_2 -> 0x623c9cd0
1884: 3 -> 0x623bd8a01723691247658127590_inner_var_3 -> 0x628d6080
1884: 4 -> 0x623bd8a01723691247658127590_inner_var_4 -> 0x628fe9c0
1884: 5 -> fetch0@fetch -> 0x6023f150
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:07:27.660329  6341 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:27.660339  6342 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:27.660372  6341 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x623bd8a01723691247658127590_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.660430  6341 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x623bd8a01723691247658127590_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:27.662375  6340 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:27.660331  6339 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:27.660333  6338 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:27.660334  6343 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:27.665372  6343 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.665428  6343 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:07:27.665473  6343 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x623bd8a01723691247658127590_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x623bd8a01723691247658127590_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.665534  6343 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.665699  6343 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.665728  6343 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x623bd8a01723691247658127590_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x623bd8a01723691247658127590_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:07:27.666342  6339 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x623bd8a01723691247658127590_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.666388  6339 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.666476  6339 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x623bd8a01723691247658127590_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x623bd8a01723691247658127590_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:07:27.666512  6339 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x623bd8a01723691247658127590_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:27.666529  6339 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.666540  6339 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x623bd8a01723691247658127590_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:07:27.669334  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623bda10) got event_name: TaskCompletion
1884: I0815 03:07:27.669371  6270 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:07:27.671252  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.671283  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.671371  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.671378  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.673543  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.673978  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.673990  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.673996  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.676009  6270 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:07:27.676156  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.676167  6270 scope.cc:202] Create variable Out
1884: I0815 03:07:27.676175  6270 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x625caf50 type is 7
1884: I0815 03:07:27.676184  6270 scope.cc:202] Create variable X
1884: I0815 03:07:27.676187  6270 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x625ca290 type is 7
1884: I0815 03:07:27.676192  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.676198  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.676270  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.676275  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.676280  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.676285  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.676357  6270 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.676373  6270 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.676443  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.676451  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.676469  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.676519  6270 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.676658  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.676710  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.676720  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.676739  6270 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.676745  6270 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6228e6f0Variable Type 7
1884: I0815 03:07:27.676765  6270 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.676784  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.676807  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.676822  6270 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.676931  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.676956  6270 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.677187  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.725378  6338 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 8710450716130284383 to 13774760641286790190 , after update, data is {current : 0, peak : 10240}.
1884: I0815 03:07:27.725417  6338 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8710450716130284383 to 16896230401215005821 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:07:27.725423  6338 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8710450716130284383 to 16896230401215005821 , after update, data is {current : 796, peak : 1600}.
1884: I0815 03:07:27.732374  6339 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 16896230401215005821 to 13774760641286790190 , after update, data is {current : 796, peak : 8000}.
1884: I0815 03:07:27.732409  6339 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 16896230401215005821 to 13774760641286790190 , after update, data is {current : 796, peak : 8000}.
1884: I0815 03:07:27.736361  6341 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13273578191364386087 to 13774760641286790190 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:07:27.736385  6341 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13273578191364386087 to 13774760641286790190 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:07:27.740382  6343 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 03:07:27.740418  6343 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 6401600, peak : 8800800}.
1884: I0815 03:07:27.740424  6343 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 13774760641286790190 to 6227015612790772271 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:07:27.752310  6270 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 03:07:27.752388  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:07:27.753629  6270 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:07:27.754513  6270 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 03:07:27.754546  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:07:27.755934  6270 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 03:07:27.755956  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:07:27.756722  6270 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 03:07:27.757751  6270 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 03:07:27.757776  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:07:27.763404  6270 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 03:07:27.763440  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:07:27.764210  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.764236  6270 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:07:27.764240  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.764247  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.766446  6270 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 03:07:27.766476  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:07:27.767573  6270 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 03:07:27.767599  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:07:27.768630  6270 pybind.cc:1827] need skip: 0
1884: I0815 03:07:27.768954  6270 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:07:27.770882  6270 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:07:27.774760  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.774781  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.774787  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.776844  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.776867  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.776875  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.776880  6270 scope.cc:202] Create variable learning_rate_0
1884: I0815 03:07:27.776887  6270 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61f2bb90 type is 7
1884: I0815 03:07:27.776891  6270 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:07:27.776894  6270 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61f2c4f0 type is 7
1884: I0815 03:07:27.776898  6270 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:07:27.776902  6270 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61f2c5a0 type is 7
1884: I0815 03:07:27.776970  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.776974  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.776978  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.776983  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.777041  6270 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777055  6270 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777077  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:07:27.777251  6270 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777261  6270 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777341  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.777386  6270 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777393  6270 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.777421  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.783694  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.785394  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.785895  6270 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:07:27.786150  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.786487  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.786722  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.786739  6270 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:07:27.786813  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.786818  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.786823  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.786929  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.786939  6270 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:07:27.788579  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.790006  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.796496  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.796742  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.796754  6270 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:07:27.796763  6270 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x624cd9b0 type is 7
1884: I0815 03:07:27.796772  6270 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:07:27.796775  6270 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x624cd730 type is 7
1884: I0815 03:07:27.796779  6270 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 03:07:27.796782  6270 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x624cd820 type is 7
1884: I0815 03:07:27.796787  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.796792  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.796795  6270 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:07:27.796798  6270 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x624ceda0 type is 7
1884: I0815 03:07:27.796802  6270 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61f2bb90 type is 7
1884: I0815 03:07:27.796806  6270 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61f2c4f0 type is 7
1884: I0815 03:07:27.796811  6270 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 03:07:27.796814  6270 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x624ced80 type is 7
1884: I0815 03:07:27.796818  6270 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:07:27.796820  6270 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x624cf2e0 type is 7
1884: I0815 03:07:27.796823  6270 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61f2c5a0 type is 7
1884: I0815 03:07:27.796828  6270 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 03:07:27.796830  6270 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x624cf550 type is 7
1884: I0815 03:07:27.796834  6270 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 03:07:27.796836  6270 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x624cf790 type is 7
1884: I0815 03:07:27.796840  6270 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:07:27.796844  6270 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x624cf9f0 type is 7
1884: I0815 03:07:27.796955  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.796970  6270 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:07:27.797041  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.797046  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.797051  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.797056  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.797124  6270 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.797138  6270 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.797158  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:07:27.797348  6270 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.797358  6270 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.797379  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:07:27.797470  6270 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:07:27.797557  6270 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 03:07:27.798738  6270 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.798760  6270 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.798832  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.798897  6270 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.798904  6270 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.798918  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:07:27.798943  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.799010  6270 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.799016  6270 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.799029  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:07:27.799121  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.799129  6270 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.799144  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:27.799258  6270 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.800408  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.800503  6270 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800515  6270 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800535  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:07:27.800575  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.800631  6270 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800639  6270 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800654  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:07:27.800771  6270 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800779  6270 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800803  6270 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:27.800849  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.800856  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.800873  6270 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.800879  6270 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62a728b0Variable Type 7
1884: I0815 03:07:27.800901  6270 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.800917  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.800935  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.800947  6270 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.800998  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.801023  6270 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:07:27.801048  6270 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.801055  6270 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.801069  6270 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:07:27.801072  6270 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62a73d00Variable Type 7
1884: I0815 03:07:27.801085  6270 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:07:27.801095  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.801108  6270 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.801117  6270 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:27.801151  6270 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:07:27.801162  6270 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 03:07:27.801625  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:07:27.801662  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:07:27.801679  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:07:27.801714  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:07:27.801746  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.801762  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:07:27.812975  6270 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:07:27.813035  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:07:27.813979  6270 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:07:27.814004  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:07:27.815495  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.817425  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.818269  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.818395  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.818910  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.827221  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.829739  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.833016  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.835284  6270 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 03:07:27.836246  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.836267  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.836272  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.840911  6270 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:07:27.840938  6270 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3954ca0 type is 9
1884: I0815 03:07:27.840947  6270 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x1bec520 type is 10
1884: I0815 03:07:27.840953  6270 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x61f2c4f0 type is 7
1884: I0815 03:07:27.840957  6270 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x61f2c5a0 type is 7
1884: I0815 03:07:27.840962  6270 scope.cc:202] Create variable saved_params
1884: I0815 03:07:27.840965  6270 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x61fd9d40 type is 17
1884: I0815 03:07:27.841003  6270 interpreter_util.cc:594] Static build: 0
1884: I0815 03:07:27.841007  6270 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:07:27.841012  6270 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:07:27.841017  6270 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:07:27.841074  6270 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.841089  6270 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:07:27.841992  6270 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:07:27.842031  6270 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:07:27.842094  6270 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 03:07:27.843402  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.843468  6270 scope.cc:202] Create variable feed
1884: I0815 03:07:27.843475  6270 naive_executor.cc:189] 0x62db5370 Create persistable variable feed, which pointer is 0x62ee2400
1884: I0815 03:07:27.843482  6270 scope.cc:202] Create variable fetch
1884: I0815 03:07:27.843484  6270 naive_executor.cc:189] 0x62db5370 Create persistable variable fetch, which pointer is 0x62db78a0
1884: I0815 03:07:27.843488  6270 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:07:27.843490  6270 naive_executor.cc:189] 0x62db5370 Create persistable variable linear_0.b_0, which pointer is 0x62db54d0
1884: I0815 03:07:27.843497  6270 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:07:27.843498  6270 naive_executor.cc:189] 0x62db5370 Create persistable variable linear_0.w_0, which pointer is 0x62db7c30
1884: I0815 03:07:27.843514  6270 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 03:07:27.843848  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.843946  6270 program_converter.cc:296] is_legacy_program : 0
1884: I0815 03:07:27.844009  6270 executor.cc:183] Old Executor is Running.
1884: I0815 03:07:27.844089  6270 executor.cc:92] Creating Variables for block 0
1884: I0815 03:07:27.844095  6270 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 03:07:27.844098  6270 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x62db54d0 type is 7
1884: I0815 03:07:27.844102  6270 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 03:07:27.844105  6270 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x62db7c30 type is 7
1884: I0815 03:07:27.844143  6270 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.844229  6270 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 03:07:27.844270  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.844274  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:27.848466  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.848603  6270 graph.cc:149] create OpNode by feed
1884: I0815 03:07:27.848642  6270 graph.cc:149] create OpNode by matmul_v2
1884: I0815 03:07:27.848656  6270 graph.cc:149] create OpNode by elementwise_add
1884: I0815 03:07:27.848668  6270 graph.cc:149] create OpNode by abs
1884: I0815 03:07:27.848676  6270 graph.cc:149] create OpNode by assign_value
1884: I0815 03:07:27.848691  6270 graph.cc:149] create OpNode by multinomial
1884: I0815 03:07:27.848699  6270 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:07:27.848716  6270 graph.cc:149] create OpNode by scale
1884: I0815 03:07:27.848726  6270 graph.cc:149] create OpNode by scale
1884: I0815 03:07:27.848737  6270 graph.cc:149] create OpNode by fetch
1884: I0815 03:07:27.848752  6270 graph.cc:149] create OpNode by fetch
1884: I0815 03:07:27.848770  6270 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 03:07:27.850103  6270 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 03:07:27.850109  6270 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 03:07:27.850189  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.850194  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 03:07:27.850311  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.851624  6270 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:07:27.851706  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.851709  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 03:07:27.851749  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.851752  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 03:07:27.851799  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.851855  6270 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:07:27.851886  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.851889  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 03:07:27.851907  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.851917  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.851938  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.851941  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 03:07:27.851980  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.851999  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.852021  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852025  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 03:07:27.852066  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852140  6270 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:07:27.852166  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852170  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 03:07:27.852200  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852217  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.852237  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852241  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 03:07:27.852269  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852421  6270 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:07:27.852448  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852452  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 03:07:27.852488  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852506  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.852526  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852530  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 03:07:27.852550  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852561  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.852582  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852586  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 03:07:27.852607  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852617  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.852638  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852639  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 03:07:27.852663  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852730  6270 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:07:27.852757  6270 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:07:27.852769  6270 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:07:27.852782  6270 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 03:07:27.852803  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.852806  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 03:07:27.852828  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.852867  6270 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:07:27.852885  6270 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:07:27.852895  6270 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:07:27.852905  6270 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:07:27.852936  6270 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:07:27.852946  6270 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 03:07:27.854230  6270 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:07:27.854276  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.854280  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 03:07:27.854324  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.854343  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.854370  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.854374  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 03:07:27.854398  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.854445  6270 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:07:27.854475  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.854477  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 03:07:27.854494  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.854508  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.854528  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.854532  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 03:07:27.854565  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.854645  6270 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 03:07:27.854665  6270 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:07:27.854677  6270 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:07:27.854689  6270 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:07:27.854702  6270 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:07:27.854715  6270 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:07:27.854728  6270 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 03:07:27.854749  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.854816  6270 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 03:07:27.854835  6270 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:07:27.854846  6270 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:07:27.854857  6270 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:07:27.854869  6270 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:07:27.854880  6270 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:07:27.854893  6270 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 03:07:27.854936  6270 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:07:27.855239  6270 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:07:27.855268  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.855270  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 03:07:27.855324  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855381  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855412  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855458  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855482  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855521  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855543  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855578  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855597  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855628  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855645  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855674  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855686  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855711  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855722  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855743  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855752  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855769  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855792  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.855795  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 03:07:27.855821  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855859  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855883  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.855886  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 03:07:27.855895  6270 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:07:27.855898  6270 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:07:27.855947  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.855966  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.855990  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.855993  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:07:27.856001  6270 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:07:27.856003  6270 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:07:27.856042  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.856062  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.856086  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.856088  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 03:07:27.856096  6270 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:07:27.856098  6270 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:07:27.856129  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.856146  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.856168  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.856169  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:07:27.856177  6270 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:07:27.856179  6270 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:07:27.856216  6270 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:07:27.856235  6270 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:07:27.856256  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.856259  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 03:07:27.856271  6270 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 03:07:27.861382  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.861410  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 03:07:27.861544  6270 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:07:27.861580  6270 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.861604  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:07:27.861687  6270 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 03:07:27.861708  6270 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:07:27.861742  6270 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:07:27.861770  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.861773  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 03:07:27.862776  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.862795  6270 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 03:07:27.862851  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.862856  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:27.863481  6270 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 03:07:27.863691  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.863762  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:27.863765  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:27.864173  6270 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:07:27.864425  6270 graph.h:183] deleting __fuse_statis__
1884: I0815 03:07:27.864432  6270 graph.h:183] deleting pass_recorder
1884: I0815 03:07:27.864439  6270 graph.h:183] deleting stale_program_op_descs
1884: I0815 03:07:27.864531  6270 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 03:07:27.864540  6270 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:07:27.864543  6270 naive_executor.cc:195] 0x62db5370 Create variable abs_0.tmp_0, which pointer is 0x620080f0
1884: I0815 03:07:27.864552  6270 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:07:27.864554  6270 naive_executor.cc:195] 0x62db5370 Create variable gaussian_0.tmp_0, which pointer is 0x62dad100
1884: I0815 03:07:27.864565  6270 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:07:27.864568  6270 naive_executor.cc:195] 0x62db5370 Create variable linear_0.tmp_1, which pointer is 0x62b84e50
1884: I0815 03:07:27.864573  6270 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:07:27.864575  6270 naive_executor.cc:195] 0x62db5370 Create variable multinomial_0.tmp_0, which pointer is 0x62b848f0
1884: I0815 03:07:27.864578  6270 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 03:07:27.864581  6270 naive_executor.cc:195] 0x62db5370 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62b84bf0
1884: I0815 03:07:27.864584  6270 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 03:07:27.864586  6270 naive_executor.cc:195] 0x62db5370 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x623c9360
1884: I0815 03:07:27.864593  6270 scope.cc:202] Create variable feed
1884: I0815 03:07:27.864598  6270 scope.cc:202] Create variable fetch
1884: I0815 03:07:27.864617  6270 naive_executor.cc:46] NaiveExecutor init with scope 0x62db5370
1884: I0815 03:07:27.864621  6270 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 03:07:27.864832  6270 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:07:27.864845  6270 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:07:27.864874  6270 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 03:07:27.864877  6270 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 03:07:27.864886  6270 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:27.864926  6270 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:07:27.865201  6270 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:27.865216  6270 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:07:27.865278  6270 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:27.867354  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 03:07:28.060307  6270 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:07:28.060465  6270 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:28.060497  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:07:28.060580  6270 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:07:28.060613  6270 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:28.060640  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:07:28.060708  6270 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:07:28.060752  6270 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:28.060770  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:07:28.060827  6270 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:07:28.060854  6270 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:07:28.060868  6270 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:07:28.060902  6270 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:07:28.060920  6270 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:28.060956  6270 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:07:28.060984  6270 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:07:28.061527  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.061542  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 03:07:28.190065  6270 pir_interpreter.cc:161] PirInterpreter(): 0x62904750 on Place(gpu:0)
1884: I0815 03:07:28.190141  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_0
1884: I0815 03:07:28.190160  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_1
1884: I0815 03:07:28.190166  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_2
1884: I0815 03:07:28.190177  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_3
1884: I0815 03:07:28.190205  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_4
1884: I0815 03:07:28.190218  6270 scope.cc:202] Create variable 0x629047501723691248190127680_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x629047501723691248190127680_inner_var_0 -> 0x62dc12a0
1884: 1 -> 0x629047501723691248190127680_inner_var_1 -> 0x624bcd10
1884: 2 -> 0x629047501723691248190127680_inner_var_2 -> 0x62daed20
1884: 3 -> linear_1.w_0 -> 0x6204c490
1884: 4 -> linear_1.b_0 -> 0x60250d20
1884: 5 -> learning_rate_1 -> 0x622ab180
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:07:28.191358  6348 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:28.191447  6348 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.191550  6348 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.191601  6348 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 03:07:28.191632  6348 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.191649  6348 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.191659  6348 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:07:28.192343  6345 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.192385  6345 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.192445  6345 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:07:28.192463  6345 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.192494  6345 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.192507  6345 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.192519  6345 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x629047501723691248190127680_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.191358  6344 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.195340  6348 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x629047501723691248190127680_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x629047501723691248190127680_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x629047501723691248190127680_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.195478  6348 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x629047501723691248190127680_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x629047501723691248190127680_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x629047501723691248190127680_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 03:07:28.198976  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x629048c0) got event_name: TaskCompletion
1884: I0815 03:07:28.191360  6347 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.191358  6346 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 03:07:28.201856  6270 pir_interpreter.cc:161] PirInterpreter(): 0x61f46ba0 on Place(gpu:0)
1884: I0815 03:07:28.201910  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_1
1884: I0815 03:07:28.201926  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_4
1884: I0815 03:07:28.201933  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_5
1884: I0815 03:07:28.201941  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_6
1884: I0815 03:07:28.201961  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_7
1884: I0815 03:07:28.201972  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_8
1884: I0815 03:07:28.201978  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_9
1884: I0815 03:07:28.202008  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_10
1884: I0815 03:07:28.202014  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_11
1884: I0815 03:07:28.202023  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_12
1884: I0815 03:07:28.202029  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_13
1884: I0815 03:07:28.202039  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_14
1884: I0815 03:07:28.202045  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_15
1884: I0815 03:07:28.202052  6270 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:07:28.202062  6270 scope.cc:202] Create variable 0x61f46ba01723691248201884344_inner_var_17
1884: I0815 03:07:28.202069  6270 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x622ab180
1884: 1 -> 0x61f46ba01723691248201884344_inner_var_1 -> 0x6023a570
1884: 2 -> linear_1.b_0 -> 0x60250d20
1884: 3 -> linear_1.w_0 -> 0x6204c490
1884: 4 -> 0x61f46ba01723691248201884344_inner_var_4 -> 0x62db4030
1884: 5 -> 0x61f46ba01723691248201884344_inner_var_5 -> 0x6204aed0
1884: 6 -> 0x61f46ba01723691248201884344_inner_var_6 -> 0x6023af60
1884: 7 -> 0x61f46ba01723691248201884344_inner_var_7 -> 0x60236510
1884: 8 -> 0x61f46ba01723691248201884344_inner_var_8 -> 0x620390c0
1884: 9 -> 0x61f46ba01723691248201884344_inner_var_9 -> 0x60264b90
1884: 10 -> 0x61f46ba01723691248201884344_inner_var_10 -> 0x625e0190
1884: 11 -> 0x61f46ba01723691248201884344_inner_var_11 -> 0x62907d70
1884: 12 -> 0x61f46ba01723691248201884344_inner_var_12 -> 0x6203e680
1884: 13 -> 0x61f46ba01723691248201884344_inner_var_13 -> 0x61f09200
1884: 14 -> 0x61f46ba01723691248201884344_inner_var_14 -> 0x620393d0
1884: 15 -> 0x61f46ba01723691248201884344_inner_var_15 -> 0x625e01b0
1884: 16 -> fetch0@fetch -> 0x625daab0
1884: 17 -> 0x61f46ba01723691248201884344_inner_var_17 -> 0x61f2cec0
1884: 18 -> fetch1@fetch -> 0x628fed70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 03:07:28.208349  6353 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:07:28.208436  6351 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:28.208417  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.208483  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:07:28.208503  6351 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.208590  6351 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.208609  6351 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.208631  6351 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:07:28.208518  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212347  6350 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.212354  6349 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.212419  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.212491  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:07:28.212534  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:07:28.212615  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:07:28.212630  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61f46ba01723691248201884344_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212677  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61f46ba01723691248201884344_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:07:28.212713  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61f46ba01723691248201884344_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212752  6353 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:07:28.212785  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61f46ba01723691248201884344_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.212813  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61f46ba01723691248201884344_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61f46ba01723691248201884344_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212850  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.208441  6352 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.212879  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61f46ba01723691248201884344_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61f46ba01723691248201884344_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.212921  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61f46ba01723691248201884344_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212942  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.212954  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61f46ba01723691248201884344_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.212970  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61f46ba01723691248201884344_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.212953  6352 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f46ba01723691248201884344_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.213001  6353 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:28.213004  6352 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:28.213094  6353 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:28.216413  6352 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f46ba01723691248201884344_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.216511  6352 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f46ba01723691248201884344_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.216542  6352 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:07:28.216567  6352 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f46ba01723691248201884344_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.217389  6353 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:28.217506  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.217525  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f46ba01723691248201884344_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61f46ba01723691248201884344_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.217586  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.217615  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.217628  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.217644  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61f46ba01723691248201884344_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.217725  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61f46ba01723691248201884344_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:07:28.217742  6353 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61f46ba01723691248201884344_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f46ba01723691248201884344_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.217772  6353 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:07:28.217783  6353 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61f46ba01723691248201884344_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f46ba01723691248201884344_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:07:28.220353  6352 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.220410  6352 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:07:28.220489  6352 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f46ba01723691248201884344_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f46ba01723691248201884344_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.220530  6352 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f46ba01723691248201884344_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.220548  6352 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:07:28.220566  6352 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f46ba01723691248201884344_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.224364  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x61f46d10) got event_name: TaskCompletion
1884: I0815 03:07:28.224434  6270 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:07:28.224493  6270 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:07:28.231611  6270 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:07:28.231669  6270 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:07:28.236614  6270 scope.cc:202] Create variable linear_1.b_0
1884: I0815 03:07:28.236704  6270 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 03:07:28.237248  6270 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482373134440"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482373134440"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 03:07:28.237529  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623c8980 on Place(cpu)
1884: I0815 03:07:28.237561  6270 scope.cc:202] Create variable 0x623c89801723691248237548976_inner_var_0
1884: I0815 03:07:28.237596  6270 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482373134440"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236912482373134440 -> 0x602214a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 03:07:28.237782  6270 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 03:07:28.238257  6356 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.242374  6356 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236912482373134440:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.250384  6358 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.250509  6355 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.251423  6356 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236912482373134440:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:07:28.253346  6357 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:28.253458  6354 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:28.254478  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623c8af0) got event_name: TaskCompletion
1884: I0815 03:07:28.261365  6356 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18379431318024195769 to 15651950018441601075 , after update, data is {current : 208, peak : 264}.
1884: I0815 03:07:28.261454  6356 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 18379431318024195769 to 15651950018441601075 , after update, data is {current : 208, peak : 264}.
1884: I0815 03:07:28.266494  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.266526  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482666995551"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482666995551"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:07:28.266986  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623c8980 on Place(cpu)
1884: I0815 03:07:28.267019  6270 scope.cc:202] Create variable 0x623c89801723691248267005514_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482666995551"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236912482666995551 -> 0x61fc1b10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:07:28.267526  6363 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.267555  6363 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236912482666995551:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.267653  6363 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236912482666995551:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.267684  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623c8af0) got event_name: TaskCompletion
1884: I0815 03:07:28.268330  6360 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.268352  6359 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:28.268407  6362 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:28.271373  6361 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.274417  6363 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 216, peak : 264}.
1884: I0815 03:07:28.274449  6363 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 216, peak : 264}.
1884: I0815 03:07:28.274616  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.274621  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236912482666995551",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236912482748243092"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236912482666995551",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236912482748243092"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:07:28.275112  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623c8980 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236912482666995551",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236912482748243092"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236912482748243092 -> 0x61fc1b10
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 03:07:28.276332  6365 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.276351  6364 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:28.276367  6365 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.276436  6365 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.276499  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623c8af0) got event_name: TaskCompletion
1884: I0815 03:07:28.277328  6367 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:28.276351  6368 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.276351  6366 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.289433  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.289467  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482896785323"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482896785323"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 03:07:28.289954  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623c8980 on Place(cpu)
1884: I0815 03:07:28.289989  6270 scope.cc:202] Create variable 0x623c89801723691248289975295_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236912482896785323"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236912482896785323 -> 0x622bdae0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:07:28.291348  6373 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:07:28.291395  6369 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:28.291414  6373 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236912482896785323:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.291514  6373 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236912482896785323:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:07:28.293846  6371 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:07:28.294325  6270 pir_interpreter.cc:1766] main_thread_blocker_(0x623c8af0) got event_name: TaskCompletion
1884: I0815 03:07:28.294355  6370 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:07:28.291348  6372 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:07:28.299356  6373 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 220, peak : 264}.
1884: I0815 03:07:28.299378  6373 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 220, peak : 264}.
1884: I0815 03:07:28.306432  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.306468  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:28.306644  6270 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 03:07:28.306744  6270 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 03:07:28.306785  6270 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482896785323"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482748243092"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482896785323"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482748243092"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 03:07:28.307698  6270 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 03:07:28.307714  6270 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 03:07:28.307765  6270 pir_interpreter.cc:161] PirInterpreter(): 0x623c8980 on Place(cpu)
1884: I0815 03:07:28.307812  6270 scope.cc:202] Create variable feed_name_0
1884: I0815 03:07:28.307829  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_5
1884: I0815 03:07:28.307852  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_6
1884: I0815 03:07:28.307862  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_7
1884: I0815 03:07:28.307869  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_8
1884: I0815 03:07:28.307888  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_9
1884: I0815 03:07:28.307898  6270 scope.cc:202] Create variable 0x623c89801723691248307784561_inner_var_10
1884: I0815 03:07:28.307926  6270 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:28.307951  6270 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:07:28.308164  6270 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:28.308178  6270 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482896785323"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236912482748243092"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236912482896785323 -> 0x622bdae0
1884: 1 -> constant_folding@_17236912482748243092 -> 0x61fc1b10
1884: 2 -> linear_1.b_0 -> 0x60240450
1884: 3 -> linear_1.w_0 -> 0x622bce40
1884: 4 -> feed_name_0 -> 0x62b78b10
1884: 5 -> 0x623c89801723691248307784561_inner_var_5 -> 0x602207d0
1884: 6 -> 0x623c89801723691248307784561_inner_var_6 -> 0x61f43170
1884: 7 -> 0x623c89801723691248307784561_inner_var_7 -> 0x628d4de0
1884: 8 -> 0x623c89801723691248307784561_inner_var_8 -> 0x6296bc30
1884: 9 -> fetch_name_0 -> 0x61fc1d60
1884: 10 -> fetch_name_1 -> 0x6296bb80
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:07:28.308883  6270 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 03:07:28.309000  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309072  6270 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:07:28.309096  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.309129  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x623c89801723691248307784561_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x623c89801723691248307784561_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309172  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x623c89801723691248307784561_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.309202  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309224  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.309238  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309273  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236912482748243092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.309293  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236912482896785323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309334  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236912482896785323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623c89801723691248307784561_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:07:28.309355  6270 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236912482896785323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:07:28.309360  6374 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:07:28.309387  6270 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236912482896785323:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623c89801723691248307784561_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:07:28.309414  6270 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:07:28.309434  6270 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:07:28.309458  6270 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:07:28.309612  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.309616  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:28.311515  6374 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.311540  6374 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 12989129100192776195 to 15651950018441601075 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.311692  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.311698  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......FFF..
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 60287 / 100000 (60.3%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([0, 0, 0, ..., 3, 2, 2], dtype=int64)
1884:  y: array([0, 0, 0, ..., 0, 0, 0])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 210220 / 300000 (70.1%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[3, 2, 3, ..., 3, 0, 1],
1884:        [2, 1, 2, ..., 2, 3, 2],
1884:        [0, 1, 1, ..., 3, 3, 3]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 99 / 100 (99%)
1884: Max absolute difference: 995
1884: Max relative difference: inf
1884:  x: array([354, 817, 979, 537, 376, 293, 470, 749, 827, 633, 321, 755, 472,
1884:        268, 930, 252, 352, 195, 507,  55, 926, 534, 618,  54, 784, 714,
1884:        257, 653, 245,  72,  99, 343, 954, 965, 173, 474, 185, 399, 810,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 5.571s
1884: 
1884: FAILED (failures=3, skipped=3)
1884: 
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: I0815 03:07:28.313992  6270 mmap_allocator.cc:348] PID: 6270, MemoryMapFdSet: set size - 0
1884: I0815 03:07:28.324923  6270 mmap_allocator.cc:348] PID: 6270, MemoryMapFdSet: set size - 0
1884: I0815 03:07:28.411382  6345 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 1456120502615146076 to 15651950018441601075 , after update, data is {current : 52, peak : 264}.
1884: I0815 03:07:28.411420  6345 thread_data_registry.h:135] Add data {current : 24, peak : 24} from thread 1456120502615146076 to 15651950018441601075 , after update, data is {current : 52, peak : 264}.
1884: I0815 03:07:28.417371  6348 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 11940181586897641491 to 15651950018441601075 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.417407  6348 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 11940181586897641491 to 15651950018441601075 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.417412  6348 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 11940181586897641491 to 15651950018441601075 , after update, data is {current : 256, peak : 768}.
1884: I0815 03:07:28.422382  6351 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 16821299056415092015 to 15651950018441601075 , after update, data is {current : 48, peak : 264}.
1884: I0815 03:07:28.422413  6351 thread_data_registry.h:135] Add data {current : 20, peak : 20} from thread 16821299056415092015 to 15651950018441601075 , after update, data is {current : 48, peak : 264}.
1884: I0815 03:07:28.426383  6352 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 15651950018441601075 to 3259993897809717693 , after update, data is {current : 768, peak : 1536}.
1884: I0815 03:07:28.426417  6352 thread_data_registry.h:135] Add data {current : 48, peak : 264} from thread 15651950018441601075 to 3259993897809717693 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.426424  6352 thread_data_registry.h:135] Add data {current : 48, peak : 264} from thread 15651950018441601075 to 3259993897809717693 , after update, data is {current : 28, peak : 264}.
1884: I0815 03:07:28.436380  6353 thread_data_registry.h:135] Add data {current : 28, peak : 264} from thread 3259993897809717693 to 6227015612790772271 , after update, data is {current : 6401792, peak : 8800800}.
1884: I0815 03:07:28.436420  6353 thread_data_registry.h:135] Add data {current : 28, peak : 264} from thread 3259993897809717693 to 6227015612790772271 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 03:07:28.436427  6353 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 3259993897809717693 to 6227015612790772271 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 03:07:28.645422  6270 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:07:28.645455  6270 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:07:28.645504  6270 mmap_allocator.cc:348] PID: 6270, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   13.73 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  13.92 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
