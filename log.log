UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0813 11:19:18.245183  1364 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0813 11:19:19.026279  1364 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=cudnn_deterministic,reallocate_gpu_memory_in_mb,async_trace_count,npu_storage_format,accuracy_check_rtol_bf16,gpugraph_enable_print_op_debug,use_fast_math,set_to_1d,enable_pir_in_executor,mkl_dir,multi_node_sample_use_gpu_table,cse_max_count,enable_blaslt_global_search,dump_chunk_info,enable_async_trace,prim_backward,use_cuda_managed_memory,prim_check_ops,gpugraph_enable_segment_merge_grads,nccl_dir,use_system_allocator,lapack_dir,prim_skip_dynamic,new_executor_sequential_run,save_static_runtime_data,sort_sum_gradient,dynamic_static_unified_comm,enable_interpretercore_launch_cinn,gpugraph_offload_param_extends,selected_gpus,multiple_of_cupti_buffer_size,accuracy_check_rtol_fp32,enable_pir_api,pir_broadcast_tree_limit,cudnn_exhaustive_search_times,prim_enabled,print_allocator_trace_info,prim_forward,static_runtime_data_save_path,mklml_dir,enable_cublas_tensor_op_math,dist_threadpool_size,allow_cinn_ops,conv_workspace_size_limit,use_stream_safe_cuda_allocator,benchmark,init_allocated_mem,local_exe_sub_scope_limit,gpugraph_storage_mode,accuracy_check_rtol_fp16,executor_log_deps_every_microseconds,fleet_executor_with_standalone,op_dir,tensorrt_dir,enable_fuse_parallel_matmul_pass,print_sub_graph_dir,auto_growth_chunk_size_in_mb,free_idle_chunk,enable_pir_in_executor_trace_run,enable_gpu_memory_usage_log,use_cinn,fraction_of_cuda_pinned_memory_to_use,sync_nccl_allreduce,cusparselt_dir,use_shm_cache,pinned_memory_as_cpu_backend,use_auto_growth_v2,eager_delete_scope,gpugraph_slot_feasign_max_num,use_pinned_memory,gpugraph_parallel_copyer_split_maxsize,use_xqa_optim,fraction_of_gpu_memory_to_use,logging_trunc_pir_py_code,graph_metapath_split_opt,manually_trans_conv_filter,accuracy_check_atol_bf16,use_mkldnn,gemm_use_half_precision_compute_type,paddle_num_threads,cusolver_dir,graph_get_neighbor_id,sync_after_alloc,initial_gpu_memory_in_mb,win_cuda_bin_dir,gpu_allocator_retry_time,cublas_dir,check_infer_symbolic,einsum_opt,new_executor_serial_run,gpugraph_hbm_table_load_factor,enable_graph_multi_node_sampling,cublaslt_exhaustive_search_times,gpugraph_offload_param_stat,custom_device_mem_record,enable_sparse_inner_gather,alloc_fill_value,enable_tracker_all2all,gpu_memory_limit_mb,trt_ibuilder_cache,enable_auto_detect_gpu_topo,gpugraph_offload_gather_copy_maxsize,tracer_onednn_ops_off,graph_embedding_split_infer_mode,allocator_strategy,pir_apply_shape_optimization_pass,pir_debug,reader_queue_speed_test_mode,enable_neighbor_list_use_uva,ir_inplace_kernel_blacklist,enable_unused_var_check,cinn_compile_thread_num,host_trace_level,fuse_parameter_groups_size,eager_delete_tensor_gb,dygraph_debug,cudnn_dir,run_kp_kernel,fraction_of_cpu_memory_to_use,use_cuda_malloc_async_allocator,enable_adjust_op_order,enable_all2all_use_fp16,nccl_blocking_wait,graph_neighbor_size_percent,search_cache_max_number,gpugraph_sparse_table_storage_mode,memory_fraction_of_eager_deletion,graph_load_in_parallel,tensor_operants_mode,nvidia_package_dir,dataloader_use_file_descriptor,cuda_dir,embedding_deterministic,initial_cpu_memory_in_mb,cache_inference_while_scope,new_executor_use_local_scope,all_blocks_convert_trt,logging_pir_py_code_dump_symbolic_dims,enable_cinn_auto_tune,gpugraph_force_device_batch_num_equal,jit_engine_type,fuse_parameter_memory_size,use_autotune,cuda_memory_async_pool_realease_threshold,inner_op_parallelism,cuda_malloc_async_pool_memory_throttle_ratio,apply_pass_to_program,deny_cinn_ops,gpugraph_enable_gpu_direct_access,auto_free_cudagraph_allocations_on_launch,new_executor_use_cuda_graph,logging_pir_py_code_dir,enable_opt_get_features,check_kernel_launch,log_memory_stats,enable_dependency_builder_debug_info,cudnn_exhaustive_search,enable_exit_when_partial_worker,call_stack_level,disable_dyshape_in_train,new_executor_use_inplace,gpugraph_merge_grads_segment_size,enable_fusion_fallback,curand_dir,use_auto_growth_pinned_allocator,enable_api_kernel_fallback,print_ir,enable_record_memory,check_nan_inf,benchmark_nccl,check_nan_inf_level,gpugraph_parallel_stream_num,add_dependency_for_communication_op,accuracy_check_atol_fp16,enable_gpu_memory_usage_log_mb,conv2d_disable_cudnn,cinn_subgraph_graphviz_dir,query_dest_rank_by_multi_node,cudnn_batchnorm_spatial_persistent,free_when_no_cache_hit,enable_dump_main_program,gpugraph_enable_hbm_table_collision_stat,enable_cse_in_dy2st,use_stride_kernel,enable_cinn_compile_cache,max_inplace_grad_add,cusparse_dir,tracer_onednn_ops_on,pir_apply_inplace_pass,convert_all_blocks,use_virtual_memory_auto_growth,gpugraph_dedup_pull_push_mode,logging_pir_py_code_int_tensor_element_limit,enable_collect_shape,get_host_by_name_time,gpugraph_load_node_list_into_hbm,allreduce_record_one_event,enable_auto_rdma_trans,new_executor_static_build,static_executor_perfstat_filepath,low_precision_op_list,accuracy_check_atol_fp32,cublaslt_device_best_config,pir_subgraph_saving_dir,prim_forward_blacklist,prim_all,tracer_profile_fname,gpugraph_debug_gpu_memory,fast_eager_deletion_mode,enable_pir_with_pt_in_dy2st,cupti_dir,prim_enable_dynamic,enable_cinn_accuracy_check 
1884: I0813 11:19:19.026397  1364 init.cc:108] After Parse: argc is 2
1884: I0813 11:19:27.911093  1364 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 11:19:27.911128  1364 dygraph_functions.cc:77659] { Input: []} 
1884: W0813 11:19:27.911767  1364 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0813 11:19:27.912117  1364 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0813 11:19:27.912910  1364 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0813 11:19:27.912981  1364 allocator_facade.cc:212] selected allocator strategy:1
1884: I0813 11:19:27.913075  1364 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0813 11:19:27.913695  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7efdbd400000), and remaining 0
1884: I0813 11:19:27.913909  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:27.913967  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:27.914050  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7efdbd400200), and remaining 0
1884: I0813 11:19:27.914077  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7efdbd400400), and remaining 0
1884: I0813 11:19:27.917805  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7efdbd400600), and remaining 0
1884: I0813 11:19:27.917925  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7efdbd400800), and remaining 0
1884: I0813 11:19:27.917989  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7efdbd400a00), and remaining 0
1884: I0813 11:19:27.918071  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:27.918092  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:27.918157  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:27.918169  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:27.919045  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194052f0 for it.
1884: I0813 11:19:27.919193  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:27.919215  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:27.919268  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7efdbd400e00), and remaining 0
1884: I0813 11:19:27.919364  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7efdbd4c4400), and remaining 0
1884: I0813 11:19:28.034750  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194052f0 for it.
1884: I0813 11:19:28.035012  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:28.035069  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.035677  1364 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7efdbd600000), and remaining 0
1884: I0813 11:19:28.043889  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x194052f0 for it.
1884: I0813 11:19:28.043990  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:28.044024  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.044061  1364 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:28.044224  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:28.045125  1364 dygraph_functions.cc:33459] Running AD API: full
1884: I0813 11:19:28.045143  1364 dygraph_functions.cc:33480] { Input: []} 
1884: I0813 11:19:28.045189  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:28.045264  1364 dygraph_functions.cc:64553] Running AD API: scale
1884: I0813 11:19:28.045290  1364 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.045351  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:28.045424  1364 dygraph_functions.cc:26170] Running AD API: exp
1884: I0813 11:19:28.045441  1364 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.045473  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:28.045645  1364 dygraph_functions.cc:72508] Running AD API: sum
1884: I0813 11:19:28.045665  1364 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.045816  1364 dygraph_functions.cc:83176] Running AD API: divide
1884: I0813 11:19:28.045841  1364 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:28.045909  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:28.048274  1364 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0813 11:19:28.048388  1364 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0813 11:19:28.048415  1364 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0813 11:19:28.048471  1364 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0813 11:19:29.470356  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.470412  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.470669  1364 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0813 11:19:29.470691  1364 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.475785  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.475823  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.476840  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.476858  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.476871  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.477629  1364 program_interpreter.cc:243] New Executor is Running.
1884: I0813 11:19:29.477643  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.477660  1364 scope.cc:202] Create variable feed
1884: I0813 11:19:29.477670  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.477684  1364 scope.cc:202] Create variable fetch
1884: I0813 11:19:29.477689  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.477700  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.477705  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.477708  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.477711  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.480027  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.480365  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.480376  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.480381  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.482012  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.482060  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.482069  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.482075  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.482082  1364 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 11:19:29.482091  1364 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x5febdb60 type is 7
1884: I0813 11:19:29.482095  1364 scope.cc:202] Create variable x
1884: I0813 11:19:29.482098  1364 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5febc620 type is 7
1884: I0813 11:19:29.482158  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.482164  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.482168  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.482172  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.482296  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.482327  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.482447  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.482458  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.482474  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.482628  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.482656  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.482674  1364 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.482683  1364 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fec4680Variable Type 7
1884: I0813 11:19:29.482710  1364 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.482731  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.482786  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.482805  1364 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.484012  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.484068  1364 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.484439  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.488108  1364 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 11:19:29.488128  1364 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 11:19:29.488210  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.488237  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.488729  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: I0813 11:19:29.488799  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.488821  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.489274  1364 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: I0813 11:19:29.489347  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.489370  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.489393  1364 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.489658  1364 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 11:19:29.489670  1364 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 11:19:29.489778  1364 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0813 11:19:29.489800  1364 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.490178  1364 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 11:19:29.490190  1364 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 11:19:29.490234  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.490253  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.490442  1364 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0813 11:19:29.490451  1364 dygraph_functions.cc:77659] { Input: []} 
1884: I0813 11:19:29.490486  1364 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0813 11:19:29.490504  1364 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0813 11:19:29.490520  1364 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.493116  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.493140  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.493191  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.493201  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.495069  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.495430  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.495445  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.495450  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.497236  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.497287  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.497298  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.497313  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5feefce0 type is 7
1884: I0813 11:19:29.497321  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.497327  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fef0050 type is 7
1884: I0813 11:19:29.497332  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.497337  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.497392  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.497399  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.497404  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.497408  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.497455  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.497475  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.497534  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.497545  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.497563  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.497781  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.497797  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.497817  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.497823  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fef67d0Variable Type 7
1884: I0813 11:19:29.497840  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.497857  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.497882  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.497900  1364 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.498574  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.498605  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.498808  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.509430  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: I0813 11:19:29.509613  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0813 11:19:29.515707  1364 pir_interpreter.cc:161] PirInterpreter(): 0x600b2a80 on Place(gpu:0)
1884: I0813 11:19:29.515751  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.515785  1364 scope.cc:202] Create variable 0x600b2a801723547969515735409_inner_var_1
1884: I0813 11:19:29.515796  1364 scope.cc:202] Create variable 0x600b2a801723547969515735409_inner_var_2
1884: I0813 11:19:29.515806  1364 scope.cc:202] Create variable 0x600b2a801723547969515735409_inner_var_3
1884: I0813 11:19:29.515816  1364 scope.cc:202] Create variable 0x600b2a801723547969515735409_inner_var_4
1884: I0813 11:19:29.515828  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.516230  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.516247  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.516250  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0813 11:19:29.516291  1364 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x600b29e0
1884: 1 -> 0x600b2a801723547969515735409_inner_var_1 -> 0x600b2a60
1884: 2 -> 0x600b2a801723547969515735409_inner_var_2 -> 0x600b3330
1884: 3 -> 0x600b2a801723547969515735409_inner_var_3 -> 0x600b21c0
1884: 4 -> 0x600b2a801723547969515735409_inner_var_4 -> 0x600b36e0
1884: 5 -> fetch0@fetch -> 0x600b3ea0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.517040  1364 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0813 11:19:29.517288  1402 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.517505  1404 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.517514  1403 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.517532  1405 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.517603  1406 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.517630  1405 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x600b2a801723547969515735409_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.517665  1407 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.517722  1405 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x600b2a801723547969515735409_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.517725  1407 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.517772  1407 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0813 11:19:29.517819  1407 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x600b2a801723547969515735409_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x600b2a801723547969515735409_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.518005  1407 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x600b2a801723547969515735409_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x600b2a801723547969515735409_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0813 11:19:29.518078  1405 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x600b2a801723547969515735409_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.518101  1405 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.519326  1405 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x600b2a801723547969515735409_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x600b2a801723547969515735409_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 11:19:29.519368  1405 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x600b2a801723547969515735409_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.519394  1405 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.519973  1405 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x600b2a801723547969515735409_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 11:19:29.520012  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x600b2bf0) got event_name: TaskCompletion
1884: I0813 11:19:29.520037  1364 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.593096  1402 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 9369304407171547164 to 12122348925893046931 , after update, data is {current : 0, peak : 800768}.
1884: I0813 11:19:29.593115  1402 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 9369304407171547164 to 17463265522944167790 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 11:19:29.593122  1402 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 9369304407171547164 to 17463265522944167790 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 11:19:29.593384  1405 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 17463265522944167790 to 820151212369438717 , after update, data is {current : 800000, peak : 2400000}.
1884: I0813 11:19:29.593403  1405 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 17463265522944167790 to 820151212369438717 , after update, data is {current : 800000, peak : 2400000}.
1884: I0813 11:19:29.593551  1407 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 12122348925893046931 to 820151212369438717 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0813 11:19:29.593565  1407 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 12122348925893046931 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.599421  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.599447  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.599503  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.599510  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.601224  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.601577  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.601591  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.601596  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.603087  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.603174  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.603185  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.603191  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fef62b0 type is 7
1884: I0813 11:19:29.603199  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.603202  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5fedb710 type is 7
1884: I0813 11:19:29.603207  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.603211  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.603266  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.603273  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.603277  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.603281  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.603341  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.603358  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.603421  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.603430  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.603444  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.603564  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.603574  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.603590  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.603596  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x2343660Variable Type 7
1884: I0813 11:19:29.603612  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.603631  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.603652  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.603667  1364 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.605244  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.605281  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.605499  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.616199  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: I0813 11:19:29.616386  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0813 11:19:29.619446  1364 pir_interpreter.cc:161] PirInterpreter(): 0x62579350 on Place(gpu:0)
1884: I0813 11:19:29.619480  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.619503  1364 scope.cc:202] Create variable 0x625793501723547969619471486_inner_var_1
1884: I0813 11:19:29.619514  1364 scope.cc:202] Create variable 0x625793501723547969619471486_inner_var_2
1884: I0813 11:19:29.619526  1364 scope.cc:202] Create variable 0x625793501723547969619471486_inner_var_3
1884: I0813 11:19:29.619536  1364 scope.cc:202] Create variable 0x625793501723547969619471486_inner_var_4
1884: I0813 11:19:29.619547  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.619882  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.619899  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.619902  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x6008d0e0
1884: 1 -> 0x625793501723547969619471486_inner_var_1 -> 0x5febe130
1884: 2 -> 0x625793501723547969619471486_inner_var_2 -> 0x600b25d0
1884: 3 -> 0x625793501723547969619471486_inner_var_3 -> 0x3bc26f0
1884: 4 -> 0x625793501723547969619471486_inner_var_4 -> 0x43c6c090
1884: 5 -> fetch0@fetch -> 0x3bc94e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.620602  1408 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.620682  1409 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.620692  1410 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.620765  1411 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.620783  1412 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.620819  1413 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.620822  1412 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625793501723547969619471486_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.620847  1413 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.620900  1413 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 11:19:29.620915  1412 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625793501723547969619471486_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.620952  1413 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625793501723547969619471486_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625793501723547969619471486_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.621088  1413 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625793501723547969619471486_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625793501723547969619471486_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.621145  1412 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625793501723547969619471486_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.621169  1412 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.623906  1412 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625793501723547969619471486_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x625793501723547969619471486_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.623955  1412 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x625793501723547969619471486_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.623978  1412 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.626010  1412 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x625793501723547969619471486_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.626060  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x625794c0) got event_name: TaskCompletion
1884: I0813 11:19:29.626082  1364 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.662340  1408 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 12122348925893046931 to 3700357058563797516 , after update, data is {current : 0, peak : 2400768}.
1884: I0813 11:19:29.662359  1408 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12122348925893046931 to 1038495853020352862 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 11:19:29.662365  1408 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12122348925893046931 to 1038495853020352862 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 11:19:29.662514  1412 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1038495853020352862 to 820151212369438717 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0813 11:19:29.662524  1412 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 1038495853020352862 to 820151212369438717 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0813 11:19:29.662698  1413 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.666424  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.666448  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.666497  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.666505  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.668066  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.668402  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.668416  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.668421  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.669973  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.670053  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.670063  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.670069  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x24431c0 type is 7
1884: I0813 11:19:29.670078  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.670082  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x287bc10 type is 7
1884: I0813 11:19:29.670086  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.670090  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.670145  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.670151  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.670156  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.670159  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.670209  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.670223  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.670284  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.670292  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.670320  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.670358  1364 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.670482  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.670537  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.670548  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.670564  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.670570  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x3bd9fe0Variable Type 7
1884: I0813 11:19:29.670588  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.670604  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.670626  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.670641  1364 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.670904  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.670926  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.671108  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.671835  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: I0813 11:19:29.672005  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0813 11:19:29.674925  1364 pir_interpreter.cc:161] PirInterpreter(): 0x625f0940 on Place(gpu:0)
1884: I0813 11:19:29.674958  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.674979  1364 scope.cc:202] Create variable 0x625f09401723547969674950285_inner_var_1
1884: I0813 11:19:29.674991  1364 scope.cc:202] Create variable 0x625f09401723547969674950285_inner_var_2
1884: I0813 11:19:29.675002  1364 scope.cc:202] Create variable 0x625f09401723547969674950285_inner_var_3
1884: I0813 11:19:29.675014  1364 scope.cc:202] Create variable 0x625f09401723547969674950285_inner_var_4
1884: I0813 11:19:29.675024  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.675357  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.675374  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.675376  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62579970
1884: 1 -> 0x625f09401723547969674950285_inner_var_1 -> 0x5fec88b0
1884: 2 -> 0x625f09401723547969674950285_inner_var_2 -> 0x5fec3880
1884: 3 -> 0x625f09401723547969674950285_inner_var_3 -> 0x5feb0520
1884: 4 -> 0x625f09401723547969674950285_inner_var_4 -> 0x3bc9d10
1884: 5 -> fetch0@fetch -> 0x43c8aa30
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.676046  1414 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.676120  1415 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.676195  1416 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.676189  1417 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.676210  1418 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.676255  1419 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.676245  1417 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625f09401723547969674950285_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.676292  1419 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.676319  1417 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625f09401723547969674950285_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.676333  1419 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0813 11:19:29.676369  1419 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625f09401723547969674950285_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625f09401723547969674950285_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.676410  1419 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.676517  1419 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.676542  1419 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625f09401723547969674950285_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625f09401723547969674950285_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0813 11:19:29.676609  1417 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625f09401723547969674950285_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.676633  1417 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.676935  1417 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625f09401723547969674950285_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x625f09401723547969674950285_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 11:19:29.676962  1417 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x625f09401723547969674950285_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.676982  1417 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.676996  1417 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x625f09401723547969674950285_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 11:19:29.677031  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x625f0ab0) got event_name: TaskCompletion
1884: I0813 11:19:29.677050  1364 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.708714  1414 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 3700357058563797516 to 4979682066939586566 , after update, data is {current : 0, peak : 3328}.
1884: I0813 11:19:29.708730  1414 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 3700357058563797516 to 3980141324054041804 , after update, data is {current : 800, peak : 1604}.
1884: I0813 11:19:29.708735  1414 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 3700357058563797516 to 3980141324054041804 , after update, data is {current : 800, peak : 1604}.
1884: I0813 11:19:29.708942  1417 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 3980141324054041804 to 4979682066939586566 , after update, data is {current : 800, peak : 2000}.
1884: I0813 11:19:29.708954  1417 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 3980141324054041804 to 4979682066939586566 , after update, data is {current : 800, peak : 2000}.
1884: I0813 11:19:29.709131  1419 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 4979682066939586566 to 820151212369438717 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0813 11:19:29.709151  1419 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 4979682066939586566 to 820151212369438717 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0813 11:19:29.709154  1419 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 4979682066939586566 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.713284  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.713312  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.713366  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.713372  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.714975  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.715317  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.715330  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.715334  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.716825  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.716905  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.716915  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.716920  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5feb3f40 type is 7
1884: I0813 11:19:29.716928  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.716933  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5feae580 type is 7
1884: I0813 11:19:29.716936  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.716940  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.716991  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.716997  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.717000  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.717003  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.717056  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.717069  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.717129  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.717137  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.717151  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.717347  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.717360  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.717376  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.717382  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x1946fe10Variable Type 7
1884: I0813 11:19:29.717399  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.717417  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.717439  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.717454  1364 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.718120  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.718148  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.718350  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.720650  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: I0813 11:19:29.720819  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0813 11:19:29.723840  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(gpu:0)
1884: I0813 11:19:29.723873  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.723896  1364 scope.cc:202] Create variable 0x5fededa01723547969723866150_inner_var_1
1884: I0813 11:19:29.723906  1364 scope.cc:202] Create variable 0x5fededa01723547969723866150_inner_var_2
1884: I0813 11:19:29.723918  1364 scope.cc:202] Create variable 0x5fededa01723547969723866150_inner_var_3
1884: I0813 11:19:29.723928  1364 scope.cc:202] Create variable 0x5fededa01723547969723866150_inner_var_4
1884: I0813 11:19:29.723939  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.724258  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.724273  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.724277  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x3c13260
1884: 1 -> 0x5fededa01723547969723866150_inner_var_1 -> 0x5fed8e20
1884: 2 -> 0x5fededa01723547969723866150_inner_var_2 -> 0x4223eb10
1884: 3 -> 0x5fededa01723547969723866150_inner_var_3 -> 0x5fed49e0
1884: 4 -> 0x5fededa01723547969723866150_inner_var_4 -> 0x3632b30
1884: 5 -> fetch0@fetch -> 0x600b1de0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.724952  1420 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.725025  1421 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.725049  1422 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.725085  1423 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.725135  1424 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.725152  1425 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.725152  1424 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fededa01723547969723866150_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.725173  1425 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.725207  1424 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fededa01723547969723866150_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.725209  1425 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0813 11:19:29.725248  1425 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fededa01723547969723866150_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547969723866150_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.725421  1425 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fededa01723547969723866150_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547969723866150_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0813 11:19:29.725484  1424 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fededa01723547969723866150_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.725507  1424 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.726713  1424 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fededa01723547969723866150_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fededa01723547969723866150_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 11:19:29.726756  1424 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fededa01723547969723866150_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.726778  1424 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.727365  1424 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fededa01723547969723866150_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0813 11:19:29.727401  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x5fedef10) got event_name: TaskCompletion
1884: I0813 11:19:29.727419  1364 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.760512  1420 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 4979682066939586566 to 3700357058563797516 , after update, data is {current : 0, peak : 800768}.
1884: I0813 11:19:29.760528  1420 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 4979682066939586566 to 1038495853020352862 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 11:19:29.760532  1420 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 4979682066939586566 to 1038495853020352862 , after update, data is {current : 800000, peak : 1600004}.
1884: I0813 11:19:29.760680  1424 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 1038495853020352862 to 820151212369438717 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0813 11:19:29.760690  1424 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 1038495853020352862 to 820151212369438717 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0813 11:19:29.760859  1425 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0813 11:19:29.760869  1425 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.765329  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.765352  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.765403  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.765411  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.767014  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.767352  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.767365  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.767369  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.768853  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.768934  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.768944  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.768949  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5fea40b0 type is 7
1884: I0813 11:19:29.768959  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.768962  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62582170 type is 7
1884: I0813 11:19:29.768966  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.768971  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.769022  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.769028  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.769032  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.769037  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.769086  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.769099  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.769156  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.769165  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.769178  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.769284  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.769294  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.769315  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.769322  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fe9f560Variable Type 7
1884: I0813 11:19:29.769337  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.769354  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.769376  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.769389  1364 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.770958  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.770994  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.771198  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.777632  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: I0813 11:19:29.777807  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0813 11:19:29.780789  1364 pir_interpreter.cc:161] PirInterpreter(): 0x6256f650 on Place(gpu:0)
1884: I0813 11:19:29.780822  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.780845  1364 scope.cc:202] Create variable 0x6256f6501723547969780815202_inner_var_1
1884: I0813 11:19:29.780855  1364 scope.cc:202] Create variable 0x6256f6501723547969780815202_inner_var_2
1884: I0813 11:19:29.780867  1364 scope.cc:202] Create variable 0x6256f6501723547969780815202_inner_var_3
1884: I0813 11:19:29.780879  1364 scope.cc:202] Create variable 0x6256f6501723547969780815202_inner_var_4
1884: I0813 11:19:29.780887  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.781211  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.781226  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.781230  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x14b0e90
1884: 1 -> 0x6256f6501723547969780815202_inner_var_1 -> 0x5feb9a00
1884: 2 -> 0x6256f6501723547969780815202_inner_var_2 -> 0x61c89f10
1884: 3 -> 0x6256f6501723547969780815202_inner_var_3 -> 0x601e6af0
1884: 4 -> 0x6256f6501723547969780815202_inner_var_4 -> 0x6255f9a0
1884: 5 -> fetch0@fetch -> 0x601e6ce0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.781919  1426 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.781996  1427 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.782011  1428 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.782070  1429 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.782084  1430 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.782119  1430 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6256f6501723547969780815202_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.782151  1431 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.782166  1430 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6256f6501723547969780815202_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.782176  1431 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.782212  1431 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 11:19:29.782248  1431 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6256f6501723547969780815202_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6256f6501723547969780815202_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.782364  1431 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6256f6501723547969780815202_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6256f6501723547969780815202_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.782423  1430 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6256f6501723547969780815202_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.782444  1430 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.785104  1430 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6256f6501723547969780815202_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6256f6501723547969780815202_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.785146  1430 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6256f6501723547969780815202_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.785166  1430 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.787119  1430 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6256f6501723547969780815202_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0813 11:19:29.787168  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x6256f7c0) got event_name: TaskCompletion
1884: I0813 11:19:29.787189  1364 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.823654  1426 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 3700357058563797516 to 4979682066939586566 , after update, data is {current : 0, peak : 2400768}.
1884: I0813 11:19:29.823668  1426 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 3700357058563797516 to 12122348925893046931 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 11:19:29.823674  1426 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 3700357058563797516 to 12122348925893046931 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0813 11:19:29.823824  1430 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 12122348925893046931 to 820151212369438717 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0813 11:19:29.823833  1430 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 12122348925893046931 to 820151212369438717 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0813 11:19:29.824012  1431 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 4979682066939586566 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.827741  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.827764  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.827812  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.827819  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.829375  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.829701  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.829715  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.829718  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.831183  1364 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0813 11:19:29.831264  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.831274  1364 scope.cc:202] Create variable Out
1884: I0813 11:19:29.831279  1364 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62579900 type is 7
1884: I0813 11:19:29.831290  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.831295  1364 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x1a79480 type is 7
1884: I0813 11:19:29.831307  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.831315  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.831367  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.831372  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.831377  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.831379  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.831429  1364 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.831442  1364 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.831499  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.831508  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.831521  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.831557  1364 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.831660  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.831703  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.831713  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.831728  1364 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.831734  1364 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62813390Variable Type 7
1884: I0813 11:19:29.831750  1364 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.831766  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.831786  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.831800  1364 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.831914  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.831935  1364 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.832111  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.832844  1364 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19459860 for it.
1884: I0813 11:19:29.833014  1364 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1941ea30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0813 11:19:29.835930  1364 pir_interpreter.cc:161] PirInterpreter(): 0x61c44ce0 on Place(gpu:0)
1884: I0813 11:19:29.835963  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.835985  1364 scope.cc:202] Create variable 0x61c44ce01723547969835955495_inner_var_1
1884: I0813 11:19:29.835996  1364 scope.cc:202] Create variable 0x61c44ce01723547969835955495_inner_var_2
1884: I0813 11:19:29.836007  1364 scope.cc:202] Create variable 0x61c44ce01723547969835955495_inner_var_3
1884: I0813 11:19:29.836019  1364 scope.cc:202] Create variable 0x61c44ce01723547969835955495_inner_var_4
1884: I0813 11:19:29.836028  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.836356  1364 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0813 11:19:29.836372  1364 scope.cc:202] Create variable X
1884: I0813 11:19:29.836376  1364 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x3bcaf20
1884: 1 -> 0x61c44ce01723547969835955495_inner_var_1 -> 0x61daca50
1884: 2 -> 0x61c44ce01723547969835955495_inner_var_2 -> 0x628138a0
1884: 3 -> 0x61c44ce01723547969835955495_inner_var_3 -> 0x3bf0550
1884: 4 -> 0x61c44ce01723547969835955495_inner_var_4 -> 0x61b46b80
1884: 5 -> fetch0@fetch -> 0x5fe6ff90
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0813 11:19:29.837042  1432 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:29.837114  1433 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.837136  1434 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.837175  1435 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.837198  1436 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.837242  1437 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.837234  1436 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61c44ce01723547969835955495_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.837268  1437 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.837286  1436 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61c44ce01723547969835955495_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.837303  1437 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0813 11:19:29.837333  1437 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61c44ce01723547969835955495_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61c44ce01723547969835955495_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.837373  1437 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.837476  1437 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.837502  1437 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61c44ce01723547969835955495_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61c44ce01723547969835955495_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0813 11:19:29.837553  1436 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61c44ce01723547969835955495_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.837574  1436 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.837766  1436 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61c44ce01723547969835955495_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61c44ce01723547969835955495_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 11:19:29.837790  1436 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61c44ce01723547969835955495_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.837806  1436 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.837816  1436 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61c44ce01723547969835955495_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0813 11:19:29.837843  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x61c44e50) got event_name: TaskCompletion
1884: I0813 11:19:29.837865  1364 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.868867  1432 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 4979682066939586566 to 3700357058563797516 , after update, data is {current : 0, peak : 10240}.
1884: I0813 11:19:29.868885  1432 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4979682066939586566 to 1038495853020352862 , after update, data is {current : 800, peak : 1604}.
1884: I0813 11:19:29.868891  1432 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 4979682066939586566 to 1038495853020352862 , after update, data is {current : 800, peak : 1604}.
1884: I0813 11:19:29.869076  1436 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 1038495853020352862 to 3700357058563797516 , after update, data is {current : 800, peak : 8000}.
1884: I0813 11:19:29.869086  1436 thread_data_registry.h:135] Add data {current : 800, peak : 1604} from thread 1038495853020352862 to 3700357058563797516 , after update, data is {current : 800, peak : 8000}.
1884: I0813 11:19:29.869239  1437 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0813 11:19:29.869249  1437 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0813 11:19:29.869253  1437 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 3700357058563797516 to 820151212369438717 , after update, data is {current : 0, peak : 2401024}.
1884: I0813 11:19:29.875344  1364 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0813 11:19:29.875391  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0813 11:19:29.876446  1364 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 11:19:29.877264  1364 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0813 11:19:29.877293  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0813 11:19:29.878583  1364 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0813 11:19:29.878608  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 11:19:29.879283  1364 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0813 11:19:29.880244  1364 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0813 11:19:29.880268  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 11:19:29.881633  1364 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0813 11:19:29.881656  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 11:19:29.882236  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.882262  1364 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0813 11:19:29.882268  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.882275  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.884294  1364 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0813 11:19:29.884325  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 11:19:29.885275  1364 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0813 11:19:29.885313  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0813 11:19:29.886241  1364 pybind.cc:1827] need skip: 0
1884: I0813 11:19:29.886531  1364 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 11:19:29.888289  1364 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0813 11:19:29.891927  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.891945  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.891950  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.893903  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.893922  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.893932  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.893937  1364 scope.cc:202] Create variable learning_rate_0
1884: I0813 11:19:29.893944  1364 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61c88a30 type is 7
1884: I0813 11:19:29.893949  1364 scope.cc:202] Create variable linear_0.b_0
1884: I0813 11:19:29.893952  1364 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x618afc80 type is 7
1884: I0813 11:19:29.893956  1364 scope.cc:202] Create variable linear_0.w_0
1884: I0813 11:19:29.893959  1364 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x618afd00 type is 7
1884: I0813 11:19:29.894021  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.894026  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.894030  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.894034  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.894088  1364 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894101  1364 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894124  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0813 11:19:29.894234  1364 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894244  1364 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894311  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.894346  1364 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894354  1364 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.894381  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.895341  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.896658  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.897104  1364 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0813 11:19:29.897322  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.897607  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.897814  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.897828  1364 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 11:19:29.897889  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.897895  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.897899  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.898002  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.898013  1364 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 11:19:29.899564  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.900873  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.901950  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.902132  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.902144  1364 scope.cc:202] Create variable abs_0.tmp_0
1884: I0813 11:19:29.902149  1364 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x61872b20 type is 7
1884: I0813 11:19:29.902158  1364 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 11:19:29.902161  1364 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x618728a0 type is 7
1884: I0813 11:19:29.902165  1364 scope.cc:202] Create variable cast_0.tmp_0
1884: I0813 11:19:29.902168  1364 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x61872990 type is 7
1884: I0813 11:19:29.902172  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.902177  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.902181  1364 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0813 11:19:29.902184  1364 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x61873f60 type is 7
1884: I0813 11:19:29.902187  1364 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61c88a30 type is 7
1884: I0813 11:19:29.902191  1364 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x618afc80 type is 7
1884: I0813 11:19:29.902196  1364 scope.cc:202] Create variable linear_0.tmp_0
1884: I0813 11:19:29.902199  1364 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x61873f40 type is 7
1884: I0813 11:19:29.902202  1364 scope.cc:202] Create variable linear_0.tmp_1
1884: I0813 11:19:29.902205  1364 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x618744a0 type is 7
1884: I0813 11:19:29.902208  1364 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x618afd00 type is 7
1884: I0813 11:19:29.902212  1364 scope.cc:202] Create variable mean_0.tmp_0
1884: I0813 11:19:29.902215  1364 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x61874710 type is 7
1884: I0813 11:19:29.902220  1364 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0813 11:19:29.902221  1364 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x61874950 type is 7
1884: I0813 11:19:29.902225  1364 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 11:19:29.902228  1364 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61874bb0 type is 7
1884: I0813 11:19:29.902312  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.902328  1364 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 11:19:29.902385  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.902392  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.902396  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.902400  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.902443  1364 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.902454  1364 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.902470  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0813 11:19:29.902558  1364 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.902568  1364 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.902586  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 11:19:29.902660  1364 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 11:19:29.902734  1364 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0813 11:19:29.903767  1364 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.903788  1364 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.903846  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.903898  1364 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.903908  1364 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.903921  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 11:19:29.903945  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.903980  1364 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.903988  1364 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904001  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 11:19:29.904080  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904090  1364 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904103  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.904202  1364 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.904270  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.904328  1364 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904340  1364 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904352  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 11:19:29.904386  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.904428  1364 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904438  1364 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904450  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0813 11:19:29.904544  1364 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904554  1364 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904573  1364 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.904614  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.904623  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.904639  1364 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.904644  1364 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61b46aa0Variable Type 7
1884: I0813 11:19:29.904661  1364 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.904677  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.904696  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904709  1364 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.904747  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.904769  1364 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0813 11:19:29.904795  1364 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.904803  1364 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.904815  1364 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0813 11:19:29.904821  1364 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61875600Variable Type 7
1884: I0813 11:19:29.904832  1364 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0813 11:19:29.904844  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.904857  1364 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.904870  1364 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.904901  1364 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0813 11:19:29.904915  1364 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0813 11:19:29.905313  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0813 11:19:29.905349  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 11:19:29.905364  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 11:19:29.905396  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0813 11:19:29.905428  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.905447  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0813 11:19:29.909252  1364 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0813 11:19:29.909286  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 11:19:29.909973  1364 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0813 11:19:29.909996  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 11:19:29.910310  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.911953  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.912761  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.912875  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.913378  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.914254  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.916309  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.917392  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.919216  1364 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0813 11:19:29.919932  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.919948  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.919952  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.921141  1364 interpreter_util.cc:1169] Creating Variables
1884: I0813 11:19:29.921159  1364 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x20c3180 type is 9
1884: I0813 11:19:29.921165  1364 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x3bfd270 type is 10
1884: I0813 11:19:29.921172  1364 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x618afc80 type is 7
1884: I0813 11:19:29.921176  1364 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x618afd00 type is 7
1884: I0813 11:19:29.921180  1364 scope.cc:202] Create variable saved_params
1884: I0813 11:19:29.921183  1364 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x6182b340 type is 17
1884: I0813 11:19:29.921211  1364 interpreter_util.cc:594] Static build: 0
1884: I0813 11:19:29.921216  1364 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0813 11:19:29.921221  1364 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0813 11:19:29.921223  1364 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0813 11:19:29.921258  1364 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.921269  1364 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0813 11:19:29.921948  1364 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0813 11:19:29.921988  1364 analysis_predictor.cc:433] Predictor::init()
1884: I0813 11:19:29.922039  1364 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0813 11:19:29.923163  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.923223  1364 scope.cc:202] Create variable feed
1884: I0813 11:19:29.923229  1364 naive_executor.cc:189] 0x6184e6e0 Create persistable variable feed, which pointer is 0x625672e0
1884: I0813 11:19:29.923234  1364 scope.cc:202] Create variable fetch
1884: I0813 11:19:29.923238  1364 naive_executor.cc:189] 0x6184e6e0 Create persistable variable fetch, which pointer is 0x6184f240
1884: I0813 11:19:29.923241  1364 scope.cc:202] Create variable linear_0.b_0
1884: I0813 11:19:29.923243  1364 naive_executor.cc:189] 0x6184e6e0 Create persistable variable linear_0.b_0, which pointer is 0x5fee5a50
1884: I0813 11:19:29.923249  1364 scope.cc:202] Create variable linear_0.w_0
1884: I0813 11:19:29.923250  1364 naive_executor.cc:189] 0x6184e6e0 Create persistable variable linear_0.w_0, which pointer is 0x6184c710
1884: I0813 11:19:29.923264  1364 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0813 11:19:29.923615  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.923703  1364 program_converter.cc:296] is_legacy_program : 0
1884: I0813 11:19:29.923748  1364 executor.cc:183] Old Executor is Running.
1884: I0813 11:19:29.923813  1364 executor.cc:92] Creating Variables for block 0
1884: I0813 11:19:29.923822  1364 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0813 11:19:29.923825  1364 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x5fee5a50 type is 7
1884: I0813 11:19:29.923828  1364 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0813 11:19:29.923831  1364 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x6184c710 type is 7
1884: I0813 11:19:29.923864  1364 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.923934  1364 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0813 11:19:29.923974  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.923980  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:29.924109  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.924224  1364 graph.cc:149] create OpNode by feed
1884: I0813 11:19:29.924260  1364 graph.cc:149] create OpNode by matmul_v2
1884: I0813 11:19:29.924276  1364 graph.cc:149] create OpNode by elementwise_add
1884: I0813 11:19:29.924291  1364 graph.cc:149] create OpNode by abs
1884: I0813 11:19:29.924309  1364 graph.cc:149] create OpNode by assign_value
1884: I0813 11:19:29.924327  1364 graph.cc:149] create OpNode by multinomial
1884: I0813 11:19:29.924336  1364 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0813 11:19:29.924350  1364 graph.cc:149] create OpNode by scale
1884: I0813 11:19:29.924362  1364 graph.cc:149] create OpNode by scale
1884: I0813 11:19:29.924373  1364 graph.cc:149] create OpNode by fetch
1884: I0813 11:19:29.924391  1364 graph.cc:149] create OpNode by fetch
1884: I0813 11:19:29.924409  1364 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0813 11:19:29.925576  1364 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0813 11:19:29.925585  1364 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0813 11:19:29.925653  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.925659  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0813 11:19:29.925767  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926010  1364 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0813 11:19:29.926071  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926077  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0813 11:19:29.926110  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926115  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0813 11:19:29.926151  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926211  1364 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 11:19:29.926240  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926245  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0813 11:19:29.926259  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926272  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.926294  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926304  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0813 11:19:29.926342  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926363  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.926388  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926393  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0813 11:19:29.926434  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926507  1364 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 11:19:29.926537  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926541  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0813 11:19:29.926573  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926591  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.926613  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926618  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0813 11:19:29.926647  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926796  1364 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0813 11:19:29.926824  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926829  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0813 11:19:29.926858  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926875  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.926898  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926903  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0813 11:19:29.926924  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.926939  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.926959  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.926964  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0813 11:19:29.926986  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.927000  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.927021  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.927026  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0813 11:19:29.927052  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.927119  1364 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0813 11:19:29.927150  1364 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 11:19:29.927165  1364 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 11:19:29.927178  1364 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0813 11:19:29.927202  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.927207  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0813 11:19:29.927227  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.927268  1364 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0813 11:19:29.927286  1364 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 11:19:29.927297  1364 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 11:19:29.927314  1364 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 11:19:29.927345  1364 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0813 11:19:29.927357  1364 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0813 11:19:29.928507  1364 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 11:19:29.928552  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.928558  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0813 11:19:29.928586  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.928606  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.928633  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.928638  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0813 11:19:29.928661  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.928709  1364 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0813 11:19:29.928738  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.928745  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0813 11:19:29.928764  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.928779  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.928802  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.928807  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0813 11:19:29.928834  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.928920  1364 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0813 11:19:29.928949  1364 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 11:19:29.928964  1364 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 11:19:29.928979  1364 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 11:19:29.928994  1364 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0813 11:19:29.929008  1364 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0813 11:19:29.929024  1364 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0813 11:19:29.929046  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929118  1364 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0813 11:19:29.929141  1364 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0813 11:19:29.929154  1364 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0813 11:19:29.929169  1364 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0813 11:19:29.929184  1364 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0813 11:19:29.929198  1364 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0813 11:19:29.929214  1364 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0813 11:19:29.929260  1364 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0813 11:19:29.929546  1364 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 11:19:29.929575  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.929581  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0813 11:19:29.929626  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929687  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.929723  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929769  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.929797  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929838  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.929862  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929900  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.929921  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.929955  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.929972  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930003  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930019  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930047  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930059  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930083  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930094  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930114  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930138  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930143  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0813 11:19:29.930168  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930210  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930235  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930240  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0813 11:19:29.930251  1364 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0813 11:19:29.930254  1364 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0813 11:19:29.930296  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930325  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930351  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930356  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0813 11:19:29.930364  1364 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0813 11:19:29.930367  1364 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0813 11:19:29.930408  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930428  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930452  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930457  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0813 11:19:29.930466  1364 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0813 11:19:29.930469  1364 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0813 11:19:29.930500  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930517  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930541  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930545  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0813 11:19:29.930553  1364 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0813 11:19:29.930555  1364 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0813 11:19:29.930593  1364 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0813 11:19:29.930613  1364 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0813 11:19:29.930636  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930640  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0813 11:19:29.930653  1364 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0813 11:19:29.930693  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930698  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0813 11:19:29.930764  1364 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 11:19:29.930783  1364 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.930800  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0813 11:19:29.930847  1364 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0813 11:19:29.930864  1364 scope.cc:202] Create variable assign_0.tmp_0
1884: I0813 11:19:29.930891  1364 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0813 11:19:29.930913  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.930918  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0813 11:19:29.931783  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.931802  1364 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0813 11:19:29.931852  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.931857  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:29.932461  1364 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0813 11:19:29.932670  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.932741  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.932746  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:29.933151  1364 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0813 11:19:29.933363  1364 graph.h:183] deleting __fuse_statis__
1884: I0813 11:19:29.933372  1364 graph.h:183] deleting pass_recorder
1884: I0813 11:19:29.933378  1364 graph.h:183] deleting stale_program_op_descs
1884: I0813 11:19:29.933456  1364 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0813 11:19:29.933466  1364 scope.cc:202] Create variable abs_0.tmp_0
1884: I0813 11:19:29.933470  1364 naive_executor.cc:195] 0x6184e6e0 Create variable abs_0.tmp_0, which pointer is 0x61c5ea10
1884: I0813 11:19:29.933475  1364 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0813 11:19:29.933477  1364 naive_executor.cc:195] 0x6184e6e0 Create variable gaussian_0.tmp_0, which pointer is 0x62811370
1884: I0813 11:19:29.933488  1364 scope.cc:202] Create variable linear_0.tmp_1
1884: I0813 11:19:29.933493  1364 naive_executor.cc:195] 0x6184e6e0 Create variable linear_0.tmp_1, which pointer is 0x6268fdc0
1884: I0813 11:19:29.933497  1364 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0813 11:19:29.933501  1364 naive_executor.cc:195] 0x6184e6e0 Create variable multinomial_0.tmp_0, which pointer is 0x6268f860
1884: I0813 11:19:29.933503  1364 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0813 11:19:29.933506  1364 naive_executor.cc:195] 0x6184e6e0 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x6268fb60
1884: I0813 11:19:29.933509  1364 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0813 11:19:29.933512  1364 naive_executor.cc:195] 0x6184e6e0 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x6268e160
1884: I0813 11:19:29.933518  1364 scope.cc:202] Create variable feed
1884: I0813 11:19:29.933521  1364 scope.cc:202] Create variable fetch
1884: I0813 11:19:29.933542  1364 naive_executor.cc:46] NaiveExecutor init with scope 0x6184e6e0
1884: I0813 11:19:29.933548  1364 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0813 11:19:29.933734  1364 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0813 11:19:29.933748  1364 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0813 11:19:29.933773  1364 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0813 11:19:29.933779  1364 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0813 11:19:29.933784  1364 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:29.933815  1364 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 11:19:29.934015  1364 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:29.934031  1364 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 11:19:29.934077  1364 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.934100  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0813 11:19:29.950107  1364 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0813 11:19:29.950198  1364 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.950225  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0813 11:19:29.950286  1364 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0813 11:19:29.950330  1364 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.950356  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0813 11:19:29.950424  1364 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0813 11:19:29.950466  1364 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.950482  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 11:19:29.950536  1364 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0813 11:19:29.950565  1364 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0813 11:19:29.950582  1364 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0813 11:19:29.950615  1364 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0813 11:19:29.950634  1364 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:29.950666  1364 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 11:19:29.950691  1364 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0813 11:19:29.951155  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:29.951165  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0813 11:19:29.989503  1364 pir_interpreter.cc:161] PirInterpreter(): 0x61bf30c0 on Place(gpu:0)
1884: I0813 11:19:29.989539  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_0
1884: I0813 11:19:29.989555  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_1
1884: I0813 11:19:29.989564  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_2
1884: I0813 11:19:29.989573  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_3
1884: I0813 11:19:29.989599  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_4
1884: I0813 11:19:29.989614  1364 scope.cc:202] Create variable 0x61bf30c01723547969989530177_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x61bf30c01723547969989530177_inner_var_0 -> 0x505ad10
1884: 1 -> 0x61bf30c01723547969989530177_inner_var_1 -> 0x3bc8a20
1884: 2 -> 0x61bf30c01723547969989530177_inner_var_2 -> 0x61d934f0
1884: 3 -> linear_1.w_0 -> 0x6008d0c0
1884: 4 -> linear_1.b_0 -> 0x600b2040
1884: 5 -> learning_rate_1 -> 0x600b53d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0813 11:19:29.990394  1438 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.990407  1439 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.990437  1440 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.990468  1441 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.990509  1442 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.990504  1441 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990515  1439 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990522  1440 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990535  1442 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990559  1441 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 11:19:29.990566  1439 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.990583  1440 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61bf30c01723547969989530177_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.990592  1442 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.990640  1442 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0813 11:19:29.990664  1442 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990682  1442 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.990694  1442 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 11:19:29.990708  1442 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61bf30c01723547969989530177_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61bf30c01723547969989530177_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61bf30c01723547969989530177_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.990768  1442 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61bf30c01723547969989530177_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61bf30c01723547969989530177_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61bf30c01723547969989530177_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0813 11:19:29.990823  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x61bf3230) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0813 11:19:29.992782  1364 pir_interpreter.cc:161] PirInterpreter(): 0x4babc200 on Place(gpu:0)
1884: I0813 11:19:29.992817  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_1
1884: I0813 11:19:29.992833  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_4
1884: I0813 11:19:29.992842  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_5
1884: I0813 11:19:29.992849  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_6
1884: I0813 11:19:29.992871  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_7
1884: I0813 11:19:29.992882  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_8
1884: I0813 11:19:29.992892  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_9
1884: I0813 11:19:29.992916  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_10
1884: I0813 11:19:29.992926  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_11
1884: I0813 11:19:29.992933  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_12
1884: I0813 11:19:29.992942  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_13
1884: I0813 11:19:29.992950  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_14
1884: I0813 11:19:29.992959  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_15
1884: I0813 11:19:29.992966  1364 scope.cc:202] Create variable fetch0@fetch
1884: I0813 11:19:29.992976  1364 scope.cc:202] Create variable 0x4babc2001723547969992804588_inner_var_17
1884: I0813 11:19:29.992982  1364 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x600b53d0
1884: 1 -> 0x4babc2001723547969992804588_inner_var_1 -> 0x61dac3a0
1884: 2 -> linear_1.b_0 -> 0x600b2040
1884: 3 -> linear_1.w_0 -> 0x6008d0c0
1884: 4 -> 0x4babc2001723547969992804588_inner_var_4 -> 0x6008c1a0
1884: 5 -> 0x4babc2001723547969992804588_inner_var_5 -> 0x5fec7be0
1884: 6 -> 0x4babc2001723547969992804588_inner_var_6 -> 0x600b2910
1884: 7 -> 0x4babc2001723547969992804588_inner_var_7 -> 0x61770060
1884: 8 -> 0x4babc2001723547969992804588_inner_var_8 -> 0x62542340
1884: 9 -> 0x4babc2001723547969992804588_inner_var_9 -> 0x5fea2100
1884: 10 -> 0x4babc2001723547969992804588_inner_var_10 -> 0x5febdd10
1884: 11 -> 0x4babc2001723547969992804588_inner_var_11 -> 0x600b4740
1884: 12 -> 0x4babc2001723547969992804588_inner_var_12 -> 0x60087dd0
1884: 13 -> 0x4babc2001723547969992804588_inner_var_13 -> 0x53b63e0
1884: 14 -> 0x4babc2001723547969992804588_inner_var_14 -> 0x600b0880
1884: 15 -> 0x4babc2001723547969992804588_inner_var_15 -> 0x624d1dc0
1884: 16 -> fetch0@fetch -> 0x600890e0
1884: 17 -> 0x4babc2001723547969992804588_inner_var_17 -> 0x5fe6f9b0
1884: 18 -> fetch1@fetch -> 0x5fec4440
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0813 11:19:29.994590  1443 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:29.994702  1444 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:29.994729  1445 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:29.994773  1446 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:29.994803  1445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.994807  1446 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.994836  1447 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0813 11:19:29.994838  1446 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:29.994832  1445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 11:19:29.994874  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.994899  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 11:19:29.994921  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.994954  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.994976  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 11:19:29.994990  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 11:19:29.995039  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0813 11:19:29.995055  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x4babc2001723547969992804588_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995088  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x4babc2001723547969992804588_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0813 11:19:29.995110  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x4babc2001723547969992804588_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995142  1447 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 11:19:29.995177  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x4babc2001723547969992804588_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 11:19:29.995204  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x4babc2001723547969992804588_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x4babc2001723547969992804588_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995241  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.995256  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x4babc2001723547969992804588_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x4babc2001723547969992804588_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 11:19:29.995285  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x4babc2001723547969992804588_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995309  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.995314  1445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4babc2001723547969992804588_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995321  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x4babc2001723547969992804588_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0813 11:19:29.995337  1445 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.995337  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x4babc2001723547969992804588_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995358  1447 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.995409  1445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4babc2001723547969992804588_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:29.995437  1445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4babc2001723547969992804588_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995452  1447 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.995455  1445 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.995469  1445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4babc2001723547969992804588_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:29.995502  1447 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.995564  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.995584  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4babc2001723547969992804588_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x4babc2001723547969992804588_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0813 11:19:29.995615  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995623  1445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995637  1445 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0813 11:19:29.995640  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.995654  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0813 11:19:29.995672  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x4babc2001723547969992804588_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995671  1445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4babc2001723547969992804588_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 11:19:29.995697  1445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4babc2001723547969992804588_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995712  1445 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.995724  1445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4babc2001723547969992804588_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 11:19:29.995743  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x4babc2001723547969992804588_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 11:19:29.995762  1447 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x4babc2001723547969992804588_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4babc2001723547969992804588_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:29.995788  1447 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0813 11:19:29.995801  1447 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x4babc2001723547969992804588_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4babc2001723547969992804588_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x4babc2001723547969992804588_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0813 11:19:29.995836  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x4babc370) got event_name: TaskCompletion
1884: I0813 11:19:29.995862  1364 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0813 11:19:29.995889  1364 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0813 11:19:30.001194  1364 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0813 11:19:30.001243  1364 analysis_predictor.cc:433] Predictor::init()
1884: I0813 11:19:30.001919  1364 scope.cc:202] Create variable linear_1.b_0
1884: I0813 11:19:30.001966  1364 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0813 11:19:30.002394  1364 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700024441500"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700024441500"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0813 11:19:30.002570  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(cpu)
1884: I0813 11:19:30.002589  1364 scope.cc:202] Create variable 0x5fededa01723547970002583811_inner_var_0
1884: I0813 11:19:30.002614  1364 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700024441500"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235479700024441500 -> 0x62548ee0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0813 11:19:30.002748  1364 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0813 11:19:30.002856  1448 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:30.002996  1449 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:30.003021  1450 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:30.003098  1451 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:30.003094  1449 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17235479700024441500:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.003129  1452 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:30.003152  1449 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17235479700024441500:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0813 11:19:30.003176  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x5fedef10) got event_name: TaskCompletion
1884: I0813 11:19:30.003373  1449 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16255452183571217664 to 12235714011619907428 , after update, data is {current : -4, peak : 104}.
1884: I0813 11:19:30.003384  1449 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16255452183571217664 to 12235714011619907428 , after update, data is {current : -4, peak : 104}.
1884: I0813 11:19:30.003492  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.003500  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700035662111"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700035662111"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0813 11:19:30.003702  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(cpu)
1884: I0813 11:19:30.003721  1364 scope.cc:202] Create variable 0x5fededa01723547970003715792_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700035662111"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235479700035662111 -> 0x61876a00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0813 11:19:30.003917  1453 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:30.003983  1454 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:30.003998  1455 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:30.004029  1456 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:30.004052  1457 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:30.004047  1456 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235479700035662111:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.004099  1456 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235479700035662111:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:30.004123  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x5fedef10) got event_name: TaskCompletion
1884: I0813 11:19:30.004292  1456 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11036828479196741596 to 12235714011619907428 , after update, data is {current : 4, peak : 104}.
1884: I0813 11:19:30.004303  1456 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 11036828479196741596 to 12235714011619907428 , after update, data is {current : 4, peak : 104}.
1884: I0813 11:19:30.004395  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.004403  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235479700035662111",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235479700044733632"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235479700035662111",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235479700044733632"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0813 11:19:30.004621  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17235479700035662111",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17235479700044733632"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235479700044733632 -> 0x61876a00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0813 11:19:30.004868  1458 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:30.004923  1459 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:30.004941  1460 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:30.004973  1461 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:30.005002  1462 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:30.004998  1461 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:30.005025  1461 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:30.005049  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x5fedef10) got event_name: TaskCompletion
1884: I0813 11:19:30.005324  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.005332  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700054036273"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700054036273"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0813 11:19:30.005530  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(cpu)
1884: I0813 11:19:30.005549  1364 scope.cc:202] Create variable 0x5fededa01723547970005544092_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17235479700054036273"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235479700054036273 -> 0x5e4d14e0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0813 11:19:30.005735  1463 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:30.005784  1464 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0813 11:19:30.005800  1465 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0813 11:19:30.005844  1466 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0813 11:19:30.005857  1467 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0813 11:19:30.005868  1465 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235479700054036273:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.005926  1465 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17235479700054036273:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0813 11:19:30.005954  1364 pir_interpreter.cc:1766] main_thread_blocker_(0x5fedef10) got event_name: TaskCompletion
1884: I0813 11:19:30.006124  1465 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13964649636328268935 to 12235714011619907428 , after update, data is {current : 8, peak : 104}.
1884: I0813 11:19:30.006132  1465 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13964649636328268935 to 12235714011619907428 , after update, data is {current : 8, peak : 104}.
1884: I0813 11:19:30.006191  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.006199  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:30.006261  1364 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0813 11:19:30.006338  1364 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0813 11:19:30.006377  1364 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700054036273"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700044733632"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700054036273"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700044733632"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0813 11:19:30.007020  1364 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0813 11:19:30.007038  1364 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0813 11:19:30.007071  1364 pir_interpreter.cc:161] PirInterpreter(): 0x5fededa0 on Place(cpu)
1884: I0813 11:19:30.007100  1364 scope.cc:202] Create variable feed_name_0
1884: I0813 11:19:30.007114  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_5
1884: I0813 11:19:30.007136  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_6
1884: I0813 11:19:30.007148  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_7
1884: I0813 11:19:30.007156  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_8
1884: I0813 11:19:30.007175  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_9
1884: I0813 11:19:30.007186  1364 scope.cc:202] Create variable 0x5fededa01723547970007084950_inner_var_10
1884: I0813 11:19:30.007208  1364 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:30.007229  1364 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 11:19:30.007352  1364 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:30.007368  1364 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700054036273"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17235479700044733632"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17235479700054036273 -> 0x5e4d14e0
1884: 1 -> constant_folding@_17235479700044733632 -> 0x61876a00
1884: 2 -> linear_1.b_0 -> 0x627f0c40
1884: 3 -> linear_1.w_0 -> 0x600b5b20
1884: 4 -> feed_name_0 -> 0x4b55810
1884: 5 -> 0x5fededa01723547970007084950_inner_var_5 -> 0x6255fd10
1884: 6 -> 0x5fededa01723547970007084950_inner_var_6 -> 0x626cc950
1884: 7 -> 0x5fededa01723547970007084950_inner_var_7 -> 0x618739f0
1884: 8 -> 0x5fededa01723547970007084950_inner_var_8 -> 0x627faee0
1884: 9 -> fetch_name_0 -> 0x5a92d640
1884: 10 -> fetch_name_1 -> 0x5feae370
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0813 11:19:30.007922  1364 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0813 11:19:30.007980  1468 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0813 11:19:30.007977  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008026  1364 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0813 11:19:30.008047  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:30.008078  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x5fededa01723547970007084950_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x5fededa01723547970007084950_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008117  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x5fededa01723547970007084950_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:30.008150  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008172  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:30.008186  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008216  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17235479700044733632:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 11:19:30.008241  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17235479700054036273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008271  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17235479700054036273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x5fededa01723547970007084950_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0813 11:19:30.008298  1364 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17235479700054036273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0813 11:19:30.008337  1364 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17235479700054036273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5fededa01723547970007084950_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0813 11:19:30.008363  1364 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07201MB]
1884: I0813 11:19:30.008383  1364 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 8.3931MB]
1884: I0813 11:19:30.008405  1364 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0813 11:19:30.008525  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.008533  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:30.008579  1468 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 16255452183571217664 to 12235714011619907428 , after update, data is {current : -184, peak : 104}.
1884: I0813 11:19:30.008589  1468 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 16255452183571217664 to 12235714011619907428 , after update, data is {current : -184, peak : 104}.
1884: I0813 11:19:30.008623  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.008630  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:30.010390  1364 mmap_allocator.cc:348] PID: 1364, MemoryMapFdSet: set size - 0
1884: I0813 11:19:30.022008  1364 mmap_allocator.cc:348] PID: 1364, MemoryMapFdSet: set size - 0
1884: I0813 11:19:30.090382  1440 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7877122029425429889 to 12235714011619907428 , after update, data is {current : -180, peak : 104}.
1884: I0813 11:19:30.090404  1440 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7877122029425429889 to 12235714011619907428 , after update, data is {current : -180, peak : 104}.
1884: I0813 11:19:30.090413  1441 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3980141324054041804 to 12235714011619907428 , after update, data is {current : -164, peak : 104}.
1884: I0813 11:19:30.090431  1441 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 3980141324054041804 to 12235714011619907428 , after update, data is {current : -164, peak : 104}.
1884: I0813 11:19:30.090442  1439 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1038495853020352862 to 12235714011619907428 , after update, data is {current : -160, peak : 104}.
1884: I0813 11:19:30.090459  1439 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1038495853020352862 to 12235714011619907428 , after update, data is {current : -160, peak : 104}.
1884: I0813 11:19:30.090605  1442 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 12122348925893046931 to 12235714011619907428 , after update, data is {current : -184, peak : 104}.
1884: I0813 11:19:30.090618  1442 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 12122348925893046931 to 12235714011619907428 , after update, data is {current : -184, peak : 104}.
1884: I0813 11:19:30.090623  1442 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 12122348925893046931 to 17463265522944167790 , after update, data is {current : 256, peak : 768}.
1884: I0813 11:19:30.090884  1445 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 17463265522944167790 to 12235714011619907428 , after update, data is {current : 768, peak : 1536}.
1884: I0813 11:19:30.090893  1445 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 17463265522944167790 to 12235714011619907428 , after update, data is {current : 24, peak : 280}.
1884: I0813 11:19:30.090898  1445 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 17463265522944167790 to 12235714011619907428 , after update, data is {current : 24, peak : 280}.
1884: I0813 11:19:30.090960  1446 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12818182337197774485 to 12235714011619907428 , after update, data is {current : 28, peak : 280}.
1884: I0813 11:19:30.090970  1446 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12818182337197774485 to 12235714011619907428 , after update, data is {current : 28, peak : 280}.
1884: I0813 11:19:30.091085  1447 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 12235714011619907428 to 820151212369438717 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0813 11:19:30.091094  1447 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 12235714011619907428 to 820151212369438717 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0813 11:19:30.091100  1447 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 12235714011619907428 to 820151212369438717 , after update, data is {current : 1536, peak : 2401024}.
1884: I0813 11:19:30.225064  1364 onednn_context.cc:104] Clearing DNNL cache.
1884: I0813 11:19:30.225091  1364 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0813 11:19:30.225129  1364 mmap_allocator.cc:348] PID: 1364, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............   Passed   13.11 sec

The following tests passed:
	test_multinomial_op

100% tests passed, 0 tests failed out of 1

Total Test time (real) =  13.28 sec
