UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1889
    Start 1889: test_multinomial_op

1889: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1889: Environment variables: 
1889:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1889:  FLAGS_PIR_NO_CHECK=True
1889: Test timeout computed to be: 10000000
1889: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1889: WARNING: Logging before InitGoogleLogging() is written to STDERR
1889: I0815 05:33:26.503245 13485 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1889: I0815 05:33:27.287607 13485 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=prim_skip_dynamic,cusparse_dir,dist_threadpool_size,local_exe_sub_scope_limit,enable_neighbor_list_use_uva,memory_fraction_of_eager_deletion,pir_apply_inplace_pass,enable_fuse_parallel_matmul_pass,graph_load_in_parallel,enable_dependency_builder_debug_info,fast_eager_deletion_mode,new_executor_sequential_run,use_cinn,sync_after_alloc,cuda_dir,check_infer_symbolic,gpugraph_parallel_stream_num,init_allocated_mem,benchmark,call_stack_level,cublaslt_exhaustive_search_times,benchmark_nccl,tracer_onednn_ops_off,inner_op_parallelism,reader_queue_speed_test_mode,gpugraph_parallel_copyer_split_maxsize,use_auto_growth_v2,new_executor_use_local_scope,nccl_dir,tracer_onednn_ops_on,nvidia_package_dir,cudnn_deterministic,trt_min_group_size,logging_trunc_pir_py_code,mklml_dir,gemm_use_half_precision_compute_type,gpugraph_force_device_batch_num_equal,free_when_no_cache_hit,gpugraph_enable_hbm_table_collision_stat,initial_gpu_memory_in_mb,fuse_parameter_groups_size,free_idle_chunk,gpugraph_load_node_list_into_hbm,jit_engine_type,conv2d_disable_cudnn,use_system_allocator,prim_enable_dynamic,enable_exit_when_partial_worker,cupti_dir,prim_forward_blacklist,cudnn_exhaustive_search,cse_max_count,use_stream_safe_cuda_allocator,prim_forward,gpugraph_enable_print_op_debug,convert_all_blocks,enable_async_trace,enable_pir_in_executor_trace_run,print_allocator_trace_info,fraction_of_gpu_memory_to_use,cudnn_exhaustive_search_times,npu_storage_format,cuda_memory_async_pool_realease_threshold,cinn_subgraph_graphviz_dir,gpu_allocator_retry_time,use_autotune,enable_gpu_memory_usage_log,static_executor_perfstat_filepath,gpu_memory_limit_mb,enable_gpu_memory_usage_log_mb,check_nan_inf_level,print_sub_graph_dir,pir_subgraph_saving_dir,cublaslt_device_best_config,host_trace_level,use_cuda_managed_memory,gpugraph_slot_feasign_max_num,tensorrt_dir,dygraph_debug,graph_get_neighbor_id,enable_cinn_accuracy_check,set_to_1d,enable_cinn_compile_cache,gpugraph_offload_param_extends,gpugraph_offload_param_stat,cinn_compile_thread_num,eager_delete_tensor_gb,enable_collect_shape,accuracy_check_rtol_fp16,pir_broadcast_tree_limit,apply_pass_to_program,tracer_profile_fname,graph_neighbor_size_percent,alloc_fill_value,enable_unused_var_check,use_virtual_memory_auto_growth,static_runtime_data_save_path,check_kernel_launch,use_pinned_memory,log_memory_stats,enable_pir_api,low_precision_op_list,custom_device_mem_record,gpugraph_dedup_pull_push_mode,initial_cpu_memory_in_mb,get_host_by_name_time,max_inplace_grad_add,query_dest_rank_by_multi_node,trt_ibuilder_cache,cuda_malloc_async_pool_memory_throttle_ratio,manually_trans_conv_filter,use_xqa_optim,gpugraph_debug_gpu_memory,sort_sum_gradient,enable_cinn_auto_tune,add_dependency_for_communication_op,new_executor_use_cuda_graph,gpugraph_hbm_table_load_factor,nccl_blocking_wait,use_auto_growth_pinned_allocator,search_cache_max_number,op_dir,graph_metapath_split_opt,new_executor_serial_run,dataloader_use_file_descriptor,enable_pir_with_pt_in_dy2st,einsum_opt,new_executor_use_inplace,embedding_deterministic,use_stride_kernel,gpugraph_offload_gather_copy_maxsize,enable_all2all_use_fp16,accuracy_check_atol_fp16,curand_dir,disable_dyshape_in_train,enable_sparse_inner_gather,paddle_num_threads,eager_delete_scope,auto_growth_chunk_size_in_mb,fuse_parameter_memory_size,pinned_memory_as_cpu_backend,new_executor_static_build,ir_inplace_kernel_blacklist,use_cuda_malloc_async_allocator,deny_cinn_ops,tensor_operants_mode,cusparselt_dir,sync_nccl_allreduce,cudnn_batchnorm_spatial_persistent,accuracy_check_atol_bf16,prim_enabled,lapack_dir,prim_check_ops,cublas_dir,allocator_strategy,pir_debug,conv_workspace_size_limit,prim_backward,multiple_of_cupti_buffer_size,allreduce_record_one_event,enable_auto_rdma_trans,use_fast_math,dynamic_static_unified_comm,enable_cublas_tensor_op_math,all_blocks_convert_trt,enable_adjust_op_order,logging_pir_py_code_int_tensor_element_limit,auto_free_cudagraph_allocations_on_launch,use_shm_cache,accuracy_check_rtol_bf16,logging_pir_py_code_dir,gpugraph_enable_gpu_direct_access,reallocate_gpu_memory_in_mb,multi_node_sample_use_gpu_table,pir_apply_shape_optimization_pass,async_trace_count,print_ir,cache_inference_while_scope,graph_embedding_split_infer_mode,gpugraph_enable_segment_merge_grads,selected_gpus,enable_graph_multi_node_sampling,run_kp_kernel,mkl_dir,use_mkldnn,fraction_of_cpu_memory_to_use,enable_opt_get_features,dump_chunk_info,save_static_runtime_data,enable_pir_in_executor,gpugraph_merge_grads_segment_size,check_nan_inf,enable_dump_main_program,enable_tracker_all2all,accuracy_check_atol_fp32,gpugraph_storage_mode,fraction_of_cuda_pinned_memory_to_use,win_cuda_bin_dir,gpugraph_sparse_table_storage_mode,accuracy_check_rtol_fp32,cudnn_dir,prim_all,allow_cinn_ops,executor_log_deps_every_microseconds,cusolver_dir,enable_auto_detect_gpu_topo,logging_pir_py_code_dump_symbolic_dims,enable_api_kernel_fallback,fleet_executor_with_standalone,enable_interpretercore_launch_cinn,enable_fusion_fallback,enable_blaslt_global_search,enable_record_memory,enable_cse_in_dy2st 
1889: I0815 05:33:27.287729 13485 init.cc:108] After Parse: argc is 2
1889: I0815 05:33:34.924549 13485 dygraph_functions.cc:77615] Running AD API: uniform
1889: I0815 05:33:34.924587 13485 dygraph_functions.cc:77636] { Input: []} 
1889: W0815 05:33:34.925237 13485 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1889: I0815 05:33:34.925563 13485 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1889: W0815 05:33:34.926388 13485 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1889: I0815 05:33:34.926474 13485 allocator_facade.cc:212] selected allocator strategy:1
1889: I0815 05:33:34.926563 13485 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1889: I0815 05:33:34.927229 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f283f600000), and remaining 0
1889: I0815 05:33:34.927505 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:34.927597 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:34.927701 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f283f600200), and remaining 0
1889: I0815 05:33:34.927742 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f283f600400), and remaining 0
1889: I0815 05:33:34.931520 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f283f600600), and remaining 0
1889: I0815 05:33:34.931653 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f283f600800), and remaining 0
1889: I0815 05:33:34.931739 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f283f600a00), and remaining 0
1889: I0815 05:33:34.931836 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:34.931860 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:34.931931 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:34.931944 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:34.932781 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x2bc9ae10 for it.
1889: I0815 05:33:34.932937 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:34.932965 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:34.933023 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f283f600e00), and remaining 0
1889: I0815 05:33:34.933091 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f283f6c4400), and remaining 0
1889: I0815 05:33:35.052647 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x2bc9ae10 for it.
1889: I0815 05:33:35.052843 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:35.052889 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.053476 13485 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f283f800000), and remaining 0
1889: I0815 05:33:35.061013 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x2bc9ae10 for it.
1889: I0815 05:33:35.061117 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:35.061151 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.061188 13485 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:35.061376 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:35.062321 13485 dygraph_functions.cc:33459] Running AD API: full
1889: I0815 05:33:35.062340 13485 dygraph_functions.cc:33480] { Input: []} 
1889: I0815 05:33:35.062399 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:35.062474 13485 dygraph_functions.cc:64553] Running AD API: scale
1889: I0815 05:33:35.062502 13485 dygraph_functions.cc:64610] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.062569 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:35.062647 13485 dygraph_functions.cc:26170] Running AD API: exp
1889: I0815 05:33:35.062666 13485 dygraph_functions.cc:26227] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.062703 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:35.062873 13485 dygraph_functions.cc:72485] Running AD API: sum
1889: I0815 05:33:35.062896 13485 dygraph_functions.cc:72542] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.063076 13485 dygraph_functions.cc:83153] Running AD API: divide
1889: I0815 05:33:35.063102 13485 dygraph_functions.cc:83226] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]),  
1889: ( y , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:35.063176 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:35.065152 13485 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1889: I0815 05:33:35.065245 13485 dygraph_functions.cc:87315] Running AD API: softmax
1889: I0815 05:33:35.065274 13485 dygraph_functions.cc:87372] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: W0815 05:33:35.065344 13485 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1889: I0815 05:33:36.411631 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.411692 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.411955 13485 dygraph_functions.cc:75627] Running AD API: transpose
1889: I0815 05:33:36.411974 13485 dygraph_functions.cc:75684] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.416332 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.416373 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.417711 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.417732 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.417748 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.418506 13485 program_interpreter.cc:243] New Executor is Running.
1889: I0815 05:33:36.418521 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.418546 13485 scope.cc:202] Create variable feed
1889: I0815 05:33:36.418558 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.418569 13485 scope.cc:202] Create variable fetch
1889: I0815 05:33:36.418572 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.418586 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.418591 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.418596 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.418599 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.420924 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.421269 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.421283 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.421288 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.422978 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.423029 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.423038 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.423045 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.423053 13485 scope.cc:202] Create variable multinomial_0.tmp_0
1889: I0815 05:33:36.423059 13485 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61f23270 type is 7
1889: I0815 05:33:36.423065 13485 scope.cc:202] Create variable x
1889: I0815 05:33:36.423071 13485 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5542f50 type is 7
1889: I0815 05:33:36.423130 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.423136 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.423139 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.423143 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.423280 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.423308 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.423437 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.423449 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.423465 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.423642 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.423672 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.423691 13485 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.423696 13485 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61f3db00Variable Type 7
1889: I0815 05:33:36.423717 13485 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.423743 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.423795 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.423815 13485 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.425040 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.425096 13485 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.425503 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.430016 13485 dygraph_functions.cc:77615] Running AD API: uniform
1889: I0815 05:33:36.430037 13485 dygraph_functions.cc:77636] { Input: []} 
1889: I0815 05:33:36.430138 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.430171 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.430657 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: I0815 05:33:36.430734 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.430757 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.431152 13485 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: I0815 05:33:36.431218 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.431241 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.431267 13485 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.431555 13485 dygraph_functions.cc:77615] Running AD API: uniform
1889: I0815 05:33:36.431567 13485 dygraph_functions.cc:77636] { Input: []} 
1889: I0815 05:33:36.431670 13485 dygraph_functions.cc:87169] Running AD API: set_value_
1889: I0815 05:33:36.431691 13485 dygraph_functions.cc:87213] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.432070 13485 dygraph_functions.cc:77615] Running AD API: uniform
1889: I0815 05:33:36.432081 13485 dygraph_functions.cc:77636] { Input: []} 
1889: I0815 05:33:36.432122 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.432142 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.432333 13485 dygraph_functions.cc:77615] Running AD API: uniform
1889: I0815 05:33:36.432343 13485 dygraph_functions.cc:77636] { Input: []} 
1889: I0815 05:33:36.432380 13485 dygraph_functions.cc:53762] Running AD API: multinomial
1889: I0815 05:33:36.432399 13485 dygraph_functions.cc:53816] { Input: [ 
1889: ( x , [[ Not specified tensor log level ]]), ]} 
1889: I0815 05:33:36.432415 13485 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.435086 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.435113 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.435168 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.435177 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.437127 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.437505 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.437520 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.437526 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.439329 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.439384 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.439395 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.439405 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61f685b0 type is 7
1889: I0815 05:33:36.439412 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.439419 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61f68920 type is 7
1889: I0815 05:33:36.439424 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.439429 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.439486 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.439494 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.439498 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.439502 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.439558 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.439574 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.439640 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.439651 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.439668 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.439927 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.439944 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.439965 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.439972 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61f6f070Variable Type 7
1889: I0815 05:33:36.439991 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.440011 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.440035 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.440052 13485 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.440744 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.440784 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.440979 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.454365 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: I0815 05:33:36.454603 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889: }
1889: 
1889: I0815 05:33:36.461006 13485 pir_interpreter.cc:161] PirInterpreter(): 0x6212c3d0 on Place(gpu:0)
1889: I0815 05:33:36.461061 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.461094 13485 scope.cc:202] Create variable 0x6212c3d01723700016461038075_inner_var_1
1889: I0815 05:33:36.461105 13485 scope.cc:202] Create variable 0x6212c3d01723700016461038075_inner_var_2
1889: I0815 05:33:36.461117 13485 scope.cc:202] Create variable 0x6212c3d01723700016461038075_inner_var_3
1889: I0815 05:33:36.461127 13485 scope.cc:202] Create variable 0x6212c3d01723700016461038075_inner_var_4
1889: I0815 05:33:36.461138 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.461591 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.461608 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.461613 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: I0815 05:33:36.461663 13485 pir_interpreter.cc:1455] New Executor is Running ...
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x6212c330
1889: 1 -> 0x6212c3d01723700016461038075_inner_var_1 -> 0x6212c3b0
1889: 2 -> 0x6212c3d01723700016461038075_inner_var_2 -> 0x6212cc80
1889: 3 -> 0x6212c3d01723700016461038075_inner_var_3 -> 0x6212bb10
1889: 4 -> 0x6212c3d01723700016461038075_inner_var_4 -> 0x6212d030
1889: 5 -> fetch0@fetch -> 0x6212d840
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.462446 13485 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1889: I0815 05:33:36.462695 13523 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.462847 13525 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.462847 13524 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.462955 13526 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.462972 13527 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.463057 13526 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6212c3d01723700016461038075_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.463099 13528 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.463176 13528 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.463196 13526 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6212c3d01723700016461038075_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.463228 13528 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1889: I0815 05:33:36.463281 13528 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6212c3d01723700016461038075_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6212c3d01723700016461038075_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.463506 13528 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6212c3d01723700016461038075_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6212c3d01723700016461038075_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1889: I0815 05:33:36.463591 13526 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6212c3d01723700016461038075_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.463626 13526 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.464911 13526 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6212c3d01723700016461038075_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x6212c3d01723700016461038075_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1889: I0815 05:33:36.464965 13526 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6212c3d01723700016461038075_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.464998 13526 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.465606 13526 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6212c3d01723700016461038075_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1889: I0815 05:33:36.465659 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x6212c540) got event_name: TaskCompletion
1889: I0815 05:33:36.465693 13485 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.541669 13523 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 16806961198187347962 to 4485270463441655231 , after update, data is {current : 0, peak : 800768}.
1889: I0815 05:33:36.541687 13523 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 16806961198187347962 to 9573413560389832194 , after update, data is {current : 800000, peak : 1600004}.
1889: I0815 05:33:36.541693 13523 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 16806961198187347962 to 9573413560389832194 , after update, data is {current : 800000, peak : 1600004}.
1889: I0815 05:33:36.541937 13526 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 9573413560389832194 to 5505790893920834842 , after update, data is {current : 800000, peak : 2400000}.
1889: I0815 05:33:36.541954 13526 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 9573413560389832194 to 5505790893920834842 , after update, data is {current : 800000, peak : 2400000}.
1889: I0815 05:33:36.542117 13528 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 4485270463441655231 to 5505790893920834842 , after update, data is {current : 3203088, peak : 3203847}.
1889: I0815 05:33:36.542138 13528 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 4485270463441655231 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.548130 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.548161 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.548221 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.548228 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.550019 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.550395 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.550410 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.550415 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.551926 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.552016 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.552026 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.552033 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x645e2240 type is 7
1889: I0815 05:33:36.552040 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.552047 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x645fe1d0 type is 7
1889: I0815 05:33:36.552050 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.552055 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.552110 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.552116 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.552120 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.552124 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.552179 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.552196 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.552260 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.552269 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.552284 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.552419 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.552431 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.552446 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.552453 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61f0e930Variable Type 7
1889: I0815 05:33:36.552469 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.552488 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.552510 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.552525 13485 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.554111 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.554149 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.554375 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.559264 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: I0815 05:33:36.559465 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889: }
1889: 
1889: I0815 05:33:36.562608 13485 pir_interpreter.cc:161] PirInterpreter(): 0x64606ac0 on Place(gpu:0)
1889: I0815 05:33:36.562644 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.562666 13485 scope.cc:202] Create variable 0x64606ac01723700016562634425_inner_var_1
1889: I0815 05:33:36.562677 13485 scope.cc:202] Create variable 0x64606ac01723700016562634425_inner_var_2
1889: I0815 05:33:36.562690 13485 scope.cc:202] Create variable 0x64606ac01723700016562634425_inner_var_3
1889: I0815 05:33:36.562700 13485 scope.cc:202] Create variable 0x64606ac01723700016562634425_inner_var_4
1889: I0815 05:33:36.562711 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.563043 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.563059 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.563063 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x45d04590
1889: 1 -> 0x64606ac01723700016562634425_inner_var_1 -> 0x5554170
1889: 2 -> 0x64606ac01723700016562634425_inner_var_2 -> 0x6212b1b0
1889: 3 -> 0x64606ac01723700016562634425_inner_var_3 -> 0x45ce5910
1889: 4 -> 0x64606ac01723700016562634425_inner_var_4 -> 0x6212baf0
1889: 5 -> fetch0@fetch -> 0x553f750
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.563786 13529 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.563867 13530 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.563891 13531 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.563938 13532 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.563972 13533 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.564023 13534 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.564031 13533 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64606ac01723700016562634425_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.564044 13534 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.564085 13534 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1889: I0815 05:33:36.564088 13533 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x64606ac01723700016562634425_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.564127 13534 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64606ac01723700016562634425_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64606ac01723700016562634425_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.564260 13534 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x64606ac01723700016562634425_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x64606ac01723700016562634425_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.564325 13533 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64606ac01723700016562634425_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.564348 13533 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.567080 13533 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x64606ac01723700016562634425_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x64606ac01723700016562634425_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.567126 13533 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x64606ac01723700016562634425_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.567148 13533 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.569160 13533 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x64606ac01723700016562634425_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.569211 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x64606c30) got event_name: TaskCompletion
1889: I0815 05:33:36.569237 13485 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.607702 13529 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 4485270463441655231 to 9190478044816721629 , after update, data is {current : 0, peak : 2400768}.
1889: I0815 05:33:36.607720 13529 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4485270463441655231 to 7245226169759526010 , after update, data is {current : 2400000, peak : 4800004}.
1889: I0815 05:33:36.607728 13529 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 4485270463441655231 to 7245226169759526010 , after update, data is {current : 2400000, peak : 4800004}.
1889: I0815 05:33:36.607892 13533 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7245226169759526010 to 5505790893920834842 , after update, data is {current : 3200000, peak : 4800004}.
1889: I0815 05:33:36.607903 13533 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7245226169759526010 to 5505790893920834842 , after update, data is {current : 3200000, peak : 4800004}.
1889: I0815 05:33:36.608093 13534 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.612232 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.612264 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.612332 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.612341 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.614116 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.614487 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.614502 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.614508 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.616026 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.616135 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.616147 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.616156 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x552c200 type is 7
1889: I0815 05:33:36.616164 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.616169 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61f6ee70 type is 7
1889: I0815 05:33:36.616174 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.616179 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.616231 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.616238 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.616241 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.616245 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.616305 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.616320 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.616387 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.616395 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.616410 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.616451 13485 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.616586 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.616652 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.616662 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.616677 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.616684 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x645f5fd0Variable Type 7
1889: I0815 05:33:36.616701 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.616719 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.616741 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.616755 13485 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.616986 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.617008 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.617204 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.617985 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: I0815 05:33:36.618161 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889: }
1889: 
1889: I0815 05:33:36.621209 13485 pir_interpreter.cc:161] PirInterpreter(): 0x61f50180 on Place(gpu:0)
1889: I0815 05:33:36.621244 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.621268 13485 scope.cc:202] Create variable 0x61f501801723700016621236066_inner_var_1
1889: I0815 05:33:36.621279 13485 scope.cc:202] Create variable 0x61f501801723700016621236066_inner_var_2
1889: I0815 05:33:36.621289 13485 scope.cc:202] Create variable 0x61f501801723700016621236066_inner_var_3
1889: I0815 05:33:36.621308 13485 scope.cc:202] Create variable 0x61f501801723700016621236066_inner_var_4
1889: I0815 05:33:36.621320 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.621637 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.621654 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.621657 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x4d65a790
1889: 1 -> 0x61f501801723700016621236066_inner_var_1 -> 0x61ab9ed0
1889: 2 -> 0x61f501801723700016621236066_inner_var_2 -> 0x64606dc0
1889: 3 -> 0x61f501801723700016621236066_inner_var_3 -> 0x62106170
1889: 4 -> 0x61f501801723700016621236066_inner_var_4 -> 0x5557470
1889: 5 -> fetch0@fetch -> 0x557b290
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.622355 13535 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.622443 13536 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.622473 13537 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.622514 13538 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.622545 13539 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.622594 13540 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.622586 13539 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f501801723700016621236066_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.622627 13540 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.622642 13539 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f501801723700016621236066_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.622664 13540 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1889: I0815 05:33:36.622696 13540 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f501801723700016621236066_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f501801723700016621236066_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.622742 13540 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.622859 13540 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.622889 13540 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f501801723700016621236066_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f501801723700016621236066_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1889: I0815 05:33:36.622941 13539 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f501801723700016621236066_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.622962 13539 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.623289 13539 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f501801723700016621236066_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61f501801723700016621236066_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1889: I0815 05:33:36.623324 13539 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f501801723700016621236066_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.623344 13539 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.623355 13539 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f501801723700016621236066_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1889: I0815 05:33:36.623385 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x61f502f0) got event_name: TaskCompletion
1889: I0815 05:33:36.623410 13485 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.656666 13535 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 9190478044816721629 to 18299877630603382627 , after update, data is {current : 0, peak : 3328}.
1889: I0815 05:33:36.656684 13535 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9190478044816721629 to 18299877630603382627 , after update, data is {current : -804, peak : 2000}.
1889: I0815 05:33:36.656690 13535 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9190478044816721629 to 18299877630603382627 , after update, data is {current : -804, peak : 2000}.
1889: I0815 05:33:36.656980 13539 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 4485270463441655231 to 18299877630603382627 , after update, data is {current : 800, peak : 2000}.
1889: I0815 05:33:36.656999 13539 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 4485270463441655231 to 18299877630603382627 , after update, data is {current : 800, peak : 2000}.
1889: I0815 05:33:36.657123 13540 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 18299877630603382627 to 5505790893920834842 , after update, data is {current : 3200800, peak : 4800004}.
1889: I0815 05:33:36.657135 13540 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 18299877630603382627 to 5505790893920834842 , after update, data is {current : 3200800, peak : 4800004}.
1889: I0815 05:33:36.657140 13540 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 18299877630603382627 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.661564 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.661595 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.661653 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.661660 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.663388 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.663749 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.663763 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.663769 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.665282 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.665385 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.665396 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.665403 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5540790 type is 7
1889: I0815 05:33:36.665413 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.665416 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5529ef0 type is 7
1889: I0815 05:33:36.665421 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.665426 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.665478 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.665485 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.665489 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.665493 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.665544 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.665560 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.665623 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.665632 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.665645 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.665884 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.665895 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.665912 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.665918 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x55496e0Variable Type 7
1889: I0815 05:33:36.665935 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.665953 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.665977 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.665992 13485 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.666710 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.666741 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.666949 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.669775 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: I0815 05:33:36.669957 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889: }
1889: 
1889: I0815 05:33:36.673095 13485 pir_interpreter.cc:161] PirInterpreter(): 0x61f1da80 on Place(gpu:0)
1889: I0815 05:33:36.673131 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.673154 13485 scope.cc:202] Create variable 0x61f1da801723700016673123521_inner_var_1
1889: I0815 05:33:36.673166 13485 scope.cc:202] Create variable 0x61f1da801723700016673123521_inner_var_2
1889: I0815 05:33:36.673177 13485 scope.cc:202] Create variable 0x61f1da801723700016673123521_inner_var_3
1889: I0815 05:33:36.673187 13485 scope.cc:202] Create variable 0x61f1da801723700016673123521_inner_var_4
1889: I0815 05:33:36.673198 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.673544 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.673563 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.673566 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x61f1d6d0
1889: 1 -> 0x61f1da801723700016673123521_inner_var_1 -> 0x554b250
1889: 2 -> 0x61f1da801723700016673123521_inner_var_2 -> 0x4a50c9d0
1889: 3 -> 0x61f1da801723700016673123521_inner_var_3 -> 0x642889c0
1889: 4 -> 0x61f1da801723700016673123521_inner_var_4 -> 0x64752d50
1889: 5 -> fetch0@fetch -> 0x61f57160
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.674268 13541 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.674366 13542 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.674393 13543 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.674438 13544 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.674471 13545 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.674520 13546 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.674527 13545 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f1da801723700016673123521_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.674543 13546 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.674567 13545 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f1da801723700016673123521_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.674578 13546 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1889: I0815 05:33:36.674609 13546 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f1da801723700016673123521_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f1da801723700016673123521_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.674839 13546 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f1da801723700016673123521_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f1da801723700016673123521_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1889: I0815 05:33:36.674898 13545 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f1da801723700016673123521_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.674918 13545 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.676112 13545 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f1da801723700016673123521_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x61f1da801723700016673123521_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1889: I0815 05:33:36.676153 13545 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f1da801723700016673123521_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.676175 13545 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.676731 13545 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f1da801723700016673123521_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1889: I0815 05:33:36.676775 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x61f1dbf0) got event_name: TaskCompletion
1889: I0815 05:33:36.676800 13485 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.681162 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.681190 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.681246 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.681255 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.683172 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.683597 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.683614 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.683619 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.685498 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.685595 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.685608 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.685617 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61f0be00 type is 7
1889: I0815 05:33:36.685626 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.685631 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61f0b140 type is 7
1889: I0815 05:33:36.685635 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.685640 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.685705 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.685714 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.685719 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.685724 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.685778 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.685796 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.685864 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.685878 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.685897 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.686134 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.686149 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.686168 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.686175 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x641eedb0Variable Type 7
1889: I0815 05:33:36.686194 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.686216 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.686242 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.686259 13485 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.686960 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.686997 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.687235 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.725778 13541 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 18299877630603382627 to 9190478044816721629 , after update, data is {current : 0, peak : 800768}.
1889: I0815 05:33:36.725802 13541 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 18299877630603382627 to 7245226169759526010 , after update, data is {current : 800000, peak : 1600004}.
1889: I0815 05:33:36.725807 13541 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 18299877630603382627 to 7245226169759526010 , after update, data is {current : 800000, peak : 1600004}.
1889: I0815 05:33:36.725987 13545 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 7245226169759526010 to 5505790893920834842 , after update, data is {current : 4000800, peak : 4800004}.
1889: I0815 05:33:36.726001 13545 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 7245226169759526010 to 5505790893920834842 , after update, data is {current : 4000800, peak : 4800800}.
1889: I0815 05:33:36.726204 13546 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 3205168, peak : 3207136}.
1889: I0815 05:33:36.726217 13546 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.731523 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.731552 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.731606 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.731613 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.733282 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.733640 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.733656 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.733660 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.735157 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.735245 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.735256 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.735262 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x556b1e0 type is 7
1889: I0815 05:33:36.735271 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.735275 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5542ba0 type is 7
1889: I0815 05:33:36.735278 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.735282 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.735347 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.735354 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.735358 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.735363 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.735412 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.735426 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.735486 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.735495 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.735509 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.735641 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.735651 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.735666 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.735672 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x646057d0Variable Type 7
1889: I0815 05:33:36.735687 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.735704 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.735725 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.735739 13485 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.737383 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.737421 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.737635 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.741946 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: I0815 05:33:36.742125 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889: }
1889: 
1889: I0815 05:33:36.745215 13485 pir_interpreter.cc:161] PirInterpreter(): 0x64700110 on Place(gpu:0)
1889: I0815 05:33:36.745250 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.745273 13485 scope.cc:202] Create variable 0x647001101723700016745242271_inner_var_1
1889: I0815 05:33:36.745285 13485 scope.cc:202] Create variable 0x647001101723700016745242271_inner_var_2
1889: I0815 05:33:36.745296 13485 scope.cc:202] Create variable 0x647001101723700016745242271_inner_var_3
1889: I0815 05:33:36.745316 13485 scope.cc:202] Create variable 0x647001101723700016745242271_inner_var_4
1889: I0815 05:33:36.745327 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.745653 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.745669 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.745673 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x61f55260
1889: 1 -> 0x647001101723700016745242271_inner_var_1 -> 0x64003980
1889: 2 -> 0x647001101723700016745242271_inner_var_2 -> 0x61f19560
1889: 3 -> 0x647001101723700016745242271_inner_var_3 -> 0x61f550d0
1889: 4 -> 0x647001101723700016745242271_inner_var_4 -> 0x5535830
1889: 5 -> fetch0@fetch -> 0x36972e0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.746392 13547 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.746465 13548 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.746488 13549 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.746551 13551 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.746562 13550 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.746593 13550 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x647001101723700016745242271_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.746655 13550 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x647001101723700016745242271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.746721 13552 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.746743 13552 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.746768 13552 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1889: I0815 05:33:36.746801 13552 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x647001101723700016745242271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x647001101723700016745242271_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.746907 13552 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x647001101723700016745242271_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x647001101723700016745242271_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.746965 13550 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x647001101723700016745242271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.747009 13550 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.749866 13550 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x647001101723700016745242271_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x647001101723700016745242271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.749920 13550 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x647001101723700016745242271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.749946 13550 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.752288 13550 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x647001101723700016745242271_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1889: I0815 05:33:36.752349 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x64700280) got event_name: TaskCompletion
1889: I0815 05:33:36.752377 13485 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.758003 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.758029 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.758082 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.758092 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.759990 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.760406 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.760421 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.760426 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.762259 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.762375 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.762390 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.762405 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6411e0a0 type is 7
1889: I0815 05:33:36.762413 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.762420 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6411da60 type is 7
1889: I0815 05:33:36.762425 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.762430 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.762493 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.762501 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.762506 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.762509 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.762562 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.762576 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.762642 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.762653 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.762671 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.762790 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.762802 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.762820 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.762828 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6212fd00Variable Type 7
1889: I0815 05:33:36.762845 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.762866 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.762888 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.762904 13485 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.764575 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.764611 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.764807 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.811187 13547 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 9190478044816721629 to 18299877630603382627 , after update, data is {current : 0, peak : 2400768}.
1889: I0815 05:33:36.811214 13547 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 9190478044816721629 to 11378414668569756947 , after update, data is {current : 2400000, peak : 4800004}.
1889: I0815 05:33:36.811219 13547 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 9190478044816721629 to 11378414668569756947 , after update, data is {current : 2400000, peak : 4800004}.
1889: I0815 05:33:36.811403 13550 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 11378414668569756947 to 5505790893920834842 , after update, data is {current : 6400800, peak : 6400800}.
1889: I0815 05:33:36.811419 13550 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 11378414668569756947 to 5505790893920834842 , after update, data is {current : 6400800, peak : 8800800}.
1889: I0815 05:33:36.811602 13552 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 18299877630603382627 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.816265 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.816291 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.816358 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.816366 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.818085 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.818449 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.818464 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.818470 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.820008 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.820102 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.820114 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.820122 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62101100 type is 7
1889: I0815 05:33:36.820135 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.820138 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x45ce8bc0 type is 7
1889: I0815 05:33:36.820142 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.820147 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.820202 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.820209 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.820212 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.820215 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.820268 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.820281 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.820354 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.820364 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.820402 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.820443 13485 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.820564 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.820621 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.820631 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.820647 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.820653 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x645fe0f0Variable Type 7
1889: I0815 05:33:36.820670 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.820689 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.820711 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.820725 13485 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.820823 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.820845 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.821044 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.821830 13485 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad3a020 for it.
1889: I0815 05:33:36.822008 13485 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ac529e0 for it.
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1889:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1889:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889: }
1889: 
1889: I0815 05:33:36.825282 13485 pir_interpreter.cc:161] PirInterpreter(): 0x61f21940 on Place(gpu:0)
1889: I0815 05:33:36.825326 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.825350 13485 scope.cc:202] Create variable 0x61f219401723700016825318506_inner_var_1
1889: I0815 05:33:36.825361 13485 scope.cc:202] Create variable 0x61f219401723700016825318506_inner_var_2
1889: I0815 05:33:36.825381 13485 scope.cc:202] Create variable 0x61f219401723700016825318506_inner_var_3
1889: I0815 05:33:36.825392 13485 scope.cc:202] Create variable 0x61f219401723700016825318506_inner_var_4
1889: I0815 05:33:36.825403 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:36.825763 13485 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1889: I0815 05:33:36.825780 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.825784 13485 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1889:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1889:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 2 )  = pd_op.full
1889: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1889: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1889: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> X -> 0x61f384e0
1889: 1 -> 0x61f219401723700016825318506_inner_var_1 -> 0x61f06a70
1889: 2 -> 0x61f219401723700016825318506_inner_var_2 -> 0x64889450
1889: 3 -> 0x61f219401723700016825318506_inner_var_3 -> 0x45cf7670
1889: 4 -> 0x61f219401723700016825318506_inner_var_4 -> 0x61eea340
1889: 5 -> fetch0@fetch -> 0x5553dc0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 2 
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.multinomial]->
1889: 2 downstreams: 3[pd_op.memcpy_d2h]->
1889: 3 downstreams: 4[pd_op.fetch]->
1889: 4 downstreams: 
1889: I0815 05:33:36.826472 13553 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:36.826555 13554 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:36.826584 13555 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:36.826620 13556 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:36.826649 13557 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:36.826694 13558 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:36.826692 13557 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f219401723700016825318506_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.826714 13558 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.826742 13558 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1889: I0815 05:33:36.826750 13557 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f219401723700016825318506_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:36.826776 13558 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f219401723700016825318506_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f219401723700016825318506_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.826812 13558 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.826921 13558 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.826946 13558 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f219401723700016825318506_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x61f219401723700016825318506_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1889: I0815 05:33:36.827005 13557 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f219401723700016825318506_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.827026 13557 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.827195 13557 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f219401723700016825318506_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x61f219401723700016825318506_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1889: I0815 05:33:36.827220 13557 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f219401723700016825318506_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:36.827239 13557 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.827252 13557 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f219401723700016825318506_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1889: I0815 05:33:36.827286 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x61f21ab0) got event_name: TaskCompletion
1889: I0815 05:33:36.827313 13485 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1889: I0815 05:33:36.828689 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.828713 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.828763 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.828771 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.830647 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.831041 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.831055 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.831060 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.832917 13485 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1889: I0815 05:33:36.832999 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.833012 13485 scope.cc:202] Create variable Out
1889: I0815 05:33:36.833024 13485 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x642e5db0 type is 7
1889: I0815 05:33:36.833030 13485 scope.cc:202] Create variable X
1889: I0815 05:33:36.833039 13485 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x642e50f0 type is 7
1889: I0815 05:33:36.833045 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.833050 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.833112 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.833119 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.833124 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.833128 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.833174 13485 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.833189 13485 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.833248 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.833258 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.833276 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.833318 13485 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.833412 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.833459 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.833472 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.833492 13485 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.833500 13485 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x64887300Variable Type 7
1889: I0815 05:33:36.833516 13485 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.833535 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.833559 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.833575 13485 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.833683 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.833706 13485 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.833914 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.869591 13553 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 18299877630603382627 to 9190478044816721629 , after update, data is {current : 0, peak : 10240}.
1889: I0815 05:33:36.869613 13553 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 18299877630603382627 to 9190478044816721629 , after update, data is {current : -804, peak : 8000}.
1889: I0815 05:33:36.869618 13553 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 18299877630603382627 to 9190478044816721629 , after update, data is {current : -804, peak : 8000}.
1889: I0815 05:33:36.869908 13557 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 7245226169759526010 to 9190478044816721629 , after update, data is {current : 800, peak : 8000}.
1889: I0815 05:33:36.869928 13557 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 7245226169759526010 to 9190478044816721629 , after update, data is {current : 800, peak : 8000}.
1889: I0815 05:33:36.870043 13558 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 6401600, peak : 6408800}.
1889: I0815 05:33:36.870054 13558 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 6401600, peak : 8800800}.
1889: I0815 05:33:36.870059 13558 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 9190478044816721629 to 5505790893920834842 , after update, data is {current : 0, peak : 2401024}.
1889: I0815 05:33:36.877043 13485 op_desc.cc:1111] CompileTime infer shape on uniform_random
1889: I0815 05:33:36.877103 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1889: I0815 05:33:36.878185 13485 op_desc.cc:1111] CompileTime infer shape on fill_constant
1889: I0815 05:33:36.879000 13485 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1889: I0815 05:33:36.879033 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1889: I0815 05:33:36.880358 13485 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1889: I0815 05:33:36.880389 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1889: I0815 05:33:36.881073 13485 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1889: I0815 05:33:36.882041 13485 op_desc.cc:1111] CompileTime infer shape on abs
1889: I0815 05:33:36.882069 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1889: I0815 05:33:36.883421 13485 op_desc.cc:1111] CompileTime infer shape on assign_value
1889: I0815 05:33:36.883445 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1889: I0815 05:33:36.884035 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.884063 13485 op_desc.cc:1111] CompileTime infer shape on multinomial
1889: I0815 05:33:36.884069 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.884076 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.886098 13485 op_desc.cc:1111] CompileTime infer shape on cast
1889: I0815 05:33:36.886124 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1889: I0815 05:33:36.887063 13485 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1889: I0815 05:33:36.887094 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1889: I0815 05:33:36.888010 13485 pybind.cc:1827] need skip: 0
1889: I0815 05:33:36.888315 13485 op_desc.cc:1111] CompileTime infer shape on fill_constant
1889: I0815 05:33:36.890074 13485 op_desc.cc:1111] CompileTime infer shape on fill_constant
1889: I0815 05:33:36.893787 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.893806 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.893812 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.895824 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.895844 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.895856 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.895860 13485 scope.cc:202] Create variable learning_rate_0
1889: I0815 05:33:36.895874 13485 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x64bc7f60 type is 7
1889: I0815 05:33:36.895877 13485 scope.cc:202] Create variable linear_0.b_0
1889: I0815 05:33:36.895880 13485 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d43480 type is 7
1889: I0815 05:33:36.895885 13485 scope.cc:202] Create variable linear_0.w_0
1889: I0815 05:33:36.895888 13485 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d434e0 type is 7
1889: I0815 05:33:36.895956 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.895962 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.895967 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.895969 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.896030 13485 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896047 13485 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896073 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1889: I0815 05:33:36.896225 13485 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896236 13485 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896309 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.896358 13485 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896366 13485 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.896395 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.897413 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.898747 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.899195 13485 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1889: I0815 05:33:36.900147 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.900465 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.900686 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.900702 13485 operator.cc:1021] found Attribute with Variable type: num_samples
1889: I0815 05:33:36.900769 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.900776 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.900780 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.900887 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.900897 13485 operator.cc:1021] found Attribute with Variable type: num_samples
1889: I0815 05:33:36.902457 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.903795 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.904919 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.905119 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.905134 13485 scope.cc:202] Create variable abs_0.tmp_0
1889: I0815 05:33:36.905143 13485 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x6401ce10 type is 7
1889: I0815 05:33:36.905153 13485 scope.cc:202] Create variable assign_0.tmp_0
1889: I0815 05:33:36.905155 13485 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x6401cb90 type is 7
1889: I0815 05:33:36.905159 13485 scope.cc:202] Create variable cast_0.tmp_0
1889: I0815 05:33:36.905162 13485 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x6401cc80 type is 7
1889: I0815 05:33:36.905167 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.905171 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.905176 13485 scope.cc:202] Create variable gaussian_0.tmp_0
1889: I0815 05:33:36.905179 13485 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x6401e250 type is 7
1889: I0815 05:33:36.905184 13485 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x64bc7f60 type is 7
1889: I0815 05:33:36.905187 13485 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d43480 type is 7
1889: I0815 05:33:36.905192 13485 scope.cc:202] Create variable linear_0.tmp_0
1889: I0815 05:33:36.905195 13485 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x6401e230 type is 7
1889: I0815 05:33:36.905200 13485 scope.cc:202] Create variable linear_0.tmp_1
1889: I0815 05:33:36.905202 13485 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x6401e700 type is 7
1889: I0815 05:33:36.905206 13485 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d434e0 type is 7
1889: I0815 05:33:36.905210 13485 scope.cc:202] Create variable mean_0.tmp_0
1889: I0815 05:33:36.905212 13485 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x6401e970 type is 7
1889: I0815 05:33:36.905217 13485 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1889: I0815 05:33:36.905220 13485 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x6401ebb0 type is 7
1889: I0815 05:33:36.905223 13485 scope.cc:202] Create variable multinomial_0.tmp_0
1889: I0815 05:33:36.905226 13485 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x6401ee10 type is 7
1889: I0815 05:33:36.905328 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.905344 13485 operator.cc:1021] found Attribute with Variable type: num_samples
1889: I0815 05:33:36.905407 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.905413 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.905418 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.905421 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.905480 13485 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.905496 13485 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.905519 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1889: I0815 05:33:36.905669 13485 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.905679 13485 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.905701 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1889: I0815 05:33:36.905781 13485 matmul_kernel_impl.h:374] MatMul's case 8
1889: I0815 05:33:36.905861 13485 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1889: I0815 05:33:36.906991 13485 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907011 13485 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907078 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.907150 13485 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907159 13485 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907173 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1889: I0815 05:33:36.907198 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.907238 13485 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907246 13485 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907258 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1889: I0815 05:33:36.907361 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907370 13485 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907383 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.907495 13485 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.907581 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.907644 13485 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907656 13485 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907671 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1889: I0815 05:33:36.907706 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.907756 13485 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907766 13485 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907780 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1889: I0815 05:33:36.907893 13485 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907904 13485 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.907927 13485 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:36.907971 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.907980 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.907996 13485 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.908004 13485 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63cfee90Variable Type 7
1889: I0815 05:33:36.908022 13485 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.908041 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.908061 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.908074 13485 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.908113 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.908138 13485 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1889: I0815 05:33:36.908164 13485 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.908174 13485 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.908185 13485 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1889: I0815 05:33:36.908192 13485 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6476cf40Variable Type 7
1889: I0815 05:33:36.908205 13485 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1889: I0815 05:33:36.908216 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.908231 13485 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.908243 13485 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:36.908277 13485 data_transfer.cc:232] Run memcpy_d2h done.
1889: I0815 05:33:36.908289 13485 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1889: I0815 05:33:36.908761 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1889: I0815 05:33:36.908798 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1889: I0815 05:33:36.908816 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1889: I0815 05:33:36.908851 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1889: I0815 05:33:36.908885 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.908903 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1889: I0815 05:33:36.913352 13485 op_desc.cc:1111] CompileTime infer shape on scale
1889: I0815 05:33:36.913395 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1889: I0815 05:33:36.914166 13485 op_desc.cc:1111] CompileTime infer shape on scale
1889: I0815 05:33:36.914189 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1889: I0815 05:33:36.914600 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.916332 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.917179 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.917312 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.917824 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.918740 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.920833 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.921890 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.923679 13485 op_desc.cc:1111] CompileTime infer shape on save_combine
1889: I0815 05:33:36.924531 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.924551 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.924556 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.925746 13485 interpreter_util.cc:1169] Creating Variables
1889: I0815 05:33:36.925766 13485 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x5543530 type is 9
1889: I0815 05:33:36.925776 13485 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45cf7b40 type is 10
1889: I0815 05:33:36.925781 13485 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x63d43480 type is 7
1889: I0815 05:33:36.925784 13485 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x63d434e0 type is 7
1889: I0815 05:33:36.925789 13485 scope.cc:202] Create variable saved_params
1889: I0815 05:33:36.925792 13485 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x64bbc3d0 type is 17
1889: I0815 05:33:36.925823 13485 interpreter_util.cc:594] Static build: 0
1889: I0815 05:33:36.925829 13485 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1889: I0815 05:33:36.925832 13485 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1889: I0815 05:33:36.925837 13485 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1889: I0815 05:33:36.925887 13485 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.925904 13485 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1889: I0815 05:33:36.926723 13485 analysis_predictor.cc:2434] create AnalysisPredictor
1889: I0815 05:33:36.926766 13485 analysis_predictor.cc:433] Predictor::init()
1889: I0815 05:33:36.926826 13485 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1889: I0815 05:33:36.928012 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.928072 13485 scope.cc:202] Create variable feed
1889: I0815 05:33:36.928078 13485 naive_executor.cc:189] 0x6212f160 Create persistable variable feed, which pointer is 0x63cdaee0
1889: I0815 05:33:36.928084 13485 scope.cc:202] Create variable fetch
1889: I0815 05:33:36.928087 13485 naive_executor.cc:189] 0x6212f160 Create persistable variable fetch, which pointer is 0x641c5f60
1889: I0815 05:33:36.928092 13485 scope.cc:202] Create variable linear_0.b_0
1889: I0815 05:33:36.928093 13485 naive_executor.cc:189] 0x6212f160 Create persistable variable linear_0.b_0, which pointer is 0x641c5d10
1889: I0815 05:33:36.928098 13485 scope.cc:202] Create variable linear_0.w_0
1889: I0815 05:33:36.928102 13485 naive_executor.cc:189] 0x6212f160 Create persistable variable linear_0.w_0, which pointer is 0x63c258f0
1889: I0815 05:33:36.928115 13485 analysis_predictor.cc:2023] AnalysisPredictor::PrepareArgument
1889: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1889: I0815 05:33:36.928467 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.928560 13485 program_converter.cc:296] is_legacy_program : 0
1889: I0815 05:33:36.928608 13485 executor.cc:183] Old Executor is Running.
1889: I0815 05:33:36.928673 13485 executor.cc:92] Creating Variables for block 0
1889: I0815 05:33:36.928680 13485 executor.cc:107] Initialize Variable linear_0.b_0
1889: I0815 05:33:36.928683 13485 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x641c5d10 type is 7
1889: I0815 05:33:36.928686 13485 executor.cc:107] Initialize Variable linear_0.w_0
1889: I0815 05:33:36.928689 13485 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x63c258f0 type is 7
1889: I0815 05:33:36.928725 13485 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.928810 13485 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1889: I0815 05:33:36.928855 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.928861 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:36.929003 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.929112 13485 graph.cc:149] create OpNode by feed
1889: I0815 05:33:36.929154 13485 graph.cc:149] create OpNode by matmul_v2
1889: I0815 05:33:36.929169 13485 graph.cc:149] create OpNode by elementwise_add
1889: I0815 05:33:36.929184 13485 graph.cc:149] create OpNode by abs
1889: I0815 05:33:36.929195 13485 graph.cc:149] create OpNode by assign_value
1889: I0815 05:33:36.929214 13485 graph.cc:149] create OpNode by multinomial
1889: I0815 05:33:36.929224 13485 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1889: I0815 05:33:36.929242 13485 graph.cc:149] create OpNode by scale
1889: I0815 05:33:36.929255 13485 graph.cc:149] create OpNode by scale
1889: I0815 05:33:36.929266 13485 graph.cc:149] create OpNode by fetch
1889: I0815 05:33:36.929283 13485 graph.cc:149] create OpNode by fetch
1889: I0815 05:33:36.929312 13485 graph.cc:224] kStaleProgramOpDescs.size: 10
1889: [1m[35m--- Running analysis [ir_analysis_pass][0m
1889: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1889: I0815 05:33:36.930686 13485 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1889: I0815 05:33:36.930696 13485 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1889: I0815 05:33:36.930768 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.930776 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1889: I0815 05:33:36.930892 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931144 13485 graph_pattern_detector.cc:126] 5 nodes marked
1889: I0815 05:33:36.931208 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931213 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1889: I0815 05:33:36.931248 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931253 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1889: I0815 05:33:36.931291 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931357 13485 graph_pattern_detector.cc:126] 2 nodes marked
1889: I0815 05:33:36.931388 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931393 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1889: I0815 05:33:36.931408 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931421 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.931442 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931447 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1889: I0815 05:33:36.931484 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931505 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.931529 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931533 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1889: I0815 05:33:36.931576 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931653 13485 graph_pattern_detector.cc:126] 2 nodes marked
1889: I0815 05:33:36.931681 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931686 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1889: I0815 05:33:36.931717 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931736 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.931758 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931763 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1889: I0815 05:33:36.931793 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.931943 13485 graph_pattern_detector.cc:126] 4 nodes marked
1889: I0815 05:33:36.931972 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.931977 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1889: I0815 05:33:36.932009 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.932025 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.932047 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.932052 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1889: I0815 05:33:36.932075 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.932089 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.932111 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.932116 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1889: I0815 05:33:36.932138 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.932152 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.932173 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.932178 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1889: I0815 05:33:36.932201 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.932269 13485 graph_pattern_detector.cc:126] 5 nodes marked
1889: I0815 05:33:36.932304 13485 graph_pattern_detector.cc:250] step 1 get records: 1
1889: I0815 05:33:36.932319 13485 graph_pattern_detector.cc:250] step 2 get records: 1
1889: I0815 05:33:36.932335 13485 graph_pattern_detector.cc:250] step 3 get records: 0
1889: I0815 05:33:36.932359 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.932364 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1889: I0815 05:33:36.932386 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.932426 13485 graph_pattern_detector.cc:126] 4 nodes marked
1889: I0815 05:33:36.932446 13485 graph_pattern_detector.cc:250] step 1 get records: 1
1889: I0815 05:33:36.932457 13485 graph_pattern_detector.cc:250] step 2 get records: 1
1889: I0815 05:33:36.932469 13485 graph_pattern_detector.cc:250] step 3 get records: 1
1889: I0815 05:33:36.932498 13485 graph_pattern_detector.cc:100] optimizing #0 subgraph
1889: I0815 05:33:36.932510 13485 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1889: I0815 05:33:36.933764 13485 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1889: I0815 05:33:36.933818 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.933825 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1889: I0815 05:33:36.933853 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.933873 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.933900 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.933905 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1889: I0815 05:33:36.933928 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.933976 13485 graph_pattern_detector.cc:126] 2 nodes marked
1889: I0815 05:33:36.934007 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.934012 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1889: I0815 05:33:36.934029 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.934044 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.934067 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.934072 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [fc_fuse_pass][0m
1889: I0815 05:33:36.934101 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.934190 13485 graph_pattern_detector.cc:126] 6 nodes marked
1889: I0815 05:33:36.934214 13485 graph_pattern_detector.cc:250] step 1 get records: 1
1889: I0815 05:33:36.934229 13485 graph_pattern_detector.cc:250] step 2 get records: 1
1889: I0815 05:33:36.934244 13485 graph_pattern_detector.cc:250] step 3 get records: 1
1889: I0815 05:33:36.934259 13485 graph_pattern_detector.cc:250] step 4 get records: 1
1889: I0815 05:33:36.934274 13485 graph_pattern_detector.cc:250] step 5 get records: 1
1889: I0815 05:33:36.934288 13485 graph_pattern_detector.cc:250] step 6 get records: 0
1889: I0815 05:33:36.934320 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.934394 13485 graph_pattern_detector.cc:126] 7 nodes marked
1889: I0815 05:33:36.934417 13485 graph_pattern_detector.cc:250] step 1 get records: 1
1889: I0815 05:33:36.934429 13485 graph_pattern_detector.cc:250] step 2 get records: 1
1889: I0815 05:33:36.934442 13485 graph_pattern_detector.cc:250] step 3 get records: 1
1889: I0815 05:33:36.934455 13485 graph_pattern_detector.cc:250] step 4 get records: 1
1889: I0815 05:33:36.934471 13485 graph_pattern_detector.cc:250] step 5 get records: 1
1889: I0815 05:33:36.934486 13485 graph_pattern_detector.cc:250] step 6 get records: 1
1889: I0815 05:33:36.934530 13485 graph_pattern_detector.cc:100] optimizing #0 subgraph
1889: I0815 05:33:36.934819 13485 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1889: I0815 05:33:36.934851 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.934856 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1889: I0815 05:33:36.934904 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.934967 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935002 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935050 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935077 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935119 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935143 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935181 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935202 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935235 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935254 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935284 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935307 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935333 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935348 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935370 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935382 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935401 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935428 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935433 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1889: I0815 05:33:36.935459 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935500 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935526 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935531 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1889: I0815 05:33:36.935542 13485 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1889: I0815 05:33:36.935545 13485 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1889: I0815 05:33:36.935591 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935612 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935638 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935643 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1889: I0815 05:33:36.935652 13485 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1889: I0815 05:33:36.935655 13485 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1889: I0815 05:33:36.935696 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935717 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935742 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935747 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1889: I0815 05:33:36.935755 13485 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1889: I0815 05:33:36.935757 13485 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1889: I0815 05:33:36.935789 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935807 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935830 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935835 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1889: I0815 05:33:36.935843 13485 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1889: I0815 05:33:36.935845 13485 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1889: I0815 05:33:36.935883 13485 graph_pattern_detector.cc:106] mark pdnodes in graph
1889: I0815 05:33:36.935904 13485 graph_pattern_detector.cc:126] 0 nodes marked
1889: I0815 05:33:36.935926 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935931 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [is_test_pass][0m
1889: I0815 05:33:36.935943 13485 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1889: I0815 05:33:36.935986 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.935992 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [32m--- Running IR pass [constant_folding_pass][0m
1889: I0815 05:33:36.936060 13485 scope.cc:202] Create variable assign_0.tmp_0
1889: I0815 05:33:36.936085 13485 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.936105 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1889: I0815 05:33:36.936167 13485 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1889: I0815 05:33:36.936185 13485 scope.cc:202] Create variable assign_0.tmp_0
1889: I0815 05:33:36.936214 13485 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1889: I0815 05:33:36.936237 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.936242 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1889: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1889: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1889: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1889: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1889: I0815 05:33:36.937194 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.937216 13485 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1889: I0815 05:33:36.937270 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.937276 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:36.937901 13485 graph_helper.cc:774] Graph to program need convert 1 sub graph
1889: I0815 05:33:36.938112 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.938182 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.938189 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:36.938601 13485 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1889: I0815 05:33:36.938818 13485 graph.h:183] deleting __fuse_statis__
1889: I0815 05:33:36.938827 13485 graph.h:183] deleting pass_recorder
1889: I0815 05:33:36.938832 13485 graph.h:183] deleting stale_program_op_descs
1889: I0815 05:33:36.938921 13485 analysis_predictor.cc:2332] ======= ir optimization completed =======
1889: I0815 05:33:36.938932 13485 scope.cc:202] Create variable abs_0.tmp_0
1889: I0815 05:33:36.938936 13485 naive_executor.cc:195] 0x6212f160 Create variable abs_0.tmp_0, which pointer is 0x63ff1d80
1889: I0815 05:33:36.938942 13485 scope.cc:202] Create variable gaussian_0.tmp_0
1889: I0815 05:33:36.938947 13485 naive_executor.cc:195] 0x6212f160 Create variable gaussian_0.tmp_0, which pointer is 0x645eb8e0
1889: I0815 05:33:36.938959 13485 scope.cc:202] Create variable linear_0.tmp_1
1889: I0815 05:33:36.938963 13485 naive_executor.cc:195] 0x6212f160 Create variable linear_0.tmp_1, which pointer is 0x641b71f0
1889: I0815 05:33:36.938968 13485 scope.cc:202] Create variable multinomial_0.tmp_0
1889: I0815 05:33:36.938970 13485 naive_executor.cc:195] 0x6212f160 Create variable multinomial_0.tmp_0, which pointer is 0x641b6c90
1889: I0815 05:33:36.938973 13485 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1889: I0815 05:33:36.938977 13485 naive_executor.cc:195] 0x6212f160 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x641b6f90
1889: I0815 05:33:36.938979 13485 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1889: I0815 05:33:36.938982 13485 naive_executor.cc:195] 0x6212f160 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x642fa330
1889: I0815 05:33:36.938989 13485 scope.cc:202] Create variable feed
1889: I0815 05:33:36.938993 13485 scope.cc:202] Create variable fetch
1889: I0815 05:33:36.939019 13485 naive_executor.cc:46] NaiveExecutor init with scope 0x6212f160
1889: I0815 05:33:36.939025 13485 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1889: I0815 05:33:36.939227 13485 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1889: I0815 05:33:36.939241 13485 operator.cc:1021] found Attribute with Variable type: num_samples
1889: I0815 05:33:36.939271 13485 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1889: I0815 05:33:36.939277 13485 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1889: I0815 05:33:36.939282 13485 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:36.939354 13485 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: I0815 05:33:36.939563 13485 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:36.939577 13485 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: I0815 05:33:36.939628 13485 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.939656 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1889: I0815 05:33:36.967149 13485 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1889: I0815 05:33:36.967284 13485 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.967331 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1889: I0815 05:33:36.967413 13485 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1889: I0815 05:33:36.967453 13485 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.967483 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1889: I0815 05:33:36.967554 13485 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1889: I0815 05:33:36.967605 13485 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.967626 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1889: I0815 05:33:36.967679 13485 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1889: I0815 05:33:36.967715 13485 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1889: I0815 05:33:36.967734 13485 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1889: I0815 05:33:36.967777 13485 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1889: I0815 05:33:36.967797 13485 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:36.967839 13485 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: I0815 05:33:36.967868 13485 reset_tensor_array.cc:45] Collect 0 arrays
1889: I0815 05:33:36.968394 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:36.968407 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: IR before lowering = {
1889:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1889:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1889:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1889:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1889:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1889:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1889:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1889:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1889:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1889:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1889:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1889:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1889:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1889:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1889:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1889:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1889: }
1889: 
1889: I0815 05:33:37.010928 13485 pir_interpreter.cc:161] PirInterpreter(): 0x61f195a0 on Place(gpu:0)
1889: I0815 05:33:37.011003 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_0
1889: I0815 05:33:37.011023 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_1
1889: I0815 05:33:37.011035 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_2
1889: I0815 05:33:37.011055 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_3
1889: I0815 05:33:37.011086 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_4
1889: I0815 05:33:37.011106 13485 scope.cc:202] Create variable 0x61f195a01723700017010989505_inner_var_5
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1889:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1889:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1889:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1889:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1889:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1889:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1889:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1889:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 0 )  = pd_op.full_int_array
1889: 1: ( 1 )  = pd_op.full
1889: 2: ( 2 )  = pd_op.full
1889: 3: ( 3 )  = pd_op.uniform ( 1 )  ( 2 )  ( 0 ) 
1889: 4: ( 4 )  = pd_op.full
1889: 5: ( 5 )  = pd_op.full
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> 0x61f195a01723700017010989505_inner_var_0 -> 0x6411a6c0
1889: 1 -> 0x61f195a01723700017010989505_inner_var_1 -> 0x64082330
1889: 2 -> 0x61f195a01723700017010989505_inner_var_2 -> 0x646fa4c0
1889: 3 -> linear_1.w_0 -> 0x63c390f0
1889: 4 -> linear_1.b_0 -> 0x63c29640
1889: 5 -> learning_rate_1 -> 0x641bf7e0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 3 
1889: 1 -> 3 
1889: 2 -> 3 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 
1889: 2 downstreams: 3[pd_op.uniform]->
1889: 3 downstreams: 
1889: 4 downstreams: 
1889: 5 downstreams: 
1889: I0815 05:33:37.011997 13559 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.012013 13560 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.012037 13561 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.012077 13562 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.012122 13563 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:37.012115 13562 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1889: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012123 13561 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012136 13560 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012158 13563 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012176 13562 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1889: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1889: I0815 05:33:37.012208 13560 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.012208 13561 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f195a01723700017010989505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.012216 13563 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.012281 13563 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1889: I0815 05:33:37.012312 13563 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012331 13563 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.012343 13563 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1889: I0815 05:33:37.012357 13563 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61f195a01723700017010989505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f195a01723700017010989505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f195a01723700017010989505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.012416 13563 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61f195a01723700017010989505_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f195a01723700017010989505_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f195a01723700017010989505_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1889: I0815 05:33:37.012475 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x61f19710) got event_name: TaskCompletion
1889: IR before lowering = {
1889:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1889:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1889:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1889:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1889:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1889:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1889:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1889:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1889:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1889:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1889:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1889:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1889:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1889:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1889:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1889:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1889:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1889:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1889:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1889:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1889:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1889:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1889:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1889:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1889: }
1889: 
1889: I0815 05:33:37.014747 13485 pir_interpreter.cc:161] PirInterpreter(): 0x61f577d0 on Place(gpu:0)
1889: I0815 05:33:37.014787 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_1
1889: I0815 05:33:37.014806 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_4
1889: I0815 05:33:37.014818 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_5
1889: I0815 05:33:37.014834 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_6
1889: I0815 05:33:37.014864 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_7
1889: I0815 05:33:37.014879 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_8
1889: I0815 05:33:37.014891 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_9
1889: I0815 05:33:37.014925 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_10
1889: I0815 05:33:37.014937 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_11
1889: I0815 05:33:37.014947 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_12
1889: I0815 05:33:37.014961 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_13
1889: I0815 05:33:37.014974 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_14
1889: I0815 05:33:37.014986 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_15
1889: I0815 05:33:37.014995 13485 scope.cc:202] Create variable fetch0@fetch
1889: I0815 05:33:37.015009 13485 scope.cc:202] Create variable 0x61f577d01723700017014772010_inner_var_17
1889: I0815 05:33:37.015022 13485 scope.cc:202] Create variable fetch1@fetch
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1889:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1889:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1889:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1889:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1889:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1889:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1889:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1889:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1889:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1889:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1889:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1889:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1889:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1889:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1889: 1: ( 4 )  = pd_op.full_int_array
1889: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1889: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1889: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1889: 5: ( 8 )  = pd_op.abs ( 7 ) 
1889: 6: ( 9 )  = pd_op.full
1889: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1889: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1889: 9: ( 11 )  = pd_op.cast ( 10 ) 
1889: 10: ( 12 )  = pd_op.mean ( 11 ) 
1889: 11: ( 13 )  = pd_op.full
1889: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1889: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1889: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1889: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1889: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> learning_rate_1 -> 0x641bf7e0
1889: 1 -> 0x61f577d01723700017014772010_inner_var_1 -> 0x63c393d0
1889: 2 -> linear_1.b_0 -> 0x63c29640
1889: 3 -> linear_1.w_0 -> 0x63c390f0
1889: 4 -> 0x61f577d01723700017014772010_inner_var_4 -> 0x63c2a2f0
1889: 5 -> 0x61f577d01723700017014772010_inner_var_5 -> 0x64abf2c0
1889: 6 -> 0x61f577d01723700017014772010_inner_var_6 -> 0x63c312d0
1889: 7 -> 0x61f577d01723700017014772010_inner_var_7 -> 0x63c165f0
1889: 8 -> 0x61f577d01723700017014772010_inner_var_8 -> 0x63d43e90
1889: 9 -> 0x61f577d01723700017014772010_inner_var_9 -> 0x63fe9970
1889: 10 -> 0x61f577d01723700017014772010_inner_var_10 -> 0x6460ce30
1889: 11 -> 0x61f577d01723700017014772010_inner_var_11 -> 0x64106890
1889: 12 -> 0x61f577d01723700017014772010_inner_var_12 -> 0x61f2ab20
1889: 13 -> 0x61f577d01723700017014772010_inner_var_13 -> 0x1ac98b80
1889: 14 -> 0x61f577d01723700017014772010_inner_var_14 -> 0x641ee760
1889: 15 -> 0x61f577d01723700017014772010_inner_var_15 -> 0x6460ce50
1889: 16 -> fetch0@fetch -> 0x645b68a0
1889: 17 -> 0x61f577d01723700017014772010_inner_var_17 -> 0x63cdc140
1889: 18 -> fetch1@fetch -> 0x64aa5d20
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 1 -> 2 
1889: 2 -> 3 
1889: 3 -> 4 
1889: 4 -> 5 13 
1889: 5 -> 8 
1889: 6 -> 7 
1889: 7 -> 8 
1889: 8 -> 9 15 
1889: 9 -> 10 
1889: 10 -> 12 
1889: 11 -> 12 
1889: 13 -> 14 
1889: 15 -> 16 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1889: 0 downstreams: 
1889: 1 downstreams: 2[pd_op.gaussian]->
1889: 2 downstreams: 3[pd_op.matmul]->
1889: 3 downstreams: 4[pd_op.add_]->
1889: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1889: 5 downstreams: 
1889: 6 downstreams: 7[pd_op.assign_value_]->
1889: 7 downstreams: 8[pd_op.multinomial]->
1889: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1889: 9 downstreams: 10[pd_op.mean]->
1889: 10 downstreams: 
1889: 11 downstreams: 12[pd_op.full_like]->
1889: 12 downstreams: 
1889: 13 downstreams: 14[pd_op.fetch]->
1889: 14 downstreams: 
1889: 15 downstreams: 16[pd_op.fetch]->
1889: 16 downstreams: 
1889: I0815 05:33:37.016860 13564 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.016983 13565 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.016993 13566 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.017076 13567 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.017081 13566 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017089 13565 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017139 13565 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.017141 13566 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1889: I0815 05:33:37.017158 13568 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1889: I0815 05:33:37.017208 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017246 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1889: I0815 05:33:37.017273 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017330 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.017402 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1889: I0815 05:33:37.017421 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1889: I0815 05:33:37.017482 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1889: I0815 05:33:37.017498 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61f577d01723700017014772010_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017539 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x61f577d01723700017014772010_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1889: I0815 05:33:37.017567 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61f577d01723700017014772010_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017603 13568 matmul_kernel_impl.h:374] MatMul's case 8
1889: I0815 05:33:37.017642 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x61f577d01723700017014772010_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.017671 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61f577d01723700017014772010_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61f577d01723700017014772010_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017709 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.017725 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x61f577d01723700017014772010_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x61f577d01723700017014772010_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.017755 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x61f577d01723700017014772010_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017776 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.017777 13566 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f577d01723700017014772010_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017788 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x61f577d01723700017014772010_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.017802 13566 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:37.017804 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61f577d01723700017014772010_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017828 13568 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:37.017881 13566 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f577d01723700017014772010_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.017910 13566 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f577d01723700017014772010_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.017925 13568 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:37.017932 13566 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1889: I0815 05:33:37.017947 13566 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f577d01723700017014772010_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.017966 13568 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:37.018030 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.018052 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x61f577d01723700017014772010_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x61f577d01723700017014772010_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.018085 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.018092 13566 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.018108 13566 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1889: I0815 05:33:37.018112 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.018131 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.018151 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x61f577d01723700017014772010_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.018150 13566 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1889: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x61f577d01723700017014772010_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.018179 13566 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x61f577d01723700017014772010_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.018196 13566 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1889: I0815 05:33:37.018210 13566 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.fetch), inputs:{0x61f577d01723700017014772010_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.018237 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x61f577d01723700017014772010_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1889: I0815 05:33:37.018257 13568 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61f577d01723700017014772010_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f577d01723700017014772010_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.018285 13568 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1889: I0815 05:33:37.018297 13568 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1889: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x61f577d01723700017014772010_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61f577d01723700017014772010_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x61f577d01723700017014772010_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1889: I0815 05:33:37.018343 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x61f57940) got event_name: TaskCompletion
1889: I0815 05:33:37.018370 13485 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1889: I0815 05:33:37.018404 13485 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1889: I0815 05:33:37.024216 13485 analysis_predictor.cc:2434] create AnalysisPredictor
1889: I0815 05:33:37.024268 13485 analysis_predictor.cc:433] Predictor::init()
1889: I0815 05:33:37.024983 13485 scope.cc:202] Create variable linear_1.b_0
1889: I0815 05:33:37.025032 13485 scope.cc:202] Create variable linear_1.w_0
1889: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1889: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1889: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1889: I0815 05:33:37.025496 13485 print_statistics.cc:50] --- detected [1] subgraphs!
1889: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1889: IR before lowering = {
1889:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170255537410"} : (builtin.tensor<2xi64>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170255537410"} : (cpu_tensor<2xi64>) -> 
1889: }
1889: 
1889: I0815 05:33:37.025696 13485 pir_interpreter.cc:161] PirInterpreter(): 0x63cd8bd0 on Place(cpu)
1889: I0815 05:33:37.025719 13485 scope.cc:202] Create variable 0x63cd8bd01723700017025711950_inner_var_0
1889: I0815 05:33:37.025748 13485 pir_interpreter.cc:1539] New Executor is Running ...
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170255537410"} : (cpu_tensor<2xi64>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 0 )  = pd_op.full_int_array
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> constant_folding@_17237000170255537410 -> 0x647089c0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.full_int_array]->
1889: 0 downstreams: 
1889: I0815 05:33:37.025892 13485 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1889: I0815 05:33:37.026008 13569 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:37.026181 13570 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.026206 13571 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.026261 13572 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.026271 13571 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1889: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17237000170255537410:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.026329 13571 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1889: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17237000170255537410:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1889: I0815 05:33:37.026347 13573 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.026355 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x63cd8d40) got event_name: TaskCompletion
1889: I0815 05:33:37.026605 13571 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 4222271522128453585 to 8395220095989831334 , after update, data is {current : 224, peak : 280}.
1889: I0815 05:33:37.026614 13571 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 4222271522128453585 to 8395220095989831334 , after update, data is {current : 224, peak : 280}.
1889: I0815 05:33:37.026674 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.026682 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: IR before lowering = {
1889:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170267556511"} : (builtin.tensor<1xi64>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170267556511"} : (cpu_tensor<1xi64>) -> 
1889: }
1889: 
1889: I0815 05:33:37.026916 13485 pir_interpreter.cc:161] PirInterpreter(): 0x63cd8bd0 on Place(cpu)
1889: I0815 05:33:37.026938 13485 scope.cc:202] Create variable 0x63cd8bd01723700017026932003_inner_var_0
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170267556511"} : (cpu_tensor<1xi64>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 0 )  = pd_op.full
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> constant_folding@_17237000170267556511 -> 0x63d4d7d0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.full]->
1889: 0 downstreams: 
1889: I0815 05:33:37.027179 13574 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:37.027245 13575 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.027267 13576 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.027297 13577 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.027330 13578 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.027326 13577 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17237000170267556511:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.027374 13577 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17237000170267556511:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.027397 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x63cd8d40) got event_name: TaskCompletion
1889: I0815 05:33:37.027570 13577 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 12223365892404116004 to 8395220095989831334 , after update, data is {current : 232, peak : 280}.
1889: I0815 05:33:37.027577 13577 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 12223365892404116004 to 8395220095989831334 , after update, data is {current : 232, peak : 280}.
1889: I0815 05:33:37.027673 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.027683 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: IR before lowering = {
1889:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17237000170267556511",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1889:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1889:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17237000170277629962"} : (builtin.tensor<1xi64>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17237000170267556511",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1889:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1889:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17237000170277629962"} : (cpu_tensor<1xi64>) -> 
1889: }
1889: 
1889: I0815 05:33:37.027935 13485 pir_interpreter.cc:161] PirInterpreter(): 0x63cd8bd0 on Place(cpu)
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17237000170267556511",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1889:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1889:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17237000170277629962"} : (cpu_tensor<1xi64>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> constant_folding@_17237000170277629962 -> 0x63d4d7d0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.assign_value_]->
1889: 0 downstreams: 
1889: I0815 05:33:37.028211 13579 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:37.028288 13580 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.028314 13581 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.028348 13582 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.028373 13583 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.028370 13582 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.028395 13582 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.028419 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x63cd8d40) got event_name: TaskCompletion
1889: I0815 05:33:37.028694 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.028702 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: IR before lowering = {
1889:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170287811333"} : (builtin.tensor<1xf32>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170287811333"} : (cpu_tensor<1xf32>) -> 
1889: }
1889: 
1889: I0815 05:33:37.028928 13485 pir_interpreter.cc:161] PirInterpreter(): 0x63cd8bd0 on Place(cpu)
1889: I0815 05:33:37.028950 13485 scope.cc:202] Create variable 0x63cd8bd01723700017028943685_inner_var_0
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1889:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17237000170287811333"} : (cpu_tensor<1xf32>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 0 )  = pd_op.full
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> constant_folding@_17237000170287811333 -> 0x646f6830
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.full]->
1889: 0 downstreams: 
1889: I0815 05:33:37.029155 13584 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:37.029222 13585 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1889: I0815 05:33:37.029242 13586 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1889: I0815 05:33:37.029273 13587 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1889: I0815 05:33:37.029299 13588 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1889: I0815 05:33:37.029296 13587 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17237000170287811333:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.029336 13587 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1889: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17237000170287811333:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1889: I0815 05:33:37.029361 13485 pir_interpreter.cc:1766] main_thread_blocker_(0x63cd8d40) got event_name: TaskCompletion
1889: I0815 05:33:37.029536 13587 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12223365892404116004 to 8395220095989831334 , after update, data is {current : 236, peak : 280}.
1889: I0815 05:33:37.029544 13587 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12223365892404116004 to 8395220095989831334 , after update, data is {current : 236, peak : 280}.
1889: I0815 05:33:37.029639 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.029647 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:37.029716 13485 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1889: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1889: I0815 05:33:37.029784 13485 print_statistics.cc:50] --- detected [2] subgraphs!
1889: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1889: I0815 05:33:37.029829 13485 print_statistics.cc:50] --- detected [2] subgraphs!
1889: IR before lowering = {
1889:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170287811333"} : () -> builtin.tensor<1xf32>
1889:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170277629962"} : () -> builtin.tensor<1xi64>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1889:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1889:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1889:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1889:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1889:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1889:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1889:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1889:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1889: }
1889: 
1889: IR after lowering = {
1889:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170287811333"} : () -> cpu_tensor<1xf32>
1889:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170277629962"} : () -> cpu_tensor<1xi64>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1889:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1889:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1889:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1889:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1889:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1889:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1889:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1889: }
1889: 
1889: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1889: [1m[35m--- Running PIR pass [inplace_pass][0m
1889: I0815 05:33:37.030607 13485 print_statistics.cc:50] --- detected [2] subgraphs!
1889: I0815 05:33:37.030627 13485 analysis_predictor.cc:1019] ======= pir optimization completed =======
1889: I0815 05:33:37.030665 13485 pir_interpreter.cc:161] PirInterpreter(): 0x63cd8bd0 on Place(cpu)
1889: I0815 05:33:37.030699 13485 scope.cc:202] Create variable feed_name_0
1889: I0815 05:33:37.030717 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_5
1889: I0815 05:33:37.030745 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_6
1889: I0815 05:33:37.030759 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_7
1889: I0815 05:33:37.030771 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_8
1889: I0815 05:33:37.030797 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_9
1889: I0815 05:33:37.030812 13485 scope.cc:202] Create variable 0x63cd8bd01723700017030681356_inner_var_10
1889: I0815 05:33:37.030840 13485 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:37.030865 13485 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: I0815 05:33:37.031018 13485 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:37.031036 13485 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: ======================== The network executed by pir interpreter ========================
1889: {
1889:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170287811333"} : () -> cpu_tensor<1xf32>
1889:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17237000170277629962"} : () -> cpu_tensor<1xi64>
1889:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1889:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1889:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1889:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1889:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1889:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1889:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1889:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1889:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1889: }
1889: 
1889: ======================== The instruction executed by pir interpreter ========================
1889: {outputs} =  instruction_name[idx] ({inputs})
1889: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1889: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1889: 2: ( 7 )  = pd_op.abs ( 6 ) 
1889: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1889: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1889: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1889: ---------------------------var_id -> var_name -> variable*---------------------------
1889: 0 -> constant_folding@_17237000170287811333 -> 0x646f6830
1889: 1 -> constant_folding@_17237000170277629962 -> 0x63d4d7d0
1889: 2 -> linear_1.b_0 -> 0x64bc3e30
1889: 3 -> linear_1.w_0 -> 0x641cc8e0
1889: 4 -> feed_name_0 -> 0x63fea160
1889: 5 -> 0x63cd8bd01723700017030681356_inner_var_5 -> 0x646f67b0
1889: 6 -> 0x63cd8bd01723700017030681356_inner_var_6 -> 0x621315e0
1889: 7 -> 0x63cd8bd01723700017030681356_inner_var_7 -> 0x64bc5e30
1889: 8 -> 0x63cd8bd01723700017030681356_inner_var_8 -> 0x61f19e70
1889: 9 -> fetch_name_0 -> 0x63c3b7e0
1889: 10 -> fetch_name_1 -> 0x64bc5df0
1889: 
1889: 
1889: ======================= The dependency of all instruction ========================
1889: id -> down_stream_id
1889: 0 -> 1 
1889: 1 -> 2 
1889: 2 -> 3 4 
1889: 3 -> 5 
1889: 
1889: 
1889: ======================== pir interpreter trace order ========================
1889: 
1889: Leaf nodes: 0[pd_op.matmul]->
1889: 0 downstreams: 1[pd_op.add_]->
1889: 1 downstreams: 2[pd_op.abs]->
1889: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1889: 3 downstreams: 5[pd_op.scale]->
1889: 4 downstreams: 
1889: 5 downstreams: 
1889: I0815 05:33:37.031666 13485 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1889: I0815 05:33:37.031728 13589 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1889: I0815 05:33:37.031725 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.031795 13485 matmul_kernel_impl.h:374] MatMul's case 8
1889: I0815 05:33:37.031821 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.031857 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x63cd8bd01723700017030681356_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x63cd8bd01723700017030681356_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.031906 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x63cd8bd01723700017030681356_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.031944 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.abs), inputs:{0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.031970 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.abs), inputs:{0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.031989 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.032027 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17237000170277629962:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.032056 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17237000170287811333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.032094 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17237000170287811333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x63cd8bd01723700017030681356_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1889: I0815 05:33:37.032126 13485 pir_interpreter.cc:1876] 
1889: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1889: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17237000170287811333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1889: I0815 05:33:37.032159 13485 pir_interpreter.cc:1916] 
1889: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1889: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17237000170287811333:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x63cd8bd01723700017030681356_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1889: I0815 05:33:37.032191 13485 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1889: I0815 05:33:37.032215 13485 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1889: I0815 05:33:37.032243 13485 reset_tensor_array.cc:45] Collect 0 arrays
1889: I0815 05:33:37.032379 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.032388 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:37.032440 13589 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2794720189754769404 to 8395220095989831334 , after update, data is {current : 44, peak : 280}.
1889: I0815 05:33:37.032449 13589 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2794720189754769404 to 8395220095989831334 , after update, data is {current : 44, peak : 280}.
1889: I0815 05:33:37.032487 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.032496 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: test_multinomial_op failed
1889:  ......sss......FFF..
1889: ======================================================================
1889: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1889: ----------------------------------------------------------------------
1889: Traceback (most recent call last):
1889:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1889:     self.check_output(check_pir=True)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2939, in check_output
1889:     res = self.check_output_with_place(
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2760, in check_output_with_place
1889:     static_checker.check()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2320, in check
1889:     self.compare_outputs_with_expects()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2310, in compare_outputs_with_expects
1889:     self.compare_single_output_with_expect(out_name, expect)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2288, in compare_single_output_with_expect
1889:     self._compare_numpy(name, actual_np, expect_np)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2251, in _compare_numpy
1889:     np.testing.assert_allclose(
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1889:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1889:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1889:     return func(*args, **kwds)
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1889:     raise AssertionError(msg)
1889: AssertionError: 
1889: Not equal to tolerance rtol=1e-05, atol=0
1889: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1889: Mismatched elements: 60646 / 100000 (60.6%)
1889: Max absolute difference: 3
1889: Max relative difference: inf
1889:  x: array([3, 0, 0, ..., 3, 3, 3], dtype=int64)
1889:  y: array([0, 0, 0, ..., 0, 0, 0])
1889: 
1889: ======================================================================
1889: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1889: ----------------------------------------------------------------------
1889: Traceback (most recent call last):
1889:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1889:     self.check_output(check_pir=True)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2939, in check_output
1889:     res = self.check_output_with_place(
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2760, in check_output_with_place
1889:     static_checker.check()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2320, in check
1889:     self.compare_outputs_with_expects()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2310, in compare_outputs_with_expects
1889:     self.compare_single_output_with_expect(out_name, expect)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2288, in compare_single_output_with_expect
1889:     self._compare_numpy(name, actual_np, expect_np)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2251, in _compare_numpy
1889:     np.testing.assert_allclose(
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1889:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1889:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1889:     return func(*args, **kwds)
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1889:     raise AssertionError(msg)
1889: AssertionError: 
1889: Not equal to tolerance rtol=1e-05, atol=0
1889: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1889: Mismatched elements: 209701 / 300000 (69.9%)
1889: Max absolute difference: 3
1889: Max relative difference: inf
1889:  x: array([[0, 0, 3, ..., 0, 3, 0],
1889:        [3, 2, 2, ..., 0, 2, 1],
1889:        [3, 2, 0, ..., 1, 1, 0]], dtype=int64)
1889:  y: array([[0, 0, 0, ..., 0, 0, 0],
1889:        [0, 0, 0, ..., 0, 0, 0],
1889:        [0, 0, 0, ..., 0, 0, 0]])
1889: 
1889: ======================================================================
1889: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1889: ----------------------------------------------------------------------
1889: Traceback (most recent call last):
1889:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1889:     self.check_output(check_pir=True)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2939, in check_output
1889:     res = self.check_output_with_place(
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2760, in check_output_with_place
1889:     static_checker.check()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2320, in check
1889:     self.compare_outputs_with_expects()
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2310, in compare_outputs_with_expects
1889:     self.compare_single_output_with_expect(out_name, expect)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2288, in compare_single_output_with_expect
1889:     self._compare_numpy(name, actual_np, expect_np)
1889:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2251, in _compare_numpy
1889:     np.testing.assert_allclose(
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1889:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1889:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1889:     return func(*args, **kwds)
1889:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1889:     raise AssertionError(msg)
1889: AssertionError: 
1889: Not equal to tolerance rtol=1e-05, atol=0
1889: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1889: Mismatched elements: 100 / 100 (100%)
1889: Max absolute difference: 988
1889: Max relative difference: inf
1889:  x: array([612, 823, 488, 387, 185, 935,  19, 518, 634, 478, 579, 144, 351,
1889:        947, 149, 548,  37, 690,  84,  93, 884, 367, 469, 771, 317, 813,
1889:        791,  58, 535, 449, 514, 836, 966, 256, 988,  21, 381,  73, 329,...
1889:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1889:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1889:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1889: 
1889: ----------------------------------------------------------------------
1889: Ran 20 tests in 4.595s
1889: 
1889: FAILED (failures=3, skipped=3)
1889: 
1889: I0815 05:33:37.034590 13485 mmap_allocator.cc:348] PID: 13485, MemoryMapFdSet: set size - 0
1889: I0815 05:33:37.047238 13485 mmap_allocator.cc:348] PID: 13485, MemoryMapFdSet: set size - 0
1889: I0815 05:33:37.133854 13562 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 1758375079691976970 to 8395220095989831334 , after update, data is {current : 60, peak : 280}.
1889: I0815 05:33:37.133883 13562 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 1758375079691976970 to 8395220095989831334 , after update, data is {current : 60, peak : 280}.
1889: I0815 05:33:37.133921 13561 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16806961198187347962 to 8395220095989831334 , after update, data is {current : 64, peak : 280}.
1889: I0815 05:33:37.133941 13561 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 16806961198187347962 to 8395220095989831334 , after update, data is {current : 64, peak : 280}.
1889: I0815 05:33:37.133949 13560 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7245226169759526010 to 8395220095989831334 , after update, data is {current : 68, peak : 280}.
1889: I0815 05:33:37.133966 13560 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7245226169759526010 to 8395220095989831334 , after update, data is {current : 68, peak : 280}.
1889: I0815 05:33:37.134457 13563 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 18299877630603382627 to 8395220095989831334 , after update, data is {current : 44, peak : 280}.
1889: I0815 05:33:37.134470 13563 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 18299877630603382627 to 8395220095989831334 , after update, data is {current : 44, peak : 280}.
1889: I0815 05:33:37.134476 13563 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 18299877630603382627 to 8395220095989831334 , after update, data is {current : 256, peak : 768}.
1889: I0815 05:33:37.134828 13566 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 8395220095989831334 to 10542136155864245475 , after update, data is {current : 768, peak : 1536}.
1889: I0815 05:33:37.134837 13566 thread_data_registry.h:135] Add data {current : 44, peak : 280} from thread 8395220095989831334 to 8361723555516351518 , after update, data is {current : 48, peak : 280}.
1889: I0815 05:33:37.134841 13566 thread_data_registry.h:135] Add data {current : 44, peak : 280} from thread 8395220095989831334 to 8361723555516351518 , after update, data is {current : 48, peak : 280}.
1889: I0815 05:33:37.134903 13565 thread_data_registry.h:135] Add data {current : 48, peak : 280} from thread 8361723555516351518 to 10542136155864245475 , after update, data is {current : 28, peak : 280}.
1889: I0815 05:33:37.134912 13565 thread_data_registry.h:135] Add data {current : 48, peak : 280} from thread 8361723555516351518 to 10542136155864245475 , after update, data is {current : 28, peak : 280}.
1889: I0815 05:33:37.135072 13568 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 10542136155864245475 to 5505790893920834842 , after update, data is {current : 6401792, peak : 8800800}.
1889: I0815 05:33:37.135084 13568 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 10542136155864245475 to 5505790893920834842 , after update, data is {current : 6401792, peak : 6408800}.
1889: I0815 05:33:37.135089 13568 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 10542136155864245475 to 5505790893920834842 , after update, data is {current : 1536, peak : 2401024}.
1889: I0815 05:33:37.304057 13485 onednn_context.cc:104] Clearing DNNL cache.
1889: I0815 05:33:37.304091 13485 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1889: I0815 05:33:37.304138 13485 mmap_allocator.cc:348] PID: 13485, MemoryMapFdSet: set size - 0
1/1 Test #1889: test_multinomial_op ..............***Failed   11.94 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  12.12 sec

The following tests FAILED:
	1889 - test_multinomial_op (Failed)
