UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1906
    Start 1906: test_nanmedian

1906: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_nanmedian"
1906: Environment variables: 
1906:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1906: Test timeout computed to be: 10000000
1906: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1906: WARNING: Logging before InitGoogleLogging() is written to STDERR
1906: I0815 06:17:53.486327 13604 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1906: I0815 06:17:54.275104 13604 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=use_mkldnn,tracer_onednn_ops_on,enable_tracker_all2all,dump_chunk_info,gpugraph_merge_grads_segment_size,fast_eager_deletion_mode,get_host_by_name_time,cusolver_dir,enable_graph_multi_node_sampling,use_cuda_malloc_async_allocator,embedding_deterministic,enable_exit_when_partial_worker,gpugraph_load_node_list_into_hbm,pir_apply_shape_optimization_pass,accuracy_check_atol_bf16,call_stack_level,sort_sum_gradient,static_executor_perfstat_filepath,tracer_profile_fname,op_dir,enable_unused_var_check,tensor_operants_mode,use_pinned_memory,free_idle_chunk,nccl_blocking_wait,multiple_of_cupti_buffer_size,new_executor_use_cuda_graph,cinn_subgraph_graphviz_dir,prim_forward_blacklist,gpugraph_dedup_pull_push_mode,ir_inplace_kernel_blacklist,gpugraph_slot_feasign_max_num,gpugraph_debug_gpu_memory,new_executor_sequential_run,dynamic_static_unified_comm,einsum_opt,mklml_dir,dataloader_use_file_descriptor,enable_gpu_memory_usage_log_mb,dygraph_debug,alloc_fill_value,disable_dyshape_in_train,memory_fraction_of_eager_deletion,enable_blaslt_global_search,trt_min_group_size,low_precision_op_list,async_trace_count,benchmark,cublaslt_exhaustive_search_times,enable_all2all_use_fp16,custom_device_mem_record,gemm_use_half_precision_compute_type,print_allocator_trace_info,all_blocks_convert_trt,prim_skip_dynamic,pir_apply_inplace_pass,enable_neighbor_list_use_uva,cinn_compile_thread_num,jit_engine_type,reader_queue_speed_test_mode,prim_enable_dynamic,enable_api_kernel_fallback,new_executor_static_build,use_stream_safe_cuda_allocator,initial_gpu_memory_in_mb,gpugraph_parallel_copyer_split_maxsize,multi_node_sample_use_gpu_table,enable_cublas_tensor_op_math,free_when_no_cache_hit,enable_async_trace,mkl_dir,accuracy_check_rtol_bf16,use_system_allocator,gpu_memory_limit_mb,check_nan_inf_level,run_kp_kernel,gpugraph_storage_mode,nccl_dir,pinned_memory_as_cpu_backend,enable_cinn_auto_tune,convert_all_blocks,gpugraph_parallel_stream_num,auto_growth_chunk_size_in_mb,check_infer_symbolic,pir_subgraph_saving_dir,sync_nccl_allreduce,executor_log_deps_every_microseconds,enable_record_memory,fraction_of_cpu_memory_to_use,static_runtime_data_save_path,use_xqa_optim,prim_backward,enable_interpretercore_launch_cinn,accuracy_check_rtol_fp16,enable_cse_in_dy2st,use_cinn,max_inplace_grad_add,use_auto_growth_pinned_allocator,gpugraph_offload_param_extends,logging_pir_py_code_dump_symbolic_dims,cudnn_dir,prim_forward,graph_neighbor_size_percent,enable_cinn_compile_cache,inner_op_parallelism,cse_max_count,enable_fuse_parallel_matmul_pass,use_shm_cache,manually_trans_conv_filter,logging_pir_py_code_dir,cudnn_deterministic,cupti_dir,host_trace_level,enable_pir_in_executor,use_cuda_managed_memory,accuracy_check_atol_fp16,enable_auto_rdma_trans,nvidia_package_dir,gpugraph_enable_print_op_debug,prim_all,fraction_of_cuda_pinned_memory_to_use,cublaslt_device_best_config,gpugraph_enable_segment_merge_grads,graph_load_in_parallel,initial_cpu_memory_in_mb,enable_auto_detect_gpu_topo,conv_workspace_size_limit,win_cuda_bin_dir,paddle_num_threads,use_virtual_memory_auto_growth,use_stride_kernel,dist_threadpool_size,lapack_dir,enable_adjust_op_order,search_cache_max_number,use_fast_math,selected_gpus,gpugraph_hbm_table_load_factor,new_executor_serial_run,enable_pir_with_pt_in_dy2st,print_ir,print_sub_graph_dir,gpugraph_offload_param_stat,eager_delete_tensor_gb,tracer_onednn_ops_off,enable_cinn_accuracy_check,new_executor_use_local_scope,enable_sparse_inner_gather,gpugraph_sparse_table_storage_mode,enable_gpu_memory_usage_log,deny_cinn_ops,accuracy_check_atol_fp32,cudnn_batchnorm_spatial_persistent,fuse_parameter_memory_size,add_dependency_for_communication_op,prim_enabled,enable_pir_in_executor_trace_run,local_exe_sub_scope_limit,prim_check_ops,cublas_dir,log_memory_stats,fleet_executor_with_standalone,use_auto_growth_v2,conv2d_disable_cudnn,enable_fusion_fallback,pir_broadcast_tree_limit,benchmark_nccl,logging_trunc_pir_py_code,cuda_dir,check_kernel_launch,save_static_runtime_data,sync_after_alloc,enable_dump_main_program,init_allocated_mem,allocator_strategy,allow_cinn_ops,cudnn_exhaustive_search,npu_storage_format,enable_opt_get_features,enable_dependency_builder_debug_info,gpugraph_offload_gather_copy_maxsize,fraction_of_gpu_memory_to_use,allreduce_record_one_event,gpugraph_force_device_batch_num_equal,eager_delete_scope,cuda_malloc_async_pool_memory_throttle_ratio,graph_embedding_split_infer_mode,accuracy_check_rtol_fp32,query_dest_rank_by_multi_node,cusparse_dir,fuse_parameter_groups_size,trt_ibuilder_cache,enable_pir_api,reallocate_gpu_memory_in_mb,pir_debug,logging_pir_py_code_int_tensor_element_limit,graph_metapath_split_opt,set_to_1d,gpu_allocator_retry_time,gpugraph_enable_hbm_table_collision_stat,gpugraph_enable_gpu_direct_access,new_executor_use_inplace,cusparselt_dir,cache_inference_while_scope,apply_pass_to_program,enable_collect_shape,check_nan_inf,tensorrt_dir,graph_get_neighbor_id,use_autotune,auto_free_cudagraph_allocations_on_launch,cudnn_exhaustive_search_times,cuda_memory_async_pool_realease_threshold,curand_dir 
1906: I0815 06:17:54.275223 13604 init.cc:108] After Parse: argc is 2
1906: I0815 06:17:59.024392 13604 scope.cc:202] Create variable X
1906: I0815 06:17:59.024469 13604 scope.cc:202] Create variable Out
1906: I0815 06:17:59.024487 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:17:59.024634 13604 op_registry.cc:112] CreateOp directly from OpDesc is deprecated. It should only beused in unit tests. Use CreateOp(const OpDesc& op_desc) instead.
1906: I0815 06:17:59.025259 13604 allocator_facade.cc:212] selected allocator strategy:1
1906: I0815 06:17:59.025547 13604 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1906: I0815 06:18:01.477645 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.477715 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.477846 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.477857 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.478902 13604 op_desc.cc:1111] CompileTime infer shape on mean
1906: I0815 06:18:01.478930 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.478972 13604 op_desc.cc:1111] CompileTime infer shape on mean
1906: I0815 06:18:01.478979 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.479470 13604 pybind.cc:1827] need skip: 0
1906: I0815 06:18:01.479568 13604 pybind.cc:1827] need skip: 0
1906: I0815 06:18:01.479995 13604 op_desc.cc:1111] CompileTime infer shape on fill_constant
1906: I0815 06:18:01.480413 13604 op_desc.cc:1111] CompileTime infer shape on mean_grad
1906: I0815 06:18:01.480433 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1906: I0815 06:18:01.480520 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1906: I0815 06:18:01.480537 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1906: I0815 06:18:01.484170 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.484854 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.484874 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.484886 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.487922 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.487941 13604 scope.cc:202] Create variable feed
1906: I0815 06:18:01.488013 13604 program_interpreter.cc:243] New Executor is Running.
1906: I0815 06:18:01.488020 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.488027 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:18:01.488035 13604 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4449ae90 type is 7
1906: I0815 06:18:01.488050 13604 scope.cc:202] Create variable Out
1906: I0815 06:18:01.488056 13604 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4449b380 type is 7
1906: I0815 06:18:01.488060 13604 scope.cc:202] Create variable Out@GRAD
1906: I0815 06:18:01.488063 13604 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x4449b830 type is 7
1906: I0815 06:18:01.488067 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.488070 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4449c370 type is 7
1906: I0815 06:18:01.488075 13604 scope.cc:202] Create variable X@GRAD
1906: I0815 06:18:01.488077 13604 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x4449c5e0 type is 7
1906: I0815 06:18:01.488082 13604 scope.cc:202] Create variable _generated_var_0
1906: I0815 06:18:01.488085 13604 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x4449c820 type is 7
1906: I0815 06:18:01.488090 13604 scope.cc:202] Create variable _generated_var_0@GRAD
1906: I0815 06:18:01.488092 13604 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x4449ca80 type is 7
1906: I0815 06:18:01.488097 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4449ae10 type is 9
1906: I0815 06:18:01.488101 13604 scope.cc:202] Create variable fetch
1906: I0815 06:18:01.488106 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4449c800 type is 10
1906: I0815 06:18:01.488231 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.488240 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.488243 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.488247 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: W0815 06:18:01.488870 13604 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1906: I0815 06:18:01.489231 13604 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1906: W0815 06:18:01.490278 13604 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1906: I0815 06:18:01.490501 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.490527 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.490682 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.490692 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.490710 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.495102 13604 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495121 13604 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495138 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.495218 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.495326 13604 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495338 13604 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495388 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.495409 13604 interpreter_util.cc:647] Standalone Executor is Used.
1906: I0815 06:18:01.495441 13604 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495450 13604 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495466 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1906: I0815 06:18:01.495577 13604 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495589 13604 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495605 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1906: I0815 06:18:01.495726 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.495750 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.495771 13604 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.495779 13604 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4451ae90Variable Type 7
1906: I0815 06:18:01.495805 13604 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.495828 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.495875 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.495896 13604 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.496016 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.496049 13604 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.496623 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.496668 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.497706 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.497727 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.497768 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.497777 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.498402 13604 op_desc.cc:1111] CompileTime infer shape on mean
1906: I0815 06:18:01.498420 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.498452 13604 op_desc.cc:1111] CompileTime infer shape on mean
1906: I0815 06:18:01.498459 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.498728 13604 pybind.cc:1827] need skip: 0
1906: I0815 06:18:01.498782 13604 pybind.cc:1827] need skip: 0
1906: I0815 06:18:01.499109 13604 op_desc.cc:1111] CompileTime infer shape on fill_constant
1906: I0815 06:18:01.499188 13604 op_desc.cc:1111] CompileTime infer shape on mean_grad
1906: I0815 06:18:01.499197 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1906: I0815 06:18:01.499262 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1906: I0815 06:18:01.499272 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1906: I0815 06:18:01.501287 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.501780 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.501796 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.501801 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.504576 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.504593 13604 scope.cc:202] Create variable feed
1906: I0815 06:18:01.504621 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.504629 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:18:01.504633 13604 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4457e7d0 type is 7
1906: I0815 06:18:01.504639 13604 scope.cc:202] Create variable Out
1906: I0815 06:18:01.504642 13604 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4457ed50 type is 7
1906: I0815 06:18:01.504647 13604 scope.cc:202] Create variable Out@GRAD
1906: I0815 06:18:01.504649 13604 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x4457f200 type is 7
1906: I0815 06:18:01.504653 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.504657 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4457fae0 type is 7
1906: I0815 06:18:01.504660 13604 scope.cc:202] Create variable X@GRAD
1906: I0815 06:18:01.504663 13604 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x4457fd50 type is 7
1906: I0815 06:18:01.504673 13604 scope.cc:202] Create variable _generated_var_0
1906: I0815 06:18:01.504678 13604 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x4457ff90 type is 7
1906: I0815 06:18:01.504683 13604 scope.cc:202] Create variable _generated_var_0@GRAD
1906: I0815 06:18:01.504685 13604 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x445801f0 type is 7
1906: I0815 06:18:01.504689 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4457f150 type is 9
1906: I0815 06:18:01.504695 13604 scope.cc:202] Create variable fetch
1906: I0815 06:18:01.504698 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4457ff70 type is 10
1906: I0815 06:18:01.504801 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.504807 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.504812 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.504814 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.504855 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.504869 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.504916 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.504925 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.504942 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.505424 13604 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505440 13604 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505455 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.505503 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.505574 13604 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505584 13604 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505628 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.505661 13604 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505669 13604 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505684 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1906: I0815 06:18:01.505777 13604 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505787 13604 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.505802 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1906: I0815 06:18:01.505905 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.505918 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.505934 13604 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.505939 13604 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x445815c0Variable Type 7
1906: I0815 06:18:01.505954 13604 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.505970 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.505988 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.506002 13604 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.506070 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.506095 13604 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.506526 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1906: I0815 06:18:01.506561 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.508122 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.508328 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: I0815 06:18:01.508383 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.509836 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.509910 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.510668 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x4449b850)  to GradNodeAccumulation (addr: 0x43620840)
1906: I0815 06:18:01.510816 13604 dygraph_functions.cc:51757] Running AD API: mean
1906: I0815 06:18:01.510843 13604 dygraph_functions.cc:51814] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.510927 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.510951 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from MeanGradNode (addr: 0x3de31a10)  to NanmedianGradNode (addr: 0x4449b850)
1906: I0815 06:18:01.511068 13604 backward.cc:442] Run in Backward
1906: I0815 06:18:01.511080 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.511103 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.511157 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.511191 13604 backward.cc:255] Preparing GradNode:MeanGradNode addr:0x3de31a10
1906: I0815 06:18:01.511207 13604 nodes.cc:25338] Running AD API GRAD: mean_grad
1906: I0815 06:18:01.511247 13604 nodes.cc:25394] { Input: [ 
1906: ( grad_out , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.511324 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.511348 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.511359 13604 backward.cc:335] Node: MeanGradNode addr:0x3de31a10, Found pending node: NanmedianGradNode addr: 0x4449b850
1906: I0815 06:18:01.511368 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.511399 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x4449b850
1906: I0815 06:18:01.511409 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.511433 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.511489 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.511514 13604 backward.cc:335] Node: NanmedianGradNode addr:0x4449b850, Found pending node: GradNodeAccumulation addr: 0x43620840
1906: I0815 06:18:01.511521 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.511538 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43620840
1906: I0815 06:18:01.511550 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.511559 13604 accumulation_node.cc:40] Move Tensor ptr: 0x43a8fc10
1906: I0815 06:18:01.511562 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.511567 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: /home/code/Paddle/build/python/paddle/base/framework.py:667: VisibleDeprecationWarning: [93m
1906: Warning:
1906: API "paddle.base.dygraph.tensor_patch_methods.gradient" is deprecated since 2.1.0, and will be removed in future versions.
1906:     Reason: Please use tensor.grad, which returns the tensor value of the gradient. [0m
1906:   return func(*args, **kwargs)
1906: I0815 06:18:01.520257 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.520427 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.520478 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> builtin.tensor<100x100xf16>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1906:     (%3) = "pd_op.mean" (%1) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1906:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1906:     (%5) = "pd_op.full_like" (%3, %4) {dtype:(pd_op.DataType)float16,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<1xf32>) -> builtin.tensor<f16>
1906:     (%6) = "pd_op.mean_grad" (%1, %5) {axis:(pd_op.IntArray)[],keepdim:false,reduce_all:false,stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<f16>) -> builtin.tensor<f16>
1906:     (%7) = "pd_op.nanmedian_grad" (%0, %2, %6) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false]} : (builtin.tensor<100x100xf16>, builtin.tensor<2xi64>, builtin.tensor<f16>) -> builtin.tensor<100x100xf16>
1906:     (%8) = "pd_op.fetch" (%7) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<100x100xf16>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1906:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1906:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1906:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1906:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1906:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1906:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1906: }
1906: 
1906: I0815 06:18:01.582692 13604 pir_interpreter.cc:161] PirInterpreter(): 0x468868d0 on Place(gpu:0)
1906: I0815 06:18:01.582741 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.582772 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_1
1906: I0815 06:18:01.582782 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_2
1906: I0815 06:18:01.582789 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_3
1906: I0815 06:18:01.582798 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_4
1906: I0815 06:18:01.582805 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_5
1906: I0815 06:18:01.582813 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_6
1906: I0815 06:18:01.582820 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_7
1906: I0815 06:18:01.582828 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_8
1906: I0815 06:18:01.582835 13604 scope.cc:202] Create variable 0x468868d01723702681582724524_inner_var_9
1906: I0815 06:18:01.582844 13604 scope.cc:202] Create variable fetch0@fetch
1906: I0815 06:18:01.583256 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: I0815 06:18:01.583271 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.583274 13604 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1906: I0815 06:18:01.583326 13604 pir_interpreter.cc:1455] New Executor is Running ...
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1906:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1906:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1906:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1906:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1906:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1906:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 4 )  = pd_op.mean ( 2 ) 
1906: 3: ( 5 )  = pd_op.full
1906: 4: ( 6 )  = pd_op.full_like ( 5 )  ( 4 ) 
1906: 5: ( 7 )  = pd_op.mean_grad ( 6 )  ( 2 ) 
1906: 6: ( 8 )  = pd_op.nanmedian_grad ( 7 )  ( 3 )  ( 1 ) 
1906: 7: ( 9 )  = pd_op.memcpy_d2h ( 8 ) 
1906: 8: ( 10 )  = pd_op.fetch ( 9 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x46886870
1906: 1 -> 0x468868d01723702681582724524_inner_var_1 -> 0x468868b0
1906: 2 -> 0x468868d01723702681582724524_inner_var_2 -> 0x468836e0
1906: 3 -> 0x468868d01723702681582724524_inner_var_3 -> 0x46885fc0
1906: 4 -> 0x468868d01723702681582724524_inner_var_4 -> 0x46884e60
1906: 5 -> 0x468868d01723702681582724524_inner_var_5 -> 0x46886f00
1906: 6 -> 0x468868d01723702681582724524_inner_var_6 -> 0x46887320
1906: 7 -> 0x468868d01723702681582724524_inner_var_7 -> 0x46887740
1906: 8 -> 0x468868d01723702681582724524_inner_var_8 -> 0x46885780
1906: 9 -> 0x468868d01723702681582724524_inner_var_9 -> 0x46887b60
1906: 10 -> fetch0@fetch -> 0x46888370
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 
1906: 1 -> 2 
1906: 2 -> 4 
1906: 3 -> 4 
1906: 4 -> 5 
1906: 5 -> 6 
1906: 6 -> 7 
1906: 7 -> 8 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->3[pd_op.full]->
1906: 0 downstreams: 1[pd_op.nanmedian]->
1906: 1 downstreams: 2[pd_op.mean]->
1906: 2 downstreams: 
1906: 3 downstreams: 4[pd_op.full_like]->
1906: 4 downstreams: 5[pd_op.mean_grad]->
1906: 5 downstreams: 6[pd_op.nanmedian_grad]->
1906: 6 downstreams: 7[pd_op.memcpy_d2h]->
1906: 7 downstreams: 8[pd_op.fetch]->
1906: 8 downstreams: 
1906: I0815 06:18:01.584264 13604 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1906: I0815 06:18:01.584461 13641 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1906: I0815 06:18:01.584585 13643 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.584585 13642 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.584681 13644 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.584692 13645 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.584721 13642 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1906: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x468868d01723702681582724524_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.584751 13646 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.584828 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.584887 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.584895 13642 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1906: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x468868d01723702681582724524_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1906: I0815 06:18:01.584945 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x468868d01723702681582724524_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_3:[dtype=;place=;dim=;lod={};, 0x468868d01723702681582724524_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.585458 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x468868d01723702681582724524_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x468868d01723702681582724524_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.585490 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x468868d01723702681582724524_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.585548 13646 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.585564 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x468868d01723702681582724524_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.585592 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x468868d01723702681582724524_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x468868d01723702681582724524_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.585645 13646 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.585711 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x468868d01723702681582724524_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x468868d01723702681582724524_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.585772 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x468868d01723702681582724524_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x468868d01723702681582724524_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.585845 13646 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.585862 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x468868d01723702681582724524_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x468868d01723702681582724524_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.585886 13646 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x468868d01723702681582724524_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x468868d01723702681582724524_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x468868d01723702681582724524_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.585970 13646 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x468868d01723702681582724524_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x468868d01723702681582724524_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x468868d01723702681582724524_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.586061 13642 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x468868d01723702681582724524_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.586100 13642 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.586211 13642 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_0
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x468868d01723702681582724524_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x468868d01723702681582724524_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.586248 13642 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x468868d01723702681582724524_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.586275 13642 tensor_utils.cc:57] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.586314 13642 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_0
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x468868d01723702681582724524_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.586352 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x46886a40) got event_name: TaskCompletion
1906: I0815 06:18:01.586381 13604 tensor_util.cc:48] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.589473 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.589504 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.589567 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.589578 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.591573 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.592144 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.592612 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.592629 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.592634 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.594905 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.594925 13604 scope.cc:202] Create variable feed
1906: I0815 06:18:01.594959 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.594969 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:18:01.594974 13604 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4693d530 type is 7
1906: I0815 06:18:01.594981 13604 scope.cc:202] Create variable Out
1906: I0815 06:18:01.594988 13604 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4693c8d0 type is 7
1906: I0815 06:18:01.594993 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.594997 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4693d2c0 type is 7
1906: I0815 06:18:01.595003 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4693d510 type is 9
1906: I0815 06:18:01.595008 13604 scope.cc:202] Create variable fetch
1906: I0815 06:18:01.595013 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4693da50 type is 10
1906: I0815 06:18:01.595089 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.595096 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.595100 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.595105 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.595161 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.595176 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.595249 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.595260 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.595280 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.595798 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.595815 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.595834 13604 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.595842 13604 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46941a50Variable Type 7
1906: I0815 06:18:01.595862 13604 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.595885 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.595911 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.595930 13604 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.595980 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.596007 13604 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.596060 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.596071 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.596087 13604 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.596094 13604 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46945b60Variable Type 7
1906: I0815 06:18:01.596108 13604 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.596122 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.596143 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.596158 13604 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.596194 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.596225 13604 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1906: I0815 06:18:01.596531 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.596562 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.597906 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.597934 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.597985 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.597997 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.599958 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.600560 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.600996 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.601011 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.601015 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.603225 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.603309 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.603322 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:18:01.603327 13604 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4524ceb0 type is 7
1906: I0815 06:18:01.603335 13604 scope.cc:202] Create variable Out
1906: I0815 06:18:01.603341 13604 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4524c1e0 type is 7
1906: I0815 06:18:01.603346 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.603350 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4524cbe0 type is 7
1906: I0815 06:18:01.603355 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4693d510 type is 9
1906: I0815 06:18:01.603361 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4693da50 type is 10
1906: I0815 06:18:01.603435 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.603442 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.603447 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.603451 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.603492 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.603505 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.603560 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.603570 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.603588 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.604117 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.604135 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.604153 13604 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.604161 13604 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45251350Variable Type 7
1906: I0815 06:18:01.604178 13604 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.604195 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.604218 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.604235 13604 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.604277 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.604307 13604 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.604357 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.604367 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.604382 13604 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.604390 13604 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45251370Variable Type 7
1906: I0815 06:18:01.604403 13604 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.604418 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.604437 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.604452 13604 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.604488 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.604511 13604 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1906: I0815 06:18:01.604779 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.604807 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.605393 13641 thread_data_registry.h:135] Add data {current : -20004, peak : 0} from thread 10859854511715344113 to 2409697520245028391 , after update, data is {current : -20004, peak : 16}.
1906: I0815 06:18:01.605415 13641 thread_data_registry.h:135] Add data {current : -20024, peak : 0} from thread 10859854511715344113 to 2409697520245028391 , after update, data is {current : 0, peak : 330659}.
1906: I0815 06:18:01.605583 13642 thread_data_registry.h:135] Add data {current : 40004, peak : 40004} from thread 4649890891860076216 to 2409697520245028391 , after update, data is {current : 20000, peak : 40004}.
1906: I0815 06:18:01.605777 13646 thread_data_registry.h:135] Add data {current : 20000, peak : 40004} from thread 2409697520245028391 to 12390818033688177718 , after update, data is {current : 20038, peak : 40004}.
1906: I0815 06:18:01.605788 13646 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 2409697520245028391 to 12390818033688177718 , after update, data is {current : 140000, peak : 520631}.
1906: I0815 06:18:01.723524 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: I0815 06:18:01.723935 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.724007 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.724617 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.724664 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.725910 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: I0815 06:18:01.726068 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.726125 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.726472 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.726506 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.728726 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: I0815 06:18:01.728881 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.728932 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1906:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: I0815 06:18:01.732482 13604 pir_interpreter.cc:161] PirInterpreter(): 0x46936680 on Place(gpu:0)
1906: I0815 06:18:01.732522 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.732544 13604 scope.cc:202] Create variable 0x469366801723702681732510723_inner_var_1
1906: I0815 06:18:01.732556 13604 scope.cc:202] Create variable 0x469366801723702681732510723_inner_var_2
1906: I0815 06:18:01.732568 13604 scope.cc:202] Create variable 0x469366801723702681732510723_inner_var_3
1906: I0815 06:18:01.732575 13604 scope.cc:202] Create variable 0x469366801723702681732510723_inner_var_4
1906: I0815 06:18:01.732586 13604 scope.cc:202] Create variable fetch0@fetch
1906: I0815 06:18:01.732970 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: I0815 06:18:01.732986 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.732990 13604 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1906: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x4694c6d0
1906: 1 -> 0x469366801723702681732510723_inner_var_1 -> 0x46886890
1906: 2 -> 0x469366801723702681732510723_inner_var_2 -> 0x46885190
1906: 3 -> 0x469366801723702681732510723_inner_var_3 -> 0x46888350
1906: 4 -> 0x469366801723702681732510723_inner_var_4 -> 0x1976dc60
1906: 5 -> fetch0@fetch -> 0x4698eda0
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 
1906: 1 -> 2 
1906: 2 -> 3 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->
1906: 0 downstreams: 1[pd_op.nanmedian]->
1906: 1 downstreams: 2[pd_op.memcpy_d2h]->
1906: 2 downstreams: 3[pd_op.fetch]->
1906: 3 downstreams: 
1906: I0815 06:18:01.733780 13648 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1906: I0815 06:18:01.733913 13649 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.733983 13650 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.734043 13651 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.734107 13652 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.734155 13653 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.734206 13653 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.734258 13653 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.734305 13653 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x469366801723702681732510723_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_3:[dtype=;place=;dim=;lod={};, 0x469366801723702681732510723_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.734840 13653 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x469366801723702681732510723_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x469366801723702681732510723_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.734928 13652 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x469366801723702681732510723_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.734973 13652 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.735051 13652 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x469366801723702681732510723_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x469366801723702681732510723_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.735090 13652 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x469366801723702681732510723_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.735116 13652 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.735134 13652 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x469366801723702681732510723_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.735172 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x469367f0) got event_name: TaskCompletion
1906: I0815 06:18:01.735198 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.735713 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.735865 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.735908 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1906:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: I0815 06:18:01.738009 13604 pir_interpreter.cc:161] PirInterpreter(): 0x46951bc0 on Place(gpu:0)
1906: I0815 06:18:01.738036 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.738051 13604 scope.cc:202] Create variable 0x46951bc01723702681738030579_inner_var_1
1906: I0815 06:18:01.738060 13604 scope.cc:202] Create variable 0x46951bc01723702681738030579_inner_var_2
1906: I0815 06:18:01.738068 13604 scope.cc:202] Create variable 0x46951bc01723702681738030579_inner_var_3
1906: I0815 06:18:01.738077 13604 scope.cc:202] Create variable 0x46951bc01723702681738030579_inner_var_4
1906: I0815 06:18:01.738085 13604 scope.cc:202] Create variable fetch0@fetch
1906: I0815 06:18:01.738358 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: I0815 06:18:01.738371 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.738375 13604 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1906: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x445a8730
1906: 1 -> 0x46951bc01723702681738030579_inner_var_1 -> 0x445a87b0
1906: 2 -> 0x46951bc01723702681738030579_inner_var_2 -> 0x4691f0a0
1906: 3 -> 0x46951bc01723702681738030579_inner_var_3 -> 0x4697b5e0
1906: 4 -> 0x46951bc01723702681738030579_inner_var_4 -> 0x43720770
1906: 5 -> fetch0@fetch -> 0x468855d0
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 
1906: 1 -> 2 
1906: 2 -> 3 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->
1906: 0 downstreams: 1[pd_op.nanmedian]->
1906: 1 downstreams: 2[pd_op.memcpy_d2h]->
1906: 2 downstreams: 3[pd_op.fetch]->
1906: 3 downstreams: 
1906: I0815 06:18:01.738900 13654 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1906: I0815 06:18:01.739106 13655 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.739161 13656 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.739199 13657 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.739287 13658 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.739374 13659 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.739423 13659 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.739462 13659 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.739500 13659 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46951bc01723702681738030579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_3:[dtype=;place=;dim=;lod={};, 0x46951bc01723702681738030579_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.739946 13659 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46951bc01723702681738030579_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x46951bc01723702681738030579_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.740022 13658 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46951bc01723702681738030579_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.740063 13658 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.740125 13658 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46951bc01723702681738030579_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46951bc01723702681738030579_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.740159 13658 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46951bc01723702681738030579_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.740177 13658 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.740188 13658 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46951bc01723702681738030579_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.740217 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x46951d30) got event_name: TaskCompletion
1906: I0815 06:18:01.740237 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.741500 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a941f0 for it.
1906: I0815 06:18:01.741677 13604 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.741727 13604 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1906:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: I0815 06:18:01.744189 13604 pir_interpreter.cc:161] PirInterpreter(): 0x45255e00 on Place(gpu:0)
1906: I0815 06:18:01.744218 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.744236 13604 scope.cc:202] Create variable 0x45255e001723702681744212786_inner_var_1
1906: I0815 06:18:01.744247 13604 scope.cc:202] Create variable 0x45255e001723702681744212786_inner_var_2
1906: I0815 06:18:01.744257 13604 scope.cc:202] Create variable 0x45255e001723702681744212786_inner_var_3
1906: I0815 06:18:01.744268 13604 scope.cc:202] Create variable 0x45255e001723702681744212786_inner_var_4
1906: I0815 06:18:01.744277 13604 scope.cc:202] Create variable fetch0@fetch
1906: I0815 06:18:01.744604 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: I0815 06:18:01.744621 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.744624 13604 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1906:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1906:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1906: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x452526c0
1906: 1 -> 0x45255e001723702681744212786_inner_var_1 -> 0x46924d00
1906: 2 -> 0x45255e001723702681744212786_inner_var_2 -> 0x46924f60
1906: 3 -> 0x45255e001723702681744212786_inner_var_3 -> 0x468895e0
1906: 4 -> 0x45255e001723702681744212786_inner_var_4 -> 0x45256d90
1906: 5 -> fetch0@fetch -> 0x46917d60
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 
1906: 1 -> 2 
1906: 2 -> 3 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->
1906: 0 downstreams: 1[pd_op.nanmedian]->
1906: 1 downstreams: 2[pd_op.memcpy_d2h]->
1906: 2 downstreams: 3[pd_op.fetch]->
1906: 3 downstreams: 
1906: I0815 06:18:01.745229 13660 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1906: I0815 06:18:01.745476 13661 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.745498 13662 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.745563 13663 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.745692 13664 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.745707 13665 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.745756 13665 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.745783 13665 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1906: I0815 06:18:01.745811 13665 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45255e001723702681744212786_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_3:[dtype=;place=;dim=;lod={};, 0x45255e001723702681744212786_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.746243 13665 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x45255e001723702681744212786_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x45255e001723702681744212786_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.746309 13664 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45255e001723702681744212786_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.746338 13664 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.746393 13664 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x45255e001723702681744212786_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x45255e001723702681744212786_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.746419 13664 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x45255e001723702681744212786_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.746438 13664 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.746452 13664 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x45255e001723702681744212786_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.746480 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x45255f70) got event_name: TaskCompletion
1906: I0815 06:18:01.746498 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: E0815 06:18:01.746596 13604 pir.cc:2522] The op: pd_op.nanmedian does not implement InferSymbolicShapeInterface.
1906: I0815 06:18:01.747725 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.747749 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.747800 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.747812 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.749523 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.750005 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.750435 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.750449 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.750454 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.752328 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.752400 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.752410 13604 scope.cc:202] Create variable MedianIndex
1906: I0815 06:18:01.752415 13604 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4454f9e0 type is 7
1906: I0815 06:18:01.752422 13604 scope.cc:202] Create variable Out
1906: I0815 06:18:01.752429 13604 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4454f200 type is 7
1906: I0815 06:18:01.752432 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.752435 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4454fb70 type is 7
1906: I0815 06:18:01.752439 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4693d510 type is 9
1906: I0815 06:18:01.752444 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4693da50 type is 10
1906: I0815 06:18:01.752516 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.752521 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.752525 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.752529 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.752578 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.752590 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.752653 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.752662 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.752678 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.753119 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.753134 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.753149 13604 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.753156 13604 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4531cf70Variable Type 7
1906: I0815 06:18:01.753175 13604 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.753190 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.753212 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.753227 13604 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.753268 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.753293 13604 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.753352 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.753362 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.753374 13604 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.753379 13604 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45320f20Variable Type 7
1906: I0815 06:18:01.753392 13604 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.753404 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.753422 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.753434 13604 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.753465 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.753492 13604 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1906: I0815 06:18:01.753788 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.753814 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.754240 13604 pybind.cc:1827] need skip: 0
1906: I0815 06:18:01.797114 13654 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 17361193375481789183 to 9123513193734749703 , after update, data is {current : -4, peak : 0}.
1906: I0815 06:18:01.797133 13654 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 17361193375481789183 to 9123513193734749703 , after update, data is {current : -36, peak : 0}.
1906: I0815 06:18:01.797318 13658 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 7379413609658164921 to 9123513193734749703 , after update, data is {current : 0, peak : 4}.
1906: I0815 06:18:01.797514 13659 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 5224684630679285382 to 9123513193734749703 , after update, data is {current : 0, peak : 16}.
1906: I0815 06:18:01.797537 13659 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 5224684630679285382 to 9123513193734749703 , after update, data is {current : -18, peak : 330659}.
1906: I0815 06:18:01.797734 13660 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 9123513193734749703 to 11875815371280096905 , after update, data is {current : 4, peak : 16}.
1906: I0815 06:18:01.797745 13660 thread_data_registry.h:135] Add data {current : -18, peak : 330659} from thread 9123513193734749703 to 15681967349506363665 , after update, data is {current : 0, peak : 330659}.
1906: I0815 06:18:01.797915 13664 thread_data_registry.h:135] Add data {current : 4, peak : 16} from thread 11875815371280096905 to 15681967349506363665 , after update, data is {current : 4, peak : 16}.
1906: I0815 06:18:01.798116 13665 thread_data_registry.h:135] Add data {current : 4, peak : 16} from thread 15681967349506363665 to 2409697520245028391 , after update, data is {current : 2, peak : 16}.
1906: I0815 06:18:01.798138 13665 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 15681967349506363665 to 2409697520245028391 , after update, data is {current : -18, peak : 330659}.
1906: I0815 06:18:01.802594 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.802717 13604 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 512(0x7f881de18e00), and remaining 0
1906: I0815 06:18:01.802829 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.802866 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.803114 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.803946 13604 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.804029 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.804056 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.804220 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.804905 13604 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.804978 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.805006 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.805156 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.805788 13604 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.805859 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.805886 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.806039 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:464: RuntimeWarning: All-NaN slice encountered
1906:   np_res = np.nanmedian(data)
1906: I0815 06:18:01.806744 13604 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.806802 13604 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f881de19000), and remaining 0
1906: I0815 06:18:01.806854 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.806877 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.807348 13604 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.807415 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.807440 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.807893 13604 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.807956 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.807981 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.808400 13604 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.808465 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.808490 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.809051 13604 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.809119 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.809144 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.809330 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.809962 13604 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.810034 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.810058 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.810214 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.810896 13604 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.810968 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.810994 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.811179 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.811801 13604 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.811874 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.811900 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.812057 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.812724 13604 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.812798 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.812824 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.813015 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.813669 13604 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.813742 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.813767 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.813923 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.814637 13604 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.814713 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.814738 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.814908 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.815586 13604 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.815662 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.815688 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.815842 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.816550 13604 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.816622 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.816648 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.816829 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.817461 13604 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.817535 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.817560 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.817718 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.818241 13604 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.818320 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.818346 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.818495 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.819177 13604 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.819244 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.819269 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.819425 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.820076 13604 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.820145 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.820171 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.820420 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:478: RuntimeWarning: All-NaN slice encountered
1906:   np_res = np.nanmedian(data, axis)
1906: I0815 06:18:01.822290 13604 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.822372 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.822398 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.822582 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.823930 13604 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.824004 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.824030 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.824234 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.825564 13604 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.825639 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.825665 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.825850 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.827072 13604 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.827147 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.827174 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.827430 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.828660 13604 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.828733 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.828759 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.828943 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.830235 13604 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.830318 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.830345 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.830525 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.831820 13604 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.831893 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.831919 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.832098 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.833325 13604 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.833400 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.833425 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.833657 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.834966 13604 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.835042 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.835067 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.835253 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.836493 13604 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.836568 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.836596 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.836798 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.838018 13604 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.838095 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.838121 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.838315 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.839596 13604 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.839673 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.839699 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.839879 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.841081 13604 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.841156 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.841183 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.841377 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.842653 13604 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.842728 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.842756 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.842933 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.844152 13604 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.844228 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.844254 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.844448 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.845655 13604 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.845731 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.845758 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.845937 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.846691 13604 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.846764 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.846791 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.846978 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.849694 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.849725 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.850569 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.850592 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.851343 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.851366 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.852128 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.852150 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.852888 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.852910 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.855331 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.855855 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.856374 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.856878 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.857393 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.858220 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.858238 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.858244 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.863057 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.863140 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.863153 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.863162 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x46bf1980 type is 7
1906: I0815 06:18:01.863171 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4693d510 type is 9
1906: I0815 06:18:01.863179 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4693da50 type is 10
1906: I0815 06:18:01.863184 13604 scope.cc:202] Create variable nanmedian_0.tmp_0
1906: I0815 06:18:01.863188 13604 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x46bf0ba0 type is 7
1906: I0815 06:18:01.863193 13604 scope.cc:202] Create variable nanmedian_0.tmp_1
1906: I0815 06:18:01.863197 13604 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x46bf1750 type is 7
1906: I0815 06:18:01.863204 13604 scope.cc:202] Create variable nanmedian_1.tmp_0
1906: I0815 06:18:01.863207 13604 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x45255080 type is 7
1906: I0815 06:18:01.863212 13604 scope.cc:202] Create variable nanmedian_1.tmp_1
1906: I0815 06:18:01.863219 13604 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x4691d6e0 type is 7
1906: I0815 06:18:01.863224 13604 scope.cc:202] Create variable nanmedian_2.tmp_0
1906: I0815 06:18:01.863229 13604 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x46c0bb60 type is 7
1906: I0815 06:18:01.863233 13604 scope.cc:202] Create variable nanmedian_2.tmp_1
1906: I0815 06:18:01.863237 13604 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x46bf1b60 type is 7
1906: I0815 06:18:01.863242 13604 scope.cc:202] Create variable nanmedian_3.tmp_0
1906: I0815 06:18:01.863245 13604 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x46bf1d70 type is 7
1906: I0815 06:18:01.863250 13604 scope.cc:202] Create variable nanmedian_3.tmp_1
1906: I0815 06:18:01.863255 13604 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x46bf1fd0 type is 7
1906: I0815 06:18:01.863260 13604 scope.cc:202] Create variable nanmedian_4.tmp_0
1906: I0815 06:18:01.863263 13604 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x46bf2230 type is 7
1906: I0815 06:18:01.863271 13604 scope.cc:202] Create variable nanmedian_4.tmp_1
1906: I0815 06:18:01.863276 13604 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x46bf2490 type is 7
1906: I0815 06:18:01.863417 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.863425 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.863430 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.863435 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.863502 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.863517 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.863591 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.863601 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.863619 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.863791 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.863996 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864008 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864025 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.864158 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.864358 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864372 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864387 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.864516 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.864703 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864714 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.864732 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.864879 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.865087 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.865099 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.865115 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.865265 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.865463 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865476 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865494 13604 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.865502 13604 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46c0fdd0Variable Type 7
1906: I0815 06:18:01.865525 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.865543 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.865569 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.865588 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.865628 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.865649 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.865693 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865703 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865720 13604 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.865726 13604 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46ccb9f0Variable Type 7
1906: I0815 06:18:01.865741 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.865754 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.865774 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.865788 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.865823 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.865849 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1906: I0815 06:18:01.865900 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865911 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.865926 13604 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.865932 13604 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46bf0450Variable Type 7
1906: I0815 06:18:01.865947 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.865959 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.865978 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.865993 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.866025 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.866039 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1906: I0815 06:18:01.866083 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.866094 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.866107 13604 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.866114 13604 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46bd4820Variable Type 7
1906: I0815 06:18:01.866127 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.866140 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.866158 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.866171 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.866204 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.866217 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1906: I0815 06:18:01.866256 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.866266 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.866281 13604 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.866287 13604 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45326000Variable Type 7
1906: I0815 06:18:01.866308 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.866322 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.866340 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.866354 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.866386 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.866400 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1906: I0815 06:18:01.867012 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.867048 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.867067 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.867087 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.867106 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1906:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1906:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1906:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1906:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1906:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906: }
1906: 
1906: I0815 06:18:01.873298 13604 pir_interpreter.cc:161] PirInterpreter(): 0x44552c40 on Place(gpu:0)
1906: I0815 06:18:01.873339 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.873361 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_1
1906: I0815 06:18:01.873373 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_2
1906: I0815 06:18:01.873384 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_3
1906: I0815 06:18:01.873394 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_4
1906: I0815 06:18:01.873402 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_5
1906: I0815 06:18:01.873414 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_6
1906: I0815 06:18:01.873422 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_7
1906: I0815 06:18:01.873431 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_8
1906: I0815 06:18:01.873441 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_9
1906: I0815 06:18:01.873448 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_10
1906: I0815 06:18:01.873458 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_11
1906: I0815 06:18:01.873471 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_12
1906: I0815 06:18:01.873481 13604 scope.cc:202] Create variable fetch0@fetch
1906: I0815 06:18:01.873495 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_14
1906: I0815 06:18:01.873504 13604 scope.cc:202] Create variable fetch1@fetch
1906: I0815 06:18:01.873512 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_16
1906: I0815 06:18:01.873522 13604 scope.cc:202] Create variable fetch2@fetch
1906: I0815 06:18:01.873530 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_18
1906: I0815 06:18:01.873539 13604 scope.cc:202] Create variable fetch3@fetch
1906: I0815 06:18:01.873548 13604 scope.cc:202] Create variable 0x44552c401723702681873330942_inner_var_20
1906: I0815 06:18:01.873556 13604 scope.cc:202] Create variable fetch4@fetch
1906: I0815 06:18:01.873840 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: I0815 06:18:01.873855 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.873858 13604 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1906:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1906: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1906: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1906: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1906: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1906: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1906: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1906: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1906: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1906: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1906: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1906: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1906: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1906: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x46bf0810
1906: 1 -> 0x44552c401723702681873330942_inner_var_1 -> 0x46bab450
1906: 2 -> 0x44552c401723702681873330942_inner_var_2 -> 0x4532bd30
1906: 3 -> 0x44552c401723702681873330942_inner_var_3 -> 0x46d12260
1906: 4 -> 0x44552c401723702681873330942_inner_var_4 -> 0x46d11e10
1906: 5 -> 0x44552c401723702681873330942_inner_var_5 -> 0x46bab660
1906: 6 -> 0x44552c401723702681873330942_inner_var_6 -> 0x44551760
1906: 7 -> 0x44552c401723702681873330942_inner_var_7 -> 0x44551b80
1906: 8 -> 0x44552c401723702681873330942_inner_var_8 -> 0x46bfca70
1906: 9 -> 0x44552c401723702681873330942_inner_var_9 -> 0x46bf1490
1906: 10 -> 0x44552c401723702681873330942_inner_var_10 -> 0x46bd53a0
1906: 11 -> 0x44552c401723702681873330942_inner_var_11 -> 0x46d07f80
1906: 12 -> 0x44552c401723702681873330942_inner_var_12 -> 0x4524bbf0
1906: 13 -> fetch0@fetch -> 0x46890540
1906: 14 -> 0x44552c401723702681873330942_inner_var_14 -> 0x46d083a0
1906: 15 -> fetch1@fetch -> 0x46bfed80
1906: 16 -> 0x44552c401723702681873330942_inner_var_16 -> 0x46890520
1906: 17 -> fetch2@fetch -> 0x46c0f030
1906: 18 -> 0x44552c401723702681873330942_inner_var_18 -> 0x46bfed60
1906: 19 -> fetch3@fetch -> 0x46c0fa40
1906: 20 -> 0x44552c401723702681873330942_inner_var_20 -> 0x46c0f010
1906: 21 -> fetch4@fetch -> 0x4691a7d0
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 2 3 4 5 
1906: 1 -> 6 
1906: 2 -> 8 
1906: 3 -> 10 
1906: 4 -> 12 
1906: 5 -> 14 
1906: 6 -> 7 
1906: 8 -> 9 
1906: 10 -> 11 
1906: 12 -> 13 
1906: 14 -> 15 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->
1906: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1906: 1 downstreams: 6[pd_op.memcpy_d2h]->
1906: 2 downstreams: 8[pd_op.memcpy_d2h]->
1906: 3 downstreams: 10[pd_op.memcpy_d2h]->
1906: 4 downstreams: 12[pd_op.memcpy_d2h]->
1906: 5 downstreams: 14[pd_op.memcpy_d2h]->
1906: 6 downstreams: 7[pd_op.fetch]->
1906: 7 downstreams: 
1906: 8 downstreams: 9[pd_op.fetch]->
1906: 9 downstreams: 
1906: 10 downstreams: 11[pd_op.fetch]->
1906: 11 downstreams: 
1906: 12 downstreams: 13[pd_op.fetch]->
1906: 13 downstreams: 
1906: 14 downstreams: 15[pd_op.fetch]->
1906: 15 downstreams: 
1906: I0815 06:18:01.875303 13666 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.875324 13667 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.875349 13668 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.875416 13669 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.875424 13670 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.875455 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.875490 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1906: I0815 06:18:01.875535 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_3:[dtype=;place=;dim=;lod={};, 0x44552c401723702681873330942_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.875715 13670 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.875869 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x44552c401723702681873330942_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.875922 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_5:[dtype=;place=;dim=;lod={};, 0x44552c401723702681873330942_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.875941 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.875974 13669 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.876052 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876072 13670 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.876112 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876137 13669 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.876152 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876248 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x44552c401723702681873330942_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.876291 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_7:[dtype=;place=;dim=;lod={};, 0x44552c401723702681873330942_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876298 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876317 13669 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.876374 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876425 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876441 13669 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.876451 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876469 13670 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.876611 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x44552c401723702681873330942_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.876652 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_9:[dtype=;place=;dim=;lod={};, 0x44552c401723702681873330942_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876659 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876672 13669 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.876704 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876744 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.876758 13669 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.876768 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.876821 13670 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.876974 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x44552c401723702681873330942_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.877015 13670 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_11:[dtype=;place=;dim=;lod={};, 0x44552c401723702681873330942_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.877022 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.877035 13669 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.877065 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.877135 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.877151 13669 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877161 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.877193 13670 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.877347 13670 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44552c401723702681873330942_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x44552c401723702681873330942_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.877394 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.877408 13669 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.877435 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44552c401723702681873330942_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44552c401723702681873330942_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.877468 13669 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.877483 13669 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877494 13669 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44552c401723702681873330942_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.877521 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x44552db0) got event_name: TaskCompletion
1906: I0815 06:18:01.877540 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877565 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877575 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877584 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.877595 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.879030 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.879117 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.879144 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.879325 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.879426 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46d12700)  to GradNodeAccumulation (addr: 0x43620840)
1906: I0815 06:18:01.879556 13604 backward.cc:459] Run in Grad
1906: I0815 06:18:01.879577 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.879630 13604 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x46d12700 to ptr: 0x45329040
1906: I0815 06:18:01.879642 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.879679 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.879714 13604 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x43620840 to ptr: 0x468868d0
1906: I0815 06:18:01.879739 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x45329040
1906: I0815 06:18:01.879748 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.879782 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.879843 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.879853 13604 backward.cc:335] Node: NanmedianGradNode addr:0x45329040, Found pending node: GradNodeAccumulation addr: 0x468868d0
1906: I0815 06:18:01.879859 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.879885 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x468868d0
1906: I0815 06:18:01.879892 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.879896 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.879904 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.879909 13604 backward.cc:435] Finish Backward
1906: I0815 06:18:01.880627 13604 dygraph_functions.cc:77615] Running AD API: uniform
1906: I0815 06:18:01.880646 13604 dygraph_functions.cc:77636] { Input: []} 
1906: I0815 06:18:01.880753 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.880777 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.880920 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.880995 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46d12700)  to GradNodeAccumulation (addr: 0x43620840)
1906: I0815 06:18:01.881089 13604 backward.cc:442] Run in Backward
1906: I0815 06:18:01.881098 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.881104 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.881135 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.881163 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x46d12700
1906: I0815 06:18:01.881172 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.881201 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.881254 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.881281 13604 backward.cc:335] Node: NanmedianGradNode addr:0x46d12700, Found pending node: GradNodeAccumulation addr: 0x43620840
1906: I0815 06:18:01.881289 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.881318 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43620840
1906: I0815 06:18:01.881326 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.881330 13604 accumulation_node.cc:40] Move Tensor ptr: 0x4524b6a0
1906: I0815 06:18:01.881335 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.881338 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.884327 13604 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1906: I0815 06:18:01.884838 13604 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.884958 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.884984 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.885154 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x45247060)  to GradNodeAccumulation (addr: 0x19a6f2b0)
1906: I0815 06:18:01.885257 13604 backward.cc:442] Run in Backward
1906: I0815 06:18:01.885265 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.885272 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.885313 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.885337 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x45247060
1906: I0815 06:18:01.885345 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.885375 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.885422 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.885447 13604 backward.cc:335] Node: NanmedianGradNode addr:0x45247060, Found pending node: GradNodeAccumulation addr: 0x19a6f2b0
1906: I0815 06:18:01.885455 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.885473 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x19a6f2b0
1906: I0815 06:18:01.885479 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.885483 13604 accumulation_node.cc:40] Move Tensor ptr: 0x468840b0
1906: I0815 06:18:01.885488 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.885491 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.886229 13604 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.886318 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.886345 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.886550 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.886651 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46d12700)  to GradNodeAccumulation (addr: 0x19a6f2b0)
1906: I0815 06:18:01.886767 13604 backward.cc:459] Run in Grad
1906: I0815 06:18:01.886777 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.886790 13604 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x46d12700 to ptr: 0x46bff270
1906: I0815 06:18:01.886799 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.886829 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.886854 13604 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x19a6f2b0 to ptr: 0x468868d0
1906: I0815 06:18:01.886873 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x46bff270
1906: I0815 06:18:01.886881 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.886909 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.887041 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.887051 13604 backward.cc:335] Node: NanmedianGradNode addr:0x46bff270, Found pending node: GradNodeAccumulation addr: 0x468868d0
1906: I0815 06:18:01.887056 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.887073 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x468868d0
1906: I0815 06:18:01.887079 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.887084 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.887089 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.887092 13604 backward.cc:435] Finish Backward
1906: I0815 06:18:01.887992 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.888072 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.888098 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.888258 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.889688 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.889744 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.889767 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.894785 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.894819 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.896040 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.896065 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.897064 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.897184 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.897220 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.897475 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.898089 13604 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.898171 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.898198 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.898388 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.899024 13604 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.899099 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.899127 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.899294 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.900087 13604 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.900180 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.900211 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.900417 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.901018 13604 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.901072 13604 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f881de19400), and remaining 0
1906: I0815 06:18:01.901129 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.901156 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.901654 13604 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.901729 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.901757 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.902271 13604 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.902355 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.902383 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.902855 13604 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.902928 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.902954 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.903471 13604 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.903542 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.903568 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.903760 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.904341 13604 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.904420 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.904448 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.904618 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.905220 13604 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.905294 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.905334 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.905498 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.906075 13604 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.906152 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.906180 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.906364 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.906970 13604 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.907043 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.907070 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.907234 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.907820 13604 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.907897 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.907925 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.908099 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.908728 13604 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.908805 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.908835 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.908993 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.909601 13604 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.909684 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.909713 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.909879 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.910516 13604 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.910593 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.910620 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.910782 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.911365 13604 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.911443 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.911471 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.911643 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.912151 13604 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.912204 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.912225 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.912354 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.912948 13604 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.913014 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.913039 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.913182 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.913825 13604 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.913899 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.913928 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.914129 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.914813 13604 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.914883 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.914908 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.915076 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.915746 13604 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.915817 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.915841 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.916013 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.916659 13604 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.916728 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.916754 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.916922 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.917568 13604 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.917639 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.917665 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.917840 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.918457 13604 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.918526 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.918551 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.918720 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.919339 13604 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.919409 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.919435 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.919607 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.920233 13604 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.920312 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.920338 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.920509 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.921144 13604 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.921212 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.921238 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.921422 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.922034 13604 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.922102 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.922128 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.922312 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.922914 13604 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.922982 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.923008 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.923182 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.923790 13604 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.923861 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.923885 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.924057 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.924672 13604 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.924742 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.924767 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.924940 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.925551 13604 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.925621 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.925647 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.925818 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.926450 13604 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.926519 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.926544 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.926714 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.927343 13604 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.927412 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.927438 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.927606 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.928273 13604 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.928360 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.928390 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.928577 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.929219 13604 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.929288 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.929323 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.929493 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.931636 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.931663 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.932418 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.932440 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.933166 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.933187 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.933936 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.933959 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.934695 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.934717 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.936326 13669 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 17361193375481789183 to 2413979341601591119 , after update, data is {current : 0, peak : 1260}.
1906: I0815 06:18:01.936336 13669 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 17361193375481789183 to 2413979341601591119 , after update, data is {current : 20, peak : 24}.
1906: I0815 06:18:01.936686 13670 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 2413979341601591119 to 12390818033688177718 , after update, data is {current : 0, peak : 260}.
1906: I0815 06:18:01.936697 13670 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 2413979341601591119 to 2409697520245028391 , after update, data is {current : 22, peak : 24}.
1906: I0815 06:18:01.936702 13670 thread_data_registry.h:135] Add data {current : 0, peak : 1260} from thread 2413979341601591119 to 2409697520245028391 , after update, data is {current : -18, peak : 330659}.
1906: I0815 06:18:01.938426 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.938956 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.939482 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.939998 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.940521 13604 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1906: I0815 06:18:01.941378 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.941396 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.941401 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.946166 13604 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1906: I0815 06:18:01.946216 13604 interpreter_util.cc:1169] Creating Variables
1906: I0815 06:18:01.946228 13604 scope.cc:202] Create variable X
1906: I0815 06:18:01.946235 13604 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x46d1c8f0 type is 7
1906: I0815 06:18:01.946244 13604 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4693d510 type is 9
1906: I0815 06:18:01.946251 13604 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4693da50 type is 10
1906: I0815 06:18:01.946257 13604 scope.cc:202] Create variable nanmedian_0.tmp_0
1906: I0815 06:18:01.946261 13604 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x46d1bec0 type is 7
1906: I0815 06:18:01.946266 13604 scope.cc:202] Create variable nanmedian_0.tmp_1
1906: I0815 06:18:01.946270 13604 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x46d1b910 type is 7
1906: I0815 06:18:01.946275 13604 scope.cc:202] Create variable nanmedian_1.tmp_0
1906: I0815 06:18:01.946278 13604 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x46d1ca70 type is 7
1906: I0815 06:18:01.946283 13604 scope.cc:202] Create variable nanmedian_1.tmp_1
1906: I0815 06:18:01.946287 13604 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x46d1cc80 type is 7
1906: I0815 06:18:01.946292 13604 scope.cc:202] Create variable nanmedian_2.tmp_0
1906: I0815 06:18:01.946295 13604 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x46d1ce90 type is 7
1906: I0815 06:18:01.946311 13604 scope.cc:202] Create variable nanmedian_2.tmp_1
1906: I0815 06:18:01.946317 13604 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x46bf9220 type is 7
1906: I0815 06:18:01.946322 13604 scope.cc:202] Create variable nanmedian_3.tmp_0
1906: I0815 06:18:01.946326 13604 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x46bf93f0 type is 7
1906: I0815 06:18:01.946331 13604 scope.cc:202] Create variable nanmedian_3.tmp_1
1906: I0815 06:18:01.946334 13604 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x46bf9650 type is 7
1906: I0815 06:18:01.946339 13604 scope.cc:202] Create variable nanmedian_4.tmp_0
1906: I0815 06:18:01.946343 13604 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x46bf98b0 type is 7
1906: I0815 06:18:01.946348 13604 scope.cc:202] Create variable nanmedian_4.tmp_1
1906: I0815 06:18:01.946352 13604 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x46bf9b10 type is 7
1906: I0815 06:18:01.946485 13604 interpreter_util.cc:594] Static build: 0
1906: I0815 06:18:01.946492 13604 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1906: I0815 06:18:01.946497 13604 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1906: I0815 06:18:01.946502 13604 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1906: I0815 06:18:01.946557 13604 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.946571 13604 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.946633 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.946643 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.946660 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.946833 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.947037 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947049 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947069 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.947202 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.947394 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947407 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947424 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.947551 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.947733 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947746 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.947760 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.947908 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.948110 13604 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.948123 13604 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.948143 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.948292 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.948498 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948510 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948527 13604 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.948536 13604 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46c102c0Variable Type 7
1906: I0815 06:18:01.948555 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.948571 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.948596 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.948613 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.948652 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.948671 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1906: I0815 06:18:01.948714 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948724 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948740 13604 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.948746 13604 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46d1bf70Variable Type 7
1906: I0815 06:18:01.948760 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.948774 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.948793 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.948807 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.948840 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.948868 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1906: I0815 06:18:01.948915 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948925 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.948940 13604 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.948946 13604 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46d0a5c0Variable Type 7
1906: I0815 06:18:01.948961 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.948973 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.948992 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.949005 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.949039 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.949054 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1906: I0815 06:18:01.949096 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.949105 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.949119 13604 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.949126 13604 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46968970Variable Type 7
1906: I0815 06:18:01.949139 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.949152 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.949169 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.949183 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.949215 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.949229 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1906: I0815 06:18:01.949267 13604 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.949277 13604 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1906: I0815 06:18:01.949291 13604 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1906: I0815 06:18:01.949297 13604 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x46c7eb40Variable Type 7
1906: I0815 06:18:01.949321 13604 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1906: I0815 06:18:01.949332 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.949350 13604 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1906: I0815 06:18:01.949364 13604 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.949398 13604 data_transfer.cc:232] Run memcpy_d2h done.
1906: I0815 06:18:01.949410 13604 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1906: I0815 06:18:01.949999 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.950033 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.950054 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.950074 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: I0815 06:18:01.950093 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1906: IR before lowering = {
1906:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1906:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1906:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1906:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1906:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1906:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1906:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1906: }
1906: 
1906: IR after lowering = {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906: }
1906: 
1906: I0815 06:18:01.955363 13604 pir_interpreter.cc:161] PirInterpreter(): 0x46d0ae40 on Place(gpu:0)
1906: I0815 06:18:01.955399 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_1
1906: I0815 06:18:01.955410 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_2
1906: I0815 06:18:01.955418 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_3
1906: I0815 06:18:01.955425 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_4
1906: I0815 06:18:01.955433 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_5
1906: I0815 06:18:01.955440 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_6
1906: I0815 06:18:01.955448 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_7
1906: I0815 06:18:01.955454 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_8
1906: I0815 06:18:01.955461 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_9
1906: I0815 06:18:01.955469 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_10
1906: I0815 06:18:01.955475 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_11
1906: I0815 06:18:01.955485 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_12
1906: I0815 06:18:01.955497 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_14
1906: I0815 06:18:01.955509 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_16
1906: I0815 06:18:01.955520 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_18
1906: I0815 06:18:01.955531 13604 scope.cc:202] Create variable 0x46d0ae401723702681955384885_inner_var_20
1906: I0815 06:18:01.955762 13604 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1906: ======================== The network executed by pir interpreter ========================
1906: {
1906:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1906:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1906:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1906:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1906:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1906: }
1906: 
1906: ======================== The instruction executed by pir interpreter ========================
1906: {outputs} =  instruction_name[idx] ({inputs})
1906: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1906: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1906: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1906: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1906: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1906: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1906: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1906: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1906: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1906: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1906: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1906: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1906: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1906: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1906: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1906: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1906: ---------------------------var_id -> var_name -> variable*---------------------------
1906: 0 -> X -> 0x46d18fb0
1906: 1 -> 0x46d0ae401723702681955384885_inner_var_1 -> 0x46913550
1906: 2 -> 0x46d0ae401723702681955384885_inner_var_2 -> 0x468914e0
1906: 3 -> 0x46d0ae401723702681955384885_inner_var_3 -> 0x46c01670
1906: 4 -> 0x46d0ae401723702681955384885_inner_var_4 -> 0x45241520
1906: 5 -> 0x46d0ae401723702681955384885_inner_var_5 -> 0x46c0ccf0
1906: 6 -> 0x46d0ae401723702681955384885_inner_var_6 -> 0x46ba4eb0
1906: 7 -> 0x46d0ae401723702681955384885_inner_var_7 -> 0x19b17630
1906: 8 -> 0x46d0ae401723702681955384885_inner_var_8 -> 0x46ccb8d0
1906: 9 -> 0x46d0ae401723702681955384885_inner_var_9 -> 0x46d05e70
1906: 10 -> 0x46d0ae401723702681955384885_inner_var_10 -> 0x46ccd0c0
1906: 11 -> 0x46d0ae401723702681955384885_inner_var_11 -> 0x46c7da90
1906: 12 -> 0x46d0ae401723702681955384885_inner_var_12 -> 0x46946810
1906: 13 -> fetch0@fetch -> 0x46890540
1906: 14 -> 0x46d0ae401723702681955384885_inner_var_14 -> 0x469520c0
1906: 15 -> fetch1@fetch -> 0x46bfed80
1906: 16 -> 0x46d0ae401723702681955384885_inner_var_16 -> 0x4690e100
1906: 17 -> fetch2@fetch -> 0x46c0f030
1906: 18 -> 0x46d0ae401723702681955384885_inner_var_18 -> 0x46890ed0
1906: 19 -> fetch3@fetch -> 0x46c0fa40
1906: 20 -> 0x46d0ae401723702681955384885_inner_var_20 -> 0x46919890
1906: 21 -> fetch4@fetch -> 0x4691a7d0
1906: 
1906: 
1906: ======================= The dependency of all instruction ========================
1906: id -> down_stream_id
1906: 0 -> 1 2 3 4 5 
1906: 1 -> 6 
1906: 2 -> 8 
1906: 3 -> 10 
1906: 4 -> 12 
1906: 5 -> 14 
1906: 6 -> 7 
1906: 8 -> 9 
1906: 10 -> 11 
1906: 12 -> 13 
1906: 14 -> 15 
1906: 
1906: 
1906: ======================== pir interpreter trace order ========================
1906: 
1906: Leaf nodes: 0[pd_op.shadow_feed]->
1906: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1906: 1 downstreams: 6[pd_op.memcpy_d2h]->
1906: 2 downstreams: 8[pd_op.memcpy_d2h]->
1906: 3 downstreams: 10[pd_op.memcpy_d2h]->
1906: 4 downstreams: 12[pd_op.memcpy_d2h]->
1906: 5 downstreams: 14[pd_op.memcpy_d2h]->
1906: 6 downstreams: 7[pd_op.fetch]->
1906: 7 downstreams: 
1906: 8 downstreams: 9[pd_op.fetch]->
1906: 9 downstreams: 
1906: 10 downstreams: 11[pd_op.fetch]->
1906: 11 downstreams: 
1906: 12 downstreams: 13[pd_op.fetch]->
1906: 13 downstreams: 
1906: 14 downstreams: 15[pd_op.fetch]->
1906: 15 downstreams: 
1906: I0815 06:18:01.956955 13671 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1906: I0815 06:18:01.957017 13672 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1906: I0815 06:18:01.957070 13673 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1906: I0815 06:18:01.957088 13675 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1906: I0815 06:18:01.957132 13674 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1906: I0815 06:18:01.957126 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.957168 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1906: I0815 06:18:01.957208 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_3:[dtype=;place=;dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.957433 13675 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.957592 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.957646 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_5:[dtype=;place=;dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.957664 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.957695 13674 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.957754 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.957813 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.957832 13675 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.957834 13674 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.957842 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.957975 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.958019 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_7:[dtype=;place=;dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958024 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958039 13674 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.958070 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958128 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958148 13674 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.958153 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958194 13675 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.958339 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.958381 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_9:[dtype=;place=;dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958388 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958405 13674 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.958436 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958493 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958511 13674 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.958518 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958547 13675 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.958700 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.958741 13675 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_11:[dtype=;place=;dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958747 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.958761 13674 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.958799 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958854 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958873 13674 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.958878 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.958926 13675 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.959079 13675 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1906: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x46d0ae401723702681955384885_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x46d0ae401723702681955384885_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1906: I0815 06:18:01.959125 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1906: I0815 06:18:01.959141 13674 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1906: I0815 06:18:01.959167 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1906: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x46d0ae401723702681955384885_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x46d0ae401723702681955384885_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.959199 13674 pir_interpreter.cc:1876] 
1906: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.959216 13674 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.959221 13674 pir_interpreter.cc:1916] 
1906: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1906: After: Place(cpu) Op(pd_op.fetch), inputs:{0x46d0ae401723702681955384885_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1906: I0815 06:18:01.959247 13604 pir_interpreter.cc:1766] main_thread_blocker_(0x46d0afb0) got event_name: TaskCompletion
1906: I0815 06:18:01.959270 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.959292 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.959309 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.959321 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.959333 13604 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1906: I0815 06:18:01.960744 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19a6f2b0 for it.
1906: I0815 06:18:01.960831 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.960857 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.961036 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.961130 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x4454fb90)  to GradNodeAccumulation (addr: 0x19a6f2b0)
1906: I0815 06:18:01.961242 13604 backward.cc:459] Run in Grad
1906: I0815 06:18:01.961251 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.961266 13604 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x4454fb90 to ptr: 0x46c03a70
1906: I0815 06:18:01.961274 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.961316 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.961341 13604 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x19a6f2b0 to ptr: 0x468868d0
1906: I0815 06:18:01.961361 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x46c03a70
1906: I0815 06:18:01.961369 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.961400 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.961459 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.961468 13604 backward.cc:335] Node: NanmedianGradNode addr:0x46c03a70, Found pending node: GradNodeAccumulation addr: 0x468868d0
1906: I0815 06:18:01.961473 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.961500 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x468868d0
1906: I0815 06:18:01.961508 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.961512 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.961516 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.961521 13604 backward.cc:435] Finish Backward
1906: I0815 06:18:01.962148 13604 dygraph_functions.cc:77615] Running AD API: uniform
1906: I0815 06:18:01.962165 13604 dygraph_functions.cc:77636] { Input: []} 
1906: I0815 06:18:01.962246 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.962268 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.962419 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.962493 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x4454fb90)  to GradNodeAccumulation (addr: 0x19a6f2b0)
1906: I0815 06:18:01.962574 13604 backward.cc:442] Run in Backward
1906: I0815 06:18:01.962581 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.962589 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.962616 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.962638 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x4454fb90
1906: I0815 06:18:01.962646 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.962673 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.962725 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.962751 13604 backward.cc:335] Node: NanmedianGradNode addr:0x4454fb90, Found pending node: GradNodeAccumulation addr: 0x19a6f2b0
1906: I0815 06:18:01.962759 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.962778 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x19a6f2b0
1906: I0815 06:18:01.962785 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.962790 13604 accumulation_node.cc:40] Move Tensor ptr: 0x46d0a140
1906: I0815 06:18:01.962793 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.962797 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.963232 13604 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.963353 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.963380 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.963544 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46c03a70)  to GradNodeAccumulation (addr: 0x43620840)
1906: I0815 06:18:01.963641 13604 backward.cc:442] Run in Backward
1906: I0815 06:18:01.963649 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.963655 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.963686 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.963708 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x46c03a70
1906: I0815 06:18:01.963716 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.963743 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.963790 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.963814 13604 backward.cc:335] Node: NanmedianGradNode addr:0x46c03a70, Found pending node: GradNodeAccumulation addr: 0x43620840
1906: I0815 06:18:01.963821 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.963840 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x43620840
1906: I0815 06:18:01.963847 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.963851 13604 accumulation_node.cc:40] Move Tensor ptr: 0x44012a70
1906: I0815 06:18:01.963855 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.963858 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.964562 13604 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.964645 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.964671 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.964857 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.964948 13604 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x46c03a70)  to GradNodeAccumulation (addr: 0x43620840)
1906: I0815 06:18:01.965061 13604 backward.cc:459] Run in Grad
1906: I0815 06:18:01.965071 13604 backward.cc:113] Start Backward
1906: I0815 06:18:01.965083 13604 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x46c03a70 to ptr: 0x4454fb90
1906: I0815 06:18:01.965092 13604 backward.cc:197] Fill grad input tensor 0 with 1.0
1906: I0815 06:18:01.965121 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.965144 13604 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x43620840 to ptr: 0x468868d0
1906: I0815 06:18:01.965163 13604 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x4454fb90
1906: I0815 06:18:01.965171 13604 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1906: I0815 06:18:01.965199 13604 nodes.cc:26740] { Input: [ 
1906: ( out_grad , [[ Not specified tensor log level ]]),  
1906: ( x , [[ Not specified tensor log level ]]),  
1906: ( medians , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.965327 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.965337 13604 backward.cc:335] Node: NanmedianGradNode addr:0x4454fb90, Found pending node: GradNodeAccumulation addr: 0x468868d0
1906: I0815 06:18:01.965342 13604 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1906: I0815 06:18:01.965360 13604 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x468868d0
1906: I0815 06:18:01.965368 13604 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.965373 13604 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1906: I0815 06:18:01.965377 13604 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1906: I0815 06:18:01.965382 13604 backward.cc:435] Finish Backward
1906: I0815 06:18:01.966252 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.966339 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.966367 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.966527 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.967864 13604 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x43620840 for it.
1906: I0815 06:18:01.967907 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.967926 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.969985 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.970009 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.970911 13604 op_desc.cc:1111] CompileTime infer shape on nanmedian
1906: I0815 06:18:01.970933 13604 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1906: I0815 06:18:01.971715 13604 dygraph_functions.cc:33459] Running AD API: full
1906: I0815 06:18:01.971732 13604 dygraph_functions.cc:33480] { Input: []} 
1906: I0815 06:18:01.971827 13604 dygraph_functions.cc:33459] Running AD API: full
1906: I0815 06:18:01.971836 13604 dygraph_functions.cc:33480] { Input: []} 
1906: I0815 06:18:01.971894 13604 dygraph_functions.cc:33459] Running AD API: full
1906: I0815 06:18:01.971901 13604 dygraph_functions.cc:33480] { Input: []} 
1906: I0815 06:18:01.971951 13604 dygraph_functions.cc:82204] Running AD API: arange
1906: I0815 06:18:01.971984 13604 dygraph_functions.cc:82268] { Input: [ 
1906: ( start , [[ Not specified tensor log level ]]),  
1906: ( end , [[ Not specified tensor log level ]]),  
1906: ( step , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.972131 13604 dygraph_functions.cc:62112] Running AD API: reshape
1906: I0815 06:18:01.972149 13604 dygraph_functions.cc:62169] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.972167 13604 api.cc:40807] Perform View between Output and Input Tensor, share allocation and inplace version.
1906: I0815 06:18:01.972240 13604 dygraph_functions.cc:14224] Running AD API: cast
1906: I0815 06:18:01.972254 13604 dygraph_functions.cc:14268] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.972333 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=200, vec_size=1, block_size=64, grid_size=4, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.972397 13604 dygraph_functions.cc:54355] Running AD API: nanmedian
1906: I0815 06:18:01.972414 13604 dygraph_functions.cc:54415] { Input: [ 
1906: ( x , [[ Not specified tensor log level ]]), ]} 
1906: I0815 06:18:01.972581 13604 gpu_launch_config.h:156] Get 1-D launch config: numel=100, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1906: I0815 06:18:01.973861 13604 mmap_allocator.cc:348] PID: 13604, MemoryMapFdSet: set size - 0
1906: I0815 06:18:01.985543 13604 mmap_allocator.cc:348] PID: 13604, MemoryMapFdSet: set size - 0
1906: I0815 06:18:02.046097 13674 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 8056776557582274967 to 8166455017933216152 , after update, data is {current : 0, peak : 1252}.
1906: I0815 06:18:02.046108 13674 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 8056776557582274967 to 8166455017933216152 , after update, data is {current : 0, peak : 16}.
1906: I0815 06:18:02.046372 13675 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 8166455017933216152 to 12390818033688177718 , after update, data is {current : 0, peak : 260}.
1906: I0815 06:18:02.046384 13675 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 8166455017933216152 to 2409697520245028391 , after update, data is {current : 22, peak : 24}.
1906: I0815 06:18:02.046389 13675 thread_data_registry.h:135] Add data {current : 0, peak : 1252} from thread 8166455017933216152 to 2409697520245028391 , after update, data is {current : -18, peak : 330659}.
1906: I0815 06:18:02.046995 13648 thread_data_registry.h:135] Add data {current : 22, peak : 24} from thread 2409697520245028391 to 4649890891860076216 , after update, data is {current : 26, peak : 26}.
1906: I0815 06:18:02.047006 13648 thread_data_registry.h:135] Add data {current : -18, peak : 330659} from thread 2409697520245028391 to 1571964316202147174 , after update, data is {current : 0, peak : 330659}.
1906: I0815 06:18:02.047284 13652 thread_data_registry.h:135] Add data {current : 26, peak : 26} from thread 4649890891860076216 to 1571964316202147174 , after update, data is {current : 26, peak : 26}.
1906: I0815 06:18:02.047504 13653 thread_data_registry.h:135] Add data {current : 26, peak : 26} from thread 1571964316202147174 to 12390818033688177718 , after update, data is {current : 20026, peak : 40004}.
1906: I0815 06:18:02.047514 13653 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 1571964316202147174 to 12390818033688177718 , after update, data is {current : 162560, peak : 560633}.
1906: I0815 06:18:02.181115 13604 mmap_allocator.cc:348] PID: 13604, MemoryMapFdSet: set size - 0
1/1 Test #1906: test_nanmedian ...................   Passed    9.61 sec

The following tests passed:
	test_nanmedian

100% tests passed, 0 tests failed out of 1

Total Test time (real) =   9.79 sec
