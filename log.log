UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:03:50.950598  6857 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:03:51.743412  6857 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=paddle_num_threads,static_runtime_data_save_path,use_autotune,cse_max_count,accuracy_check_atol_fp16,enable_dump_main_program,graph_embedding_split_infer_mode,executor_log_deps_every_microseconds,gpu_memory_limit_mb,benchmark,enable_graph_multi_node_sampling,enable_collect_shape,fraction_of_cpu_memory_to_use,enable_api_kernel_fallback,cudnn_deterministic,dump_chunk_info,dataloader_use_file_descriptor,prim_backward,run_kp_kernel,use_fast_math,ir_inplace_kernel_blacklist,log_memory_stats,enable_fusion_fallback,enable_fuse_parallel_matmul_pass,mkl_dir,query_dest_rank_by_multi_node,enable_opt_get_features,use_xqa_optim,allreduce_record_one_event,cusolver_dir,einsum_opt,win_cuda_bin_dir,prim_enabled,print_ir,enable_cinn_compile_cache,enable_pir_api,check_kernel_launch,pir_subgraph_saving_dir,gpu_allocator_retry_time,eager_delete_scope,gpugraph_dedup_pull_push_mode,logging_trunc_pir_py_code,init_allocated_mem,graph_get_neighbor_id,new_executor_static_build,trt_ibuilder_cache,accuracy_check_atol_bf16,pir_apply_inplace_pass,auto_growth_chunk_size_in_mb,tracer_onednn_ops_on,gpugraph_storage_mode,gpugraph_offload_gather_copy_maxsize,allocator_strategy,dygraph_debug,embedding_deterministic,cusparse_dir,enable_cublas_tensor_op_math,prim_forward_blacklist,use_virtual_memory_auto_growth,cudnn_batchnorm_spatial_persistent,selected_gpus,cudnn_exhaustive_search,op_dir,curand_dir,logging_pir_py_code_int_tensor_element_limit,cinn_subgraph_graphviz_dir,fast_eager_deletion_mode,use_cinn,disable_dyshape_in_train,multiple_of_cupti_buffer_size,enable_tracker_all2all,max_inplace_grad_add,cuda_dir,gpugraph_enable_gpu_direct_access,enable_gpu_memory_usage_log_mb,enable_auto_rdma_trans,gpugraph_offload_param_stat,accuracy_check_rtol_fp32,print_allocator_trace_info,static_executor_perfstat_filepath,cudnn_dir,benchmark_nccl,use_auto_growth_v2,sync_after_alloc,gpugraph_enable_print_op_debug,add_dependency_for_communication_op,enable_dependency_builder_debug_info,cupti_dir,sort_sum_gradient,enable_exit_when_partial_worker,pir_apply_shape_optimization_pass,tensor_operants_mode,free_idle_chunk,new_executor_use_local_scope,gpugraph_debug_gpu_memory,initial_cpu_memory_in_mb,cinn_compile_thread_num,initial_gpu_memory_in_mb,gpugraph_slot_feasign_max_num,fuse_parameter_groups_size,enable_sparse_inner_gather,tracer_onednn_ops_off,conv_workspace_size_limit,use_auto_growth_pinned_allocator,enable_unused_var_check,enable_pir_in_executor_trace_run,check_nan_inf,conv2d_disable_cudnn,fleet_executor_with_standalone,gemm_use_half_precision_compute_type,print_sub_graph_dir,logging_pir_py_code_dump_symbolic_dims,allow_cinn_ops,accuracy_check_rtol_bf16,new_executor_sequential_run,nvidia_package_dir,alloc_fill_value,graph_metapath_split_opt,lapack_dir,jit_engine_type,convert_all_blocks,npu_storage_format,memory_fraction_of_eager_deletion,use_mkldnn,accuracy_check_atol_fp32,cusparselt_dir,prim_all,logging_pir_py_code_dir,cuda_memory_async_pool_realease_threshold,enable_cinn_accuracy_check,prim_enable_dynamic,graph_neighbor_size_percent,pir_broadcast_tree_limit,enable_interpretercore_launch_cinn,enable_cse_in_dy2st,nccl_dir,search_cache_max_number,enable_pir_with_pt_in_dy2st,gpugraph_merge_grads_segment_size,enable_all2all_use_fp16,call_stack_level,tensorrt_dir,enable_gpu_memory_usage_log,accuracy_check_rtol_fp16,cuda_malloc_async_pool_memory_throttle_ratio,gpugraph_parallel_stream_num,host_trace_level,auto_free_cudagraph_allocations_on_launch,reader_queue_speed_test_mode,check_nan_inf_level,manually_trans_conv_filter,fraction_of_gpu_memory_to_use,inner_op_parallelism,gpugraph_parallel_copyer_split_maxsize,nccl_blocking_wait,gpugraph_sparse_table_storage_mode,low_precision_op_list,enable_auto_detect_gpu_topo,prim_skip_dynamic,gpugraph_offload_param_extends,set_to_1d,multi_node_sample_use_gpu_table,free_when_no_cache_hit,new_executor_use_cuda_graph,graph_load_in_parallel,enable_neighbor_list_use_uva,prim_forward,tracer_profile_fname,get_host_by_name_time,gpugraph_force_device_batch_num_equal,cublaslt_exhaustive_search_times,reallocate_gpu_memory_in_mb,gpugraph_hbm_table_load_factor,all_blocks_convert_trt,use_cuda_managed_memory,use_stride_kernel,sync_nccl_allreduce,apply_pass_to_program,prim_check_ops,enable_blaslt_global_search,use_cuda_malloc_async_allocator,cache_inference_while_scope,enable_record_memory,check_infer_symbolic,cublaslt_device_best_config,enable_async_trace,use_stream_safe_cuda_allocator,eager_delete_tensor_gb,fraction_of_cuda_pinned_memory_to_use,pinned_memory_as_cpu_backend,fuse_parameter_memory_size,enable_cinn_auto_tune,enable_pir_in_executor,gpugraph_load_node_list_into_hbm,local_exe_sub_scope_limit,gpugraph_enable_segment_merge_grads,use_pinned_memory,deny_cinn_ops,use_system_allocator,new_executor_serial_run,custom_device_mem_record,async_trace_count,cublas_dir,pir_debug,new_executor_use_inplace,dist_threadpool_size,save_static_runtime_data,mklml_dir,use_shm_cache,cudnn_exhaustive_search_times,enable_adjust_op_order,dynamic_static_unified_comm,gpugraph_enable_hbm_table_collision_stat 
1884: I0815 04:03:51.743522  6857 init.cc:108] After Parse: argc is 2
1884: I0815 04:03:59.162279  6857 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:03:59.162319  6857 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:03:59.162972  6857 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:03:59.163429  6857 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:03:59.164208  6857 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:03:59.164290  6857 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:03:59.164386  6857 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:03:59.165031  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f82dc000000), and remaining 0
1884: I0815 04:03:59.165275  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.165338  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.165421  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f82dc000200), and remaining 0
1884: I0815 04:03:59.165446  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f82dc000400), and remaining 0
1884: I0815 04:03:59.169183  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f82dc000600), and remaining 0
1884: I0815 04:03:59.169318  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f82dc000800), and remaining 0
1884: I0815 04:03:59.169380  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f82dc000a00), and remaining 0
1884: I0815 04:03:59.169462  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.169482  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.169548  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.169561  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.170493  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c65880 for it.
1884: I0815 04:03:59.170630  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.170653  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.170708  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f82dc000e00), and remaining 0
1884: I0815 04:03:59.170780  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f82dc0c4400), and remaining 0
1884: I0815 04:03:59.273978  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c65880 for it.
1884: I0815 04:03:59.274174  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.274216  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.274814  6857 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f82dc200000), and remaining 0
1884: I0815 04:03:59.283046  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c65880 for it.
1884: I0815 04:03:59.283145  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:03:59.283178  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.283215  6857 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:03:59.283432  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:03:59.284354  6857 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:03:59.284372  6857 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:03:59.284422  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:03:59.284492  6857 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:03:59.284518  6857 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.284574  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:03:59.284642  6857 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:03:59.284660  6857 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.284694  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:03:59.284850  6857 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:03:59.284869  6857 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.285019  6857 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:03:59.285044  6857 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:03:59.285111  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:03:59.288460  6857 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:03:59.288553  6857 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:03:59.288579  6857 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:03:59.288641  6857 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:04:00.780469  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.780526  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.780786  6857 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:04:00.780805  6857 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.785354  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.785389  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.786278  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.786296  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.786317  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.787015  6857 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:04:00.787029  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:00.787045  6857 scope.cc:202] Create variable feed
1884: I0815 04:04:00.787053  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:00.787061  6857 scope.cc:202] Create variable fetch
1884: I0815 04:04:00.787066  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:00.787077  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:00.787083  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.787087  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.787091  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.789419  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:00.789768  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.789781  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.789785  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.791414  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:00.791460  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:00.791469  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:00.791476  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:00.791482  6857 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:04:00.791491  6857 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x442934b0 type is 7
1884: I0815 04:04:00.791494  6857 scope.cc:202] Create variable x
1884: I0815 04:04:00.791498  6857 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x604c57b0 type is 7
1884: I0815 04:04:00.791555  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:00.791561  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.791565  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.791569  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.791682  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.791703  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.791805  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.791816  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.791831  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.791980  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.792008  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.792027  6857 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:00.792033  6857 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x604cd850Variable Type 7
1884: I0815 04:04:00.792054  6857 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:00.792074  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.792124  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.792142  6857 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.793360  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:00.793408  6857 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:00.793789  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.799130  6857 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:04:00.799150  6857 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:04:00.799238  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.799266  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.799723  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: I0815 04:04:00.799793  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.799816  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.800231  6857 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: I0815 04:04:00.800294  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.800328  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.800351  6857 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.800599  6857 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:04:00.800611  6857 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:04:00.800711  6857 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:04:00.800734  6857 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.801098  6857 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:04:00.801110  6857 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:04:00.801151  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.801170  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.801364  6857 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:04:00.801374  6857 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:04:00.801410  6857 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:04:00.801429  6857 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:04:00.801445  6857 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.803974  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.803997  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.804047  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.804057  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.805907  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:00.806260  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.806275  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.806280  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.808043  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:00.808090  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:00.808100  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:00.808106  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x604f8e00 type is 7
1884: I0815 04:04:00.808116  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.808120  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x604f9170 type is 7
1884: I0815 04:04:00.808125  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:00.808131  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:00.808188  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:00.808197  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.808200  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.808205  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.808249  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.808264  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.808326  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.808338  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.808357  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.808594  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.808609  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.808627  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:00.808635  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x604ff8c0Variable Type 7
1884: I0815 04:04:00.808651  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:00.808669  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.808694  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.808710  6857 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.809401  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:00.809439  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:00.809607  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.819458  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: I0815 04:04:00.819640  6857 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19cadd00 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:04:00.825368  6857 pir_interpreter.cc:161] PirInterpreter(): 0x606bc4e0 on Place(gpu:0)
1884: I0815 04:04:00.825408  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.825438  6857 scope.cc:202] Create variable 0x606bc4e01723694640825395222_inner_var_1
1884: I0815 04:04:00.825448  6857 scope.cc:202] Create variable 0x606bc4e01723694640825395222_inner_var_2
1884: I0815 04:04:00.825459  6857 scope.cc:202] Create variable 0x606bc4e01723694640825395222_inner_var_3
1884: I0815 04:04:00.825469  6857 scope.cc:202] Create variable 0x606bc4e01723694640825395222_inner_var_4
1884: I0815 04:04:00.825477  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:00.825862  6857 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:04:00.825877  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.825882  6857 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:04:00.825922  6857 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x606bc440
1884: 1 -> 0x606bc4e01723694640825395222_inner_var_1 -> 0x606bc4c0
1884: 2 -> 0x606bc4e01723694640825395222_inner_var_2 -> 0x606bcd70
1884: 3 -> 0x606bc4e01723694640825395222_inner_var_3 -> 0x606bb6f0
1884: 4 -> 0x606bc4e01723694640825395222_inner_var_4 -> 0x606bd120
1884: 5 -> fetch0@fetch -> 0x606bd930
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:04:00.826653  6857 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:04:00.826877  6895 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:00.826990  6896 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:00.827109  6897 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:00.827129  6898 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:00.827194  6899 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:00.827224  6898 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x606bc4e01723694640825395222_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.827304  6900 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:00.827335  6898 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x606bc4e01723694640825395222_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:00.827347  6900 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.827391  6900 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:04:00.827436  6900 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x606bc4e01723694640825395222_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x606bc4e01723694640825395222_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.827629  6900 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x606bc4e01723694640825395222_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x606bc4e01723694640825395222_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:04:00.827697  6898 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x606bc4e01723694640825395222_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.827720  6898 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.828945  6898 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x606bc4e01723694640825395222_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x606bc4e01723694640825395222_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:04:00.828987  6898 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x606bc4e01723694640825395222_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.829015  6898 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:00.829603  6898 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x606bc4e01723694640825395222_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:04:00.829641  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x606bc650) got event_name: TaskCompletion
1884: I0815 04:04:00.829665  6857 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:00.903020  6895 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 12625370169531976897 to 8508920150491387091 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:04:00.903045  6895 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 12625370169531976897 to 10144981446558951206 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:04:00.903050  6895 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 12625370169531976897 to 10144981446558951206 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:04:00.903224  6898 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 10144981446558951206 to 440294949534955838 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:04:00.903239  6898 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 10144981446558951206 to 440294949534955838 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:04:00.903417  6900 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:04:00.903429  6900 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:04:00.909286  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.909319  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.909375  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.909384  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.911160  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:00.911518  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.911532  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.911537  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.913072  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:00.913161  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:00.913170  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:00.913177  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x606bea20 type is 7
1884: I0815 04:04:00.913183  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.913187  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x420b0f0 type is 7
1884: I0815 04:04:00.913192  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:00.913195  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:00.913251  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:00.913257  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.913261  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.913265  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.913323  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.913338  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.913393  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.913401  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.913415  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.913547  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.913558  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.913573  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:00.913579  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x41f7030Variable Type 7
1884: I0815 04:04:00.913594  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:00.913610  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.913631  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.913645  6857 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.915226  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:00.915263  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:00.915462  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.924158  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19cadd00 for it.
1884: I0815 04:04:00.924350  6857 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:04:00.927376  6857 pir_interpreter.cc:161] PirInterpreter(): 0x4c125800 on Place(gpu:0)
1884: I0815 04:04:00.927409  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.927431  6857 scope.cc:202] Create variable 0x4c1258001723694640927401117_inner_var_1
1884: I0815 04:04:00.927443  6857 scope.cc:202] Create variable 0x4c1258001723694640927401117_inner_var_2
1884: I0815 04:04:00.927454  6857 scope.cc:202] Create variable 0x4c1258001723694640927401117_inner_var_3
1884: I0815 04:04:00.927465  6857 scope.cc:202] Create variable 0x4c1258001723694640927401117_inner_var_4
1884: I0815 04:04:00.927477  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:00.927796  6857 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:04:00.927811  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.927815  6857 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x41f6230
1884: 1 -> 0x4c1258001723694640927401117_inner_var_1 -> 0x60696410
1884: 2 -> 0x4c1258001723694640927401117_inner_var_2 -> 0x421bbe0
1884: 3 -> 0x4c1258001723694640927401117_inner_var_3 -> 0x604d26e0
1884: 4 -> 0x4c1258001723694640927401117_inner_var_4 -> 0x42187a0
1884: 5 -> fetch0@fetch -> 0x60696d50
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:04:00.928511  6901 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:00.928608  6903 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:00.928619  6902 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:00.928638  6904 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:00.928678  6905 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:00.928725  6905 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4c1258001723694640927401117_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.928829  6905 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4c1258001723694640927401117_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:00.928903  6906 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:00.928934  6906 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.928967  6906 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:04:00.929013  6906 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4c1258001723694640927401117_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4c1258001723694640927401117_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.929148  6906 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x4c1258001723694640927401117_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x4c1258001723694640927401117_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:04:00.929239  6905 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4c1258001723694640927401117_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.929296  6905 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.932159  6905 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x4c1258001723694640927401117_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x4c1258001723694640927401117_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:04:00.932214  6905 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x4c1258001723694640927401117_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.932240  6905 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:00.934309  6905 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x4c1258001723694640927401117_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:04:00.934360  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x4c125970) got event_name: TaskCompletion
1884: I0815 04:04:00.934384  6857 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:00.972472  6901 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8508920150491387091 to 9334651537925342488 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:04:00.972486  6901 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8508920150491387091 to 5833715745318962027 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:04:00.972492  6901 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8508920150491387091 to 5833715745318962027 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:04:00.972666  6905 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 5833715745318962027 to 440294949534955838 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:04:00.972676  6905 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 5833715745318962027 to 440294949534955838 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:04:00.972862  6906 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 9334651537925342488 to 440294949534955838 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:04:00.976562  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.976584  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.976632  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:00.976640  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.978215  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:00.978542  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.978556  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.978561  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.980027  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:00.980109  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:00.980119  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:00.980125  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x42311b0 type is 7
1884: I0815 04:04:00.980135  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.980139  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x48329880 type is 7
1884: I0815 04:04:00.980144  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:00.980149  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:00.980201  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:00.980206  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:00.980211  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:00.980214  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:00.980257  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.980270  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.980331  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.980340  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.980355  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:00.980394  6857 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.980530  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:00.980592  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.980602  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:00.980616  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:00.980623  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62e01490Variable Type 7
1884: I0815 04:04:00.980638  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:00.980654  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.980675  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:00.980690  6857 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.980976  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:00.980996  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:00.981166  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:00.981897  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: I0815 04:04:00.982064  6857 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19cadd00 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:04:00.984941  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62ce56b0 on Place(gpu:0)
1884: I0815 04:04:00.984973  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.984993  6857 scope.cc:202] Create variable 0x62ce56b01723694640984964523_inner_var_1
1884: I0815 04:04:00.985004  6857 scope.cc:202] Create variable 0x62ce56b01723694640984964523_inner_var_2
1884: I0815 04:04:00.985016  6857 scope.cc:202] Create variable 0x62ce56b01723694640984964523_inner_var_3
1884: I0815 04:04:00.985028  6857 scope.cc:202] Create variable 0x62ce56b01723694640984964523_inner_var_4
1884: I0815 04:04:00.985038  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:00.985368  6857 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:04:00.985384  6857 scope.cc:202] Create variable X
1884: I0815 04:04:00.985388  6857 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x604a6b20
1884: 1 -> 0x62ce56b01723694640984964523_inner_var_1 -> 0x442740e0
1884: 2 -> 0x62ce56b01723694640984964523_inner_var_2 -> 0x13d5850
1884: 3 -> 0x62ce56b01723694640984964523_inner_var_3 -> 0x6049f500
1884: 4 -> 0x62ce56b01723694640984964523_inner_var_4 -> 0x41dd040
1884: 5 -> fetch0@fetch -> 0x604a7100
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:04:00.986038  6907 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:00.986127  6908 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:00.986133  6909 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:00.986172  6910 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:00.986204  6911 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:00.986229  6911 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62ce56b01723694640984964523_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.986287  6911 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62ce56b01723694640984964523_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:00.986358  6912 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:00.986382  6912 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.986406  6912 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:04:00.986441  6912 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62ce56b01723694640984964523_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62ce56b01723694640984964523_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.986478  6912 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.986582  6912 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:00.986608  6912 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62ce56b01723694640984964523_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62ce56b01723694640984964523_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:04:00.986673  6911 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62ce56b01723694640984964523_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.986714  6911 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:00.986977  6911 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62ce56b01723694640984964523_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62ce56b01723694640984964523_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:04:00.987010  6911 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62ce56b01723694640984964523_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:00.987031  6911 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:04:00.987049  6911 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62ce56b01723694640984964523_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:04:00.987090  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62ce5820) got event_name: TaskCompletion
1884: I0815 04:04:00.987116  6857 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.018710  6907 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:04:01.018723  6907 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:04:01.018728  6907 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:04:01.018919  6911 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1948461582148395452 to 8508920150491387091 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:04:01.018929  6911 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1948461582148395452 to 8508920150491387091 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:04:01.019091  6912 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:04:01.019102  6912 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:04:01.019107  6912 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:04:01.024399  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.024422  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.024467  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.024475  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.026053  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.026388  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.026401  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.026405  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.027947  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:01.028023  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.028033  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:01.028038  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x604ae590 type is 7
1884: I0815 04:04:01.028045  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.028048  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x604e9170 type is 7
1884: I0815 04:04:01.028053  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.028059  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.028110  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.028115  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.028120  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.028122  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.028167  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.028179  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.028234  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.028241  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.028255  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.028395  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.028406  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.028421  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.028427  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62ceb290Variable Type 7
1884: I0815 04:04:01.028441  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.028458  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.028478  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.028492  6857 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.030125  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.030160  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:01.030362  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.034657  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: I0815 04:04:01.034833  6857 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19caf5a0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:04:01.037820  6857 pir_interpreter.cc:161] PirInterpreter(): 0x625e9130 on Place(gpu:0)
1884: I0815 04:04:01.037853  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.037873  6857 scope.cc:202] Create variable 0x625e91301723694641037845043_inner_var_1
1884: I0815 04:04:01.037884  6857 scope.cc:202] Create variable 0x625e91301723694641037845043_inner_var_2
1884: I0815 04:04:01.037896  6857 scope.cc:202] Create variable 0x625e91301723694641037845043_inner_var_3
1884: I0815 04:04:01.037906  6857 scope.cc:202] Create variable 0x625e91301723694641037845043_inner_var_4
1884: I0815 04:04:01.037918  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:01.038231  6857 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:04:01.038247  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.038251  6857 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x625e8fb0
1884: 1 -> 0x625e91301723694641037845043_inner_var_1 -> 0x607dea40
1884: 2 -> 0x625e91301723694641037845043_inner_var_2 -> 0x5170880
1884: 3 -> 0x625e91301723694641037845043_inner_var_3 -> 0x62df0fa0
1884: 4 -> 0x625e91301723694641037845043_inner_var_4 -> 0x604c9990
1884: 5 -> fetch0@fetch -> 0x60691570
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:04:01.038925  6913 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.038997  6914 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.039024  6915 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.039101  6916 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.039101  6917 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.039146  6918 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:01.039137  6916 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625e91301723694641037845043_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.039180  6918 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.039207  6916 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x625e91301723694641037845043_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.039229  6918 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:04:01.039266  6918 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625e91301723694641037845043_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625e91301723694641037845043_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.039402  6918 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x625e91301723694641037845043_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x625e91301723694641037845043_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:04:01.039459  6916 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625e91301723694641037845043_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.039484  6916 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.042232  6916 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x625e91301723694641037845043_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x625e91301723694641037845043_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:04:01.042280  6916 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x625e91301723694641037845043_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.042309  6916 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.044376  6916 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x625e91301723694641037845043_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:04:01.044426  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x625e92a0) got event_name: TaskCompletion
1884: I0815 04:04:01.044448  6857 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.050190  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.050215  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.050262  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.050271  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.052090  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.052489  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.052505  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.052510  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.054374  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:01.054454  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.054467  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:01.054473  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x604dea80 type is 7
1884: I0815 04:04:01.054481  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.054484  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6254f790 type is 7
1884: I0815 04:04:01.054489  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.054498  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.054559  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.054566  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.054571  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.054576  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.054622  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.054641  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.054699  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.054714  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.054734  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.054864  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.054876  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.054894  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.054901  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62c9f4f0Variable Type 7
1884: I0815 04:04:01.054919  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.054937  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.054960  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.054976  6857 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.056634  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.056669  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:01.056845  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.102392  6913 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8508920150491387091 to 9334651537925342488 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:04:01.102407  6913 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8508920150491387091 to 7309683420481334815 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:04:01.102416  6913 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8508920150491387091 to 7309683420481334815 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:04:01.102651  6916 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7309683420481334815 to 440294949534955838 , after update, data is {current : 5600800, peak : 5600800}.
1884: I0815 04:04:01.102663  6916 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 7309683420481334815 to 440294949534955838 , after update, data is {current : 5600800, peak : 8000800}.
1884: I0815 04:04:01.102807  6918 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 9334651537925342488 to 440294949534955838 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:04:01.107517  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.107549  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.107602  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.107610  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.109310  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.109645  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.109658  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.109663  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.111182  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:01.111268  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.111279  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:01.111284  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x606be370 type is 7
1884: I0815 04:04:01.111290  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.111297  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x19cbf680 type is 7
1884: I0815 04:04:01.111308  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.111312  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.111364  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.111371  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.111375  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.111378  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.111423  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.111435  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.111490  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.111500  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.111513  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.111549  6857 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.111675  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.111721  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.111730  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.111744  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.111752  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x604bcd80Variable Type 7
1884: I0815 04:04:01.111766  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.111783  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.111804  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.111819  6857 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.111922  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.111943  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:01.112125  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.112880  6857 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19caf5a0 for it.
1884: I0815 04:04:01.113054  6857 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c807f0 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:04:01.116020  6857 pir_interpreter.cc:161] PirInterpreter(): 0x6256a810 on Place(gpu:0)
1884: I0815 04:04:01.116053  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.116073  6857 scope.cc:202] Create variable 0x6256a8101723694641116045160_inner_var_1
1884: I0815 04:04:01.116086  6857 scope.cc:202] Create variable 0x6256a8101723694641116045160_inner_var_2
1884: I0815 04:04:01.116094  6857 scope.cc:202] Create variable 0x6256a8101723694641116045160_inner_var_3
1884: I0815 04:04:01.116106  6857 scope.cc:202] Create variable 0x6256a8101723694641116045160_inner_var_4
1884: I0815 04:04:01.116117  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:01.116452  6857 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:04:01.116468  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.116472  6857 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62550100
1884: 1 -> 0x6256a8101723694641116045160_inner_var_1 -> 0x63096880
1884: 2 -> 0x6256a8101723694641116045160_inner_var_2 -> 0x422f710
1884: 3 -> 0x6256a8101723694641116045160_inner_var_3 -> 0x19c64c20
1884: 4 -> 0x6256a8101723694641116045160_inner_var_4 -> 0x630968e0
1884: 5 -> fetch0@fetch -> 0x60478ae0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:04:01.117126  6919 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.117210  6920 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.117223  6921 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.117267  6922 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.117309  6923 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.117377  6924 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:01.117388  6923 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6256a8101723694641116045160_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.117403  6924 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.117441  6924 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:04:01.117448  6923 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6256a8101723694641116045160_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.117476  6924 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6256a8101723694641116045160_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6256a8101723694641116045160_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.117517  6924 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.117626  6924 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.117653  6924 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6256a8101723694641116045160_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6256a8101723694641116045160_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:04:01.117731  6923 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6256a8101723694641116045160_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.117774  6923 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.117929  6923 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6256a8101723694641116045160_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x6256a8101723694641116045160_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:04:01.117959  6923 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6256a8101723694641116045160_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.117978  6923 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.117992  6923 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6256a8101723694641116045160_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:04:01.118023  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x6256a980) got event_name: TaskCompletion
1884: I0815 04:04:01.118048  6857 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.119426  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.119448  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.119498  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.119508  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.121317  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.121701  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.121716  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.121721  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.123519  6857 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:04:01.123600  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.123610  6857 scope.cc:202] Create variable Out
1884: I0815 04:04:01.123620  6857 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x6049c150 type is 7
1884: I0815 04:04:01.123628  6857 scope.cc:202] Create variable X
1884: I0815 04:04:01.123634  6857 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6049b490 type is 7
1884: I0815 04:04:01.123641  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.123646  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.123709  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.123716  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.123721  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.123726  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.123767  6857 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.123781  6857 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.123837  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.123847  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.123865  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.123898  6857 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.123989  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.124038  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.124050  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.124071  6857 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.124078  6857 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6307e8f0Variable Type 7
1884: I0815 04:04:01.124095  6857 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.124114  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.124136  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.124152  6857 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.124238  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.124260  6857 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:01.124459  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.157362  6919 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:04:01.157377  6919 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:04:01.157382  6919 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 9334651537925342488 to 8508920150491387091 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:04:01.157579  6923 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1948461582148395452 to 8508920150491387091 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:04:01.157591  6923 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 1948461582148395452 to 8508920150491387091 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:04:01.157752  6924 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 5601600, peak : 5608800}.
1884: I0815 04:04:01.157763  6924 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 5601600, peak : 8000800}.
1884: I0815 04:04:01.157768  6924 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 8508920150491387091 to 440294949534955838 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:04:01.164043  6857 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:04:01.164088  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:04:01.165108  6857 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:04:01.165918  6857 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:04:01.165947  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:04:01.167187  6857 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:04:01.167212  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:04:01.167910  6857 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:04:01.168864  6857 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:04:01.168890  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:04:01.170163  6857 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:04:01.170186  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:04:01.170768  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.170795  6857 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:04:01.170801  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.170809  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.172684  6857 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:04:01.172710  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:04:01.173640  6857 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:04:01.173667  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:04:01.174530  6857 pybind.cc:1827] need skip: 0
1884: I0815 04:04:01.174798  6857 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:04:01.176493  6857 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:04:01.179920  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.179939  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.179944  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.181875  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.181895  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.181901  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.181913  6857 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:04:01.181917  6857 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6219bd10 type is 7
1884: I0815 04:04:01.181921  6857 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:04:01.181926  6857 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6219cc70 type is 7
1884: I0815 04:04:01.181929  6857 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:04:01.181932  6857 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6219ccf0 type is 7
1884: I0815 04:04:01.181991  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.181998  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.182001  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.182005  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.182046  6857 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182058  6857 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182076  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:04:01.182186  6857 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182196  6857 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182255  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.182291  6857 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182307  6857 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.182333  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.183265  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.184592  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.185046  6857 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:04:01.185262  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.185555  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.185760  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.185774  6857 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:04:01.185837  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.185844  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.185848  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.185945  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.185957  6857 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:04:01.187518  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.188841  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.189934  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.190115  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.190127  6857 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:04:01.190135  6857 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x62b79c80 type is 7
1884: I0815 04:04:01.190140  6857 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:04:01.190145  6857 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x62b79a00 type is 7
1884: I0815 04:04:01.190148  6857 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:04:01.190151  6857 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x62b79af0 type is 7
1884: I0815 04:04:01.190155  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.190160  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.190164  6857 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:04:01.190168  6857 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x62b7c030 type is 7
1884: I0815 04:04:01.190172  6857 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x6219bd10 type is 7
1884: I0815 04:04:01.190176  6857 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6219cc70 type is 7
1884: I0815 04:04:01.190181  6857 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:04:01.190184  6857 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x62b7c010 type is 7
1884: I0815 04:04:01.190188  6857 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:04:01.190191  6857 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x62b7c570 type is 7
1884: I0815 04:04:01.190194  6857 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6219ccf0 type is 7
1884: I0815 04:04:01.190199  6857 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:04:01.190202  6857 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x62b7c7e0 type is 7
1884: I0815 04:04:01.190207  6857 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:04:01.190209  6857 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x62b7ca20 type is 7
1884: I0815 04:04:01.190213  6857 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:04:01.190217  6857 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x62b7cc80 type is 7
1884: I0815 04:04:01.190294  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.190316  6857 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:04:01.190374  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.190380  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.190384  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.190388  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.190431  6857 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.190443  6857 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.190457  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:04:01.190555  6857 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.190567  6857 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.190583  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:04:01.190649  6857 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:04:01.190723  6857 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:04:01.191787  6857 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.191805  6857 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.191862  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.191923  6857 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.191933  6857 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.191946  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:04:01.191968  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.192001  6857 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192010  6857 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192021  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:04:01.192098  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192107  6857 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192121  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.192215  6857 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.192293  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.192355  6857 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192366  6857 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192379  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:04:01.192410  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.192452  6857 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192461  6857 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192474  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:04:01.192571  6857 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192581  6857 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192600  6857 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.192641  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.192651  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.192665  6857 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.192672  6857 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x630868a0Variable Type 7
1884: I0815 04:04:01.192687  6857 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.192703  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.192720  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192734  6857 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.192772  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.192795  6857 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:04:01.192819  6857 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.192828  6857 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.192840  6857 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:04:01.192847  6857 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62b7d550Variable Type 7
1884: I0815 04:04:01.192858  6857 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:04:01.192869  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.192884  6857 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.192896  6857 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.192929  6857 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:04:01.192942  6857 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:04:01.193343  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:04:01.193375  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:04:01.193392  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:04:01.193423  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:04:01.193452  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.193470  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:04:01.198341  6857 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:04:01.198375  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:04:01.199052  6857 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:04:01.199074  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:04:01.199402  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.201045  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.201871  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.201990  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.202497  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.203380  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.205463  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.206501  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.208246  6857 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:04:01.209012  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.209028  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.209031  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.210173  6857 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:04:01.210191  6857 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x422f140 type is 9
1884: I0815 04:04:01.210196  6857 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x41ff790 type is 10
1884: I0815 04:04:01.210203  6857 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x6219cc70 type is 7
1884: I0815 04:04:01.210207  6857 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x6219ccf0 type is 7
1884: I0815 04:04:01.210211  6857 scope.cc:202] Create variable saved_params
1884: I0815 04:04:01.210215  6857 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x62f5e340 type is 17
1884: I0815 04:04:01.210242  6857 interpreter_util.cc:594] Static build: 0
1884: I0815 04:04:01.210248  6857 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:04:01.210251  6857 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:04:01.210255  6857 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:04:01.210289  6857 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.210307  6857 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:04:01.210943  6857 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:04:01.210983  6857 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:04:01.211035  6857 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:04:01.212154  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.212211  6857 scope.cc:202] Create variable feed
1884: I0815 04:04:01.212219  6857 naive_executor.cc:189] 0x62cd7c00 Create persistable variable feed, which pointer is 0x6224be20
1884: I0815 04:04:01.212224  6857 scope.cc:202] Create variable fetch
1884: I0815 04:04:01.212227  6857 naive_executor.cc:189] 0x62cd7c00 Create persistable variable fetch, which pointer is 0x6224bcc0
1884: I0815 04:04:01.212231  6857 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:04:01.212234  6857 naive_executor.cc:189] 0x62cd7c00 Create persistable variable linear_0.b_0, which pointer is 0x62cd7d60
1884: I0815 04:04:01.212239  6857 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:04:01.212241  6857 naive_executor.cc:189] 0x62cd7c00 Create persistable variable linear_0.w_0, which pointer is 0x62c91560
1884: I0815 04:04:01.212256  6857 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:04:01.212594  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.212677  6857 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:04:01.212720  6857 executor.cc:183] Old Executor is Running.
1884: I0815 04:04:01.212785  6857 executor.cc:92] Creating Variables for block 0
1884: I0815 04:04:01.212792  6857 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:04:01.212795  6857 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x62cd7d60 type is 7
1884: I0815 04:04:01.212800  6857 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:04:01.212802  6857 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x62c91560 type is 7
1884: I0815 04:04:01.212831  6857 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.212899  6857 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:04:01.212939  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.212945  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.213075  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.213179  6857 graph.cc:149] create OpNode by feed
1884: I0815 04:04:01.213212  6857 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:04:01.213227  6857 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:04:01.213240  6857 graph.cc:149] create OpNode by abs
1884: I0815 04:04:01.213251  6857 graph.cc:149] create OpNode by assign_value
1884: I0815 04:04:01.213267  6857 graph.cc:149] create OpNode by multinomial
1884: I0815 04:04:01.213277  6857 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:04:01.213294  6857 graph.cc:149] create OpNode by scale
1884: I0815 04:04:01.213317  6857 graph.cc:149] create OpNode by scale
1884: I0815 04:04:01.213330  6857 graph.cc:149] create OpNode by fetch
1884: I0815 04:04:01.213348  6857 graph.cc:149] create OpNode by fetch
1884: I0815 04:04:01.213369  6857 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:04:01.214533  6857 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:04:01.214541  6857 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:04:01.214605  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.214612  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:04:01.214715  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.214964  6857 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:04:01.215023  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215029  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:04:01.215059  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215065  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:04:01.215102  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215162  6857 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:04:01.215193  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215198  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:04:01.215215  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215229  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215250  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215255  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:04:01.215293  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215323  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215348  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215353  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:04:01.215395  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215472  6857 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:04:01.215499  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215504  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:04:01.215535  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215555  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215577  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215582  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:04:01.215608  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215757  6857 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:04:01.215786  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215792  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:04:01.215822  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215839  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215862  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215866  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:04:01.215886  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215900  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215922  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215927  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:04:01.215948  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.215962  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.215983  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.215988  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:04:01.216012  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.216078  6857 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:04:01.216109  6857 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:04:01.216125  6857 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:04:01.216140  6857 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:04:01.216163  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.216169  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:04:01.216192  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.216230  6857 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:04:01.216250  6857 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:04:01.216262  6857 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:04:01.216274  6857 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:04:01.216310  6857 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:04:01.216320  6857 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:04:01.217486  6857 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:04:01.217533  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.217540  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:04:01.217564  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.217584  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.217612  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.217617  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:04:01.217639  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.217687  6857 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:04:01.217718  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.217723  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:04:01.217741  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.217756  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.217779  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.217784  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:04:01.217818  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.217904  6857 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:04:01.217929  6857 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:04:01.217944  6857 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:04:01.217959  6857 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:04:01.217975  6857 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:04:01.217990  6857 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:04:01.218005  6857 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:04:01.218029  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218101  6857 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:04:01.218124  6857 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:04:01.218137  6857 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:04:01.218150  6857 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:04:01.218164  6857 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:04:01.218179  6857 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:04:01.218195  6857 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:04:01.218240  6857 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:04:01.218504  6857 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:04:01.218534  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.218539  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:04:01.218587  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218647  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218680  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218726  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218755  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218796  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218822  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218858  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218879  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218914  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218931  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.218962  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.218978  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219004  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219018  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219041  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219053  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219072  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219097  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219102  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:04:01.219128  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219167  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219192  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219198  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:04:01.219208  6857 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:04:01.219210  6857 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:04:01.219257  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219280  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219310  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219316  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:04:01.219326  6857 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:04:01.219328  6857 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:04:01.219370  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219393  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219417  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219424  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:04:01.219431  6857 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:04:01.219434  6857 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:04:01.219466  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219485  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219507  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219513  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:04:01.219521  6857 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:04:01.219524  6857 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:04:01.219563  6857 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:04:01.219583  6857 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:04:01.219606  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219611  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:04:01.219623  6857 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:04:01.219662  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219667  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:04:01.219733  6857 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:04:01.219753  6857 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.219769  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:04:01.219816  6857 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:04:01.219833  6857 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:04:01.219858  6857 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:04:01.219882  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.219887  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:04:01.220753  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.220768  6857 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:04:01.220819  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.220825  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.221438  6857 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:04:01.221649  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.221721  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.221727  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.222129  6857 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:04:01.222332  6857 graph.h:183] deleting __fuse_statis__
1884: I0815 04:04:01.222340  6857 graph.h:183] deleting pass_recorder
1884: I0815 04:04:01.222347  6857 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:04:01.222424  6857 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:04:01.222433  6857 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:04:01.222437  6857 naive_executor.cc:195] 0x62cd7c00 Create variable abs_0.tmp_0, which pointer is 0x62284310
1884: I0815 04:04:01.222443  6857 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:04:01.222448  6857 naive_executor.cc:195] 0x62cd7c00 Create variable gaussian_0.tmp_0, which pointer is 0x621c47a0
1884: I0815 04:04:01.222460  6857 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:04:01.222465  6857 naive_executor.cc:195] 0x62cd7c00 Create variable linear_0.tmp_1, which pointer is 0x62cda8f0
1884: I0815 04:04:01.222468  6857 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:04:01.222471  6857 naive_executor.cc:195] 0x62cd7c00 Create variable multinomial_0.tmp_0, which pointer is 0x62cda390
1884: I0815 04:04:01.222476  6857 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:04:01.222478  6857 naive_executor.cc:195] 0x62cd7c00 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62cda690
1884: I0815 04:04:01.222481  6857 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:04:01.222484  6857 naive_executor.cc:195] 0x62cd7c00 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x62549ab0
1884: I0815 04:04:01.222491  6857 scope.cc:202] Create variable feed
1884: I0815 04:04:01.222494  6857 scope.cc:202] Create variable fetch
1884: I0815 04:04:01.222512  6857 naive_executor.cc:46] NaiveExecutor init with scope 0x62cd7c00
1884: I0815 04:04:01.222518  6857 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:04:01.222702  6857 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:04:01.222715  6857 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:04:01.222741  6857 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:04:01.222748  6857 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:04:01.222754  6857 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.222783  6857 helper.h:475] Init predictor : [cpu current allocated memory: 5.3423MB], [cpu current reserved memory: 5.3423MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:04:01.222975  6857 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.222992  6857 helper.h:475] before run : [cpu current allocated memory: 5.34235MB], [cpu current reserved memory: 5.34235MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:04:01.223037  6857 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.223060  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:04:01.252272  6857 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:04:01.252393  6857 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.252421  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:04:01.252489  6857 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:04:01.252521  6857 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.252545  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:04:01.252614  6857 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:04:01.252658  6857 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.252676  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:04:01.252729  6857 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:04:01.252758  6857 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:04:01.252774  6857 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:04:01.252810  6857 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:04:01.252827  6857 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.252861  6857 helper.h:475] after run : [cpu current allocated memory: 5.34283MB], [cpu current reserved memory: 5.34283MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:04:01.252887  6857 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:04:01.253365  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.253376  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:04:01.294210  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62b7b7c0 on Place(gpu:0)
1884: I0815 04:04:01.294240  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_0
1884: I0815 04:04:01.294255  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_1
1884: I0815 04:04:01.294263  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_2
1884: I0815 04:04:01.294272  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_3
1884: I0815 04:04:01.294310  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_4
1884: I0815 04:04:01.294327  6857 scope.cc:202] Create variable 0x62b7b7c01723694641294231826_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x62b7b7c01723694641294231826_inner_var_0 -> 0x442758a0
1884: 1 -> 0x62b7b7c01723694641294231826_inner_var_1 -> 0x6226caa0
1884: 2 -> 0x62b7b7c01723694641294231826_inner_var_2 -> 0x43d0a70
1884: 3 -> linear_1.w_0 -> 0x62fa69b0
1884: 4 -> linear_1.b_0 -> 0x604ebd60
1884: 5 -> learning_rate_1 -> 0x604c70a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:04:01.295105  6925 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.295125  6926 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.295150  6927 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.295182  6928 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.295222  6929 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:01.295220  6928 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295233  6926 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295248  6929 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295235  6927 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295254  6928 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:04:01.295293  6929 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.295295  6926 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.295302  6927 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62b7b7c01723694641294231826_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.295325  6929 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:04:01.295343  6929 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295358  6929 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.295367  6929 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:04:01.295382  6929 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x62b7b7c01723694641294231826_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62b7b7c01723694641294231826_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62b7b7c01723694641294231826_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.295434  6929 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x62b7b7c01723694641294231826_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62b7b7c01723694641294231826_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62b7b7c01723694641294231826_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:04:01.295482  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62b7b930) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:04:01.297619  6857 pir_interpreter.cc:161] PirInterpreter(): 0x6224c720 on Place(gpu:0)
1884: I0815 04:04:01.297652  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_1
1884: I0815 04:04:01.297667  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_4
1884: I0815 04:04:01.297677  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_5
1884: I0815 04:04:01.297684  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_6
1884: I0815 04:04:01.297708  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_7
1884: I0815 04:04:01.297719  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_8
1884: I0815 04:04:01.297729  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_9
1884: I0815 04:04:01.297756  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_10
1884: I0815 04:04:01.297767  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_11
1884: I0815 04:04:01.297775  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_12
1884: I0815 04:04:01.297784  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_13
1884: I0815 04:04:01.297796  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_14
1884: I0815 04:04:01.297806  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_15
1884: I0815 04:04:01.297814  6857 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:04:01.297825  6857 scope.cc:202] Create variable 0x6224c7201723694641297641397_inner_var_17
1884: I0815 04:04:01.297834  6857 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x604c70a0
1884: 1 -> 0x6224c7201723694641297641397_inner_var_1 -> 0x63097440
1884: 2 -> linear_1.b_0 -> 0x604ebd60
1884: 3 -> linear_1.w_0 -> 0x62fa69b0
1884: 4 -> 0x6224c7201723694641297641397_inner_var_4 -> 0x59c2b8f0
1884: 5 -> 0x6224c7201723694641297641397_inner_var_5 -> 0x62b8e410
1884: 6 -> 0x6224c7201723694641297641397_inner_var_6 -> 0x6229ce40
1884: 7 -> 0x6224c7201723694641297641397_inner_var_7 -> 0x62fa7450
1884: 8 -> 0x6224c7201723694641297641397_inner_var_8 -> 0x622b3580
1884: 9 -> 0x6224c7201723694641297641397_inner_var_9 -> 0x6307d0b0
1884: 10 -> 0x6224c7201723694641297641397_inner_var_10 -> 0x62b8e260
1884: 11 -> 0x6224c7201723694641297641397_inner_var_11 -> 0x606c1dc0
1884: 12 -> 0x6224c7201723694641297641397_inner_var_12 -> 0x6222a8a0
1884: 13 -> 0x6224c7201723694641297641397_inner_var_13 -> 0x6228d900
1884: 14 -> 0x6224c7201723694641297641397_inner_var_14 -> 0x621d82c0
1884: 15 -> 0x6224c7201723694641297641397_inner_var_15 -> 0x59c2b9b0
1884: 16 -> fetch0@fetch -> 0x606c2610
1884: 17 -> 0x6224c7201723694641297641397_inner_var_17 -> 0x62c8d120
1884: 18 -> fetch1@fetch -> 0x62ad4500
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:04:01.299465  6930 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.299595  6931 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.299603  6932 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.299675  6933 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.299680  6932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.299685  6931 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.299711  6931 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.299716  6932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:04:01.299733  6934 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:04:01.299784  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.299818  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:04:01.299846  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.299888  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.299924  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:04:01.299940  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:04:01.299984  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:04:01.300001  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x6224c7201723694641297641397_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300036  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x6224c7201723694641297641397_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:04:01.300061  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x6224c7201723694641297641397_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300093  6934 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:04:01.300123  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x6224c7201723694641297641397_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.300149  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x6224c7201723694641297641397_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x6224c7201723694641297641397_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300187  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.300202  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x6224c7201723694641297641397_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x6224c7201723694641297641397_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.300230  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x6224c7201723694641297641397_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300251  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.300251  6932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6224c7201723694641297641397_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300261  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x6224c7201723694641297641397_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.300272  6932 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.300276  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x6224c7201723694641297641397_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300295  6934 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.300333  6932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6224c7201723694641297641397_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.300360  6932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6224c7201723694641297641397_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300369  6934 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.300377  6932 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.300390  6932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6224c7201723694641297641397_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.300419  6934 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.300480  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.300498  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6224c7201723694641297641397_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x6224c7201723694641297641397_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.300529  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300536  6932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300550  6932 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:04:01.300552  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.300565  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.300581  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x6224c7201723694641297641397_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300582  6932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6224c7201723694641297641397_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.300606  6932 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6224c7201723694641297641397_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300621  6932 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.300633  6932 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6224c7201723694641297641397_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.300647  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x6224c7201723694641297641397_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:04:01.300663  6934 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x6224c7201723694641297641397_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6224c7201723694641297641397_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.300688  6934 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:04:01.300704  6934 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x6224c7201723694641297641397_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6224c7201723694641297641397_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6224c7201723694641297641397_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:04:01.300738  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x6224c890) got event_name: TaskCompletion
1884: I0815 04:04:01.300761  6857 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.300786  6857 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:04:01.306249  6857 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:04:01.306296  6857 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:04:01.306946  6857 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:04:01.306994  6857 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:04:01.307403  6857 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413074547740"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413074547740"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:04:01.307575  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62cf1830 on Place(cpu)
1884: I0815 04:04:01.307595  6857 scope.cc:202] Create variable 0x62cf18301723694641307589184_inner_var_0
1884: I0815 04:04:01.307617  6857 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413074547740"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236946413074547740 -> 0x63081ea0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:04:01.307745  6857 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:04:01.307850  6935 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.307999  6936 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.308044  6937 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.308070  6938 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.308079  6936 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236946413074547740:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.308111  6936 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236946413074547740:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:04:01.308125  6939 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.308133  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62cf19a0) got event_name: TaskCompletion
1884: I0815 04:04:01.308377  6936 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13605281445671434995 to 2664012504097671172 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:04:01.308387  6936 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13605281445671434995 to 2664012504097671172 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:04:01.308439  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.308446  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413085081261"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413085081261"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:04:01.308641  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62cf1830 on Place(cpu)
1884: I0815 04:04:01.308660  6857 scope.cc:202] Create variable 0x62cf18301723694641308654903_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413085081261"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236946413085081261 -> 0x622be5a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:04:01.308854  6940 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.308907  6941 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.308928  6942 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.308956  6943 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.308979  6944 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.308977  6943 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236946413085081261:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.309023  6943 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236946413085081261:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.309046  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62cf19a0) got event_name: TaskCompletion
1884: I0815 04:04:01.309207  6943 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 3089038154725411819 to 2664012504097671172 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:04:01.309216  6943 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 3089038154725411819 to 2664012504097671172 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:04:01.309319  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.309326  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236946413085081261",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236946413093984802"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236946413085081261",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236946413093984802"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:04:01.309545  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62cf1830 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236946413085081261",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236946413093984802"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236946413093984802 -> 0x622be5a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:04:01.309787  6945 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.309839  6946 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.309855  6947 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.309883  6948 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.309906  6949 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.309904  6948 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.309927  6948 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.309950  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62cf19a0) got event_name: TaskCompletion
1884: I0815 04:04:01.310213  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.310220  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413102919273"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413102919273"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:04:01.310446  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62cf1830 on Place(cpu)
1884: I0815 04:04:01.310465  6857 scope.cc:202] Create variable 0x62cf18301723694641310460347_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236946413102919273"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236946413102919273 -> 0x62cd6840
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:04:01.310647  6950 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.310719  6951 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:04:01.310737  6952 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:04:01.310768  6953 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:04:01.310791  6954 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:04:01.310787  6953 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236946413102919273:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.310817  6953 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236946413102919273:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:04:01.310837  6857 pir_interpreter.cc:1766] main_thread_blocker_(0x62cf19a0) got event_name: TaskCompletion
1884: I0815 04:04:01.311002  6953 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3089038154725411819 to 2664012504097671172 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:04:01.311009  6953 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 3089038154725411819 to 2664012504097671172 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:04:01.311101  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.311110  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.311167  6857 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:04:01.311223  6857 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:04:01.311259  6857 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413102919273"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413093984802"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413102919273"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413093984802"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:04:01.312005  6857 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:04:01.312024  6857 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:04:01.312055  6857 pir_interpreter.cc:161] PirInterpreter(): 0x62cf1830 on Place(cpu)
1884: I0815 04:04:01.312085  6857 scope.cc:202] Create variable feed_name_0
1884: I0815 04:04:01.312099  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_5
1884: I0815 04:04:01.312121  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_6
1884: I0815 04:04:01.312134  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_7
1884: I0815 04:04:01.312142  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_8
1884: I0815 04:04:01.312162  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_9
1884: I0815 04:04:01.312175  6857 scope.cc:202] Create variable 0x62cf18301723694641312070499_inner_var_10
1884: I0815 04:04:01.312196  6857 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.312215  6857 helper.h:475] Init predictor : [cpu current allocated memory: 5.34267MB], [cpu current reserved memory: 5.34267MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:04:01.312328  6857 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.312345  6857 helper.h:475] before run : [cpu current allocated memory: 5.34272MB], [cpu current reserved memory: 5.34272MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413102919273"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236946413093984802"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236946413102919273 -> 0x62cd6840
1884: 1 -> constant_folding@_17236946413093984802 -> 0x622be5a0
1884: 2 -> linear_1.b_0 -> 0x6256a370
1884: 3 -> linear_1.w_0 -> 0x6266c4d0
1884: 4 -> feed_name_0 -> 0x62194730
1884: 5 -> 0x62cf18301723694641312070499_inner_var_5 -> 0x62c97a60
1884: 6 -> 0x62cf18301723694641312070499_inner_var_6 -> 0x604c5310
1884: 7 -> 0x62cf18301723694641312070499_inner_var_7 -> 0x62b83cb0
1884: 8 -> 0x62cf18301723694641312070499_inner_var_8 -> 0x62bb04d0
1884: 9 -> fetch_name_0 -> 0x6219b7b0
1884: 10 -> fetch_name_1 -> 0x62bafde0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:04:01.312891  6857 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:04:01.312948  6955 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:04:01.312947  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.312994  6857 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:04:01.313012  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.313040  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62cf18301723694641312070499_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62cf18301723694641312070499_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.313081  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x62cf18301723694641312070499_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.313113  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.313134  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.313151  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.313181  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236946413093984802:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.313207  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236946413102919273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.313236  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236946413102919273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x62cf18301723694641312070499_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:04:01.313264  6857 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236946413102919273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:04:01.313293  6857 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236946413102919273:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cf18301723694641312070499_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:04:01.313329  6857 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:04:01.313350  6857 helper.h:475] after run : [cpu current allocated memory: 5.3429MB], [cpu current reserved memory: 5.3429MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:04:01.313372  6857 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:04:01.313489  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.313496  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.313545  6955 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 13605281445671434995 to 2664012504097671172 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:04:01.313553  6955 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 13605281445671434995 to 2664012504097671172 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:04:01.313586  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.313593  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EEFF..
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 56, in setUp
1884:     self.init_data()
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 62, in init_data
1884:     self.outputs = {"Out": paddle.multinomial(self.input_np)}
1884:   File "/home/code/Paddle/build/python/paddle/tensor/random.py", line 496, in multinomial
1884:     check_variable_and_dtype(
1884:   File "/home/code/Paddle/build/python/paddle/base/data_feeder.py", line 172, in check_variable_and_dtype
1884:     check_type(input, input_name, (Variable, Value), op_name, extra_message)
1884:   File "/home/code/Paddle/build/python/paddle/base/data_feeder.py", line 203, in check_type
1884:     raise TypeError(
1884: TypeError: The type of 'x' in multinomial must be (<class 'paddle.base.framework.Variable'>, <class 'paddle.base.libpaddle.pir.Value'>), but received <class 'numpy.ndarray'>. 
1884: 
1884: ======================================================================
1884: ERROR: tearDownClass (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 486, in tearDownClass
1884:     raise AssertionError(
1884: AssertionError: This test do not have op_type in class attrs, please set self.__class__.op_type=the_real_op_type manually.
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 209989 / 300000 (70%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[2, 0, 2, ..., 3, 0, 3],
1884:        [3, 2, 2, ..., 2, 2, 2],
1884:        [0, 2, 3, ..., 2, 3, 3]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 100 / 100 (100%)
1884: Max absolute difference: 976
1884: Max relative difference: inf
1884:  x: array([511, 386, 971, 262, 868, 496, 239, 420, 382,  85, 458, 725, 696,
1884:        900, 215, 619, 684, 124, 653, 811,  94, 709, 818, 748,  19, 120,
1884:        717, 647, 341, 422, 611, 181, 576, 759, 275, 581, 223, 626, 752,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.617s
1884: 
1884: FAILED (failures=2, errors=2, skipped=3)
1884: 
1884: I0815 04:04:01.315402  6857 mmap_allocator.cc:348] PID: 6857, MemoryMapFdSet: set size - 0
1884: I0815 04:04:01.327867  6857 mmap_allocator.cc:348] PID: 6857, MemoryMapFdSet: set size - 0
1884: I0815 04:04:01.384341  6927 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 10144981446558951206 to 2664012504097671172 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:04:01.384362  6927 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 10144981446558951206 to 2664012504097671172 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:04:01.384377  6928 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10951390051173201640 to 2664012504097671172 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:04:01.384393  6928 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 10951390051173201640 to 2664012504097671172 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:04:01.384408  6926 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1948461582148395452 to 2664012504097671172 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:04:01.384423  6926 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 1948461582148395452 to 2664012504097671172 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:04:01.384647  6929 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 5833715745318962027 to 2664012504097671172 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:04:01.384661  6929 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 5833715745318962027 to 2664012504097671172 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:04:01.384666  6929 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 5833715745318962027 to 15902349701415279896 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:04:01.384907  6932 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 15902349701415279896 to 2664012504097671172 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:04:01.384915  6932 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 15902349701415279896 to 2664012504097671172 , after update, data is {current : 24, peak : 280}.
1884: I0815 04:04:01.384919  6932 thread_data_registry.h:135] Add data {current : 208, peak : 280} from thread 15902349701415279896 to 2664012504097671172 , after update, data is {current : 24, peak : 280}.
1884: I0815 04:04:01.384943  6931 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 603470380997677776 to 2664012504097671172 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:04:01.384950  6931 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 603470380997677776 to 2664012504097671172 , after update, data is {current : 28, peak : 280}.
1884: I0815 04:04:01.385108  6934 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 2664012504097671172 to 440294949534955838 , after update, data is {current : 5601792, peak : 8000800}.
1884: I0815 04:04:01.385121  6934 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 2664012504097671172 to 440294949534955838 , after update, data is {current : 5601792, peak : 5608800}.
1884: I0815 04:04:01.385125  6934 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 2664012504097671172 to 440294949534955838 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:04:01.522696  6857 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:04:01.522724  6857 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:04:01.522771  6857 mmap_allocator.cc:348] PID: 6857, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   11.61 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  11.79 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
