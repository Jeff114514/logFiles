UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:32:36.459425 17703 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:32:37.239332 17703 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=initial_gpu_memory_in_mb,enable_interpretercore_launch_cinn,cupti_dir,enable_gpu_memory_usage_log_mb,gpugraph_parallel_stream_num,init_allocated_mem,gpu_memory_limit_mb,cusparse_dir,add_dependency_for_communication_op,graph_metapath_split_opt,cublas_dir,accuracy_check_rtol_bf16,save_static_runtime_data,enable_api_kernel_fallback,pir_subgraph_saving_dir,enable_exit_when_partial_worker,cudnn_exhaustive_search,pir_apply_shape_optimization_pass,enable_pir_in_executor_trace_run,multi_node_sample_use_gpu_table,gpugraph_debug_gpu_memory,enable_all2all_use_fp16,gpugraph_enable_print_op_debug,gpugraph_enable_segment_merge_grads,gpugraph_sparse_table_storage_mode,fraction_of_cuda_pinned_memory_to_use,enable_tracker_all2all,dataloader_use_file_descriptor,auto_growth_chunk_size_in_mb,gpugraph_offload_gather_copy_maxsize,sync_after_alloc,logging_trunc_pir_py_code,cuda_dir,cudnn_exhaustive_search_times,fast_eager_deletion_mode,sync_nccl_allreduce,get_host_by_name_time,logging_pir_py_code_dir,use_auto_growth_pinned_allocator,log_memory_stats,gpugraph_offload_param_stat,new_executor_sequential_run,gpugraph_enable_hbm_table_collision_stat,tracer_onednn_ops_off,embedding_deterministic,ir_inplace_kernel_blacklist,use_xqa_optim,pir_debug,use_system_allocator,benchmark,gpugraph_hbm_table_load_factor,max_inplace_grad_add,accuracy_check_atol_fp16,tensorrt_dir,nccl_dir,einsum_opt,all_blocks_convert_trt,new_executor_use_cuda_graph,free_idle_chunk,cse_max_count,enable_fusion_fallback,cache_inference_while_scope,tensor_operants_mode,cudnn_batchnorm_spatial_persistent,cuda_memory_async_pool_realease_threshold,graph_neighbor_size_percent,disable_dyshape_in_train,enable_auto_detect_gpu_topo,custom_device_mem_record,gpugraph_storage_mode,apply_pass_to_program,paddle_num_threads,alloc_fill_value,new_executor_use_local_scope,conv_workspace_size_limit,mklml_dir,curand_dir,cublaslt_exhaustive_search_times,check_kernel_launch,mkl_dir,gpugraph_enable_gpu_direct_access,pinned_memory_as_cpu_backend,cinn_subgraph_graphviz_dir,run_kp_kernel,enable_adjust_op_order,print_ir,prim_enable_dynamic,use_shm_cache,use_stream_safe_cuda_allocator,gpugraph_merge_grads_segment_size,jit_engine_type,sort_sum_gradient,gemm_use_half_precision_compute_type,gpugraph_force_device_batch_num_equal,new_executor_use_inplace,enable_neighbor_list_use_uva,eager_delete_tensor_gb,graph_get_neighbor_id,search_cache_max_number,allocator_strategy,prim_forward,check_nan_inf,cusolver_dir,nvidia_package_dir,enable_graph_multi_node_sampling,use_pinned_memory,async_trace_count,set_to_1d,reallocate_gpu_memory_in_mb,gpugraph_dedup_pull_push_mode,cinn_compile_thread_num,enable_gpu_memory_usage_log,lapack_dir,enable_collect_shape,memory_fraction_of_eager_deletion,enable_pir_in_executor,multiple_of_cupti_buffer_size,fuse_parameter_groups_size,enable_fuse_parallel_matmul_pass,convert_all_blocks,prim_check_ops,free_when_no_cache_hit,check_infer_symbolic,enable_record_memory,nccl_blocking_wait,use_autotune,allow_cinn_ops,prim_backward,enable_sparse_inner_gather,use_cinn,npu_storage_format,cusparselt_dir,enable_pir_with_pt_in_dy2st,enable_opt_get_features,gpugraph_offload_param_extends,enable_async_trace,enable_auto_rdma_trans,cublaslt_device_best_config,reader_queue_speed_test_mode,conv2d_disable_cudnn,logging_pir_py_code_int_tensor_element_limit,fraction_of_cpu_memory_to_use,enable_cinn_accuracy_check,call_stack_level,gpugraph_parallel_copyer_split_maxsize,use_auto_growth_v2,tracer_profile_fname,accuracy_check_rtol_fp16,inner_op_parallelism,win_cuda_bin_dir,pir_broadcast_tree_limit,prim_enabled,fraction_of_gpu_memory_to_use,gpugraph_slot_feasign_max_num,query_dest_rank_by_multi_node,new_executor_static_build,new_executor_serial_run,use_cuda_managed_memory,auto_free_cudagraph_allocations_on_launch,enable_dump_main_program,pir_apply_inplace_pass,use_fast_math,print_allocator_trace_info,enable_cinn_compile_cache,accuracy_check_atol_fp32,logging_pir_py_code_dump_symbolic_dims,tracer_onednn_ops_on,benchmark_nccl,prim_all,use_mkldnn,graph_load_in_parallel,executor_log_deps_every_microseconds,deny_cinn_ops,prim_skip_dynamic,selected_gpus,static_runtime_data_save_path,use_stride_kernel,accuracy_check_atol_bf16,fuse_parameter_memory_size,use_virtual_memory_auto_growth,local_exe_sub_scope_limit,manually_trans_conv_filter,low_precision_op_list,print_sub_graph_dir,enable_pir_api,trt_ibuilder_cache,dynamic_static_unified_comm,allreduce_record_one_event,enable_dependency_builder_debug_info,fleet_executor_with_standalone,check_nan_inf_level,enable_cinn_auto_tune,initial_cpu_memory_in_mb,cudnn_dir,enable_unused_var_check,use_cuda_malloc_async_allocator,cuda_malloc_async_pool_memory_throttle_ratio,host_trace_level,graph_embedding_split_infer_mode,cudnn_deterministic,dump_chunk_info,dist_threadpool_size,eager_delete_scope,prim_forward_blacklist,gpu_allocator_retry_time,enable_cublas_tensor_op_math,accuracy_check_rtol_fp32,enable_cse_in_dy2st,gpugraph_load_node_list_into_hbm,static_executor_perfstat_filepath,dygraph_debug,enable_blaslt_global_search,op_dir 
1884: I0815 04:32:37.239440 17703 init.cc:108] After Parse: argc is 2
1884: I0815 04:32:45.167668 17703 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:32:45.167726 17703 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:32:45.168401 17703 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:32:45.168934 17703 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:32:45.169742 17703 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:32:45.169837 17703 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:32:45.169930 17703 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:32:45.170655 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f916f600000), and remaining 0
1884: I0815 04:32:45.171005 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.171072 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.171159 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f916f600200), and remaining 0
1884: I0815 04:32:45.171186 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f916f600400), and remaining 0
1884: I0815 04:32:45.174935 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f916f600600), and remaining 0
1884: I0815 04:32:45.175086 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f916f600800), and remaining 0
1884: I0815 04:32:45.175156 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f916f600a00), and remaining 0
1884: I0815 04:32:45.175257 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.175278 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.175357 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.175371 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.176812 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c57bf0 for it.
1884: I0815 04:32:45.176970 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.176995 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.177047 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f916f600e00), and remaining 0
1884: I0815 04:32:45.177125 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f916f6c4400), and remaining 0
1884: I0815 04:32:45.305791 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c57bf0 for it.
1884: I0815 04:32:45.305950 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.305984 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.306524 17703 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f916f800000), and remaining 0
1884: I0815 04:32:45.316582 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c57bf0 for it.
1884: I0815 04:32:45.316669 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:45.316695 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.316728 17703 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:45.316890 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:45.317835 17703 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:32:45.317854 17703 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:32:45.317903 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:45.317986 17703 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:32:45.318009 17703 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.318068 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:45.318161 17703 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:32:45.318178 17703 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.318214 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:45.318424 17703 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:32:45.318444 17703 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.318614 17703 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:32:45.318637 17703 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:45.318711 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:45.322659 17703 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:32:45.322772 17703 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:32:45.322796 17703 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:32:45.322861 17703 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:32:46.681746 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.681828 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.682214 17703 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:32:46.682236 17703 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.688184 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.688220 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.691054 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.691074 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.691089 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.691880 17703 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:32:46.691895 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.691910 17703 scope.cc:202] Create variable feed
1884: I0815 04:32:46.691920 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.691929 17703 scope.cc:202] Create variable fetch
1884: I0815 04:32:46.691934 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.691946 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.691951 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.691955 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.691958 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.694375 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.694722 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.694734 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.694739 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.696419 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.696466 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.696473 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.696480 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.696488 17703 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:32:46.696496 17703 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x605f48d0 type is 7
1884: I0815 04:32:46.696501 17703 scope.cc:202] Create variable x
1884: I0815 04:32:46.696509 17703 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x605f4a00 type is 7
1884: I0815 04:32:46.696570 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.696578 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.696581 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.696585 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.696712 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.696733 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.696853 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.696866 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.696887 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.697041 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.697070 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.697090 17703 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.697094 17703 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x605fca50Variable Type 7
1884: I0815 04:32:46.697122 17703 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.697145 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.697197 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.697217 17703 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.698431 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.698483 17703 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.698876 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.705161 17703 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:32:46.705179 17703 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:32:46.705269 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.705296 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.705816 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: I0815 04:32:46.705884 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.705906 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.706358 17703 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: I0815 04:32:46.706419 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.706441 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.706465 17703 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.706718 17703 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:32:46.706728 17703 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:32:46.706846 17703 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:32:46.706869 17703 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.707247 17703 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:32:46.707257 17703 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:32:46.707307 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.707327 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.707522 17703 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:32:46.707531 17703 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:32:46.707567 17703 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:32:46.707584 17703 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:32:46.707600 17703 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.710266 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.710287 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.710350 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.710358 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.712265 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.712630 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.712642 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.712647 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.714438 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.714488 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.714497 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:46.714502 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60627860 type is 7
1884: I0815 04:32:46.714510 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.714516 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x60627bd0 type is 7
1884: I0815 04:32:46.714522 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.714529 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.714584 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.714591 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.714596 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.714601 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.714650 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.714668 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.714723 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.714732 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.714749 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.714991 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.715006 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.715024 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.715031 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6062e350Variable Type 7
1884: I0815 04:32:46.715049 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.715067 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.715092 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.715107 17703 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.715814 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.715843 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.716043 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.726831 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: I0815 04:32:46.727015 17703 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c9fb20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:32:46.733232 17703 pir_interpreter.cc:161] PirInterpreter(): 0x607ea7d0 on Place(gpu:0)
1884: I0815 04:32:46.733273 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.733312 17703 scope.cc:202] Create variable 0x607ea7d01723696366733258886_inner_var_1
1884: I0815 04:32:46.733325 17703 scope.cc:202] Create variable 0x607ea7d01723696366733258886_inner_var_2
1884: I0815 04:32:46.733335 17703 scope.cc:202] Create variable 0x607ea7d01723696366733258886_inner_var_3
1884: I0815 04:32:46.733345 17703 scope.cc:202] Create variable 0x607ea7d01723696366733258886_inner_var_4
1884: I0815 04:32:46.733352 17703 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:32:46.733775 17703 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:32:46.733790 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.733794 17703 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:32:46.733839 17703 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x607ea730
1884: 1 -> 0x607ea7d01723696366733258886_inner_var_1 -> 0x607ea7b0
1884: 2 -> 0x607ea7d01723696366733258886_inner_var_2 -> 0x607eb080
1884: 3 -> 0x607ea7d01723696366733258886_inner_var_3 -> 0x607e9f10
1884: 4 -> 0x607ea7d01723696366733258886_inner_var_4 -> 0x607eb430
1884: 5 -> fetch0@fetch -> 0x607ebc40
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:32:46.734586 17703 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:32:46.734825 17741 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:46.734956 17742 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:46.735033 17743 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:46.735054 17744 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:46.735128 17745 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:46.735157 17744 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x607ea7d01723696366733258886_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.735239 17746 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:46.735256 17744 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x607ea7d01723696366733258886_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:46.735281 17746 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.735334 17746 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:32:46.735379 17746 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x607ea7d01723696366733258886_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x607ea7d01723696366733258886_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.735567 17746 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x607ea7d01723696366733258886_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x607ea7d01723696366733258886_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:32:46.735636 17744 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x607ea7d01723696366733258886_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.735659 17744 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.736868 17744 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x607ea7d01723696366733258886_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x607ea7d01723696366733258886_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:32:46.736907 17744 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x607ea7d01723696366733258886_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.736933 17744 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.737500 17744 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x607ea7d01723696366733258886_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:32:46.737537 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x607ea940) got event_name: TaskCompletion
1884: I0815 04:32:46.737562 17703 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.811194 17741 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 15549015400543909692 to 2264660949987248687 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:32:46.811218 17741 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 15549015400543909692 to 12343320959873926819 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:32:46.811224 17741 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 15549015400543909692 to 12343320959873926819 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:32:46.811416 17744 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12343320959873926819 to 6143986556339498568 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:32:46.811429 17744 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 12343320959873926819 to 6143986556339498568 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:32:46.811612 17746 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 2264660949987248687 to 6143986556339498568 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:32:46.811623 17746 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 2264660949987248687 to 6143986556339498568 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:32:46.817246 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.817271 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.817339 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.817348 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.819077 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.819429 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.819442 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.819448 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.820987 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.821084 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.821094 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:46.821100 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4640100 type is 7
1884: I0815 04:32:46.821110 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.821112 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62dcb830 type is 7
1884: I0815 04:32:46.821117 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.821121 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.821177 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.821182 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.821187 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.821192 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.821244 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.821259 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.821329 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.821338 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.821352 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.821488 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.821499 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.821513 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.821519 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60a2c220Variable Type 7
1884: I0815 04:32:46.821535 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.821553 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.821574 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.821589 17703 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.823264 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.823307 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.823513 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.827896 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c9fb20 for it.
1884: I0815 04:32:46.828076 17703 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:32:46.831115 17703 pir_interpreter.cc:161] PirInterpreter(): 0x6060f0f0 on Place(gpu:0)
1884: I0815 04:32:46.831149 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.831171 17703 scope.cc:202] Create variable 0x6060f0f01723696366831139923_inner_var_1
1884: I0815 04:32:46.831182 17703 scope.cc:202] Create variable 0x6060f0f01723696366831139923_inner_var_2
1884: I0815 04:32:46.831197 17703 scope.cc:202] Create variable 0x6060f0f01723696366831139923_inner_var_3
1884: I0815 04:32:46.831211 17703 scope.cc:202] Create variable 0x6060f0f01723696366831139923_inner_var_4
1884: I0815 04:32:46.831220 17703 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:32:46.831555 17703 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:32:46.831570 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.831574 17703 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62dcee40
1884: 1 -> 0x6060f0f01723696366831139923_inner_var_1 -> 0x605f6ae0
1884: 2 -> 0x6060f0f01723696366831139923_inner_var_2 -> 0x46296f0
1884: 3 -> 0x6060f0f01723696366831139923_inner_var_3 -> 0x62d585c0
1884: 4 -> 0x6060f0f01723696366831139923_inner_var_4 -> 0x606191c0
1884: 5 -> fetch0@fetch -> 0x605f58d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:32:46.832237 17747 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:46.832314 17748 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:46.832336 17749 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:46.832383 17750 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:46.832408 17751 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:46.832450 17752 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:46.832449 17751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6060f0f01723696366831139923_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.832471 17752 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.832504 17751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6060f0f01723696366831139923_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:46.832505 17752 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:32:46.832542 17752 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6060f0f01723696366831139923_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6060f0f01723696366831139923_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.832634 17752 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6060f0f01723696366831139923_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x6060f0f01723696366831139923_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:32:46.832688 17751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6060f0f01723696366831139923_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.832710 17751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.835289 17751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6060f0f01723696366831139923_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x6060f0f01723696366831139923_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:32:46.835338 17751 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6060f0f01723696366831139923_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.835361 17751 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.837298 17751 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6060f0f01723696366831139923_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:32:46.837349 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x6060f260) got event_name: TaskCompletion
1884: I0815 04:32:46.837373 17703 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.876678 17747 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 2264660949987248687 to 13940899254919033154 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:32:46.876696 17747 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2264660949987248687 to 6320938406279171711 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:32:46.876703 17747 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 2264660949987248687 to 6320938406279171711 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:32:46.876845 17751 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 6320938406279171711 to 6143986556339498568 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:32:46.876855 17751 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 6320938406279171711 to 6143986556339498568 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:32:46.877018 17752 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 13940899254919033154 to 6143986556339498568 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:32:46.880694 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.880715 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.880765 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.880772 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.882396 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.882726 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.882738 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.882743 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.884244 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.884356 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.884366 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:46.884372 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x464b2a0 type is 7
1884: I0815 04:32:46.884379 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.884385 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x605e6cc0 type is 7
1884: I0815 04:32:46.884390 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.884394 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.884449 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.884455 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.884459 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.884464 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.884513 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.884527 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.884585 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.884593 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.884608 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.884642 17703 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.884769 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:46.884831 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.884841 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.884856 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.884862 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x607f3cb0Variable Type 7
1884: I0815 04:32:46.884878 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.884896 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.884917 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.884932 17703 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.885201 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.885222 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.885411 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.886140 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: I0815 04:32:46.886333 17703 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c9fb20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:32:46.889254 17703 pir_interpreter.cc:161] PirInterpreter(): 0x62c80340 on Place(gpu:0)
1884: I0815 04:32:46.889287 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.889317 17703 scope.cc:202] Create variable 0x62c803401723696366889278465_inner_var_1
1884: I0815 04:32:46.889328 17703 scope.cc:202] Create variable 0x62c803401723696366889278465_inner_var_2
1884: I0815 04:32:46.889340 17703 scope.cc:202] Create variable 0x62c803401723696366889278465_inner_var_3
1884: I0815 04:32:46.889351 17703 scope.cc:202] Create variable 0x62c803401723696366889278465_inner_var_4
1884: I0815 04:32:46.889364 17703 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:32:46.889683 17703 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:32:46.889698 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.889703 17703 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62f3be40
1884: 1 -> 0x62c803401723696366889278465_inner_var_1 -> 0x62c80f20
1884: 2 -> 0x62c803401723696366889278465_inner_var_2 -> 0x605daa10
1884: 3 -> 0x62c803401723696366889278465_inner_var_3 -> 0x607ea710
1884: 4 -> 0x62c803401723696366889278465_inner_var_4 -> 0x467f3d0
1884: 5 -> fetch0@fetch -> 0x466ab00
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:32:46.890357 17753 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:46.890419 17754 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:46.890491 17756 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:46.890479 17755 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:46.890528 17757 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:46.890558 17758 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:46.890563 17755 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62c803401723696366889278465_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.890578 17758 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.890615 17758 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:32:46.890661 17755 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62c803401723696366889278465_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:46.890702 17758 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62c803401723696366889278465_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62c803401723696366889278465_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.890743 17758 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.890852 17758 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:46.890877 17758 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62c803401723696366889278465_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62c803401723696366889278465_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:32:46.890952 17755 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62c803401723696366889278465_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.890980 17755 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.891273 17755 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62c803401723696366889278465_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62c803401723696366889278465_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:32:46.891315 17755 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62c803401723696366889278465_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:46.891353 17755 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.891369 17755 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62c803401723696366889278465_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:32:46.891400 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62c804b0) got event_name: TaskCompletion
1884: I0815 04:32:46.891422 17703 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:32:46.923086 17753 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 13940899254919033154 to 2264660949987248687 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:32:46.923110 17753 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 13940899254919033154 to 2264660949987248687 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:32:46.923116 17753 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 13940899254919033154 to 2264660949987248687 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:32:46.923274 17755 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11907573098849688970 to 2264660949987248687 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:32:46.923285 17755 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11907573098849688970 to 2264660949987248687 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:32:46.923451 17758 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2264660949987248687 to 6143986556339498568 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:32:46.923465 17758 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 2264660949987248687 to 6143986556339498568 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:32:46.923470 17758 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 2264660949987248687 to 6143986556339498568 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:32:46.927258 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.927279 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.927350 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.927358 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.928900 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.929219 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.929231 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.929236 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.930737 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.930811 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.930822 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:46.930827 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x2b2a180 type is 7
1884: I0815 04:32:46.930835 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.930837 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x605ebdd0 type is 7
1884: I0815 04:32:46.930842 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.930846 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.930899 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.930905 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.930909 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.930913 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.930959 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.930972 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.931025 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.931033 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.931046 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.931272 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.931283 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.931305 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.931313 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x605e9e20Variable Type 7
1884: I0815 04:32:46.931329 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.931346 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.931367 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.931382 17703 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.932107 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.932133 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.932322 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.934923 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.934944 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.934991 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.934999 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.970999 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.971020 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.971064 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.971071 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.972674 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:46.972991 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.973003 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.973008 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.977537 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:46.977677 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:46.977689 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:46.977694 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x466c780 type is 7
1884: I0815 04:32:46.977701 17703 scope.cc:202] Create variable X
1884: I0815 04:32:46.977707 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x462ecc0 type is 7
1884: I0815 04:32:46.977711 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:46.977716 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:46.977770 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:46.977777 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:46.977782 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:46.977784 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:46.977834 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.977847 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.977901 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.977910 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.977924 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.978070 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.978080 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:46.978094 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:46.978101 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62ca2120Variable Type 7
1884: I0815 04:32:46.978116 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:46.978133 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.978152 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:46.978166 17703 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:46.979837 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:46.979871 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:46.980067 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:46.989216 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.989238 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:46.989284 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:46.989293 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.024650 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:47.024670 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.024711 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:47.024718 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.026252 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:47.026578 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.026590 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.026595 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.028102 17703 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:32:47.028182 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:47.028192 17703 scope.cc:202] Create variable Out
1884: I0815 04:32:47.028198 17703 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x607c0140 type is 7
1884: I0815 04:32:47.028205 17703 scope.cc:202] Create variable X
1884: I0815 04:32:47.028210 17703 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62cb0210 type is 7
1884: I0815 04:32:47.028215 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:47.028219 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:47.028270 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:47.028276 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.028280 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.028285 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.028338 17703 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.028352 17703 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.028404 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.028412 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.028425 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.028460 17703 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.028580 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.028632 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.028642 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.028656 17703 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:47.028663 17703 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62271e20Variable Type 7
1884: I0815 04:32:47.028679 17703 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:47.028697 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.028717 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.028729 17703 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.028861 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:47.028882 17703 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:47.029065 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.029825 17703 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19c9fb20 for it.
1884: I0815 04:32:47.030035 17703 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19c72b30 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:32:47.033015 17703 pir_interpreter.cc:161] PirInterpreter(): 0x62cafce0 on Place(gpu:0)
1884: I0815 04:32:47.033049 17703 scope.cc:202] Create variable X
1884: I0815 04:32:47.033071 17703 scope.cc:202] Create variable 0x62cafce01723696367033039708_inner_var_1
1884: I0815 04:32:47.033082 17703 scope.cc:202] Create variable 0x62cafce01723696367033039708_inner_var_2
1884: I0815 04:32:47.033092 17703 scope.cc:202] Create variable 0x62cafce01723696367033039708_inner_var_3
1884: I0815 04:32:47.033103 17703 scope.cc:202] Create variable 0x62cafce01723696367033039708_inner_var_4
1884: I0815 04:32:47.033113 17703 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:32:47.033438 17703 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:32:47.033452 17703 scope.cc:202] Create variable X
1884: I0815 04:32:47.033457 17703 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x62f27070
1884: 1 -> 0x62cafce01723696367033039708_inner_var_1 -> 0x62f270f0
1884: 2 -> 0x62cafce01723696367033039708_inner_var_2 -> 0x62dcebb0
1884: 3 -> 0x62cafce01723696367033039708_inner_var_3 -> 0x607f35f0
1884: 4 -> 0x62cafce01723696367033039708_inner_var_4 -> 0x605ff710
1884: 5 -> fetch0@fetch -> 0x605e5700
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:32:47.034143 17759 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.034226 17760 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.034284 17762 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.034314 17761 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.034361 17764 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:47.034364 17761 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367033039708_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.034385 17764 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.034427 17764 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:32:47.034436 17761 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367033039708_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.034461 17764 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62cafce01723696367033039708_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62cafce01723696367033039708_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.034503 17764 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.034505 17763 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.034613 17764 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.034639 17764 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62cafce01723696367033039708_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x62cafce01723696367033039708_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:32:47.034744 17763 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367033039708_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.034792 17763 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.034889 17763 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367033039708_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x62cafce01723696367033039708_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:32:47.034933 17763 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367033039708_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.034957 17763 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.034976 17763 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367033039708_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:32:47.035014 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62cafe50) got event_name: TaskCompletion
1884: I0815 04:32:47.035039 17703 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.066705 17759 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 2264660949987248687 to 13940899254919033154 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:32:47.066717 17759 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2264660949987248687 to 6320938406279171711 , after update, data is {current : 796, peak : 1600}.
1884: I0815 04:32:47.066722 17759 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 2264660949987248687 to 6320938406279171711 , after update, data is {current : 796, peak : 1600}.
1884: I0815 04:32:47.066941 17763 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 6320938406279171711 to 13940899254919033154 , after update, data is {current : 796, peak : 8000}.
1884: I0815 04:32:47.066957 17763 thread_data_registry.h:135] Add data {current : 796, peak : 1600} from thread 6320938406279171711 to 13940899254919033154 , after update, data is {current : 796, peak : 8000}.
1884: I0815 04:32:47.066970 17761 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12343320959873926819 to 13940899254919033154 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:32:47.066990 17761 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12343320959873926819 to 13940899254919033154 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:32:47.067132 17764 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 13940899254919033154 to 6143986556339498568 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:32:47.067145 17764 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 13940899254919033154 to 6143986556339498568 , after update, data is {current : 3201600, peak : 5600800}.
1884: I0815 04:32:47.067152 17764 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 13940899254919033154 to 6143986556339498568 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:32:47.073149 17703 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:32:47.073197 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:32:47.074249 17703 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:32:47.075080 17703 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:32:47.075110 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:32:47.076404 17703 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:32:47.076427 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:32:47.077095 17703 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:32:47.078063 17703 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:32:47.078088 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:32:47.079396 17703 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:32:47.079416 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:32:47.080000 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.080026 17703 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:32:47.080032 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.080039 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.081933 17703 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:32:47.081957 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:32:47.082911 17703 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:32:47.082937 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:32:47.083875 17703 pybind.cc:1827] need skip: 0
1884: I0815 04:32:47.084161 17703 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:32:47.085888 17703 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:32:47.089469 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.089486 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.089491 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.091459 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:47.091476 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:47.091485 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:47.091491 17703 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:32:47.091496 17703 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62078a00 type is 7
1884: I0815 04:32:47.091502 17703 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:32:47.091506 17703 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62079bb0 type is 7
1884: I0815 04:32:47.091511 17703 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:32:47.091514 17703 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x620788e0 type is 7
1884: I0815 04:32:47.091574 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:47.091581 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.091585 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.091589 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.091637 17703 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091650 17703 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091670 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:32:47.091792 17703 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091802 17703 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091861 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.091904 17703 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091913 17703 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.091938 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.092890 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.094204 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:47.094660 17703 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:32:47.094872 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.095160 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.095373 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.095389 17703 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:32:47.095453 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.095458 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.095463 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.095562 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.095574 17703 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:32:47.097105 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.098445 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.099540 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.099723 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:47.099735 17703 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:32:47.099740 17703 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x61fb8260 type is 7
1884: I0815 04:32:47.099745 17703 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:32:47.099752 17703 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x61fb7760 type is 7
1884: I0815 04:32:47.099756 17703 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:32:47.099761 17703 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x61fb7580 type is 7
1884: I0815 04:32:47.099764 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:47.099769 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:47.099774 17703 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:32:47.099779 17703 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x62045b70 type is 7
1884: I0815 04:32:47.099783 17703 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62078a00 type is 7
1884: I0815 04:32:47.099790 17703 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62079bb0 type is 7
1884: I0815 04:32:47.099794 17703 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:32:47.099798 17703 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x62045b50 type is 7
1884: I0815 04:32:47.099802 17703 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:32:47.099805 17703 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x620460b0 type is 7
1884: I0815 04:32:47.099809 17703 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x620788e0 type is 7
1884: I0815 04:32:47.099814 17703 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:32:47.099817 17703 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x62046320 type is 7
1884: I0815 04:32:47.099821 17703 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:32:47.099825 17703 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x62046560 type is 7
1884: I0815 04:32:47.099830 17703 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:32:47.099834 17703 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x620467c0 type is 7
1884: I0815 04:32:47.099915 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.099928 17703 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:32:47.099984 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:47.099992 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.099995 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.099999 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.100044 17703 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.100055 17703 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.100071 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:32:47.100175 17703 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.100185 17703 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.100203 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:32:47.100275 17703 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:32:47.100353 17703 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:32:47.101420 17703 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101441 17703 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101502 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.101564 17703 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101573 17703 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101586 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:32:47.101609 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.101649 17703 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101660 17703 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101670 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:32:47.101750 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101760 17703 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.101773 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.101874 17703 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.101953 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.102008 17703 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102020 17703 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102032 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:32:47.102068 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.102118 17703 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102128 17703 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102140 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:32:47.102241 17703 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102250 17703 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102270 17703 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.102319 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.102329 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.102344 17703 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:47.102351 17703 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62fd0c60Variable Type 7
1884: I0815 04:32:47.102367 17703 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:47.102383 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.102401 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102414 17703 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.102453 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:47.102476 17703 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:32:47.102504 17703 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.102511 17703 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.102524 17703 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:32:47.102530 17703 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62fd21d0Variable Type 7
1884: I0815 04:32:47.102543 17703 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:32:47.102555 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.102568 17703 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.102579 17703 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.102612 17703 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:32:47.102625 17703 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:32:47.103016 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:32:47.103047 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:32:47.103065 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:32:47.103097 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:32:47.103127 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.103143 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:32:47.106882 17703 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:32:47.106915 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:32:47.107607 17703 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:32:47.107627 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:32:47.107935 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.109586 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.110391 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.110507 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.111030 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.111920 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.114001 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.115056 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.116876 17703 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:32:47.117645 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.117660 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.117664 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.118805 17703 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:32:47.118822 17703 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4662130 type is 9
1884: I0815 04:32:47.118829 17703 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4628be0 type is 10
1884: I0815 04:32:47.118834 17703 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62079bb0 type is 7
1884: I0815 04:32:47.118837 17703 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x620788e0 type is 7
1884: I0815 04:32:47.118842 17703 scope.cc:202] Create variable saved_params
1884: I0815 04:32:47.118845 17703 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x61fc6110 type is 17
1884: I0815 04:32:47.118873 17703 interpreter_util.cc:594] Static build: 0
1884: I0815 04:32:47.118878 17703 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:32:47.118882 17703 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:32:47.118886 17703 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:32:47.118921 17703 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.118932 17703 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:32:47.119598 17703 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:32:47.119638 17703 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:32:47.119686 17703 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:32:47.120808 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.120864 17703 scope.cc:202] Create variable feed
1884: I0815 04:32:47.120872 17703 naive_executor.cc:189] 0x62bfc7f0 Create persistable variable feed, which pointer is 0x62dae590
1884: I0815 04:32:47.120878 17703 scope.cc:202] Create variable fetch
1884: I0815 04:32:47.120882 17703 naive_executor.cc:189] 0x62bfc7f0 Create persistable variable fetch, which pointer is 0x6209d2a0
1884: I0815 04:32:47.120885 17703 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:32:47.120888 17703 naive_executor.cc:189] 0x62bfc7f0 Create persistable variable linear_0.b_0, which pointer is 0x62bfca90
1884: I0815 04:32:47.120895 17703 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:32:47.120898 17703 naive_executor.cc:189] 0x62bfc7f0 Create persistable variable linear_0.w_0, which pointer is 0x61f6fa40
1884: I0815 04:32:47.120913 17703 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:32:47.121246 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.121340 17703 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:32:47.121387 17703 executor.cc:183] Old Executor is Running.
1884: I0815 04:32:47.121454 17703 executor.cc:92] Creating Variables for block 0
1884: I0815 04:32:47.121462 17703 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:32:47.121465 17703 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x62bfca90 type is 7
1884: I0815 04:32:47.121469 17703 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:32:47.121471 17703 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x61f6fa40 type is 7
1884: I0815 04:32:47.121502 17703 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.121573 17703 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:32:47.121614 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.121620 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.121755 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.121861 17703 graph.cc:149] create OpNode by feed
1884: I0815 04:32:47.121897 17703 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:32:47.121912 17703 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:32:47.121927 17703 graph.cc:149] create OpNode by abs
1884: I0815 04:32:47.121937 17703 graph.cc:149] create OpNode by assign_value
1884: I0815 04:32:47.121953 17703 graph.cc:149] create OpNode by multinomial
1884: I0815 04:32:47.121963 17703 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:32:47.121977 17703 graph.cc:149] create OpNode by scale
1884: I0815 04:32:47.121990 17703 graph.cc:149] create OpNode by scale
1884: I0815 04:32:47.122002 17703 graph.cc:149] create OpNode by fetch
1884: I0815 04:32:47.122018 17703 graph.cc:149] create OpNode by fetch
1884: I0815 04:32:47.122040 17703 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:32:47.123205 17703 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:32:47.123214 17703 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:32:47.123281 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.123288 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:32:47.123402 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.123651 17703 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:32:47.123708 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.123714 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:32:47.123747 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.123752 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:32:47.123792 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.123852 17703 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:32:47.123883 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.123888 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:32:47.123907 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.123920 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.123942 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.123947 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:32:47.123988 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124009 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.124032 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124037 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:32:47.124080 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124154 17703 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:32:47.124183 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124188 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:32:47.124219 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124239 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.124260 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124265 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:32:47.124296 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124451 17703 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:32:47.124480 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124485 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:32:47.124516 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124531 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.124550 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124554 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:32:47.124575 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124589 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.124610 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124615 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:32:47.124639 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124652 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.124675 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124679 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:32:47.124702 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124768 17703 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:32:47.124799 17703 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:32:47.124814 17703 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:32:47.124828 17703 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:32:47.124852 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.124857 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:32:47.124881 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.124920 17703 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:32:47.124940 17703 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:32:47.124951 17703 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:32:47.124964 17703 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:32:47.124996 17703 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:32:47.125007 17703 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:32:47.126169 17703 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:32:47.126214 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.126219 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:32:47.126246 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.126266 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.126291 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.126297 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:32:47.126328 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.126379 17703 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:32:47.126408 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.126413 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:32:47.126432 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.126447 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.126469 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.126474 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:32:47.126508 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.126595 17703 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:32:47.126622 17703 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:32:47.126637 17703 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:32:47.126653 17703 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:32:47.126668 17703 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:32:47.126682 17703 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:32:47.126698 17703 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:32:47.126722 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.126793 17703 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:32:47.126816 17703 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:32:47.126828 17703 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:32:47.126842 17703 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:32:47.126856 17703 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:32:47.126871 17703 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:32:47.126886 17703 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:32:47.126930 17703 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:32:47.127194 17703 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:32:47.127223 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.127228 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:32:47.127274 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127357 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127391 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127437 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127465 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127507 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127532 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127568 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127589 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127622 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127641 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127671 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127686 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127713 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127727 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127748 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127759 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127777 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127802 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.127807 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:32:47.127833 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127874 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.127899 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.127904 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:32:47.127916 17703 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:32:47.127920 17703 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:32:47.127967 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.127988 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.128013 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128018 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:32:47.128029 17703 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:32:47.128033 17703 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:32:47.128073 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.128094 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.128119 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128124 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:32:47.128134 17703 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:32:47.128136 17703 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:32:47.128168 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.128186 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.128208 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128213 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:32:47.128222 17703 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:32:47.128224 17703 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:32:47.128261 17703 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:32:47.128283 17703 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:32:47.128312 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128319 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:32:47.128332 17703 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:32:47.128371 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128376 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:32:47.128446 17703 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:32:47.128466 17703 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.128484 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:32:47.128535 17703 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:32:47.128552 17703 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:32:47.128578 17703 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:32:47.128602 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.128607 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:32:47.129452 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.129467 17703 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:32:47.129516 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.129523 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.130116 17703 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:32:47.130333 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.130405 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.130410 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.130815 17703 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:32:47.131012 17703 graph.h:183] deleting __fuse_statis__
1884: I0815 04:32:47.131021 17703 graph.h:183] deleting pass_recorder
1884: I0815 04:32:47.131026 17703 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:32:47.131104 17703 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:32:47.131114 17703 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:32:47.131119 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable abs_0.tmp_0, which pointer is 0x62603f20
1884: I0815 04:32:47.131124 17703 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:32:47.131126 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable gaussian_0.tmp_0, which pointer is 0x62e0a920
1884: I0815 04:32:47.131139 17703 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:32:47.131143 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable linear_0.tmp_1, which pointer is 0x62ca3330
1884: I0815 04:32:47.131147 17703 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:32:47.131150 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable multinomial_0.tmp_0, which pointer is 0x62ca2dd0
1884: I0815 04:32:47.131153 17703 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:32:47.131156 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x62ca30d0
1884: I0815 04:32:47.131160 17703 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:32:47.131163 17703 naive_executor.cc:195] 0x62bfc7f0 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x624c6ec0
1884: I0815 04:32:47.131170 17703 scope.cc:202] Create variable feed
1884: I0815 04:32:47.131173 17703 scope.cc:202] Create variable fetch
1884: I0815 04:32:47.131191 17703 naive_executor.cc:46] NaiveExecutor init with scope 0x62bfc7f0
1884: I0815 04:32:47.131197 17703 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:32:47.131392 17703 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:32:47.131407 17703 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:32:47.131434 17703 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:32:47.131439 17703 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:32:47.131448 17703 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.131479 17703 helper.h:475] Init predictor : [cpu current allocated memory: 3.05348MB], [cpu current reserved memory: 3.05348MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:32:47.131680 17703 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.131695 17703 helper.h:475] before run : [cpu current allocated memory: 3.05353MB], [cpu current reserved memory: 3.05353MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:32:47.131738 17703 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.131764 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:32:47.148097 17703 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:32:47.148154 17703 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.148173 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:32:47.148213 17703 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:32:47.148236 17703 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.148253 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:32:47.148305 17703 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:32:47.148337 17703 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.148350 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:32:47.148392 17703 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:32:47.148413 17703 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:32:47.148425 17703 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:32:47.148451 17703 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:32:47.148466 17703 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.148486 17703 helper.h:475] after run : [cpu current allocated memory: 3.05401MB], [cpu current reserved memory: 3.05401MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:32:47.148502 17703 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:32:47.148757 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.148766 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:32:47.193166 17703 pir_interpreter.cc:161] PirInterpreter(): 0x4be29f30 on Place(gpu:0)
1884: I0815 04:32:47.193205 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_0
1884: I0815 04:32:47.193222 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_1
1884: I0815 04:32:47.193230 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_2
1884: I0815 04:32:47.193240 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_3
1884: I0815 04:32:47.193266 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_4
1884: I0815 04:32:47.193280 17703 scope.cc:202] Create variable 0x4be29f301723696367193193842_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x4be29f301723696367193193842_inner_var_0 -> 0x2160590
1884: 1 -> 0x4be29f301723696367193193842_inner_var_1 -> 0x4669ff0
1884: 2 -> 0x4be29f301723696367193193842_inner_var_2 -> 0x4657840
1884: 3 -> linear_1.w_0 -> 0x62fa5dd0
1884: 4 -> linear_1.b_0 -> 0x62fd0e40
1884: 5 -> learning_rate_1 -> 0x61f9d5c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:32:47.194054 17765 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.194072 17766 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.194095 17767 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.194135 17768 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.194165 17769 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:47.194160 17767 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194170 17765 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194165 17768 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194191 17769 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194223 17767 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:32:47.194231 17765 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_0
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.194236 17768 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x4be29f301723696367193193842_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.194245 17769 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.194284 17769 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:32:47.194314 17769 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194331 17769 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.194342 17769 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:32:47.194356 17769 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x4be29f301723696367193193842_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4be29f301723696367193193842_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4be29f301723696367193193842_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.194416 17769 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x4be29f301723696367193193842_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4be29f301723696367193193842_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x4be29f301723696367193193842_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:32:47.194471 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x4be2a0a0) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:32:47.196439 17703 pir_interpreter.cc:161] PirInterpreter(): 0x62cafce0 on Place(gpu:0)
1884: I0815 04:32:47.196472 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_1
1884: I0815 04:32:47.196487 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_4
1884: I0815 04:32:47.196497 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_5
1884: I0815 04:32:47.196504 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_6
1884: I0815 04:32:47.196527 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_7
1884: I0815 04:32:47.196537 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_8
1884: I0815 04:32:47.196545 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_9
1884: I0815 04:32:47.196573 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_10
1884: I0815 04:32:47.196583 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_11
1884: I0815 04:32:47.196590 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_12
1884: I0815 04:32:47.196600 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_13
1884: I0815 04:32:47.196609 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_14
1884: I0815 04:32:47.196619 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_15
1884: I0815 04:32:47.196625 17703 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:32:47.196635 17703 scope.cc:202] Create variable 0x62cafce01723696367196460845_inner_var_17
1884: I0815 04:32:47.196642 17703 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x61f9d5c0
1884: 1 -> 0x62cafce01723696367196460845_inner_var_1 -> 0x62fcda40
1884: 2 -> linear_1.b_0 -> 0x62fd0e40
1884: 3 -> linear_1.w_0 -> 0x62fa5dd0
1884: 4 -> 0x62cafce01723696367196460845_inner_var_4 -> 0x605dac60
1884: 5 -> 0x62cafce01723696367196460845_inner_var_5 -> 0x605e89b0
1884: 6 -> 0x62cafce01723696367196460845_inner_var_6 -> 0x605c6590
1884: 7 -> 0x62cafce01723696367196460845_inner_var_7 -> 0x62fa85a0
1884: 8 -> 0x62cafce01723696367196460845_inner_var_8 -> 0x605f6430
1884: 9 -> 0x62cafce01723696367196460845_inner_var_9 -> 0x605d5bf0
1884: 10 -> 0x62cafce01723696367196460845_inner_var_10 -> 0x62f94950
1884: 11 -> 0x62cafce01723696367196460845_inner_var_11 -> 0x62dceb70
1884: 12 -> 0x62cafce01723696367196460845_inner_var_12 -> 0x60602760
1884: 13 -> 0x62cafce01723696367196460845_inner_var_13 -> 0x605e83f0
1884: 14 -> 0x62cafce01723696367196460845_inner_var_14 -> 0x624c9a60
1884: 15 -> 0x62cafce01723696367196460845_inner_var_15 -> 0x607ecbb0
1884: 16 -> fetch0@fetch -> 0x6228af90
1884: 17 -> 0x62cafce01723696367196460845_inner_var_17 -> 0x607ec9c0
1884: 18 -> fetch1@fetch -> 0x60614130
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:32:47.198257 17770 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.198354 17771 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.198372 17772 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.198443 17773 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.198444 17772 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198472 17774 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:32:47.198462 17771 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198478 17772 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:32:47.198505 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198515 17771 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.198531 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:32:47.198552 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198585 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.198614 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:32:47.198629 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:32:47.198683 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:32:47.198699 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x62cafce01723696367196460845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198731 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x62cafce01723696367196460845_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:32:47.198755 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x62cafce01723696367196460845_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198788 17774 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:32:47.198822 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x62cafce01723696367196460845_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.198848 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x62cafce01723696367196460845_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x62cafce01723696367196460845_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198884 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.198899 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x62cafce01723696367196460845_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x62cafce01723696367196460845_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.198927 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x62cafce01723696367196460845_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198947 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.198951 17771 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367196460845_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198958 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x62cafce01723696367196460845_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.198974 17771 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.198973 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x62cafce01723696367196460845_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.198995 17774 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.199054 17771 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367196460845_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.199085 17771 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367196460845_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199100 17774 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.199105 17771 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.199118 17771 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367196460845_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.199149 17774 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.199209 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.199226 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x62cafce01723696367196460845_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x62cafce01723696367196460845_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.199258 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199265 17771 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199280 17771 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:32:47.199283 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.199298 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.199321 17771 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x62cafce01723696367196460845_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.199323 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x62cafce01723696367196460845_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199344 17771 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367196460845_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199357 17771 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.199371 17771 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x62cafce01723696367196460845_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.199401 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x62cafce01723696367196460845_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:32:47.199421 17774 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x62cafce01723696367196460845_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cafce01723696367196460845_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.199446 17774 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:32:47.199456 17774 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x62cafce01723696367196460845_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x62cafce01723696367196460845_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x62cafce01723696367196460845_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:32:47.199489 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62cafe50) got event_name: TaskCompletion
1884: I0815 04:32:47.199512 17703 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.199539 17703 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:32:47.204865 17703 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:32:47.204911 17703 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:32:47.205570 17703 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:32:47.205617 17703 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:32:47.206050 17703 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672061020080"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672061020080"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:32:47.206223 17703 pir_interpreter.cc:161] PirInterpreter(): 0x621488c0 on Place(cpu)
1884: I0815 04:32:47.206243 17703 scope.cc:202] Create variable 0x621488c01723696367206237464_inner_var_0
1884: I0815 04:32:47.206269 17703 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672061020080"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236963672061020080 -> 0x62fa5f20
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:32:47.206410 17703 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:32:47.206516 17775 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.206667 17777 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.206668 17776 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.206766 17777 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236963672061020080:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.206784 17779 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.206780 17778 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.206792 17777 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236963672061020080:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:32:47.206820 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62148a30) got event_name: TaskCompletion
1884: I0815 04:32:47.207037 17777 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14537915198509952571 to 1297461247138804266 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:32:47.207046 17777 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14537915198509952571 to 1297461247138804266 , after update, data is {current : -4, peak : 104}.
1884: I0815 04:32:47.207113 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.207120 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672071850541"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672071850541"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:32:47.207326 17703 pir_interpreter.cc:161] PirInterpreter(): 0x621488c0 on Place(cpu)
1884: I0815 04:32:47.207345 17703 scope.cc:202] Create variable 0x621488c01723696367207340223_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672071850541"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236963672071850541 -> 0x6228a8d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:32:47.207540 17780 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.207599 17781 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.207612 17782 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.207645 17783 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.207665 17784 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.207664 17783 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236963672071850541:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.207717 17783 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236963672071850541:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.207742 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62148a30) got event_name: TaskCompletion
1884: I0815 04:32:47.207911 17783 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 17733610560062777864 to 1297461247138804266 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:32:47.207919 17783 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 17733610560062777864 to 1297461247138804266 , after update, data is {current : 4, peak : 104}.
1884: I0815 04:32:47.208009 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.208015 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236963672071850541",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236963672080897242"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236963672071850541",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236963672080897242"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:32:47.208236 17703 pir_interpreter.cc:161] PirInterpreter(): 0x621488c0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236963672071850541",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236963672080897242"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236963672080897242 -> 0x6228a8d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:32:47.208508 17785 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.208560 17786 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.208576 17787 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.208609 17788 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.208631 17789 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.208626 17788 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.208652 17788 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.208674 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62148a30) got event_name: TaskCompletion
1884: I0815 04:32:47.208943 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.208950 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672090213853"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672090213853"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:32:47.209148 17703 pir_interpreter.cc:161] PirInterpreter(): 0x621488c0 on Place(cpu)
1884: I0815 04:32:47.209167 17703 scope.cc:202] Create variable 0x621488c01723696367209161473_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236963672090213853"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236963672090213853 -> 0x6209c1c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:32:47.209365 17790 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.209424 17791 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:32:47.209445 17792 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:32:47.209475 17793 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:32:47.209499 17794 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:32:47.209496 17793 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236963672090213853:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.209529 17793 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236963672090213853:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:32:47.209551 17703 pir_interpreter.cc:1766] main_thread_blocker_(0x62148a30) got event_name: TaskCompletion
1884: I0815 04:32:47.209719 17793 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17733610560062777864 to 1297461247138804266 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:32:47.209728 17793 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17733610560062777864 to 1297461247138804266 , after update, data is {current : 8, peak : 104}.
1884: I0815 04:32:47.209817 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.209825 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.209883 17703 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:32:47.209939 17703 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:32:47.209976 17703 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672090213853"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672080897242"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672090213853"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672080897242"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:32:47.210629 17703 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:32:47.210646 17703 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:32:47.210678 17703 pir_interpreter.cc:161] PirInterpreter(): 0x621488c0 on Place(cpu)
1884: I0815 04:32:47.210708 17703 scope.cc:202] Create variable feed_name_0
1884: I0815 04:32:47.210722 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_5
1884: I0815 04:32:47.210744 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_6
1884: I0815 04:32:47.210755 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_7
1884: I0815 04:32:47.210763 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_8
1884: I0815 04:32:47.210784 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_9
1884: I0815 04:32:47.210798 17703 scope.cc:202] Create variable 0x621488c01723696367210692969_inner_var_10
1884: I0815 04:32:47.210819 17703 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.210839 17703 helper.h:475] Init predictor : [cpu current allocated memory: 3.05385MB], [cpu current reserved memory: 3.05385MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:32:47.210954 17703 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.210968 17703 helper.h:475] before run : [cpu current allocated memory: 3.0539MB], [cpu current reserved memory: 3.0539MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672090213853"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236963672080897242"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236963672090213853 -> 0x6209c1c0
1884: 1 -> constant_folding@_17236963672080897242 -> 0x6228a8d0
1884: 2 -> linear_1.b_0 -> 0x605fb9d0
1884: 3 -> linear_1.w_0 -> 0x62025cd0
1884: 4 -> feed_name_0 -> 0x6228ec20
1884: 5 -> 0x621488c01723696367210692969_inner_var_5 -> 0x605ce710
1884: 6 -> 0x621488c01723696367210692969_inner_var_6 -> 0x605db9f0
1884: 7 -> 0x621488c01723696367210692969_inner_var_7 -> 0x61fc7210
1884: 8 -> 0x621488c01723696367210692969_inner_var_8 -> 0x62f92220
1884: 9 -> fetch_name_0 -> 0x62081720
1884: 10 -> fetch_name_1 -> 0x62fb5290
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:32:47.211525 17703 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:32:47.211588 17795 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:32:47.211583 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211632 17703 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:32:47.211654 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.211685 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x621488c01723696367210692969_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x621488c01723696367210692969_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211724 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x621488c01723696367210692969_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.211756 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211776 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.211791 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211822 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236963672080897242:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.211846 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236963672090213853:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211874 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236963672090213853:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x621488c01723696367210692969_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:32:47.211900 17703 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236963672090213853:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:32:47.211931 17703 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236963672090213853:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x621488c01723696367210692969_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:32:47.211958 17703 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07188MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07188MB]
1884: I0815 04:32:47.211978 17703 helper.h:475] after run : [cpu current allocated memory: 3.05408MB], [cpu current reserved memory: 3.05408MB], [cpu peak allocated memory: 5.34134MB], [cpu peak reserved memory: 5.34134MB]
1884: I0815 04:32:47.212000 17703 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:32:47.212112 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.212121 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.212169 17795 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 8572961606915755827 to 1297461247138804266 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:32:47.212177 17795 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 8572961606915755827 to 1297461247138804266 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:32:47.212210 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.212217 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EE...
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2976, in check_output_customized
1884:     checker(outs)
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 82, in verify_output
1884:     print(self.calc_output('cpu'))
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 891, in calc_output
1884:     outs, _ = self._calc_output(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1539, in _calc_output
1884:     feed_map = self.feed_var(inputs, place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 816, in feed_var
1884:     tensor.set(self.inputs[var_name], place)
1884: TypeError: set(): incompatible function arguments. The following argument types are supported:
1884:     1. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CPUPlace, zero_copy: bool = False) -> None
1884:     2. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CustomPlace, zero_copy: bool = False) -> None
1884:     3. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.XPUPlace, zero_copy: bool = False) -> None
1884:     4. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CUDAPlace, zero_copy: bool = False) -> None
1884:     5. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.IPUPlace, zero_copy: bool = False) -> None
1884:     6. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CUDAPinnedPlace, zero_copy: bool = False) -> None
1884: 
1884: Invoked with: <paddle.base.libpaddle.Tensor object at 0x7f91b014f570>, array([0.69646919, 0.28613933, 0.22685145, 0.55131477]), 'cpu'
1884: 
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 66, in test_check_output
1884:     self.check_output_customized(self.verify_output, check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2976, in check_output_customized
1884:     checker(outs)
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 82, in verify_output
1884:     print(self.calc_output('cpu'))
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 891, in calc_output
1884:     outs, _ = self._calc_output(place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1539, in _calc_output
1884:     feed_map = self.feed_var(inputs, place)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 816, in feed_var
1884:     tensor.set(self.inputs[var_name], place)
1884: TypeError: set(): incompatible function arguments. The following argument types are supported:
1884:     1. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CPUPlace, zero_copy: bool = False) -> None
1884:     2. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CustomPlace, zero_copy: bool = False) -> None
1884:     3. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.XPUPlace, zero_copy: bool = False) -> None
1884:     4. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CUDAPlace, zero_copy: bool = False) -> None
1884:     5. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.IPUPlace, zero_copy: bool = False) -> None
1884:     6. (self: paddle.base.libpaddle.Tensor, array: object, place: paddle.base.libpaddle.CUDAPinnedPlace, zero_copy: bool = False) -> None
1884: 
1884: Invoked with: <paddle.base.libpaddle.Tensor object at 0x7f91b03ef570>, array([[0.69646919, 0.28613933, 0.22685145, 0.55131477],
1884:        [0.71946897, 0.42310646, 0.9807642 , 0.68482974],
1884:        [0.4809319 , 0.39211752, 0.34317802, 0.72904971]]), 'cpu'
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.507s
1884: 
1884: FAILED (errors=2, skipped=3)
1884: 
1884: I0815 04:32:47.214160 17703 mmap_allocator.cc:348] PID: 17703, MemoryMapFdSet: set size - 0
1884: I0815 04:32:47.225818 17703 mmap_allocator.cc:348] PID: 17703, MemoryMapFdSet: set size - 0
1884: I0815 04:32:47.292513 17768 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12343320959873926819 to 1297461247138804266 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:32:47.292534 17768 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12343320959873926819 to 1297461247138804266 , after update, data is {current : -180, peak : 104}.
1884: I0815 04:32:47.292551 17767 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11907573098849688970 to 1297461247138804266 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:32:47.292569 17767 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 11907573098849688970 to 1297461247138804266 , after update, data is {current : -164, peak : 104}.
1884: I0815 04:32:47.292577 17765 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13940899254919033154 to 1297461247138804266 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:32:47.292595 17765 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13940899254919033154 to 1297461247138804266 , after update, data is {current : -160, peak : 104}.
1884: I0815 04:32:47.292753 17769 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 18066208200292774381 to 1297461247138804266 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:32:47.292765 17769 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 18066208200292774381 to 1297461247138804266 , after update, data is {current : -184, peak : 104}.
1884: I0815 04:32:47.292771 17769 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 18066208200292774381 to 12005280611059025445 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:32:47.293016 17771 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 12005280611059025445 to 1297461247138804266 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:32:47.293030 17771 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 12005280611059025445 to 1297461247138804266 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:32:47.293035 17771 thread_data_registry.h:135] Add data {current : 196, peak : 268} from thread 12005280611059025445 to 1297461247138804266 , after update, data is {current : 12, peak : 268}.
1884: I0815 04:32:47.293082 17772 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13915066066958979532 to 1297461247138804266 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:32:47.293090 17772 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 13915066066958979532 to 1297461247138804266 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:32:47.293210 17774 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 1297461247138804266 to 6143986556339498568 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:32:47.293219 17774 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 1297461247138804266 to 6143986556339498568 , after update, data is {current : 3201792, peak : 5600800}.
1884: I0815 04:32:47.293224 17774 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 1297461247138804266 to 6143986556339498568 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:32:47.424757 17703 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:32:47.424785 17703 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:32:47.424834 17703 mmap_allocator.cc:348] PID: 17703, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   12.03 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  12.20 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
