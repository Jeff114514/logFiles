UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1901
    Start 1901: test_nanmedian

1901: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_nanmedian"
1901: Environment variables: 
1901:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1901: Test timeout computed to be: 10000000
1901: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1901: WARNING: Logging before InitGoogleLogging() is written to STDERR
1901: I0815 10:16:26.100926  8146 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1901: I0815 10:16:26.885686  8146 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=accuracy_check_atol_fp16,benchmark_nccl,dist_threadpool_size,use_auto_growth_pinned_allocator,cublas_dir,reallocate_gpu_memory_in_mb,use_cuda_managed_memory,enable_record_memory,query_dest_rank_by_multi_node,gpugraph_parallel_copyer_split_maxsize,enable_sparse_inner_gather,enable_pir_in_executor,gemm_use_half_precision_compute_type,enable_cinn_auto_tune,enable_fusion_fallback,memory_fraction_of_eager_deletion,run_kp_kernel,enable_unused_var_check,fraction_of_cpu_memory_to_use,graph_metapath_split_opt,use_system_allocator,nccl_dir,enable_opt_get_features,use_shm_cache,gpugraph_enable_segment_merge_grads,enable_gpu_memory_usage_log_mb,new_executor_use_local_scope,print_ir,tensor_operants_mode,gpugraph_sparse_table_storage_mode,cupti_dir,logging_pir_py_code_dir,embedding_deterministic,pinned_memory_as_cpu_backend,cudnn_batchnorm_spatial_persistent,allocator_strategy,sync_nccl_allreduce,nvidia_package_dir,accuracy_check_rtol_fp16,fleet_executor_with_standalone,cusparse_dir,pir_debug,call_stack_level,use_cuda_malloc_async_allocator,cudnn_deterministic,allow_cinn_ops,use_fast_math,enable_adjust_op_order,new_executor_use_inplace,enable_gpu_memory_usage_log,max_inplace_grad_add,use_cinn,graph_load_in_parallel,new_executor_serial_run,enable_neighbor_list_use_uva,enable_dependency_builder_debug_info,use_virtual_memory_auto_growth,enable_pir_api,selected_gpus,win_cuda_bin_dir,conv_workspace_size_limit,enable_cse_in_dy2st,gpu_memory_limit_mb,gpugraph_force_device_batch_num_equal,logging_pir_py_code_int_tensor_element_limit,reader_queue_speed_test_mode,enable_fuse_parallel_matmul_pass,enable_collect_shape,use_autotune,gpugraph_offload_gather_copy_maxsize,trt_ibuilder_cache,eager_delete_scope,prim_all,print_sub_graph_dir,pir_apply_inplace_pass,enable_cublas_tensor_op_math,npu_storage_format,pir_apply_shape_optimization_pass,deny_cinn_ops,graph_get_neighbor_id,convert_all_blocks,search_cache_max_number,enable_pir_with_pt_in_dy2st,cinn_compile_thread_num,tracer_profile_fname,all_blocks_convert_trt,jit_engine_type,use_stride_kernel,cusparselt_dir,enable_auto_detect_gpu_topo,fast_eager_deletion_mode,local_exe_sub_scope_limit,new_executor_use_cuda_graph,enable_all2all_use_fp16,gpugraph_storage_mode,new_executor_sequential_run,graph_neighbor_size_percent,manually_trans_conv_filter,enable_cinn_accuracy_check,cudnn_exhaustive_search_times,cinn_subgraph_graphviz_dir,mkl_dir,gpu_allocator_retry_time,cse_max_count,accuracy_check_atol_bf16,check_infer_symbolic,dump_chunk_info,fuse_parameter_groups_size,apply_pass_to_program,check_kernel_launch,einsum_opt,eager_delete_tensor_gb,nccl_blocking_wait,executor_log_deps_every_microseconds,enable_dump_main_program,save_static_runtime_data,cuda_malloc_async_pool_memory_throttle_ratio,prim_skip_dynamic,cuda_dir,initial_gpu_memory_in_mb,enable_api_kernel_fallback,lapack_dir,prim_forward,logging_trunc_pir_py_code,log_memory_stats,cache_inference_while_scope,use_xqa_optim,gpugraph_offload_param_stat,mklml_dir,conv2d_disable_cudnn,logging_pir_py_code_dump_symbolic_dims,auto_free_cudagraph_allocations_on_launch,enable_exit_when_partial_worker,accuracy_check_atol_fp32,alloc_fill_value,use_pinned_memory,dygraph_debug,prim_backward,use_mkldnn,check_nan_inf_level,gpugraph_hbm_table_load_factor,paddle_num_threads,prim_enabled,gpugraph_dedup_pull_push_mode,enable_interpretercore_launch_cinn,gpugraph_debug_gpu_memory,print_allocator_trace_info,cusolver_dir,gpugraph_offload_param_extends,host_trace_level,cublaslt_exhaustive_search_times,gpugraph_enable_hbm_table_collision_stat,auto_growth_chunk_size_in_mb,static_executor_perfstat_filepath,gpugraph_merge_grads_segment_size,prim_check_ops,graph_embedding_split_infer_mode,cublaslt_device_best_config,enable_tracker_all2all,ir_inplace_kernel_blacklist,fraction_of_gpu_memory_to_use,gpugraph_enable_print_op_debug,prim_forward_blacklist,curand_dir,use_auto_growth_v2,gpugraph_parallel_stream_num,check_nan_inf,enable_blaslt_global_search,fraction_of_cuda_pinned_memory_to_use,allreduce_record_one_event,cuda_memory_async_pool_realease_threshold,dynamic_static_unified_comm,sync_after_alloc,benchmark,custom_device_mem_record,init_allocated_mem,multi_node_sample_use_gpu_table,get_host_by_name_time,accuracy_check_rtol_fp32,tracer_onednn_ops_off,add_dependency_for_communication_op,prim_enable_dynamic,accuracy_check_rtol_bf16,fuse_parameter_memory_size,pir_subgraph_saving_dir,enable_async_trace,set_to_1d,enable_auto_rdma_trans,low_precision_op_list,free_when_no_cache_hit,inner_op_parallelism,async_trace_count,gpugraph_slot_feasign_max_num,static_runtime_data_save_path,op_dir,dataloader_use_file_descriptor,cudnn_dir,gpugraph_load_node_list_into_hbm,disable_dyshape_in_train,use_stream_safe_cuda_allocator,new_executor_static_build,enable_pir_in_executor_trace_run,multiple_of_cupti_buffer_size,initial_cpu_memory_in_mb,gpugraph_enable_gpu_direct_access,enable_graph_multi_node_sampling,tensorrt_dir,tracer_onednn_ops_on,cudnn_exhaustive_search,free_idle_chunk,pir_broadcast_tree_limit,enable_cinn_compile_cache,sort_sum_gradient 
1901: I0815 10:16:26.885793  8146 init.cc:108] After Parse: argc is 2
1901: I0815 10:16:32.212544  8146 scope.cc:202] Create variable X
1901: I0815 10:16:32.212615  8146 scope.cc:202] Create variable Out
1901: I0815 10:16:32.212630  8146 scope.cc:202] Create variable MedianIndex
1901: I0815 10:16:32.212780  8146 op_registry.cc:112] CreateOp directly from OpDesc is deprecated. It should only beused in unit tests. Use CreateOp(const OpDesc& op_desc) instead.
1901: I0815 10:16:32.213407  8146 allocator_facade.cc:212] selected allocator strategy:1
1901: I0815 10:16:32.213716  8146 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1901: I0815 10:16:34.683542  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.683594  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.683719  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.683729  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.684762  8146 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 10:16:34.684785  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.684827  8146 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 10:16:34.684834  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.685240  8146 pybind.cc:1827] need skip: 0
1901: I0815 10:16:34.685328  8146 pybind.cc:1827] need skip: 0
1901: I0815 10:16:34.685729  8146 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0815 10:16:34.686035  8146 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0815 10:16:34.686051  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 10:16:34.686139  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0815 10:16:34.686152  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 10:16:34.689188  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.689870  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.689886  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.689900  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.692898  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:34.692914  8146 scope.cc:202] Create variable feed
1901: I0815 10:16:34.692979  8146 program_interpreter.cc:243] New Executor is Running.
1901: I0815 10:16:34.692987  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:34.692996  8146 scope.cc:202] Create variable MedianIndex
1901: I0815 10:16:34.693006  8146 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x456a4400 type is 7
1901: I0815 10:16:34.693014  8146 scope.cc:202] Create variable Out
1901: I0815 10:16:34.693019  8146 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x456a48f0 type is 7
1901: I0815 10:16:34.693025  8146 scope.cc:202] Create variable Out@GRAD
1901: I0815 10:16:34.693028  8146 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x456a4da0 type is 7
1901: I0815 10:16:34.693033  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.693037  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x456a58e0 type is 7
1901: I0815 10:16:34.693042  8146 scope.cc:202] Create variable X@GRAD
1901: I0815 10:16:34.693045  8146 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x456a5b50 type is 7
1901: I0815 10:16:34.693051  8146 scope.cc:202] Create variable _generated_var_0
1901: I0815 10:16:34.693054  8146 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x456a5d90 type is 7
1901: I0815 10:16:34.693063  8146 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0815 10:16:34.693068  8146 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x456a5ff0 type is 7
1901: I0815 10:16:34.693073  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x456a4380 type is 9
1901: I0815 10:16:34.693078  8146 scope.cc:202] Create variable fetch
1901: I0815 10:16:34.693081  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x456a5d70 type is 10
1901: I0815 10:16:34.693205  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:34.693213  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.693219  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.693224  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: W0815 10:16:34.693850  8146 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1901: I0815 10:16:34.694159  8146 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1901: W0815 10:16:34.696205  8146 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1901: I0815 10:16:34.696439  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.696466  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.697010  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.697021  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.697041  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.701869  8146 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.701889  8146 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.701906  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.701982  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.702088  8146 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702098  8146 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702154  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.702175  8146 interpreter_util.cc:647] Standalone Executor is Used.
1901: I0815 10:16:34.702207  8146 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702216  8146 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702234  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 10:16:34.702342  8146 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702353  8146 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702371  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 10:16:34.702490  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.702514  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.702538  8146 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.702545  8146 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45723df0Variable Type 7
1901: I0815 10:16:34.702569  8146 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.702591  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.702637  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.702659  8146 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.702775  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.702808  8146 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:34.703377  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.703423  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.704438  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.704458  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.704504  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.704511  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.705135  8146 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 10:16:34.705152  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.705183  8146 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 10:16:34.705189  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.705473  8146 pybind.cc:1827] need skip: 0
1901: I0815 10:16:34.705526  8146 pybind.cc:1827] need skip: 0
1901: I0815 10:16:34.705842  8146 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0815 10:16:34.705920  8146 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0815 10:16:34.705929  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 10:16:34.705993  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0815 10:16:34.706002  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 10:16:34.708055  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.708546  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.708560  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.708565  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.711953  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:34.711970  8146 scope.cc:202] Create variable feed
1901: I0815 10:16:34.711997  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:34.712005  8146 scope.cc:202] Create variable MedianIndex
1901: I0815 10:16:34.712010  8146 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x44c125f0 type is 7
1901: I0815 10:16:34.712018  8146 scope.cc:202] Create variable Out
1901: I0815 10:16:34.712023  8146 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x452315a0 type is 7
1901: I0815 10:16:34.712028  8146 scope.cc:202] Create variable Out@GRAD
1901: I0815 10:16:34.712033  8146 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x45673a90 type is 7
1901: I0815 10:16:34.712036  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.712041  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x456864c0 type is 7
1901: I0815 10:16:34.712046  8146 scope.cc:202] Create variable X@GRAD
1901: I0815 10:16:34.712049  8146 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x45687580 type is 7
1901: I0815 10:16:34.712054  8146 scope.cc:202] Create variable _generated_var_0
1901: I0815 10:16:34.712057  8146 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x45730680 type is 7
1901: I0815 10:16:34.712064  8146 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0815 10:16:34.712069  8146 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x4566bd90 type is 7
1901: I0815 10:16:34.712072  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x45668a20 type is 9
1901: I0815 10:16:34.712078  8146 scope.cc:202] Create variable fetch
1901: I0815 10:16:34.712081  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x45730660 type is 10
1901: I0815 10:16:34.712170  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:34.712177  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.712181  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.712186  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.712229  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712244  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712291  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712306  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712325  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.712796  8146 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712811  8146 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712826  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.712872  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.712930  8146 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712940  8146 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.712977  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.713007  8146 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.713017  8146 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.713032  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 10:16:34.713101  8146 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.713112  8146 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.713128  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 10:16:34.713228  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.713241  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.713256  8146 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.713263  8146 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4526f510Variable Type 7
1901: I0815 10:16:34.713279  8146 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.713294  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.713320  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.713335  8146 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.713389  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.713410  8146 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:34.713822  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 10:16:34.713857  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.715430  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:34.715616  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: I0815 10:16:34.715669  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.716477  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:34.716532  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.717063  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x44c108a0)  to GradNodeAccumulation (addr: 0x1adce3a0)
1901: I0815 10:16:34.717195  8146 dygraph_functions.cc:51757] Running AD API: mean
1901: I0815 10:16:34.717221  8146 dygraph_functions.cc:51814] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.717320  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.717343  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from MeanGradNode (addr: 0x457684f0)  to NanmedianGradNode (addr: 0x44c108a0)
1901: I0815 10:16:34.717432  8146 backward.cc:442] Run in Backward
1901: I0815 10:16:34.717442  8146 backward.cc:113] Start Backward
1901: I0815 10:16:34.717464  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:34.717518  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.717550  8146 backward.cc:255] Preparing GradNode:MeanGradNode addr:0x457684f0
1901: I0815 10:16:34.717563  8146 nodes.cc:25338] Running AD API GRAD: mean_grad
1901: I0815 10:16:34.717604  8146 nodes.cc:25394] { Input: [ 
1901: ( grad_out , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.717679  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.717706  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:34.717717  8146 backward.cc:335] Node: MeanGradNode addr:0x457684f0, Found pending node: NanmedianGradNode addr: 0x44c108a0
1901: I0815 10:16:34.717725  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:34.717758  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x44c108a0
1901: I0815 10:16:34.717773  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:34.717799  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.717855  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:34.717880  8146 backward.cc:335] Node: NanmedianGradNode addr:0x44c108a0, Found pending node: GradNodeAccumulation addr: 0x1adce3a0
1901: I0815 10:16:34.717888  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:34.717905  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1adce3a0
1901: I0815 10:16:34.717916  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:34.717926  8146 accumulation_node.cc:40] Move Tensor ptr: 0x45689800
1901: I0815 10:16:34.717929  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:34.717934  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: /home/code/Paddle/build/python/paddle/base/framework.py:667: VisibleDeprecationWarning: [93m
1901: Warning:
1901: API "paddle.base.dygraph.tensor_patch_methods.gradient" is deprecated since 2.1.0, and will be removed in future versions.
1901:     Reason: Please use tensor.grad, which returns the tensor value of the gradient. [0m
1901:   return func(*args, **kwargs)
1901: I0815 10:16:34.725210  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:34.725358  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.725401  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.mean" (%1) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1901:     (%5) = "pd_op.full_like" (%3, %4) {dtype:(pd_op.DataType)float16,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<1xf32>) -> builtin.tensor<f16>
1901:     (%6) = "pd_op.mean_grad" (%1, %5) {axis:(pd_op.IntArray)[],keepdim:false,reduce_all:false,stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%7) = "pd_op.nanmedian_grad" (%0, %2, %6) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false]} : (builtin.tensor<100x100xf16>, builtin.tensor<2xi64>, builtin.tensor<f16>) -> builtin.tensor<100x100xf16>
1901:     (%8) = "pd_op.fetch" (%7) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<100x100xf16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: I0815 10:16:34.790036  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47accd70 on Place(gpu:0)
1901: I0815 10:16:34.790079  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.790107  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_1
1901: I0815 10:16:34.790117  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_2
1901: I0815 10:16:34.790124  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_3
1901: I0815 10:16:34.790134  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_4
1901: I0815 10:16:34.790141  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_5
1901: I0815 10:16:34.790150  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_6
1901: I0815 10:16:34.790156  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_7
1901: I0815 10:16:34.790166  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_8
1901: I0815 10:16:34.790172  8146 scope.cc:202] Create variable 0x47accd701723716994790065603_inner_var_9
1901: I0815 10:16:34.790181  8146 scope.cc:202] Create variable fetch0@fetch
1901: I0815 10:16:34.790587  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 10:16:34.790601  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.790606  8146 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: I0815 10:16:34.790648  8146 pir_interpreter.cc:1455] New Executor is Running ...
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.mean ( 2 ) 
1901: 3: ( 5 )  = pd_op.full
1901: 4: ( 6 )  = pd_op.full_like ( 5 )  ( 4 ) 
1901: 5: ( 7 )  = pd_op.mean_grad ( 6 )  ( 2 ) 
1901: 6: ( 8 )  = pd_op.nanmedian_grad ( 7 )  ( 3 )  ( 1 ) 
1901: 7: ( 9 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 8: ( 10 )  = pd_op.fetch ( 9 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47acb3b0
1901: 1 -> 0x47accd701723716994790065603_inner_var_1 -> 0x47accd50
1901: 2 -> 0x47accd701723716994790065603_inner_var_2 -> 0x47ac9b80
1901: 3 -> 0x47accd701723716994790065603_inner_var_3 -> 0x47ac9310
1901: 4 -> 0x47accd701723716994790065603_inner_var_4 -> 0x47ac8df0
1901: 5 -> 0x47accd701723716994790065603_inner_var_5 -> 0x47acd540
1901: 6 -> 0x47accd701723716994790065603_inner_var_6 -> 0x47acd960
1901: 7 -> 0x47accd701723716994790065603_inner_var_7 -> 0x47acdd80
1901: 8 -> 0x47accd701723716994790065603_inner_var_8 -> 0x47acc460
1901: 9 -> 0x47accd701723716994790065603_inner_var_9 -> 0x47ace1a0
1901: 10 -> fetch0@fetch -> 0x47ace9b0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 4 
1901: 3 -> 4 
1901: 4 -> 5 
1901: 5 -> 6 
1901: 6 -> 7 
1901: 7 -> 8 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->3[pd_op.full]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.mean]->
1901: 2 downstreams: 
1901: 3 downstreams: 4[pd_op.full_like]->
1901: 4 downstreams: 5[pd_op.mean_grad]->
1901: 5 downstreams: 6[pd_op.nanmedian_grad]->
1901: 6 downstreams: 7[pd_op.memcpy_d2h]->
1901: 7 downstreams: 8[pd_op.fetch]->
1901: 8 downstreams: 
1901: I0815 10:16:34.791572  8146 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1901: I0815 10:16:34.791776  8183 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 10:16:34.791970  8184 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:34.791975  8185 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:34.792001  8186 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:34.792114  8187 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:34.792155  8188 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:34.792127  8185 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x47accd701723716994790065603_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.792223  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.792284  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.792304  8185 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x47accd701723716994790065603_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1901: I0815 10:16:34.792351  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47accd701723716994790065603_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47accd701723716994790065603_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.792862  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47accd701723716994790065603_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47accd701723716994790065603_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.792894  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x47accd701723716994790065603_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.792946  8188 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.792963  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x47accd701723716994790065603_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.792990  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x47accd701723716994790065603_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x47accd701723716994790065603_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.793041  8188 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.793119  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x47accd701723716994790065603_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x47accd701723716994790065603_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.793146  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x47accd701723716994790065603_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x47accd701723716994790065603_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.793218  8188 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:34.793234  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x47accd701723716994790065603_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x47accd701723716994790065603_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.793258  8188 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x47accd701723716994790065603_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x47accd701723716994790065603_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x47accd701723716994790065603_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.793347  8188 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x47accd701723716994790065603_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x47accd701723716994790065603_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x47accd701723716994790065603_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.793439  8185 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47accd701723716994790065603_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.793473  8185 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.793599  8185 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47accd701723716994790065603_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47accd701723716994790065603_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.793637  8185 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47accd701723716994790065603_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.793663  8185 tensor_utils.cc:57] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.793694  8185 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47accd701723716994790065603_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.793730  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47accee0) got event_name: TaskCompletion
1901: I0815 10:16:34.793761  8146 tensor_util.cc:48] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.798014  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.798043  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.798105  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.798115  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.799322  8183 thread_data_registry.h:135] Add data {current : -20004, peak : 0} from thread 14104122751991627278 to 12896966126556168739 , after update, data is {current : -20004, peak : 16}.
1901: I0815 10:16:34.799341  8183 thread_data_registry.h:135] Add data {current : -20024, peak : 0} from thread 14104122751991627278 to 12896966126556168739 , after update, data is {current : 0, peak : 330659}.
1901: I0815 10:16:34.799516  8185 thread_data_registry.h:135] Add data {current : 40004, peak : 40004} from thread 13286838499092016549 to 12896966126556168739 , after update, data is {current : 20000, peak : 40004}.
1901: I0815 10:16:34.799679  8188 thread_data_registry.h:135] Add data {current : 20000, peak : 40004} from thread 12896966126556168739 to 9009457683004622817 , after update, data is {current : 20000, peak : 40004}.
1901: I0815 10:16:34.799690  8188 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 12896966126556168739 to 9009457683004622817 , after update, data is {current : 120000, peak : 440631}.
1901: I0815 10:16:34.801083  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.801666  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.802145  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.802158  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.802165  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.804478  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:34.804497  8146 scope.cc:202] Create variable feed
1901: I0815 10:16:34.804529  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:34.804538  8146 scope.cc:202] Create variable MedianIndex
1901: I0815 10:16:34.804544  8146 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x47aea560 type is 7
1901: I0815 10:16:34.804555  8146 scope.cc:202] Create variable Out
1901: I0815 10:16:34.804559  8146 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x47ae9af0 type is 7
1901: I0815 10:16:34.804565  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.804569  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47aea4e0 type is 7
1901: I0815 10:16:34.804575  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x47ae9b10 type is 9
1901: I0815 10:16:34.804584  8146 scope.cc:202] Create variable fetch
1901: I0815 10:16:34.804587  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x47aea8c0 type is 10
1901: I0815 10:16:34.804663  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:34.804670  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.804677  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.804682  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.804736  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.804752  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.804819  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.804831  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.804850  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.805369  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.805387  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.805408  8146 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.805415  8146 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47aee860Variable Type 7
1901: I0815 10:16:34.805434  8146 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.805455  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.805481  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.805500  8146 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.805547  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.805572  8146 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:34.805621  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.805631  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.805649  8146 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.805656  8146 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47af3740Variable Type 7
1901: I0815 10:16:34.805672  8146 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.805687  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.805707  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.805722  8146 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.805757  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.805785  8146 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 10:16:34.806067  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.806097  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.807422  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.807448  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.807502  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:34.807511  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.809441  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.810002  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:34.810446  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.810459  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.810465  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.812700  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:34.812772  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:34.812783  8146 scope.cc:202] Create variable MedianIndex
1901: I0815 10:16:34.812788  8146 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x47b20ca0 type is 7
1901: I0815 10:16:34.812798  8146 scope.cc:202] Create variable Out
1901: I0815 10:16:34.812803  8146 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x47b1ffd0 type is 7
1901: I0815 10:16:34.812808  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.812813  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47b209d0 type is 7
1901: I0815 10:16:34.812820  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x47ae9b10 type is 9
1901: I0815 10:16:34.812829  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x47aea8c0 type is 10
1901: I0815 10:16:34.812901  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:34.812908  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:34.812913  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:34.812918  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:34.812960  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.812974  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.813032  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.813042  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.813061  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:34.813591  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.813607  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.813627  8146 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.813635  8146 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47b25130Variable Type 7
1901: I0815 10:16:34.813653  8146 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.813670  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.813694  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.813710  8146 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.813751  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.813772  8146 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:34.813819  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.813830  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:34.813848  8146 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:34.813854  8146 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47b25150Variable Type 7
1901: I0815 10:16:34.813868  8146 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:34.813882  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.813901  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:34.813916  8146 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.813951  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:34.813972  8146 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 10:16:34.814241  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.814270  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:34.931427  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: I0815 10:16:34.931836  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.931900  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:34.932497  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:34.932541  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.933738  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: I0815 10:16:34.933887  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:34.933943  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.935014  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:34.935052  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:34.937425  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: I0815 10:16:34.937582  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.937631  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 10:16:34.940761  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47ad8450 on Place(gpu:0)
1901: I0815 10:16:34.940797  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.940820  8146 scope.cc:202] Create variable 0x47ad84501723716994940788247_inner_var_1
1901: I0815 10:16:34.940832  8146 scope.cc:202] Create variable 0x47ad84501723716994940788247_inner_var_2
1901: I0815 10:16:34.940842  8146 scope.cc:202] Create variable 0x47ad84501723716994940788247_inner_var_3
1901: I0815 10:16:34.940851  8146 scope.cc:202] Create variable 0x47ad84501723716994940788247_inner_var_4
1901: I0815 10:16:34.940862  8146 scope.cc:202] Create variable fetch0@fetch
1901: I0815 10:16:34.941228  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 10:16:34.941244  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.941248  8146 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47af8b40
1901: 1 -> 0x47ad84501723716994940788247_inner_var_1 -> 0x47b2d2b0
1901: 2 -> 0x47ad84501723716994940788247_inner_var_2 -> 0x1af3e5f0
1901: 3 -> 0x47ad84501723716994940788247_inner_var_3 -> 0x45721730
1901: 4 -> 0x47ad84501723716994940788247_inner_var_4 -> 0x47ad1940
1901: 5 -> fetch0@fetch -> 0x47b21840
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 10:16:34.941913  8190 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 10:16:34.942059  8191 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:34.942102  8192 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:34.942147  8193 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:34.942236  8194 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:34.942310  8195 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:34.942349  8195 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.942382  8195 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.942411  8195 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47ad84501723716994940788247_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47ad84501723716994940788247_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.942826  8195 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47ad84501723716994940788247_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47ad84501723716994940788247_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.942907  8194 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47ad84501723716994940788247_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.942950  8194 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.943022  8194 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47ad84501723716994940788247_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47ad84501723716994940788247_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.943055  8194 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47ad84501723716994940788247_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.943074  8194 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.943087  8194 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47ad84501723716994940788247_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.943120  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47ad85c0) got event_name: TaskCompletion
1901: I0815 10:16:34.943140  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.943646  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:34.943781  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.943823  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 10:16:34.945912  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47ad6c10 on Place(gpu:0)
1901: I0815 10:16:34.945938  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.945953  8146 scope.cc:202] Create variable 0x47ad6c101723716994945932742_inner_var_1
1901: I0815 10:16:34.945963  8146 scope.cc:202] Create variable 0x47ad6c101723716994945932742_inner_var_2
1901: I0815 10:16:34.945971  8146 scope.cc:202] Create variable 0x47ad6c101723716994945932742_inner_var_3
1901: I0815 10:16:34.945979  8146 scope.cc:202] Create variable 0x47ad6c101723716994945932742_inner_var_4
1901: I0815 10:16:34.945986  8146 scope.cc:202] Create variable fetch0@fetch
1901: I0815 10:16:34.946242  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 10:16:34.946255  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.946259  8146 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x457f4f80
1901: 1 -> 0x47ad6c101723716994945932742_inner_var_1 -> 0x47b06b80
1901: 2 -> 0x47ad6c101723716994945932742_inner_var_2 -> 0x47b2bdb0
1901: 3 -> 0x47ad6c101723716994945932742_inner_var_3 -> 0x47ae3560
1901: 4 -> 0x47ad6c101723716994945932742_inner_var_4 -> 0x47ad7420
1901: 5 -> fetch0@fetch -> 0x47ad2fa0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 10:16:34.946767  8196 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 10:16:34.946971  8197 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:34.947005  8198 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:34.947070  8199 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:34.947144  8200 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:34.947207  8201 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:34.947243  8201 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.947264  8201 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.947290  8201 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47ad6c101723716994945932742_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47ad6c101723716994945932742_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.947685  8201 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47ad6c101723716994945932742_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47ad6c101723716994945932742_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.947744  8200 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47ad6c101723716994945932742_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.947774  8200 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.947831  8200 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47ad6c101723716994945932742_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47ad6c101723716994945932742_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.947860  8200 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47ad6c101723716994945932742_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.947878  8200 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.947889  8200 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47ad6c101723716994945932742_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.947919  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47ad6d80) got event_name: TaskCompletion
1901: I0815 10:16:34.947943  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.949236  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adc9000 for it.
1901: I0815 10:16:34.949419  8146 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:34.949472  8146 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 10:16:34.951990  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47b45260 on Place(gpu:0)
1901: I0815 10:16:34.952020  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.952039  8146 scope.cc:202] Create variable 0x47b452601723716994952013900_inner_var_1
1901: I0815 10:16:34.952050  8146 scope.cc:202] Create variable 0x47b452601723716994952013900_inner_var_2
1901: I0815 10:16:34.952061  8146 scope.cc:202] Create variable 0x47b452601723716994952013900_inner_var_3
1901: I0815 10:16:34.952075  8146 scope.cc:202] Create variable 0x47b452601723716994952013900_inner_var_4
1901: I0815 10:16:34.952086  8146 scope.cc:202] Create variable fetch0@fetch
1901: I0815 10:16:34.952418  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 10:16:34.952435  8146 scope.cc:202] Create variable X
1901: I0815 10:16:34.952438  8146 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47ae6f20
1901: 1 -> 0x47b452601723716994952013900_inner_var_1 -> 0x47b45240
1901: 2 -> 0x47b452601723716994952013900_inner_var_2 -> 0x47b48fa0
1901: 3 -> 0x47b452601723716994952013900_inner_var_3 -> 0x47afce10
1901: 4 -> 0x47b452601723716994952013900_inner_var_4 -> 0x47b48bc0
1901: 5 -> fetch0@fetch -> 0x47b03850
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 10:16:34.953039  8202 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 10:16:34.953269  8203 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:34.953284  8204 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:34.953373  8205 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:34.953469  8206 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:34.953476  8207 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:34.953511  8207 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.953531  8207 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 10:16:34.953555  8207 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b452601723716994952013900_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47b452601723716994952013900_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.953974  8207 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b452601723716994952013900_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b452601723716994952013900_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:34.954034  8206 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b452601723716994952013900_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.954056  8206 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:34.954103  8206 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b452601723716994952013900_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b452601723716994952013900_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.954129  8206 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b452601723716994952013900_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:34.954144  8206 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.954156  8206 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b452601723716994952013900_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:34.954181  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47b453d0) got event_name: TaskCompletion
1901: I0815 10:16:34.954200  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:34.954357  8146 shape_analysis.cc:506] InferShapeOrDataForValue,  defining_op: pd_op.fetch id:48
1901: I0815 10:16:34.955673  8146 unary_infer_sym.cc:1363] NanmedianOpInferSymbolicShape
1901: I0815 10:16:35.001360  8190 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 12896966126556168739 to 5208523814054386114 , after update, data is {current : -2, peak : 16}.
1901: I0815 10:16:35.001374  8190 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 12896966126556168739 to 5208523814054386114 , after update, data is {current : 0, peak : 330659}.
1901: I0815 10:16:35.001525  8194 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 12975631832573084373 to 5208523814054386114 , after update, data is {current : 2, peak : 16}.
1901: I0815 10:16:35.001706  8195 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 13286838499092016549 to 5208523814054386114 , after update, data is {current : 2, peak : 16}.
1901: I0815 10:16:35.001718  8195 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 13286838499092016549 to 5208523814054386114 , after update, data is {current : 18, peak : 330659}.
1901: I0815 10:16:35.001863  8196 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 12551711153033439171 to 5208523814054386114 , after update, data is {current : 0, peak : 16}.
1901: I0815 10:16:35.001871  8196 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 12551711153033439171 to 5208523814054386114 , after update, data is {current : 0, peak : 330659}.
1901: I0815 10:16:35.002018  8200 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 2821430045679439767 to 5208523814054386114 , after update, data is {current : 4, peak : 16}.
1901: I0815 10:16:35.002183  8201 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 4782475663937657275 to 5208523814054386114 , after update, data is {current : 4, peak : 16}.
1901: I0815 10:16:35.002190  8201 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 4782475663937657275 to 5208523814054386114 , after update, data is {current : 18, peak : 330659}.
1901: I0815 10:16:35.002321  8202 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 6996341608318450795 to 5208523814054386114 , after update, data is {current : 2, peak : 16}.
1901: I0815 10:16:35.002328  8202 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 6996341608318450795 to 5208523814054386114 , after update, data is {current : 0, peak : 330659}.
1901: I0815 10:16:35.002470  8206 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8392895135818886201 to 5208523814054386114 , after update, data is {current : 6, peak : 16}.
1901: I0815 10:16:35.002636  8207 thread_data_registry.h:135] Add data {current : 6, peak : 16} from thread 5208523814054386114 to 9009457683004622817 , after update, data is {current : 20006, peak : 40004}.
1901: I0815 10:16:35.002645  8207 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 5208523814054386114 to 9009457683004622817 , after update, data is {current : 200000, peak : 560633}.
1901: I0815 10:16:35.007006  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.007108  8146 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 512(0x7fb1c9827c00), and remaining 0
1901: I0815 10:16:35.007193  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.007220  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.007464  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.008235  8146 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.008324  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.008351  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.008514  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.009194  8146 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.009264  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.009290  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.009454  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.010071  8146 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.010143  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.010169  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.010349  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:464: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data)
1901: I0815 10:16:35.011027  8146 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.011083  8146 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fb1c9818e00), and remaining 0
1901: I0815 10:16:35.011134  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.011158  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.011631  8146 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.011698  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.011720  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.012168  8146 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.012233  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.012255  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.012673  8146 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.012737  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.012760  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.013324  8146 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.013394  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.013419  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.013592  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.014221  8146 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.014293  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.014328  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.014494  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.015193  8146 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.015264  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.015290  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.015486  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.016099  8146 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.016171  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.016196  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.016363  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.017021  8146 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.017091  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.017117  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.017313  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.017959  8146 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.018031  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.018057  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.018213  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.018909  8146 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.018982  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.019008  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.019166  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.019831  8146 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.019905  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.019932  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.020082  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.020783  8146 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.020856  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.020881  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.021055  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.021688  8146 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.021761  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.021786  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.021941  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.022472  8146 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.022537  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.022562  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.022711  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.023397  8146 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.023464  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.023489  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.023634  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.024281  8146 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.024358  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.024384  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.024626  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:478: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data, axis)
1901: I0815 10:16:35.026504  8146 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.026576  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.026602  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.026788  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.028142  8146 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.028216  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.028242  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.028456  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.029771  8146 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.029845  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.029872  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.030050  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.031271  8146 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.031356  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.031383  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.031621  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.032850  8146 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.032924  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.032950  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.033129  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.034430  8146 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.034503  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.034529  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.034708  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.036017  8146 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.036092  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.036118  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.036298  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.037523  8146 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.037597  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.037623  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.037850  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.039163  8146 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.039239  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.039264  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.039458  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.040668  8146 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.040743  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.040769  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.040966  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.042186  8146 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.042261  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.042287  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.042477  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.043766  8146 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.043841  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.043867  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.044047  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.045253  8146 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.045336  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.045362  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.045538  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.046814  8146 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.046888  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.046914  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.047091  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.048314  8146 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.048389  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.048414  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.048594  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.049800  8146 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.049875  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.049901  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.050076  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.050808  8146 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.050880  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.050905  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.051075  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.053726  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.053755  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.054585  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.054608  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.055383  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.055406  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.056176  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.056200  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.056962  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.056983  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.059527  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.060070  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.060600  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.061118  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.061643  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.062487  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:35.062505  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:35.062511  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:35.067559  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:35.067632  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:35.067646  8146 scope.cc:202] Create variable X
1901: I0815 10:16:35.067654  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47eeb1f0 type is 7
1901: I0815 10:16:35.067662  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x47ae9b10 type is 9
1901: I0815 10:16:35.067668  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x47aea8c0 type is 10
1901: I0815 10:16:35.067674  8146 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0815 10:16:35.067682  8146 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x47eea4b0 type is 7
1901: I0815 10:16:35.067687  8146 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0815 10:16:35.067691  8146 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x47eeb0a0 type is 7
1901: I0815 10:16:35.067698  8146 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0815 10:16:35.067703  8146 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x47aea170 type is 7
1901: I0815 10:16:35.067708  8146 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0815 10:16:35.067711  8146 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x47ee9e00 type is 7
1901: I0815 10:16:35.067718  8146 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0815 10:16:35.067723  8146 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x47ee9f30 type is 7
1901: I0815 10:16:35.067729  8146 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0815 10:16:35.067734  8146 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x47eeb3f0 type is 7
1901: I0815 10:16:35.067739  8146 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0815 10:16:35.067744  8146 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x47eeb600 type is 7
1901: I0815 10:16:35.067749  8146 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0815 10:16:35.067752  8146 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x47eeb860 type is 7
1901: I0815 10:16:35.067756  8146 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0815 10:16:35.067760  8146 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x47eebac0 type is 7
1901: I0815 10:16:35.067765  8146 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0815 10:16:35.067770  8146 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x47eebd20 type is 7
1901: I0815 10:16:35.067901  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:35.067909  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:35.067914  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:35.067919  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:35.067982  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.067997  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068065  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068076  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068094  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.068269  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.068476  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068491  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068509  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.068643  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.068835  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068847  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.068863  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.068989  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.069175  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.069186  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.069202  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.069360  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.069567  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.069581  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.069597  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.069747  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.069944  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.069957  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.069974  8146 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.069983  8146 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47b11aa0Variable Type 7
1901: I0815 10:16:35.070001  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.070020  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.070044  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.070062  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.070106  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.070127  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:35.070169  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070180  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070195  8146 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.070202  8146 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47b11ae0Variable Type 7
1901: I0815 10:16:35.070217  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.070232  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.070251  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.070266  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.070307  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.070338  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 10:16:35.070390  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070400  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070415  8146 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.070422  8146 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47df5f20Variable Type 7
1901: I0815 10:16:35.070437  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.070451  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.070470  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.070484  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.070515  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.070528  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0815 10:16:35.070567  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070578  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070591  8146 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.070598  8146 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47de4000Variable Type 7
1901: I0815 10:16:35.070612  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.070626  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.070643  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.070658  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.070690  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.070703  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0815 10:16:35.070741  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070751  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.070765  8146 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.070772  8146 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x464f0910Variable Type 7
1901: I0815 10:16:35.070786  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.070799  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.070816  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.070830  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.070863  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.070875  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0815 10:16:35.071492  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.071527  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.071548  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.071568  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.071588  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0815 10:16:35.077654  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47b13100 on Place(gpu:0)
1901: I0815 10:16:35.077687  8146 scope.cc:202] Create variable X
1901: I0815 10:16:35.077705  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_1
1901: I0815 10:16:35.077718  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_2
1901: I0815 10:16:35.077728  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_3
1901: I0815 10:16:35.077736  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_4
1901: I0815 10:16:35.077745  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_5
1901: I0815 10:16:35.077754  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_6
1901: I0815 10:16:35.077762  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_7
1901: I0815 10:16:35.077770  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_8
1901: I0815 10:16:35.077780  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_9
1901: I0815 10:16:35.077788  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_10
1901: I0815 10:16:35.077797  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_11
1901: I0815 10:16:35.077806  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_12
1901: I0815 10:16:35.077817  8146 scope.cc:202] Create variable fetch0@fetch
1901: I0815 10:16:35.077831  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_14
1901: I0815 10:16:35.077840  8146 scope.cc:202] Create variable fetch1@fetch
1901: I0815 10:16:35.077849  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_16
1901: I0815 10:16:35.077859  8146 scope.cc:202] Create variable fetch2@fetch
1901: I0815 10:16:35.077867  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_18
1901: I0815 10:16:35.077876  8146 scope.cc:202] Create variable fetch3@fetch
1901: I0815 10:16:35.077884  8146 scope.cc:202] Create variable 0x47b131001723716995077679181_inner_var_20
1901: I0815 10:16:35.077893  8146 scope.cc:202] Create variable fetch4@fetch
1901: I0815 10:16:35.078171  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 10:16:35.078184  8146 scope.cc:202] Create variable X
1901: I0815 10:16:35.078188  8146 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47de9460
1901: 1 -> 0x47b131001723716995077679181_inner_var_1 -> 0x47de9950
1901: 2 -> 0x47b131001723716995077679181_inner_var_2 -> 0x47b11da0
1901: 3 -> 0x47b131001723716995077679181_inner_var_3 -> 0x47de3970
1901: 4 -> 0x47b131001723716995077679181_inner_var_4 -> 0x47de95f0
1901: 5 -> 0x47b131001723716995077679181_inner_var_5 -> 0x47eee240
1901: 6 -> 0x47b131001723716995077679181_inner_var_6 -> 0x47f267d0
1901: 7 -> 0x47b131001723716995077679181_inner_var_7 -> 0x47eeaa90
1901: 8 -> 0x47b131001723716995077679181_inner_var_8 -> 0x47e05720
1901: 9 -> 0x47b131001723716995077679181_inner_var_9 -> 0x47dfaba0
1901: 10 -> 0x47b131001723716995077679181_inner_var_10 -> 0x47b1f8f0
1901: 11 -> 0x47b131001723716995077679181_inner_var_11 -> 0x47b1fd10
1901: 12 -> 0x47b131001723716995077679181_inner_var_12 -> 0x47eee1b0
1901: 13 -> fetch0@fetch -> 0x47b1ea30
1901: 14 -> 0x47b131001723716995077679181_inner_var_14 -> 0x47b1e5f0
1901: 15 -> fetch1@fetch -> 0x47dfd470
1901: 16 -> 0x47b131001723716995077679181_inner_var_16 -> 0x47b1ea10
1901: 17 -> fetch2@fetch -> 0x47f35860
1901: 18 -> 0x47b131001723716995077679181_inner_var_18 -> 0x47dfd450
1901: 19 -> fetch3@fetch -> 0x47f28690
1901: 20 -> 0x47b131001723716995077679181_inner_var_20 -> 0x47f35840
1901: 21 -> fetch4@fetch -> 0x47e1bd30
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0815 10:16:35.079614  8208 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:35.079660  8209 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:35.079667  8210 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:35.079700  8211 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:35.079743  8212 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:35.079770  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.079807  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0815 10:16:35.079849  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47b131001723716995077679181_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080026  8212 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.080183  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b131001723716995077679181_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.080231  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_5:[dtype=;place=;dim=;lod={};, 0x47b131001723716995077679181_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080247  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080273  8211 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.080336  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.080394  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080411  8212 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.080413  8211 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.080426  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.080554  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b131001723716995077679181_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.080596  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_7:[dtype=;place=;dim=;lod={};, 0x47b131001723716995077679181_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080603  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080617  8211 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.080682  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.080724  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080739  8211 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.080744  8212 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.080749  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.080886  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b131001723716995077679181_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.080925  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_9:[dtype=;place=;dim=;lod={};, 0x47b131001723716995077679181_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080930  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.080945  8211 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.080982  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081022  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081036  8211 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081045  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081095  8212 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.081245  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b131001723716995077679181_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.081284  8212 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_11:[dtype=;place=;dim=;lod={};, 0x47b131001723716995077679181_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081290  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081316  8211 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.081346  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081403  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081418  8211 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081426  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081471  8212 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.081622  8212 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47b131001723716995077679181_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47b131001723716995077679181_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.081669  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081683  8211 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.081709  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47b131001723716995077679181_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47b131001723716995077679181_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081741  8211 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.081755  8211 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081765  8211 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47b131001723716995077679181_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.081791  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47b13270) got event_name: TaskCompletion
1901: I0815 10:16:35.081811  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081835  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081845  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081856  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.081862  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.083495  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.083581  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.083609  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.083782  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.083884  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f23a40)  to GradNodeAccumulation (addr: 0x1adce3a0)
1901: I0815 10:16:35.084012  8146 backward.cc:459] Run in Grad
1901: I0815 10:16:35.084026  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.084079  8146 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x47f23a40 to ptr: 0x47f36cf0
1901: I0815 10:16:35.084091  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.084136  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.084167  8146 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1adce3a0 to ptr: 0x47f21c50
1901: I0815 10:16:35.084193  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f36cf0
1901: I0815 10:16:35.084201  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.084235  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.084311  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.084322  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f36cf0, Found pending node: GradNodeAccumulation addr: 0x47f21c50
1901: I0815 10:16:35.084327  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.084353  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x47f21c50
1901: I0815 10:16:35.084362  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.084365  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.084371  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.084375  8146 backward.cc:435] Finish Backward
1901: I0815 10:16:35.085070  8146 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0815 10:16:35.085088  8146 dygraph_functions.cc:77659] { Input: []} 
1901: I0815 10:16:35.085193  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.085217  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.085371  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.085446  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f23a40)  to GradNodeAccumulation (addr: 0x1adce3a0)
1901: I0815 10:16:35.085543  8146 backward.cc:442] Run in Backward
1901: I0815 10:16:35.085551  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.085557  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.085588  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.085613  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f23a40
1901: I0815 10:16:35.085623  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.085650  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.085704  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.085732  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f23a40, Found pending node: GradNodeAccumulation addr: 0x1adce3a0
1901: I0815 10:16:35.085740  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.085758  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1adce3a0
1901: I0815 10:16:35.085765  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.085769  8146 accumulation_node.cc:40] Move Tensor ptr: 0x47f3c5b0
1901: I0815 10:16:35.085773  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.085777  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.088732  8146 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1901: I0815 10:16:35.089234  8146 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.089360  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.089387  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.089557  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47de8700)  to GradNodeAccumulation (addr: 0x1ae2de80)
1901: I0815 10:16:35.089660  8146 backward.cc:442] Run in Backward
1901: I0815 10:16:35.089668  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.089675  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.089706  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.089730  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47de8700
1901: I0815 10:16:35.089737  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.089766  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.089814  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.089839  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47de8700, Found pending node: GradNodeAccumulation addr: 0x1ae2de80
1901: I0815 10:16:35.089847  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.089864  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ae2de80
1901: I0815 10:16:35.089871  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.089875  8146 accumulation_node.cc:40] Move Tensor ptr: 0x45682140
1901: I0815 10:16:35.089879  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.089882  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.090641  8146 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.090723  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.090747  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.090950  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.091048  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f23a40)  to GradNodeAccumulation (addr: 0x1ae2de80)
1901: I0815 10:16:35.091166  8146 backward.cc:459] Run in Grad
1901: I0815 10:16:35.091176  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.091189  8146 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x47f23a40 to ptr: 0x47de8380
1901: I0815 10:16:35.091198  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.091228  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.091251  8146 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ae2de80 to ptr: 0x47f21c50
1901: I0815 10:16:35.091270  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47de8380
1901: I0815 10:16:35.091277  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.091315  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.091446  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.091456  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47de8380, Found pending node: GradNodeAccumulation addr: 0x47f21c50
1901: I0815 10:16:35.091462  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.091478  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x47f21c50
1901: I0815 10:16:35.091485  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.091490  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.091493  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.091498  8146 backward.cc:435] Finish Backward
1901: I0815 10:16:35.092396  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.092475  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.092501  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.092658  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.094099  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.094157  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.094179  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.101590  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.101624  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.102753  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.102777  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.103770  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.103901  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.103937  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.104202  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.104792  8146 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.104875  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.104902  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.105094  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.105710  8146 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.105785  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.105813  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.105979  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.106561  8146 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.106637  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.106664  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.106837  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.107447  8146 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.107501  8146 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fb1c9819200), and remaining 0
1901: I0815 10:16:35.107555  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.107581  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.108073  8146 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.108143  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.108170  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.108709  8146 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.108786  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.108814  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.109323  8146 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.109396  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.109423  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.109927  8146 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.109994  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.110019  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.110220  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.110788  8146 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.110863  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.110889  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.111064  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.111670  8146 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.111743  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.111771  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.111936  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.112527  8146 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.112603  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.112629  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.112800  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.113412  8146 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.113485  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.113513  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.113684  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.114265  8146 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.114352  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.114380  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.114554  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.115160  8146 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.115236  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.115262  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.115437  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.116034  8146 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.116111  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.116137  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.116314  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.116932  8146 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.117004  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.117031  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.117203  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.117774  8146 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.117849  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.117877  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.118048  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.118570  8146 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.118625  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.118645  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.118765  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.119375  8146 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.119441  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.119465  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.119612  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.120246  8146 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.120332  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.120361  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.120563  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.121243  8146 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.121320  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.121346  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.121518  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.122175  8146 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.122243  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.122268  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.122455  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.123085  8146 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.123153  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.123178  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.123358  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.123998  8146 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.124065  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.124090  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.124275  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.124884  8146 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.124953  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.124977  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.125149  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.125773  8146 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.125841  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.125866  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.126040  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.126674  8146 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.126742  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.126767  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.126938  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.127578  8146 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.127647  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.127673  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.127864  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.128485  8146 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.128554  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.128580  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.128753  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.129359  8146 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.129427  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.129452  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.129628  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.130224  8146 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.130293  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.130327  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.130499  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.131098  8146 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.131166  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.131191  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.131374  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.131970  8146 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.132040  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.132064  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.132232  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.132848  8146 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.132916  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.132941  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.133113  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.133790  8146 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.133865  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.133893  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.134080  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.134667  8146 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.134737  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.134761  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.134949  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.135687  8146 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.135763  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.135792  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.135979  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.138025  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.138047  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.138782  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.138800  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.139434  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.139452  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.140069  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.140085  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.140693  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.140710  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.142457  8211 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 12975631832573084373 to 13286838499092016549 , after update, data is {current : 0, peak : 1260}.
1901: I0815 10:16:35.142473  8211 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 12975631832573084373 to 13286838499092016549 , after update, data is {current : 20, peak : 24}.
1901: I0815 10:16:35.142999  8212 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 13286838499092016549 to 9009457683004622817 , after update, data is {current : 0, peak : 260}.
1901: I0815 10:16:35.143015  8212 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 13286838499092016549 to 9009457683004622817 , after update, data is {current : 20026, peak : 40004}.
1901: I0815 10:16:35.143020  8212 thread_data_registry.h:135] Add data {current : 0, peak : 1260} from thread 13286838499092016549 to 9009457683004622817 , after update, data is {current : 162560, peak : 560633}.
1901: I0815 10:16:35.144800  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.145272  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.145736  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.146185  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.146646  8146 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 10:16:35.147512  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:35.147529  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:35.147536  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:35.151919  8146 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 10:16:35.151973  8146 interpreter_util.cc:1169] Creating Variables
1901: I0815 10:16:35.151984  8146 scope.cc:202] Create variable X
1901: I0815 10:16:35.151993  8146 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47f0c5f0 type is 7
1901: I0815 10:16:35.152002  8146 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x47ae9b10 type is 9
1901: I0815 10:16:35.152010  8146 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x47aea8c0 type is 10
1901: I0815 10:16:35.152016  8146 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0815 10:16:35.152020  8146 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x47f0bbe0 type is 7
1901: I0815 10:16:35.152027  8146 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0815 10:16:35.152033  8146 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x47f0b700 type is 7
1901: I0815 10:16:35.152040  8146 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0815 10:16:35.152045  8146 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x47f0c770 type is 7
1901: I0815 10:16:35.152051  8146 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0815 10:16:35.152055  8146 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x47f0c850 type is 7
1901: I0815 10:16:35.152061  8146 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0815 10:16:35.152068  8146 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x47f0c930 type is 7
1901: I0815 10:16:35.152076  8146 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0815 10:16:35.152081  8146 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x47f0cb90 type is 7
1901: I0815 10:16:35.152087  8146 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0815 10:16:35.152093  8146 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x47dc4df0 type is 7
1901: I0815 10:16:35.152101  8146 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0815 10:16:35.152105  8146 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x47dc5050 type is 7
1901: I0815 10:16:35.152112  8146 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0815 10:16:35.152117  8146 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x47dc52b0 type is 7
1901: I0815 10:16:35.152122  8146 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0815 10:16:35.152127  8146 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x47dc5510 type is 7
1901: I0815 10:16:35.152251  8146 interpreter_util.cc:594] Static build: 0
1901: I0815 10:16:35.152258  8146 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 10:16:35.152264  8146 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 10:16:35.152268  8146 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 10:16:35.152359  8146 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152377  8146 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152449  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152460  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152479  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.152654  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.152849  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152863  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.152880  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.153012  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.153196  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153209  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153226  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.153357  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.153538  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153550  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153568  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.153713  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.153910  8146 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153923  8146 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.153940  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.154084  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.154273  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154285  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154314  8146 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.154322  8146 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47de3640Variable Type 7
1901: I0815 10:16:35.154345  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.154363  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.154389  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.154407  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.154449  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.154471  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 10:16:35.154517  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154528  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154544  8146 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.154552  8146 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47e2be30Variable Type 7
1901: I0815 10:16:35.154567  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.154582  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.154600  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.154615  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.154650  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.154682  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 10:16:35.154728  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154739  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154754  8146 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.154762  8146 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47e2b030Variable Type 7
1901: I0815 10:16:35.154776  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.154790  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.154808  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.154824  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.154857  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.154872  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0815 10:16:35.154914  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154925  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.154939  8146 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.154946  8146 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47dedc50Variable Type 7
1901: I0815 10:16:35.154959  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.154973  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.154990  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.155005  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.155040  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.155053  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0815 10:16:35.155092  8146 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.155102  8146 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 10:16:35.155117  8146 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 10:16:35.155123  8146 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47e24ff0Variable Type 7
1901: I0815 10:16:35.155136  8146 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 10:16:35.155150  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.155167  8146 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 10:16:35.155184  8146 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.155215  8146 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 10:16:35.155229  8146 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0815 10:16:35.155831  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.155865  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.155887  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.155908  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 10:16:35.155929  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0815 10:16:35.161913  8146 pir_interpreter.cc:161] PirInterpreter(): 0x47dc29a0 on Place(gpu:0)
1901: I0815 10:16:35.161955  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_1
1901: I0815 10:16:35.161969  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_2
1901: I0815 10:16:35.161980  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_3
1901: I0815 10:16:35.161991  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_4
1901: I0815 10:16:35.162001  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_5
1901: I0815 10:16:35.162012  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_6
1901: I0815 10:16:35.162022  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_7
1901: I0815 10:16:35.162034  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_8
1901: I0815 10:16:35.162045  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_9
1901: I0815 10:16:35.162055  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_10
1901: I0815 10:16:35.162065  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_11
1901: I0815 10:16:35.162075  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_12
1901: I0815 10:16:35.162093  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_14
1901: I0815 10:16:35.162109  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_16
1901: I0815 10:16:35.162124  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_18
1901: I0815 10:16:35.162138  8146 scope.cc:202] Create variable 0x47dc29a01723716995161936769_inner_var_20
1901: I0815 10:16:35.162422  8146 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47e1c040
1901: 1 -> 0x47dc29a01723716995161936769_inner_var_1 -> 0x47ec6710
1901: 2 -> 0x47dc29a01723716995161936769_inner_var_2 -> 0x47e1d5e0
1901: 3 -> 0x47dc29a01723716995161936769_inner_var_3 -> 0x47e125c0
1901: 4 -> 0x47dc29a01723716995161936769_inner_var_4 -> 0x47f2c240
1901: 5 -> 0x47dc29a01723716995161936769_inner_var_5 -> 0x47df5bf0
1901: 6 -> 0x47dc29a01723716995161936769_inner_var_6 -> 0x1ac60220
1901: 7 -> 0x47dc29a01723716995161936769_inner_var_7 -> 0x47e12770
1901: 8 -> 0x47dc29a01723716995161936769_inner_var_8 -> 0x1af424a0
1901: 9 -> 0x47dc29a01723716995161936769_inner_var_9 -> 0x47e1dbd0
1901: 10 -> 0x47dc29a01723716995161936769_inner_var_10 -> 0x47b06f20
1901: 11 -> 0x47dc29a01723716995161936769_inner_var_11 -> 0x47f3c420
1901: 12 -> 0x47dc29a01723716995161936769_inner_var_12 -> 0x47e25b90
1901: 13 -> fetch0@fetch -> 0x47b1ea30
1901: 14 -> 0x47dc29a01723716995161936769_inner_var_14 -> 0x1ac61c20
1901: 15 -> fetch1@fetch -> 0x47dfd470
1901: 16 -> 0x47dc29a01723716995161936769_inner_var_16 -> 0x47f2df20
1901: 17 -> fetch2@fetch -> 0x47f35860
1901: 18 -> 0x47dc29a01723716995161936769_inner_var_18 -> 0x47b46d30
1901: 19 -> fetch3@fetch -> 0x47f28690
1901: 20 -> 0x47dc29a01723716995161936769_inner_var_20 -> 0x47de8d10
1901: 21 -> fetch4@fetch -> 0x47e1bd30
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0815 10:16:35.163830  8213 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 10:16:35.163836  8214 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 10:16:35.163865  8215 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 10:16:35.163920  8216 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 10:16:35.163952  8217 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 10:16:35.163980  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164016  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0815 10:16:35.164053  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164237  8217 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.164393  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.164444  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_5:[dtype=;place=;dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164474  8216 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164515  8216 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.164566  8217 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.164614  8216 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.164739  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.164798  8216 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.164824  8216 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.164830  8216 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.164830  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_7:[dtype=;place=;dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164860  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.164896  8215 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.164970  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.164988  8217 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.165135  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165158  8215 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.165165  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165179  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.165220  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_9:[dtype=;place=;dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.165227  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.165242  8215 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.165278  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165323  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165340  8215 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.165346  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165397  8217 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.165549  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.165589  8217 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_11:[dtype=;place=;dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.165594  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.165608  8215 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.165642  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165681  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165697  8215 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.165702  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.165761  8217 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.165911  8217 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47dc29a01723716995161936769_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x47dc29a01723716995161936769_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 10:16:35.165957  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0815 10:16:35.165973  8215 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 10:16:35.166000  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47dc29a01723716995161936769_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47dc29a01723716995161936769_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.166034  8215 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.166050  8215 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.166055  8215 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47dc29a01723716995161936769_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 10:16:35.166081  8146 pir_interpreter.cc:1766] main_thread_blocker_(0x47dc2b10) got event_name: TaskCompletion
1901: I0815 10:16:35.166107  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.166137  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.166152  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.166167  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.166180  8146 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 10:16:35.167802  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ae2de80 for it.
1901: I0815 10:16:35.167901  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.167932  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.168157  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.168265  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f3f000)  to GradNodeAccumulation (addr: 0x1ae2de80)
1901: I0815 10:16:35.168408  8146 backward.cc:459] Run in Grad
1901: I0815 10:16:35.168422  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.168442  8146 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x47f3f000 to ptr: 0x47f38f80
1901: I0815 10:16:35.168455  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.168495  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.168530  8146 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ae2de80 to ptr: 0x47f10020
1901: I0815 10:16:35.168556  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f38f80
1901: I0815 10:16:35.168565  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.168603  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.168697  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.168709  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f38f80, Found pending node: GradNodeAccumulation addr: 0x47f10020
1901: I0815 10:16:35.168715  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.168745  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x47f10020
1901: I0815 10:16:35.168754  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.168759  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.168766  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.168771  8146 backward.cc:435] Finish Backward
1901: I0815 10:16:35.169486  8146 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0815 10:16:35.169504  8146 dygraph_functions.cc:77659] { Input: []} 
1901: I0815 10:16:35.169603  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.169629  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.169796  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.169881  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f3f000)  to GradNodeAccumulation (addr: 0x1ae2de80)
1901: I0815 10:16:35.169971  8146 backward.cc:442] Run in Backward
1901: I0815 10:16:35.169981  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.169989  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.170027  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.170058  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f3f000
1901: I0815 10:16:35.170068  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.170096  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.170176  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.170208  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f3f000, Found pending node: GradNodeAccumulation addr: 0x1ae2de80
1901: I0815 10:16:35.170217  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.170240  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ae2de80
1901: I0815 10:16:35.170249  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.170254  8146 accumulation_node.cc:40] Move Tensor ptr: 0x47f3c5b0
1901: I0815 10:16:35.170258  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.170264  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.170763  8146 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.170886  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.170914  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.171108  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f38f80)  to GradNodeAccumulation (addr: 0x1adce3a0)
1901: I0815 10:16:35.171216  8146 backward.cc:442] Run in Backward
1901: I0815 10:16:35.171226  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.171234  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.171269  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.171308  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f38f80
1901: I0815 10:16:35.171319  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.171350  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.171411  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.171440  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f38f80, Found pending node: GradNodeAccumulation addr: 0x1adce3a0
1901: I0815 10:16:35.171449  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.171473  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1adce3a0
1901: I0815 10:16:35.171481  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.171486  8146 accumulation_node.cc:40] Move Tensor ptr: 0x45256ab0
1901: I0815 10:16:35.171490  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.171495  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.172266  8146 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.172364  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.172394  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.172616  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.172719  8146 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47f38f80)  to GradNodeAccumulation (addr: 0x1adce3a0)
1901: I0815 10:16:35.172843  8146 backward.cc:459] Run in Grad
1901: I0815 10:16:35.172854  8146 backward.cc:113] Start Backward
1901: I0815 10:16:35.172870  8146 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x47f38f80 to ptr: 0x47f0ae30
1901: I0815 10:16:35.172879  8146 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 10:16:35.172914  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.172945  8146 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1adce3a0 to ptr: 0x47f10020
1901: I0815 10:16:35.172967  8146 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47f0ae30
1901: I0815 10:16:35.172976  8146 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 10:16:35.173007  8146 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.173156  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.173166  8146 backward.cc:335] Node: NanmedianGradNode addr:0x47f0ae30, Found pending node: GradNodeAccumulation addr: 0x47f10020
1901: I0815 10:16:35.173172  8146 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 10:16:35.173192  8146 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x47f10020
1901: I0815 10:16:35.173200  8146 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.173205  8146 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 10:16:35.173213  8146 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 10:16:35.173218  8146 backward.cc:435] Finish Backward
1901: I0815 10:16:35.174196  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.174284  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.174322  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.174507  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.175956  8146 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1adce3a0 for it.
1901: I0815 10:16:35.176005  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.176026  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.178308  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.178335  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.179322  8146 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 10:16:35.179347  8146 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 10:16:35.180178  8146 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 10:16:35.180197  8146 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 10:16:35.180315  8146 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 10:16:35.180325  8146 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 10:16:35.180397  8146 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 10:16:35.180404  8146 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 10:16:35.180469  8146 dygraph_functions.cc:82227] Running AD API: arange
1901: I0815 10:16:35.180508  8146 dygraph_functions.cc:82291] { Input: [ 
1901: ( start , [[ Not specified tensor log level ]]),  
1901: ( end , [[ Not specified tensor log level ]]),  
1901: ( step , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.180688  8146 dygraph_functions.cc:62112] Running AD API: reshape
1901: I0815 10:16:35.180713  8146 dygraph_functions.cc:62169] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.180737  8146 api.cc:40807] Perform View between Output and Input Tensor, share allocation and inplace version.
1901: I0815 10:16:35.180815  8146 dygraph_functions.cc:14224] Running AD API: cast
1901: I0815 10:16:35.180837  8146 dygraph_functions.cc:14268] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.180917  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=200, vec_size=1, block_size=64, grid_size=4, limit_blocks=2147483647, limit_threads=512
1901: I0815 10:16:35.181012  8146 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 10:16:35.181032  8146 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 10:16:35.181242  8146 gpu_launch_config.h:156] Get 1-D launch config: numel=100, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: test_nanmedian failed
1901:  ss.E.................
1901: ======================================================================
1901: ERROR: test_check_output (test_nanmedian.TestNanmedianFP16Op)
1901: ----------------------------------------------------------------------
1901: Traceback (most recent call last):
1901:   File "/home/code/Paddle/build/test/legacy_test/test_nanmedian.py", line 617, in test_check_output
1901:     self.check_output(check_pir=True)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1901:     res = self.check_output_with_place(
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2786, in check_output_with_place
1901:     symbol_checker.check()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2637, in check
1901:     self.infer_and_compare_symbol()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2645, in infer_and_compare_symbol
1901:     self.op_test._infer_and_compare_symbol(place)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1712, in _infer_and_compare_symbol
1901:     self._compare_symbol(program, outs)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1633, in _compare_symbol
1901:     shape_analysis.get_shape_or_data_for_var(
1901: RuntimeError: (PreconditionNotMet) The op operand index [1] must less than operands size[1].
1901:   [Hint: Expected index < num_operands_, but received index:1 >= num_operands_:1.] (at /home/code/Paddle/paddle/pir/src/core/operation.cc:420)
1901: 
1901: 
1901: ----------------------------------------------------------------------
1901: Ran 21 tests in 2.972s
1901: 
1901: FAILED (errors=1, skipped=2)
1901: 
1901: I0815 10:16:35.182719  8146 mmap_allocator.cc:348] PID: 8146, MemoryMapFdSet: set size - 0
1901: I0815 10:16:35.195466  8146 mmap_allocator.cc:348] PID: 8146, MemoryMapFdSet: set size - 0
1901: I0815 10:16:35.284905  8216 thread_data_registry.h:135] Add data {current : -4, peak : 0} from thread 13204967335596237484 to 3300008994213368542 , after update, data is {current : -20, peak : 0}.
1901: I0815 10:16:35.284916  8216 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 13204967335596237484 to 3300008994213368542 , after update, data is {current : 0, peak : 4}.
1901: I0815 10:16:35.284952  8215 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 3300008994213368542 to 12896966126556168739 , after update, data is {current : 0, peak : 1252}.
1901: I0815 10:16:35.284976  8215 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 3300008994213368542 to 12896966126556168739 , after update, data is {current : 0, peak : 16}.
1901: I0815 10:16:35.285190  8217 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 12896966126556168739 to 9009457683004622817 , after update, data is {current : 0, peak : 260}.
1901: I0815 10:16:35.285214  8217 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 12896966126556168739 to 9009457683004622817 , after update, data is {current : 20026, peak : 40004}.
1901: I0815 10:16:35.285220  8217 thread_data_registry.h:135] Add data {current : 0, peak : 1252} from thread 12896966126556168739 to 9009457683004622817 , after update, data is {current : 162560, peak : 560633}.
1901: I0815 10:16:35.435214  8146 mmap_allocator.cc:348] PID: 8146, MemoryMapFdSet: set size - 0
1/1 Test #1901: test_nanmedian ...................***Failed   10.27 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  10.47 sec

The following tests FAILED:
	1901 - test_nanmedian (Failed)
