UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 03:40:50.892107  6391 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 03:40:51.680249  6391 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=cudnn_batchnorm_spatial_persistent,allocator_strategy,enable_blaslt_global_search,gpu_allocator_retry_time,enable_pir_in_executor,win_cuda_bin_dir,multiple_of_cupti_buffer_size,op_dir,cinn_subgraph_graphviz_dir,accuracy_check_atol_fp32,new_executor_static_build,selected_gpus,cublas_dir,get_host_by_name_time,nvidia_package_dir,use_auto_growth_v2,enable_pir_api,enable_exit_when_partial_worker,gpu_memory_limit_mb,cudnn_exhaustive_search_times,max_inplace_grad_add,enable_cublas_tensor_op_math,enable_gpu_memory_usage_log,enable_neighbor_list_use_uva,new_executor_use_inplace,gpugraph_storage_mode,sort_sum_gradient,use_system_allocator,use_virtual_memory_auto_growth,lapack_dir,curand_dir,auto_free_cudagraph_allocations_on_launch,use_fast_math,prim_backward,host_trace_level,query_dest_rank_by_multi_node,memory_fraction_of_eager_deletion,accuracy_check_atol_fp16,sync_after_alloc,save_static_runtime_data,convert_all_blocks,multi_node_sample_use_gpu_table,einsum_opt,pir_debug,use_shm_cache,paddle_num_threads,cuda_malloc_async_pool_memory_throttle_ratio,tensor_operants_mode,gpugraph_parallel_copyer_split_maxsize,pinned_memory_as_cpu_backend,enable_cinn_compile_cache,fleet_executor_with_standalone,conv2d_disable_cudnn,fuse_parameter_memory_size,pir_subgraph_saving_dir,enable_graph_multi_node_sampling,free_when_no_cache_hit,use_stride_kernel,prim_forward,tracer_onednn_ops_off,pir_apply_shape_optimization_pass,graph_embedding_split_infer_mode,enable_cinn_accuracy_check,tracer_onednn_ops_on,cusparse_dir,use_xqa_optim,accuracy_check_atol_bf16,dynamic_static_unified_comm,pir_apply_inplace_pass,cuda_dir,new_executor_sequential_run,graph_neighbor_size_percent,gpugraph_enable_gpu_direct_access,manually_trans_conv_filter,dataloader_use_file_descriptor,initial_cpu_memory_in_mb,cache_inference_while_scope,enable_pir_with_pt_in_dy2st,apply_pass_to_program,graph_get_neighbor_id,enable_fusion_fallback,check_nan_inf_level,reader_queue_speed_test_mode,embedding_deterministic,accuracy_check_rtol_fp16,print_allocator_trace_info,add_dependency_for_communication_op,enable_unused_var_check,enable_dump_main_program,allreduce_record_one_event,ir_inplace_kernel_blacklist,enable_opt_get_features,prim_enable_dynamic,gpugraph_enable_hbm_table_collision_stat,executor_log_deps_every_microseconds,gpugraph_force_device_batch_num_equal,fraction_of_cuda_pinned_memory_to_use,gpugraph_sparse_table_storage_mode,mklml_dir,initial_gpu_memory_in_mb,enable_auto_detect_gpu_topo,dump_chunk_info,static_runtime_data_save_path,use_pinned_memory,enable_sparse_inner_gather,async_trace_count,enable_auto_rdma_trans,alloc_fill_value,print_sub_graph_dir,use_stream_safe_cuda_allocator,set_to_1d,cuda_memory_async_pool_realease_threshold,use_autotune,new_executor_serial_run,trt_ibuilder_cache,sync_nccl_allreduce,enable_fuse_parallel_matmul_pass,prim_enabled,enable_api_kernel_fallback,enable_adjust_op_order,eager_delete_scope,search_cache_max_number,npu_storage_format,dist_threadpool_size,logging_trunc_pir_py_code,gpugraph_slot_feasign_max_num,enable_interpretercore_launch_cinn,tracer_profile_fname,use_cuda_malloc_async_allocator,logging_pir_py_code_int_tensor_element_limit,check_kernel_launch,gpugraph_enable_segment_merge_grads,tensorrt_dir,enable_dependency_builder_debug_info,gpugraph_load_node_list_into_hbm,benchmark,fraction_of_cpu_memory_to_use,fraction_of_gpu_memory_to_use,eager_delete_tensor_gb,enable_async_trace,logging_pir_py_code_dir,enable_cinn_auto_tune,prim_all,local_exe_sub_scope_limit,run_kp_kernel,jit_engine_type,free_idle_chunk,use_auto_growth_pinned_allocator,graph_metapath_split_opt,mkl_dir,accuracy_check_rtol_bf16,prim_skip_dynamic,deny_cinn_ops,gpugraph_debug_gpu_memory,init_allocated_mem,cudnn_deterministic,enable_record_memory,cinn_compile_thread_num,cudnn_dir,low_precision_op_list,cudnn_exhaustive_search,new_executor_use_cuda_graph,gpugraph_offload_gather_copy_maxsize,disable_dyshape_in_train,cusparselt_dir,logging_pir_py_code_dump_symbolic_dims,prim_check_ops,pir_broadcast_tree_limit,print_ir,static_executor_perfstat_filepath,accuracy_check_rtol_fp32,enable_all2all_use_fp16,nccl_blocking_wait,gpugraph_merge_grads_segment_size,gpugraph_enable_print_op_debug,custom_device_mem_record,dygraph_debug,gemm_use_half_precision_compute_type,enable_pir_in_executor_trace_run,use_cuda_managed_memory,use_cinn,graph_load_in_parallel,gpugraph_hbm_table_load_factor,conv_workspace_size_limit,check_infer_symbolic,use_mkldnn,gpugraph_dedup_pull_push_mode,cublaslt_device_best_config,allow_cinn_ops,enable_cse_in_dy2st,enable_gpu_memory_usage_log_mb,gpugraph_parallel_stream_num,cupti_dir,gpugraph_offload_param_stat,fuse_parameter_groups_size,reallocate_gpu_memory_in_mb,enable_tracker_all2all,cublaslt_exhaustive_search_times,all_blocks_convert_trt,gpugraph_offload_param_extends,inner_op_parallelism,log_memory_stats,call_stack_level,cse_max_count,prim_forward_blacklist,new_executor_use_local_scope,enable_collect_shape,benchmark_nccl,cusolver_dir,fast_eager_deletion_mode,check_nan_inf,nccl_dir,auto_growth_chunk_size_in_mb 
1884: I0815 03:40:51.680363  6391 init.cc:108] After Parse: argc is 2
1884: I0815 03:40:59.964092  6391 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:40:59.964151  6391 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 03:40:59.964838  6391 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 03:40:59.965430  6391 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 03:40:59.966248  6391 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 03:40:59.966351  6391 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 03:40:59.966444  6391 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 03:40:59.967110  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f9a4b600000), and remaining 0
1884: I0815 03:40:59.967466  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:40:59.967528  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:40:59.967612  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f9a4b600200), and remaining 0
1884: I0815 03:40:59.967640  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f9a4b600400), and remaining 0
1884: I0815 03:40:59.971414  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f9a4b600600), and remaining 0
1884: I0815 03:40:59.971565  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7f9a4b600800), and remaining 0
1884: I0815 03:40:59.971632  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7f9a4b600a00), and remaining 0
1884: I0815 03:40:59.971719  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:40:59.971740  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:40:59.971810  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:40:59.971823  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:40:59.973260  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e5f600 for it.
1884: I0815 03:40:59.973435  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:40:59.973459  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:40:59.973511  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7f9a4b600e00), and remaining 0
1884: I0815 03:40:59.973585  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7f9a4b6c4400), and remaining 0
1884: I0815 03:41:00.081284  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e5f600 for it.
1884: I0815 03:41:00.081490  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:00.081534  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.082113  6391 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7f9a4b800000), and remaining 0
1884: I0815 03:41:00.091960  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e5f600 for it.
1884: I0815 03:41:00.092065  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:00.092099  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.092135  6391 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:00.092312  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:00.093331  6391 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 03:41:00.093349  6391 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 03:41:00.093401  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:00.093479  6391 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 03:41:00.093504  6391 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.093560  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:00.093639  6391 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 03:41:00.093658  6391 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.093693  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:00.093892  6391 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 03:41:00.093910  6391 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.094065  6391 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 03:41:00.094090  6391 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:00.094161  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:00.098002  6391 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 03:41:00.098121  6391 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 03:41:00.098145  6391 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 03:41:00.098208  6391 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 03:41:01.485131  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.485186  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.485468  6391 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 03:41:01.485487  6391 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.490511  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.490546  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.491573  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.491591  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.491603  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.492364  6391 program_interpreter.cc:243] New Executor is Running.
1884: I0815 03:41:01.492378  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.492393  6391 scope.cc:202] Create variable feed
1884: I0815 03:41:01.492403  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.492411  6391 scope.cc:202] Create variable fetch
1884: I0815 03:41:01.492416  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.492427  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.492432  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.492436  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.492439  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.494755  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.495092  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.495105  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.495110  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.496747  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.496794  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.496803  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.496809  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.496816  6391 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:41:01.496824  6391 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x60bb9220 type is 7
1884: I0815 03:41:01.496829  6391 scope.cc:202] Create variable x
1884: I0815 03:41:01.496831  6391 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x60bb9350 type is 7
1884: I0815 03:41:01.496889  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.496896  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.496899  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.496902  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.497017  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.497040  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.497148  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.497159  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.497174  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.497340  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.497368  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.497390  6391 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.497395  6391 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60bc1380Variable Type 7
1884: I0815 03:41:01.497413  6391 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.497435  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.497485  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.497505  6391 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.498739  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.498791  6391 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.499174  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.504490  6391 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:41:01.504513  6391 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:41:01.504606  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.504633  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.505092  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: I0815 03:41:01.505165  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.505187  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.505625  6391 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: I0815 03:41:01.505688  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.505712  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.505734  6391 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.505981  6391 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:41:01.505992  6391 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:41:01.506109  6391 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 03:41:01.506134  6391 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.506506  6391 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:41:01.506518  6391 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:41:01.506561  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.506582  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.506760  6391 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 03:41:01.506770  6391 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 03:41:01.506806  6391 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 03:41:01.506824  6391 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 03:41:01.506840  6391 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.509558  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.509582  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.509636  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.509645  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.511574  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.511935  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.511951  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.511956  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.513757  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.513811  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.513823  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.513828  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60bec900 type is 7
1884: I0815 03:41:01.513837  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.513841  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x60becc70 type is 7
1884: I0815 03:41:01.513846  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.513851  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.513911  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.513917  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.513922  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.513926  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.513974  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.513989  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.514046  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.514056  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.514075  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.514338  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.514355  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.514375  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.514382  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60bf33c0Variable Type 7
1884: I0815 03:41:01.514400  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.514418  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.514444  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.514461  6391 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.515182  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.515209  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.515393  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.525601  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: I0815 03:41:01.525830  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:41:01.531885  6391 pir_interpreter.cc:161] PirInterpreter(): 0x60daf970 on Place(gpu:0)
1884: I0815 03:41:01.531927  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.531957  6391 scope.cc:202] Create variable 0x60daf9701723693261531912936_inner_var_1
1884: I0815 03:41:01.531968  6391 scope.cc:202] Create variable 0x60daf9701723693261531912936_inner_var_2
1884: I0815 03:41:01.531980  6391 scope.cc:202] Create variable 0x60daf9701723693261531912936_inner_var_3
1884: I0815 03:41:01.531986  6391 scope.cc:202] Create variable 0x60daf9701723693261531912936_inner_var_4
1884: I0815 03:41:01.531997  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.532415  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.532433  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.532438  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 03:41:01.532480  6391 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60daf8d0
1884: 1 -> 0x60daf9701723693261531912936_inner_var_1 -> 0x60daf950
1884: 2 -> 0x60daf9701723693261531912936_inner_var_2 -> 0x60db0220
1884: 3 -> 0x60daf9701723693261531912936_inner_var_3 -> 0x60daf0b0
1884: 4 -> 0x60daf9701723693261531912936_inner_var_4 -> 0x60db05d0
1884: 5 -> fetch0@fetch -> 0x60db0de0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.533221  6391 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 03:41:01.533450  6429 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.533566  6430 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.533656  6431 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.533674  6432 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.533738  6433 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.533768  6432 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60daf9701723693261531912936_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.533808  6434 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.533859  6432 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60daf9701723693261531912936_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.533866  6434 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.533910  6434 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:41:01.533958  6434 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60daf9701723693261531912936_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60daf9701723693261531912936_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.534162  6434 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60daf9701723693261531912936_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60daf9701723693261531912936_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:41:01.534235  6432 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60daf9701723693261531912936_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.534258  6432 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.535512  6432 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60daf9701723693261531912936_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x60daf9701723693261531912936_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:41:01.535553  6432 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60daf9701723693261531912936_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.535579  6432 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.536166  6432 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60daf9701723693261531912936_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:41:01.536206  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x60dafae0) got event_name: TaskCompletion
1884: I0815 03:41:01.536231  6391 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.610736  6429 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 1149597515079431530 to 5689192042821843650 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:41:01.610762  6429 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 1149597515079431530 to 1367699014747194628 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:41:01.610767  6429 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 1149597515079431530 to 1367699014747194628 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:41:01.610944  6432 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 1367699014747194628 to 8712560457566753969 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:41:01.610957  6432 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 1367699014747194628 to 8712560457566753969 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 03:41:01.611131  6434 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 03:41:01.611143  6434 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.617054  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.617081  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.617138  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.617146  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.618875  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.619231  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.619246  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.619251  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.620797  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.620887  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.620899  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.620905  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x44964500 type is 7
1884: I0815 03:41:01.620913  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.620915  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6328b410 type is 7
1884: I0815 03:41:01.620919  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.620923  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.620977  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.620983  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.620987  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.620990  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.621037  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.621052  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.621106  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.621114  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.621129  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.621263  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.621274  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.621289  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.621295  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63382a40Variable Type 7
1884: I0815 03:41:01.621321  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.621340  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.621361  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.621376  6391 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.623054  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.623088  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.623283  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.629940  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: I0815 03:41:01.630126  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:41:01.633178  6391 pir_interpreter.cc:161] PirInterpreter(): 0x60bd0e10 on Place(gpu:0)
1884: I0815 03:41:01.633213  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.633235  6391 scope.cc:202] Create variable 0x60bd0e101723693261633204224_inner_var_1
1884: I0815 03:41:01.633246  6391 scope.cc:202] Create variable 0x60bd0e101723693261633204224_inner_var_2
1884: I0815 03:41:01.633258  6391 scope.cc:202] Create variable 0x60bd0e101723693261633204224_inner_var_3
1884: I0815 03:41:01.633270  6391 scope.cc:202] Create variable 0x60bd0e101723693261633204224_inner_var_4
1884: I0815 03:41:01.633280  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.633612  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.633628  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.633632  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x45bc850
1884: 1 -> 0x60bd0e101723693261633204224_inner_var_1 -> 0x60bace20
1884: 2 -> 0x60bd0e101723693261633204224_inner_var_2 -> 0x60b9b320
1884: 3 -> 0x60bd0e101723693261633204224_inner_var_3 -> 0x457fed0
1884: 4 -> 0x60bd0e101723693261633204224_inner_var_4 -> 0x60db09a0
1884: 5 -> fetch0@fetch -> 0x60d897d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.634344  6435 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.634402  6436 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.634429  6437 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.634490  6439 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.634497  6438 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.634533  6439 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60bd0e101723693261633204224_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.634549  6440 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.634588  6439 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60bd0e101723693261633204224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.634581  6440 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.634619  6440 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:41:01.634668  6440 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60bd0e101723693261633204224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60bd0e101723693261633204224_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.634778  6440 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60bd0e101723693261633204224_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60bd0e101723693261633204224_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.634828  6439 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60bd0e101723693261633204224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.634851  6439 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.637564  6439 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60bd0e101723693261633204224_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x60bd0e101723693261633204224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.637611  6439 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60bd0e101723693261633204224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.637631  6439 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.639650  6439 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60bd0e101723693261633204224_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.639693  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x60bd0f80) got event_name: TaskCompletion
1884: I0815 03:41:01.639714  6391 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.677259  6435 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 5689192042821843650 to 12278656778335387300 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:41:01.677281  6435 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 5689192042821843650 to 8947414039842106427 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:41:01.677287  6435 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 5689192042821843650 to 8947414039842106427 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:41:01.677453  6439 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 8947414039842106427 to 8712560457566753969 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 03:41:01.677464  6439 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 8947414039842106427 to 8712560457566753969 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 03:41:01.677634  6440 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.681558  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.681582  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.681634  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.681641  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.683275  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.683619  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.683634  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.683638  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.685142  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.685231  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.685242  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.685247  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60bc0260 type is 7
1884: I0815 03:41:01.685256  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.685259  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x457fdc0 type is 7
1884: I0815 03:41:01.685263  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.685268  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.685329  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.685336  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.685339  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.685343  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.685388  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.685402  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.685456  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.685463  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.685477  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.685513  6391 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.685642  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.685706  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.685716  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.685731  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.685737  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x634edf80Variable Type 7
1884: I0815 03:41:01.685752  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.685770  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.685791  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.685804  6391 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.686074  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.686093  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.686272  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.687038  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: I0815 03:41:01.687211  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:41:01.690178  6391 pir_interpreter.cc:161] PirInterpreter(): 0x632eda10 on Place(gpu:0)
1884: I0815 03:41:01.690212  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.690232  6391 scope.cc:202] Create variable 0x632eda101723693261690203323_inner_var_1
1884: I0815 03:41:01.690243  6391 scope.cc:202] Create variable 0x632eda101723693261690203323_inner_var_2
1884: I0815 03:41:01.690255  6391 scope.cc:202] Create variable 0x632eda101723693261690203323_inner_var_3
1884: I0815 03:41:01.690265  6391 scope.cc:202] Create variable 0x632eda101723693261690203323_inner_var_4
1884: I0815 03:41:01.690276  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.690603  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.690618  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.690622  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x45bbbe0
1884: 1 -> 0x632eda101723693261690203323_inner_var_1 -> 0x634e4ee0
1884: 2 -> 0x632eda101723693261690203323_inner_var_2 -> 0x633cbae0
1884: 3 -> 0x632eda101723693261690203323_inner_var_3 -> 0x633cba90
1884: 4 -> 0x632eda101723693261690203323_inner_var_4 -> 0x632edf30
1884: 5 -> fetch0@fetch -> 0x633cb920
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.691272  6441 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.691365  6442 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.691391  6443 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.691428  6444 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.691457  6445 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.691504  6446 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.691498  6445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x632eda101723693261690203323_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.691531  6446 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.691546  6445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x632eda101723693261690203323_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.691572  6446 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:41:01.691607  6446 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x632eda101723693261690203323_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x632eda101723693261690203323_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.691650  6446 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.691761  6446 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.691787  6446 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x632eda101723693261690203323_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x632eda101723693261690203323_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:41:01.691844  6445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x632eda101723693261690203323_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.691864  6445 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.692176  6445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x632eda101723693261690203323_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x632eda101723693261690203323_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:41:01.692200  6445 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x632eda101723693261690203323_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.692217  6445 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.692229  6445 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x632eda101723693261690203323_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:41:01.692255  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x632edb80) got event_name: TaskCompletion
1884: I0815 03:41:01.692276  6391 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.724658  6441 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 12278656778335387300 to 5689192042821843650 , after update, data is {current : 0, peak : 3328}.
1884: I0815 03:41:01.724684  6441 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12278656778335387300 to 5689192042821843650 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:41:01.724690  6441 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 12278656778335387300 to 5689192042821843650 , after update, data is {current : -804, peak : 2000}.
1884: I0815 03:41:01.724856  6445 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11142325096254835613 to 5689192042821843650 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:41:01.724869  6445 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 11142325096254835613 to 5689192042821843650 , after update, data is {current : 800, peak : 2000}.
1884: I0815 03:41:01.725037  6446 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 03:41:01.725049  6446 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 03:41:01.725054  6446 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.729269  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.729293  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.729354  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.729362  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.731024  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.731371  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.731385  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.731390  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.732892  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.732982  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.732993  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.732998  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x45bb100 type is 7
1884: I0815 03:41:01.733008  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.733011  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x60d85450 type is 7
1884: I0815 03:41:01.733016  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.733019  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.733070  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.733076  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.733079  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.733083  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.733129  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.733142  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.733196  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.733204  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.733217  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.733456  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.733470  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.733486  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.733492  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60ba0340Variable Type 7
1884: I0815 03:41:01.733507  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.733525  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.733546  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.733561  6391 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.734256  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.734282  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.734479  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.737527  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: I0815 03:41:01.737700  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 03:41:01.740700  6391 pir_interpreter.cc:161] PirInterpreter(): 0x60bb9370 on Place(gpu:0)
1884: I0815 03:41:01.740734  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.740756  6391 scope.cc:202] Create variable 0x60bb93701723693261740725926_inner_var_1
1884: I0815 03:41:01.740767  6391 scope.cc:202] Create variable 0x60bb93701723693261740725926_inner_var_2
1884: I0815 03:41:01.740777  6391 scope.cc:202] Create variable 0x60bb93701723693261740725926_inner_var_3
1884: I0815 03:41:01.740788  6391 scope.cc:202] Create variable 0x60bb93701723693261740725926_inner_var_4
1884: I0815 03:41:01.740799  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.741124  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.741139  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.741143  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf64>) -> gpu_tensor<4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60bd5c30
1884: 1 -> 0x60bb93701723693261740725926_inner_var_1 -> 0x62a8bc00
1884: 2 -> 0x60bb93701723693261740725926_inner_var_2 -> 0x62ba8fa0
1884: 3 -> 0x60bb93701723693261740725926_inner_var_3 -> 0x60d83640
1884: 4 -> 0x60bb93701723693261740725926_inner_var_4 -> 0x63267510
1884: 5 -> fetch0@fetch -> 0x60bdcbd0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.741819  6447 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.741897  6448 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.741914  6449 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.741961  6450 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.741990  6451 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.742033  6452 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.742033  6451 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60bb93701723693261740725926_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.742058  6452 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.742082  6451 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x60bb93701723693261740725926_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.742096  6452 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 03:41:01.742127  6452 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60bb93701723693261740725926_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60bb93701723693261740725926_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.742316  6452 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x60bb93701723693261740725926_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x60bb93701723693261740725926_inner_var_1:[dtype=double;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 03:41:01.742373  6451 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60bb93701723693261740725926_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.742393  6451 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.743552  6451 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x60bb93701723693261740725926_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x60bb93701723693261740725926_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:41:01.743587  6451 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x60bb93701723693261740725926_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.743605  6451 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.744135  6451 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x60bb93701723693261740725926_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 03:41:01.744171  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x60bb94e0) got event_name: TaskCompletion
1884: I0815 03:41:01.744192  6391 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.747882  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.747906  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.747958  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.747968  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.749833  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.750234  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.750249  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.750254  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.752125  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.752213  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.752226  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.752235  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62a96130 type is 7
1884: I0815 03:41:01.752243  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.752249  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62a95470 type is 7
1884: I0815 03:41:01.752254  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.752259  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.752336  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.752344  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.752348  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.752353  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.752401  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.752420  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.752480  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.752492  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.752512  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.752714  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.752730  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.752751  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.752759  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62a57390Variable Type 7
1884: I0815 03:41:01.752779  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.752797  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.752822  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.752839  6391 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.753557  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.753592  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.753811  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.791666  6447 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 5689192042821843650 to 12278656778335387300 , after update, data is {current : 0, peak : 800768}.
1884: I0815 03:41:01.791683  6447 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 5689192042821843650 to 8947414039842106427 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:41:01.791688  6447 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 5689192042821843650 to 8947414039842106427 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 03:41:01.791848  6451 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8947414039842106427 to 8712560457566753969 , after update, data is {current : 4000800, peak : 4800004}.
1884: I0815 03:41:01.791863  6451 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 8947414039842106427 to 8712560457566753969 , after update, data is {current : 4000800, peak : 4800800}.
1884: I0815 03:41:01.792032  6452 thread_data_registry.h:135] Add data {current : 0, peak : 1023} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 3205168, peak : 3207136}.
1884: I0815 03:41:01.792047  6452 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.797508  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.797533  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.797585  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.797592  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.799310  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.799664  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.799677  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.799682  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.801191  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.801285  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.801295  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.801308  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x45a32a0 type is 7
1884: I0815 03:41:01.801318  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.801321  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x60dadab0 type is 7
1884: I0815 03:41:01.801326  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.801330  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.801383  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.801390  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.801393  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.801398  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.801442  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.801456  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.801512  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.801520  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.801533  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.801672  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.801683  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.801698  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.801704  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x45d1e00Variable Type 7
1884: I0815 03:41:01.801719  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.801736  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.801756  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.801769  6391 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.803395  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.803427  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.803628  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.811955  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: I0815 03:41:01.812139  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 03:41:01.815212  6391 pir_interpreter.cc:161] PirInterpreter(): 0x633ddfa0 on Place(gpu:0)
1884: I0815 03:41:01.815244  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.815266  6391 scope.cc:202] Create variable 0x633ddfa01723693261815236086_inner_var_1
1884: I0815 03:41:01.815276  6391 scope.cc:202] Create variable 0x633ddfa01723693261815236086_inner_var_2
1884: I0815 03:41:01.815289  6391 scope.cc:202] Create variable 0x633ddfa01723693261815236086_inner_var_3
1884: I0815 03:41:01.815308  6391 scope.cc:202] Create variable 0x633ddfa01723693261815236086_inner_var_4
1884: I0815 03:41:01.815320  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.815646  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.815662  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.815666  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x60ba8e80
1884: 1 -> 0x633ddfa01723693261815236086_inner_var_1 -> 0x60bd76e0
1884: 2 -> 0x633ddfa01723693261815236086_inner_var_2 -> 0x45880d0
1884: 3 -> 0x633ddfa01723693261815236086_inner_var_3 -> 0x60d8b8f0
1884: 4 -> 0x633ddfa01723693261815236086_inner_var_4 -> 0x4588dc0
1884: 5 -> fetch0@fetch -> 0x2d22ef0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.816339  6453 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.816422  6454 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.816452  6455 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.816489  6456 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.816521  6457 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.816561  6458 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.816562  6457 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x633ddfa01723693261815236086_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.816581  6458 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.816612  6457 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x633ddfa01723693261815236086_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.816615  6458 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:41:01.816648  6458 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x633ddfa01723693261815236086_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x633ddfa01723693261815236086_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.816747  6458 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x633ddfa01723693261815236086_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x633ddfa01723693261815236086_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.816798  6457 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x633ddfa01723693261815236086_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.816820  6457 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.819478  6457 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x633ddfa01723693261815236086_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x633ddfa01723693261815236086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.819521  6457 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x633ddfa01723693261815236086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.819541  6457 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.821532  6457 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x633ddfa01723693261815236086_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 03:41:01.821576  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x633de110) got event_name: TaskCompletion
1884: I0815 03:41:01.821596  6391 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.829490  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.829515  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.829569  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.829577  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.831465  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.831871  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.831887  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.831892  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.833748  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.833840  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.833853  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.833863  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x60b90fd0 type is 7
1884: I0815 03:41:01.833870  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.833876  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x62ba6420 type is 7
1884: I0815 03:41:01.833881  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.833886  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.833950  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.833957  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.833962  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.833966  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.834014  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.834029  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.834092  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.834101  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.834120  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.834239  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.834252  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.834270  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.834278  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60bd4040Variable Type 7
1884: I0815 03:41:01.834295  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.834324  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.834349  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.834365  6391 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.836009  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.836043  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.836239  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.881932  6453 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 12278656778335387300 to 5689192042821843650 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 03:41:01.881959  6453 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12278656778335387300 to 11142325096254835613 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:41:01.881964  6453 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 12278656778335387300 to 11142325096254835613 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 03:41:01.882139  6457 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 11142325096254835613 to 8712560457566753969 , after update, data is {current : 6400800, peak : 6400800}.
1884: I0815 03:41:01.882155  6457 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 11142325096254835613 to 8712560457566753969 , after update, data is {current : 6400800, peak : 8800800}.
1884: I0815 03:41:01.882330  6458 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 5689192042821843650 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.887108  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.887133  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.887189  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.887197  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.888948  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.889294  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.889317  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.889322  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.890841  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.890934  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.890945  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.890954  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x629b5880 type is 7
1884: I0815 03:41:01.890961  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.890966  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6289d1e0 type is 7
1884: I0815 03:41:01.890970  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.890975  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.891028  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.891034  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.891038  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.891042  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.891088  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.891103  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.891158  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.891166  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.891180  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.891218  6391 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.891348  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.891404  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.891415  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.891430  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.891438  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x60bd1d10Variable Type 7
1884: I0815 03:41:01.891453  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.891470  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.891491  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.891506  6391 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.891628  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.891649  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.891834  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.892632  6391 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19e77690 for it.
1884: I0815 03:41:01.892812  6391 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x44986f60 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 03:41:01.895898  6391 pir_interpreter.cc:161] PirInterpreter(): 0x63843b50 on Place(gpu:0)
1884: I0815 03:41:01.895932  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.895953  6391 scope.cc:202] Create variable 0x63843b501723693261895923960_inner_var_1
1884: I0815 03:41:01.895965  6391 scope.cc:202] Create variable 0x63843b501723693261895923960_inner_var_2
1884: I0815 03:41:01.895977  6391 scope.cc:202] Create variable 0x63843b501723693261895923960_inner_var_3
1884: I0815 03:41:01.895988  6391 scope.cc:202] Create variable 0x63843b501723693261895923960_inner_var_4
1884: I0815 03:41:01.896000  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:01.896356  6391 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 03:41:01.896373  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.896378  6391 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x44979210
1884: 1 -> 0x63843b501723693261895923960_inner_var_1 -> 0x60d85ad0
1884: 2 -> 0x63843b501723693261895923960_inner_var_2 -> 0x60bd1970
1884: 3 -> 0x63843b501723693261895923960_inner_var_3 -> 0x60bccd90
1884: 4 -> 0x63843b501723693261895923960_inner_var_4 -> 0x60ba74e0
1884: 5 -> fetch0@fetch -> 0x45b59c0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 03:41:01.897065  6459 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:01.897152  6460 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:01.897212  6461 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:01.897231  6462 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:01.897269  6463 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:01.897289  6462 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63843b501723693261895923960_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.897334  6464 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:01.897369  6464 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.897397  6462 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x63843b501723693261895923960_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:01.897420  6464 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 03:41:01.897464  6464 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63843b501723693261895923960_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63843b501723693261895923960_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.897527  6464 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.897679  6464 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.897713  6464 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x63843b501723693261895923960_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x63843b501723693261895923960_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 03:41:01.897789  6462 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63843b501723693261895923960_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.897836  6462 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.897958  6462 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x63843b501723693261895923960_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x63843b501723693261895923960_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:41:01.898000  6462 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x63843b501723693261895923960_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:01.898023  6462 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.898036  6462 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x63843b501723693261895923960_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 03:41:01.898070  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x63843cc0) got event_name: TaskCompletion
1884: I0815 03:41:01.898090  6391 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 03:41:01.899456  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.899480  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.899531  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.899540  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.901371  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.901769  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.901784  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.901789  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.903609  6391 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 03:41:01.903697  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.903709  6391 scope.cc:202] Create variable Out
1884: I0815 03:41:01.903717  6391 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x63739910 type is 7
1884: I0815 03:41:01.903725  6391 scope.cc:202] Create variable X
1884: I0815 03:41:01.903731  6391 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x63737090 type is 7
1884: I0815 03:41:01.903738  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.903743  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.903806  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.903815  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.903818  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.903823  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.903869  6391 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.903884  6391 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.903944  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.903954  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.903970  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.904016  6391 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.904132  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.904186  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.904197  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.904217  6391 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.904223  6391 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x6337ce50Variable Type 7
1884: I0815 03:41:01.904242  6391 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.904260  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.904284  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.904310  6391 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.904405  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.904430  6391 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.904634  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.940107  6459 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 5689192042821843650 to 12278656778335387300 , after update, data is {current : 0, peak : 10240}.
1884: I0815 03:41:01.940133  6459 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 5689192042821843650 to 12278656778335387300 , after update, data is {current : -804, peak : 8000}.
1884: I0815 03:41:01.940140  6459 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 5689192042821843650 to 12278656778335387300 , after update, data is {current : -804, peak : 8000}.
1884: I0815 03:41:01.940399  6462 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 15477162628277092782 to 12278656778335387300 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:41:01.940425  6462 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 15477162628277092782 to 12278656778335387300 , after update, data is {current : 800, peak : 8000}.
1884: I0815 03:41:01.940584  6464 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 6401600, peak : 6408800}.
1884: I0815 03:41:01.940609  6464 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 6401600, peak : 8800800}.
1884: I0815 03:41:01.940615  6464 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 12278656778335387300 to 8712560457566753969 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 03:41:01.947657  6391 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 03:41:01.947711  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:41:01.948799  6391 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:41:01.949633  6391 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 03:41:01.949663  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:41:01.950951  6391 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 03:41:01.950976  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:41:01.951658  6391 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 03:41:01.952638  6391 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 03:41:01.952665  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:41:01.953998  6391 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 03:41:01.954020  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:41:01.954620  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:01.954648  6391 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 03:41:01.954654  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:01.954661  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.956619  6391 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 03:41:01.956643  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:41:01.957598  6391 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 03:41:01.957626  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:41:01.958549  6391 pybind.cc:1827] need skip: 0
1884: I0815 03:41:01.958837  6391 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:41:01.960592  6391 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 03:41:01.964282  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.964309  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.964316  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.966293  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.966323  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.966332  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.966337  6391 scope.cc:202] Create variable learning_rate_0
1884: I0815 03:41:01.966343  6391 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62b97c10 type is 7
1884: I0815 03:41:01.966349  6391 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:41:01.966352  6391 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62b97ab0 type is 7
1884: I0815 03:41:01.966356  6391 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:41:01.966359  6391 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62b97b60 type is 7
1884: I0815 03:41:01.966423  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.966429  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.966432  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.966436  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.966485  6391 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966498  6391 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966517  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 03:41:01.966647  6391 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966657  6391 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966720  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.966761  6391 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966770  6391 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.966797  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.967813  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.969161  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.969612  6391 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 03:41:01.969832  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.970127  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.970355  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:01.970371  6391 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:41:01.970436  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.970443  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.970448  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.970546  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:01.970558  6391 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:41:01.972092  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.973446  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.974603  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.974812  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.974824  6391 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:41:01.974833  6391 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x6184d090 type is 7
1884: I0815 03:41:01.974839  6391 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:41:01.974844  6391 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x6184ce10 type is 7
1884: I0815 03:41:01.974849  6391 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 03:41:01.974853  6391 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x6184cf00 type is 7
1884: I0815 03:41:01.974857  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.974862  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.974866  6391 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:41:01.974870  6391 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x6184e4d0 type is 7
1884: I0815 03:41:01.974875  6391 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x62b97c10 type is 7
1884: I0815 03:41:01.974879  6391 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62b97ab0 type is 7
1884: I0815 03:41:01.974884  6391 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 03:41:01.974887  6391 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x6184e4b0 type is 7
1884: I0815 03:41:01.974892  6391 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:41:01.974895  6391 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x61960c60 type is 7
1884: I0815 03:41:01.974900  6391 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62b97b60 type is 7
1884: I0815 03:41:01.974903  6391 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 03:41:01.974907  6391 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x61960ed0 type is 7
1884: I0815 03:41:01.974911  6391 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 03:41:01.974915  6391 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x61961110 type is 7
1884: I0815 03:41:01.974920  6391 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:41:01.974921  6391 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61961370 type is 7
1884: I0815 03:41:01.975006  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:01.975020  6391 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:41:01.975081  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.975088  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.975092  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.975096  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.975143  6391 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.975155  6391 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.975171  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 03:41:01.975283  6391 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.975293  6391 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.975322  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:41:01.975389  6391 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:41:01.975467  6391 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 03:41:01.976560  6391 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976580  6391 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976641  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.976701  6391 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976711  6391 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976723  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:41:01.976747  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.976789  6391 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976799  6391 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976810  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:41:01.976897  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976907  6391 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.976919  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:01.977027  6391 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.977111  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.977169  6391 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977178  6391 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977191  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:41:01.977223  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.977272  6391 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977281  6391 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977295  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 03:41:01.977409  6391 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977420  6391 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977440  6391 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:01.977483  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.977492  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.977507  6391 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.977514  6391 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x62a8bf90Variable Type 7
1884: I0815 03:41:01.977530  6391 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.977546  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.977566  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977579  6391 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.977618  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.977641  6391 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 03:41:01.977665  6391 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.977674  6391 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.977686  6391 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 03:41:01.977692  6391 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x63754b30Variable Type 7
1884: I0815 03:41:01.977705  6391 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 03:41:01.977717  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.977732  6391 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.977744  6391 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:01.977778  6391 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 03:41:01.977792  6391 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 03:41:01.978214  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 03:41:01.978250  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:41:01.978267  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:41:01.978309  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 03:41:01.978343  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.978360  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 03:41:01.982390  6391 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:41:01.982426  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:41:01.983129  6391 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 03:41:01.983152  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:41:01.983505  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.985219  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.986048  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:01.986171  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.986685  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.987593  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.989666  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.990907  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.992795  6391 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 03:41:01.993611  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.993628  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.993633  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.994824  6391 interpreter_util.cc:1169] Creating Variables
1884: I0815 03:41:01.994843  6391 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x4598870 type is 9
1884: I0815 03:41:01.994849  6391 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4068240 type is 10
1884: I0815 03:41:01.994854  6391 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x62b97ab0 type is 7
1884: I0815 03:41:01.994858  6391 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x62b97b60 type is 7
1884: I0815 03:41:01.994863  6391 scope.cc:202] Create variable saved_params
1884: I0815 03:41:01.994865  6391 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x618fc860 type is 17
1884: I0815 03:41:01.994894  6391 interpreter_util.cc:594] Static build: 0
1884: I0815 03:41:01.994899  6391 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 03:41:01.994903  6391 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 03:41:01.994906  6391 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 03:41:01.994947  6391 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.994959  6391 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 03:41:01.995760  6391 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:41:01.995803  6391 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:41:01.995857  6391 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 03:41:01.997025  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:01.997083  6391 scope.cc:202] Create variable feed
1884: I0815 03:41:01.997092  6391 naive_executor.cc:189] 0x63847880 Create persistable variable feed, which pointer is 0x62a6a230
1884: I0815 03:41:01.997097  6391 scope.cc:202] Create variable fetch
1884: I0815 03:41:01.997099  6391 naive_executor.cc:189] 0x63847880 Create persistable variable fetch, which pointer is 0x62a6a0d0
1884: I0815 03:41:01.997103  6391 scope.cc:202] Create variable linear_0.b_0
1884: I0815 03:41:01.997105  6391 naive_executor.cc:189] 0x63847880 Create persistable variable linear_0.b_0, which pointer is 0x63847090
1884: I0815 03:41:01.997110  6391 scope.cc:202] Create variable linear_0.w_0
1884: I0815 03:41:01.997113  6391 naive_executor.cc:189] 0x63847880 Create persistable variable linear_0.w_0, which pointer is 0x63844820
1884: I0815 03:41:01.997128  6391 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 03:41:01.997470  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:01.997551  6391 program_converter.cc:296] is_legacy_program : 0
1884: I0815 03:41:01.997596  6391 executor.cc:183] Old Executor is Running.
1884: I0815 03:41:01.997666  6391 executor.cc:92] Creating Variables for block 0
1884: I0815 03:41:01.997673  6391 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 03:41:01.997676  6391 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x63847090 type is 7
1884: I0815 03:41:01.997679  6391 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 03:41:01.997682  6391 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x63844820 type is 7
1884: I0815 03:41:01.997713  6391 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:01.997786  6391 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 03:41:01.997828  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:01.997833  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:01.997967  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.998070  6391 graph.cc:149] create OpNode by feed
1884: I0815 03:41:01.998103  6391 graph.cc:149] create OpNode by matmul_v2
1884: I0815 03:41:01.998118  6391 graph.cc:149] create OpNode by elementwise_add
1884: I0815 03:41:01.998134  6391 graph.cc:149] create OpNode by abs
1884: I0815 03:41:01.998145  6391 graph.cc:149] create OpNode by assign_value
1884: I0815 03:41:01.998164  6391 graph.cc:149] create OpNode by multinomial
1884: I0815 03:41:01.998173  6391 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 03:41:01.998188  6391 graph.cc:149] create OpNode by scale
1884: I0815 03:41:01.998201  6391 graph.cc:149] create OpNode by scale
1884: I0815 03:41:01.998212  6391 graph.cc:149] create OpNode by fetch
1884: I0815 03:41:01.998229  6391 graph.cc:149] create OpNode by fetch
1884: I0815 03:41:01.998250  6391 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 03:41:01.999425  6391 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 03:41:01.999433  6391 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 03:41:01.999498  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:01.999505  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 03:41:01.999614  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:01.999861  6391 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:41:01.999919  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:01.999925  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 03:41:01.999958  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:01.999963  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 03:41:02.000001  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000061  6391 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:41:02.000093  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000098  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 03:41:02.000115  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000128  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000150  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000155  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 03:41:02.000193  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000216  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000238  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000243  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 03:41:02.000284  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000380  6391 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:41:02.000409  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000414  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 03:41:02.000445  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000465  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000486  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000491  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 03:41:02.000519  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000669  6391 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:41:02.000695  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000700  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 03:41:02.000730  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000746  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000768  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000773  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 03:41:02.000793  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000808  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000829  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000834  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 03:41:02.000856  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000870  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.000891  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.000896  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 03:41:02.000919  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.000984  6391 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 03:41:02.001016  6391 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:41:02.001031  6391 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:41:02.001045  6391 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 03:41:02.001070  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.001075  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 03:41:02.001096  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.001135  6391 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 03:41:02.001155  6391 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:41:02.001166  6391 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:41:02.001179  6391 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:41:02.001209  6391 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:41:02.001220  6391 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 03:41:02.002364  6391 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:41:02.002410  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.002417  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 03:41:02.002442  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.002462  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.002489  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.002494  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 03:41:02.002516  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.002565  6391 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 03:41:02.002595  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.002600  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 03:41:02.002619  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.002633  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.002655  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.002660  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 03:41:02.002692  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.002777  6391 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 03:41:02.002806  6391 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:41:02.002821  6391 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:41:02.002837  6391 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:41:02.002852  6391 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:41:02.002866  6391 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:41:02.002882  6391 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 03:41:02.002904  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.002976  6391 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 03:41:02.003000  6391 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 03:41:02.003012  6391 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 03:41:02.003026  6391 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 03:41:02.003041  6391 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 03:41:02.003055  6391 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 03:41:02.003070  6391 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 03:41:02.003114  6391 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 03:41:02.003382  6391 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:41:02.003412  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.003417  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 03:41:02.003461  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003520  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003554  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003600  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003628  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003669  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003693  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003731  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003751  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003784  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003803  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003832  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003847  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003875  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003887  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003911  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003921  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.003940  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.003965  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.003970  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 03:41:02.003995  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.004034  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.004058  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004065  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 03:41:02.004073  6391 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:41:02.004076  6391 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:41:02.004122  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.004143  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.004168  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004173  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:41:02.004181  6391 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:41:02.004185  6391 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:41:02.004225  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.004246  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.004271  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004276  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 03:41:02.004283  6391 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 03:41:02.004287  6391 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:41:02.004324  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.004343  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.004366  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004371  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 03:41:02.004379  6391 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 03:41:02.004382  6391 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 03:41:02.004418  6391 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 03:41:02.004439  6391 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 03:41:02.004462  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004467  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 03:41:02.004478  6391 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 03:41:02.004518  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004523  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 03:41:02.004591  6391 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:41:02.004612  6391 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.004631  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 03:41:02.004690  6391 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 03:41:02.004707  6391 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 03:41:02.004734  6391 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 03:41:02.004756  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.004761  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 03:41:02.005628  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:02.005643  6391 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 03:41:02.005693  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.005699  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:02.006295  6391 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 03:41:02.006534  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:02.006610  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.006616  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:02.007025  6391 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 03:41:02.007243  6391 graph.h:183] deleting __fuse_statis__
1884: I0815 03:41:02.007251  6391 graph.h:183] deleting pass_recorder
1884: I0815 03:41:02.007256  6391 graph.h:183] deleting stale_program_op_descs
1884: I0815 03:41:02.007351  6391 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 03:41:02.007361  6391 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 03:41:02.007364  6391 naive_executor.cc:195] 0x63847880 Create variable abs_0.tmp_0, which pointer is 0x60dd1d10
1884: I0815 03:41:02.007370  6391 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 03:41:02.007375  6391 naive_executor.cc:195] 0x63847880 Create variable gaussian_0.tmp_0, which pointer is 0x628ac280
1884: I0815 03:41:02.007378  6391 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 03:41:02.007380  6391 naive_executor.cc:195] 0x63847880 Create variable linear_0.tmp_1, which pointer is 0x6338b330
1884: I0815 03:41:02.007392  6391 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 03:41:02.007397  6391 naive_executor.cc:195] 0x63847880 Create variable multinomial_0.tmp_0, which pointer is 0x6338add0
1884: I0815 03:41:02.007401  6391 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 03:41:02.007403  6391 naive_executor.cc:195] 0x63847880 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x6338b0d0
1884: I0815 03:41:02.007407  6391 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 03:41:02.007409  6391 naive_executor.cc:195] 0x63847880 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x633896d0
1884: I0815 03:41:02.007416  6391 scope.cc:202] Create variable feed
1884: I0815 03:41:02.007419  6391 scope.cc:202] Create variable fetch
1884: I0815 03:41:02.007439  6391 naive_executor.cc:46] NaiveExecutor init with scope 0x63847880
1884: I0815 03:41:02.007445  6391 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 03:41:02.007637  6391 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 03:41:02.007651  6391 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 03:41:02.007680  6391 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 03:41:02.007686  6391 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 03:41:02.007694  6391 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.007725  6391 helper.h:475] Init predictor : [cpu current allocated memory: 6.10524MB], [cpu current reserved memory: 6.10524MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:41:02.007961  6391 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.007977  6391 helper.h:475] before run : [cpu current allocated memory: 6.10529MB], [cpu current reserved memory: 6.10529MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:41:02.008026  6391 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.008051  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 03:41:02.082154  6391 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:41:02.082278  6391 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.082321  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 03:41:02.082399  6391 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:41:02.082435  6391 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.082461  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 03:41:02.082530  6391 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:41:02.082576  6391 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.082592  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:41:02.082646  6391 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 03:41:02.082676  6391 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 03:41:02.082691  6391 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 03:41:02.082727  6391 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 03:41:02.082746  6391 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.082780  6391 helper.h:475] after run : [cpu current allocated memory: 6.10577MB], [cpu current reserved memory: 6.10577MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:41:02.082808  6391 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:41:02.083287  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.083308  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 03:41:02.127511  6391 pir_interpreter.cc:161] PirInterpreter(): 0x61951f20 on Place(gpu:0)
1884: I0815 03:41:02.127574  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_0
1884: I0815 03:41:02.127591  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_1
1884: I0815 03:41:02.127600  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_2
1884: I0815 03:41:02.127609  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_3
1884: I0815 03:41:02.127637  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_4
1884: I0815 03:41:02.127652  6391 scope.cc:202] Create variable 0x61951f201723693262127564739_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 2 )  ( 1 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x61951f201723693262127564739_inner_var_0 -> 0x61869590
1884: 1 -> 0x61951f201723693262127564739_inner_var_1 -> 0x61876730
1884: 2 -> 0x61951f201723693262127564739_inner_var_2 -> 0x62a8b110
1884: 3 -> linear_1.w_0 -> 0x586f480
1884: 4 -> linear_1.b_0 -> 0x60b90580
1884: 5 -> learning_rate_1 -> 0x60bc3310
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:41:02.128443  6465 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.128459  6466 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.128482  6467 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.128533  6468 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.128561  6469 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:02.128561  6468 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128566  6467 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128578  6466 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128588  6469 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128615  6468 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:41:02.128628  6467 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.128641  6469 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.128634  6466 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x61951f201723693262127564739_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.128683  6469 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 03:41:02.128705  6469 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128721  6469 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.128731  6469 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:41:02.128746  6469 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61951f201723693262127564739_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61951f201723693262127564739_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61951f201723693262127564739_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.128803  6469 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x61951f201723693262127564739_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61951f201723693262127564739_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x61951f201723693262127564739_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 03:41:02.128855  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x61952090) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 03:41:02.130834  6391 pir_interpreter.cc:161] PirInterpreter(): 0x6290f930 on Place(gpu:0)
1884: I0815 03:41:02.130869  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_1
1884: I0815 03:41:02.130885  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_4
1884: I0815 03:41:02.130894  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_5
1884: I0815 03:41:02.130901  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_6
1884: I0815 03:41:02.130925  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_7
1884: I0815 03:41:02.130937  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_8
1884: I0815 03:41:02.130946  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_9
1884: I0815 03:41:02.130975  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_10
1884: I0815 03:41:02.130985  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_11
1884: I0815 03:41:02.130991  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_12
1884: I0815 03:41:02.130999  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_13
1884: I0815 03:41:02.131009  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_14
1884: I0815 03:41:02.131016  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_15
1884: I0815 03:41:02.131026  6391 scope.cc:202] Create variable fetch0@fetch
1884: I0815 03:41:02.131034  6391 scope.cc:202] Create variable 0x6290f9301723693262130857006_inner_var_17
1884: I0815 03:41:02.131045  6391 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x60bc3310
1884: 1 -> 0x6290f9301723693262130857006_inner_var_1 -> 0x6336ef60
1884: 2 -> linear_1.b_0 -> 0x60b90580
1884: 3 -> linear_1.w_0 -> 0x586f480
1884: 4 -> 0x6290f9301723693262130857006_inner_var_4 -> 0x6325db50
1884: 5 -> 0x6290f9301723693262130857006_inner_var_5 -> 0x6370aca0
1884: 6 -> 0x6290f9301723693262130857006_inner_var_6 -> 0x63375550
1884: 7 -> 0x6290f9301723693262130857006_inner_var_7 -> 0x5a3663a0
1884: 8 -> 0x6290f9301723693262130857006_inner_var_8 -> 0x6321dc90
1884: 9 -> 0x6290f9301723693262130857006_inner_var_9 -> 0x63751950
1884: 10 -> 0x6290f9301723693262130857006_inner_var_10 -> 0x60bd3330
1884: 11 -> 0x6290f9301723693262130857006_inner_var_11 -> 0x60bd34d0
1884: 12 -> 0x6290f9301723693262130857006_inner_var_12 -> 0x618677f0
1884: 13 -> 0x6290f9301723693262130857006_inner_var_13 -> 0x619605e0
1884: 14 -> 0x6290f9301723693262130857006_inner_var_14 -> 0x62a72cd0
1884: 15 -> 0x6290f9301723693262130857006_inner_var_15 -> 0x60bd3350
1884: 16 -> fetch0@fetch -> 0x62a68ff0
1884: 17 -> 0x6290f9301723693262130857006_inner_var_17 -> 0x637543e0
1884: 18 -> fetch1@fetch -> 0x62b90410
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 03:41:02.132692  6470 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.132792  6471 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.132807  6472 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.132880  6473 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.132894  6471 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.132892  6472 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.132972  6471 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.132977  6472 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:41:02.132990  6474 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 03:41:02.133031  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133059  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:41:02.133086  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133128  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.133177  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:41:02.133194  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:41:02.133255  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 03:41:02.133273  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x6290f9301723693262130857006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133318  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x6290f9301723693262130857006_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 03:41:02.133347  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x6290f9301723693262130857006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133378  6474 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:41:02.133412  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x6290f9301723693262130857006_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.133440  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x6290f9301723693262130857006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x6290f9301723693262130857006_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133477  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.133492  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x6290f9301723693262130857006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x6290f9301723693262130857006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.133522  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x6290f9301723693262130857006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133540  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.133545  6472 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6290f9301723693262130857006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133553  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x6290f9301723693262130857006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.133576  6472 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:02.133577  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x6290f9301723693262130857006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133600  6474 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:02.133654  6472 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6290f9301723693262130857006_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.133685  6472 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6290f9301723693262130857006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133697  6474 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:02.133704  6472 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:41:02.133719  6472 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6290f9301723693262130857006_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.133744  6474 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:02.133803  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.133821  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x6290f9301723693262130857006_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x6290f9301723693262130857006_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.133853  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133859  6472 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133873  6472 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 03:41:02.133878  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.133893  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.133908  6472 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x6290f9301723693262130857006_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.133910  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x6290f9301723693262130857006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133929  6472 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x6290f9301723693262130857006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.133946  6472 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:41:02.133957  6472 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x6290f9301723693262130857006_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.133982  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x6290f9301723693262130857006_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:41:02.134003  6474 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x6290f9301723693262130857006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6290f9301723693262130857006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.134028  6474 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 03:41:02.134039  6474 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x6290f9301723693262130857006_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6290f9301723693262130857006_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x6290f9301723693262130857006_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 03:41:02.134073  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x6290faa0) got event_name: TaskCompletion
1884: I0815 03:41:02.134095  6391 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 03:41:02.134122  6391 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 03:41:02.139533  6391 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 03:41:02.139581  6391 analysis_predictor.cc:433] Predictor::init()
1884: I0815 03:41:02.140223  6391 scope.cc:202] Create variable linear_1.b_0
1884: I0815 03:41:02.140270  6391 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 03:41:02.140689  6391 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621407398620"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621407398620"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 03:41:02.140864  6391 pir_interpreter.cc:161] PirInterpreter(): 0x60bb9370 on Place(cpu)
1884: I0815 03:41:02.140884  6391 scope.cc:202] Create variable 0x60bb93701723693262140878244_inner_var_0
1884: I0815 03:41:02.140910  6391 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621407398620"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236932621407398620 -> 0x6185c190
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 03:41:02.141041  6391 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 03:41:02.141150  6475 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:02.141310  6476 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.141319  6477 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.141392  6478 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.141402  6477 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236932621407398620:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.141436  6477 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236932621407398620:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 03:41:02.141459  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x60bb94e0) got event_name: TaskCompletion
1884: I0815 03:41:02.141520  6479 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.141691  6477 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 15169859247423511995 to 7072346966367823805 , after update, data is {current : 224, peak : 280}.
1884: I0815 03:41:02.141707  6477 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 15169859247423511995 to 7072346966367823805 , after update, data is {current : 224, peak : 280}.
1884: I0815 03:41:02.141770  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.141778  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621418436481"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621418436481"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:41:02.141980  6391 pir_interpreter.cc:161] PirInterpreter(): 0x6182ec40 on Place(cpu)
1884: I0815 03:41:02.141999  6391 scope.cc:202] Create variable 0x6182ec401723693262141993515_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621418436481"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236932621418436481 -> 0x60bb3310
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:41:02.142210  6480 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:02.142267  6481 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.142285  6482 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.142336  6483 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.142364  6484 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.142359  6483 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236932621418436481:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.142410  6483 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236932621418436481:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.142431  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x6182edb0) got event_name: TaskCompletion
1884: I0815 03:41:02.142602  6483 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 17220263839421538981 to 7072346966367823805 , after update, data is {current : 232, peak : 280}.
1884: I0815 03:41:02.142611  6483 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 17220263839421538981 to 7072346966367823805 , after update, data is {current : 232, peak : 280}.
1884: I0815 03:41:02.142702  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.142709  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236932621418436481",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236932621427848092"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236932621418436481",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236932621427848092"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 03:41:02.142933  6391 pir_interpreter.cc:161] PirInterpreter(): 0x6182ec40 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236932621418436481",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236932621427848092"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236932621427848092 -> 0x60bb3310
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 03:41:02.143178  6485 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:02.143246  6486 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.143268  6487 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.143306  6488 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.143337  6489 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.143332  6488 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.143357  6488 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.143376  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x6182edb0) got event_name: TaskCompletion
1884: I0815 03:41:02.143643  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.143651  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621437230613"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621437230613"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 03:41:02.143853  6391 pir_interpreter.cc:161] PirInterpreter(): 0x6182ec40 on Place(cpu)
1884: I0815 03:41:02.143872  6391 scope.cc:202] Create variable 0x6182ec401723693262143866880_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236932621437230613"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236932621437230613 -> 0x618543d0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 03:41:02.144052  6490 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:02.144111  6491 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 03:41:02.144129  6492 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 03:41:02.144158  6493 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 03:41:02.144181  6494 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 03:41:02.144177  6493 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236932621437230613:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.144209  6493 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236932621437230613:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 03:41:02.144232  6391 pir_interpreter.cc:1766] main_thread_blocker_(0x6182edb0) got event_name: TaskCompletion
1884: I0815 03:41:02.144403  6493 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17220263839421538981 to 7072346966367823805 , after update, data is {current : 236, peak : 280}.
1884: I0815 03:41:02.144412  6493 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17220263839421538981 to 7072346966367823805 , after update, data is {current : 236, peak : 280}.
1884: I0815 03:41:02.144502  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.144510  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:02.144572  6391 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 03:41:02.144623  6391 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 03:41:02.144659  6391 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621437230613"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621427848092"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621437230613"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621427848092"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 03:41:02.145308  6391 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 03:41:02.145327  6391 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 03:41:02.145359  6391 pir_interpreter.cc:161] PirInterpreter(): 0x6182ec40 on Place(cpu)
1884: I0815 03:41:02.145390  6391 scope.cc:202] Create variable feed_name_0
1884: I0815 03:41:02.145403  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_5
1884: I0815 03:41:02.145426  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_6
1884: I0815 03:41:02.145438  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_7
1884: I0815 03:41:02.145447  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_8
1884: I0815 03:41:02.145466  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_9
1884: I0815 03:41:02.145478  6391 scope.cc:202] Create variable 0x6182ec401723693262145373314_inner_var_10
1884: I0815 03:41:02.145500  6391 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.145522  6391 helper.h:475] Init predictor : [cpu current allocated memory: 6.10561MB], [cpu current reserved memory: 6.10561MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:41:02.145641  6391 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.145656  6391 helper.h:475] before run : [cpu current allocated memory: 6.10566MB], [cpu current reserved memory: 6.10566MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621437230613"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236932621427848092"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236932621437230613 -> 0x618543d0
1884: 1 -> constant_folding@_17236932621427848092 -> 0x60bb3310
1884: 2 -> linear_1.b_0 -> 0x619664a0
1884: 3 -> linear_1.w_0 -> 0x62ba3db0
1884: 4 -> feed_name_0 -> 0x62a8fed0
1884: 5 -> 0x6182ec401723693262145373314_inner_var_5 -> 0x628a22a0
1884: 6 -> 0x6182ec401723693262145373314_inner_var_6 -> 0x62a8e670
1884: 7 -> 0x6182ec401723693262145373314_inner_var_7 -> 0x63275260
1884: 8 -> 0x6182ec401723693262145373314_inner_var_8 -> 0x628a1e80
1884: 9 -> fetch_name_0 -> 0x628b0af0
1884: 10 -> fetch_name_1 -> 0x628a1b20
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 03:41:02.146204  6391 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 03:41:02.146261  6495 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 03:41:02.146260  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146317  6391 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 03:41:02.146337  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.146369  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x6182ec401723693262145373314_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x6182ec401723693262145373314_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146404  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x6182ec401723693262145373314_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.146435  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146452  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.146466  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146493  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236932621427848092:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.146515  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236932621437230613:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146543  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236932621437230613:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x6182ec401723693262145373314_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 03:41:02.146565  6391 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236932621437230613:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 03:41:02.146591  6391 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236932621437230613:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x6182ec401723693262145373314_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 03:41:02.146618  6391 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07201MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07964MB]
1884: I0815 03:41:02.146639  6391 helper.h:475] after run : [cpu current allocated memory: 6.10584MB], [cpu current reserved memory: 6.10584MB], [cpu peak allocated memory: 8.3931MB], [cpu peak reserved memory: 10.6819MB]
1884: I0815 03:41:02.146662  6391 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 03:41:02.146778  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.146786  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:02.146835  6495 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 17652838517313633958 to 7072346966367823805 , after update, data is {current : 44, peak : 280}.
1884: I0815 03:41:02.146843  6495 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 17652838517313633958 to 7072346966367823805 , after update, data is {current : 44, peak : 280}.
1884: I0815 03:41:02.146878  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.146884  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......FFF..
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 60025 / 100000 (60%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([1, 2, 3, ..., 0, 0, 0], dtype=int64)
1884:  y: array([0, 0, 0, ..., 0, 0, 0])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 209932 / 300000 (70%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[3, 3, 0, ..., 0, 3, 3],
1884:        [0, 3, 2, ..., 0, 0, 3],
1884:        [3, 2, 0, ..., 3, 0, 3]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 100 / 100 (100%)
1884: Max absolute difference: 996
1884: Max relative difference: inf
1884:  x: array([504, 262,  31, 496, 105, 418, 972, 604, 875, 654, 340,  58, 698,
1884:        130, 988, 550, 339, 786, 622, 932, 996, 133, 373, 748, 426, 378,
1884:        301, 349,  69, 685, 701,  19, 843, 770, 587, 765, 851, 839, 466,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.670s
1884: 
1884: FAILED (failures=3, skipped=3)
1884: 
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39717 0.16296 0.12789 0.31198]
1884: {'Out': array([0, 0, 0, ..., 0, 0, 0])}
1884: [0.39554701 0.16250763 0.12883616 0.3131092 ]
1884: [0.39331 0.16301 0.1302  0.31348]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39345 0.16188 0.13081 0.31386]
1884:  [0.25966 0.14928 0.3486  0.24246]
1884:  [0.24812 0.20061 0.178   0.37327]]
1884: {'Out': array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])}
1884: [[0.39554701 0.16250763 0.12883616 0.3131092 ]
1884:  [0.25620569 0.15066985 0.34925393 0.24387053]
1884:  [0.24723053 0.20157411 0.176416   0.37477935]]
1884: [[0.39488 0.16219 0.12896 0.31397]
1884:  [0.25447 0.14892 0.35046 0.24615]
1884:  [0.24546 0.20212 0.17652 0.3759 ]]
1884: I0815 03:41:02.148876  6391 mmap_allocator.cc:348] PID: 6391, MemoryMapFdSet: set size - 0
1884: I0815 03:41:02.161201  6391 mmap_allocator.cc:348] PID: 6391, MemoryMapFdSet: set size - 0
1884: I0815 03:41:02.230167  6466 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8947414039842106427 to 7072346966367823805 , after update, data is {current : 48, peak : 280}.
1884: I0815 03:41:02.230193  6466 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8947414039842106427 to 7072346966367823805 , after update, data is {current : 48, peak : 280}.
1884: I0815 03:41:02.230202  6468 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 1367699014747194628 to 7072346966367823805 , after update, data is {current : 64, peak : 280}.
1884: I0815 03:41:02.230221  6468 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 1367699014747194628 to 7072346966367823805 , after update, data is {current : 64, peak : 280}.
1884: I0815 03:41:02.230240  6467 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 15477162628277092782 to 7072346966367823805 , after update, data is {current : 68, peak : 280}.
1884: I0815 03:41:02.230252  6467 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 15477162628277092782 to 7072346966367823805 , after update, data is {current : 68, peak : 280}.
1884: I0815 03:41:02.230420  6469 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 11142325096254835613 to 7072346966367823805 , after update, data is {current : 44, peak : 280}.
1884: I0815 03:41:02.230432  6469 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 11142325096254835613 to 7072346966367823805 , after update, data is {current : 44, peak : 280}.
1884: I0815 03:41:02.230437  6469 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 11142325096254835613 to 7072346966367823805 , after update, data is {current : 256, peak : 768}.
1884: I0815 03:41:02.230695  6472 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 7072346966367823805 to 9733862684893097145 , after update, data is {current : 768, peak : 1536}.
1884: I0815 03:41:02.230708  6472 thread_data_registry.h:135] Add data {current : 44, peak : 280} from thread 7072346966367823805 to 9733862684893097145 , after update, data is {current : 24, peak : 280}.
1884: I0815 03:41:02.230713  6472 thread_data_registry.h:135] Add data {current : 44, peak : 280} from thread 7072346966367823805 to 9733862684893097145 , after update, data is {current : 24, peak : 280}.
1884: I0815 03:41:02.230763  6471 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8922143757157754564 to 9733862684893097145 , after update, data is {current : 28, peak : 280}.
1884: I0815 03:41:02.230772  6471 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 8922143757157754564 to 9733862684893097145 , after update, data is {current : 28, peak : 280}.
1884: I0815 03:41:02.230901  6474 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 9733862684893097145 to 8712560457566753969 , after update, data is {current : 6401792, peak : 8800800}.
1884: I0815 03:41:02.230911  6474 thread_data_registry.h:135] Add data {current : 28, peak : 280} from thread 9733862684893097145 to 8712560457566753969 , after update, data is {current : 6401792, peak : 6408800}.
1884: I0815 03:41:02.230914  6474 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 9733862684893097145 to 8712560457566753969 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 03:41:02.379624  6391 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 03:41:02.379664  6391 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 03:41:02.379710  6391 mmap_allocator.cc:348] PID: 6391, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   12.56 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  12.74 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
