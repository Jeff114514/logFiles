UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1884
    Start 1884: test_multinomial_op

1884: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_multinomial_op"
1884: Environment variables: 
1884:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1884:  FLAGS_PIR_NO_CHECK=True
1884: Test timeout computed to be: 10000000
1884: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1884: WARNING: Logging before InitGoogleLogging() is written to STDERR
1884: I0815 04:07:01.591953  6969 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1884: I0815 04:07:02.368793  6969 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=benchmark,graph_get_neighbor_id,gemm_use_half_precision_compute_type,enable_async_trace,memory_fraction_of_eager_deletion,pir_apply_inplace_pass,use_cuda_managed_memory,cusparselt_dir,enable_cublas_tensor_op_math,gpugraph_parallel_copyer_split_maxsize,enable_blaslt_global_search,cse_max_count,gpugraph_debug_gpu_memory,cublaslt_device_best_config,print_sub_graph_dir,host_trace_level,initial_gpu_memory_in_mb,use_stride_kernel,new_executor_use_inplace,paddle_num_threads,enable_gpu_memory_usage_log,dataloader_use_file_descriptor,prim_enabled,enable_pir_in_executor_trace_run,print_allocator_trace_info,trt_ibuilder_cache,enable_fuse_parallel_matmul_pass,enable_auto_detect_gpu_topo,nccl_blocking_wait,nccl_dir,new_executor_static_build,eager_delete_tensor_gb,enable_dependency_builder_debug_info,gpugraph_dedup_pull_push_mode,prim_forward_blacklist,cusparse_dir,enable_pir_with_pt_in_dy2st,tracer_profile_fname,enable_interpretercore_launch_cinn,nvidia_package_dir,cudnn_deterministic,enable_graph_multi_node_sampling,use_fast_math,enable_dump_main_program,pir_debug,cinn_subgraph_graphviz_dir,logging_pir_py_code_dump_symbolic_dims,use_auto_growth_pinned_allocator,sort_sum_gradient,fuse_parameter_groups_size,static_executor_perfstat_filepath,cudnn_exhaustive_search,tensor_operants_mode,alloc_fill_value,enable_cinn_compile_cache,enable_exit_when_partial_worker,use_cinn,pinned_memory_as_cpu_backend,graph_embedding_split_infer_mode,mklml_dir,fraction_of_cpu_memory_to_use,cuda_dir,prim_skip_dynamic,accuracy_check_rtol_fp16,op_dir,accuracy_check_atol_fp32,custom_device_mem_record,new_executor_serial_run,tensorrt_dir,reader_queue_speed_test_mode,auto_free_cudagraph_allocations_on_launch,call_stack_level,cublas_dir,add_dependency_for_communication_op,manually_trans_conv_filter,cupti_dir,dygraph_debug,gpugraph_load_node_list_into_hbm,use_system_allocator,gpugraph_sparse_table_storage_mode,use_stream_safe_cuda_allocator,sync_nccl_allreduce,max_inplace_grad_add,enable_neighbor_list_use_uva,local_exe_sub_scope_limit,inner_op_parallelism,cudnn_dir,tracer_onednn_ops_on,gpugraph_slot_feasign_max_num,fraction_of_cuda_pinned_memory_to_use,jit_engine_type,graph_metapath_split_opt,new_executor_use_local_scope,einsum_opt,mkl_dir,cusolver_dir,benchmark_nccl,dump_chunk_info,save_static_runtime_data,gpugraph_offload_param_extends,gpu_allocator_retry_time,npu_storage_format,logging_trunc_pir_py_code,enable_adjust_op_order,use_pinned_memory,cuda_memory_async_pool_realease_threshold,fast_eager_deletion_mode,ir_inplace_kernel_blacklist,enable_collect_shape,low_precision_op_list,allreduce_record_one_event,gpugraph_enable_hbm_table_collision_stat,gpugraph_offload_gather_copy_maxsize,disable_dyshape_in_train,prim_backward,enable_fusion_fallback,enable_auto_rdma_trans,gpugraph_force_device_batch_num_equal,new_executor_use_cuda_graph,cublaslt_exhaustive_search_times,eager_delete_scope,multi_node_sample_use_gpu_table,dynamic_static_unified_comm,use_mkldnn,tracer_onednn_ops_off,use_virtual_memory_auto_growth,use_auto_growth_v2,cinn_compile_thread_num,prim_forward,set_to_1d,accuracy_check_rtol_fp32,fraction_of_gpu_memory_to_use,log_memory_stats,new_executor_sequential_run,logging_pir_py_code_dir,fleet_executor_with_standalone,enable_all2all_use_fp16,use_xqa_optim,gpugraph_enable_print_op_debug,all_blocks_convert_trt,init_allocated_mem,gpu_memory_limit_mb,check_kernel_launch,enable_tracker_all2all,gpugraph_storage_mode,enable_cinn_accuracy_check,gpugraph_merge_grads_segment_size,enable_pir_in_executor,cudnn_batchnorm_spatial_persistent,pir_apply_shape_optimization_pass,prim_check_ops,convert_all_blocks,gpugraph_parallel_stream_num,print_ir,sync_after_alloc,cudnn_exhaustive_search_times,use_shm_cache,accuracy_check_atol_bf16,enable_opt_get_features,conv2d_disable_cudnn,accuracy_check_atol_fp16,curand_dir,cuda_malloc_async_pool_memory_throttle_ratio,allow_cinn_ops,query_dest_rank_by_multi_node,reallocate_gpu_memory_in_mb,enable_gpu_memory_usage_log_mb,enable_unused_var_check,graph_neighbor_size_percent,embedding_deterministic,allocator_strategy,apply_pass_to_program,lapack_dir,use_cuda_malloc_async_allocator,logging_pir_py_code_int_tensor_element_limit,dist_threadpool_size,graph_load_in_parallel,auto_growth_chunk_size_in_mb,run_kp_kernel,multiple_of_cupti_buffer_size,get_host_by_name_time,pir_subgraph_saving_dir,enable_cse_in_dy2st,prim_enable_dynamic,enable_cinn_auto_tune,prim_all,initial_cpu_memory_in_mb,static_runtime_data_save_path,conv_workspace_size_limit,check_infer_symbolic,gpugraph_enable_gpu_direct_access,check_nan_inf,gpugraph_offload_param_stat,use_autotune,check_nan_inf_level,gpugraph_hbm_table_load_factor,fuse_parameter_memory_size,win_cuda_bin_dir,enable_api_kernel_fallback,executor_log_deps_every_microseconds,selected_gpus,search_cache_max_number,cache_inference_while_scope,accuracy_check_rtol_bf16,async_trace_count,enable_record_memory,free_when_no_cache_hit,enable_sparse_inner_gather,deny_cinn_ops,gpugraph_enable_segment_merge_grads,free_idle_chunk,enable_pir_api,pir_broadcast_tree_limit 
1884: I0815 04:07:02.368898  6969 init.cc:108] After Parse: argc is 2
1884: I0815 04:07:11.718987  6969 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:07:11.719040  6969 dygraph_functions.cc:77659] { Input: []} 
1884: W0815 04:07:11.719719  6969 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1884: I0815 04:07:11.720252  6969 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1884: W0815 04:07:11.721122  6969 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1884: I0815 04:07:11.721213  6969 allocator_facade.cc:212] selected allocator strategy:1
1884: I0815 04:07:11.721310  6969 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1884: I0815 04:07:11.721957  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fa3eba00000), and remaining 0
1884: I0815 04:07:11.722311  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.722373  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.722455  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fa3eba00200), and remaining 0
1884: I0815 04:07:11.722482  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fa3eba00400), and remaining 0
1884: I0815 04:07:11.726583  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fa3eba00600), and remaining 0
1884: I0815 04:07:11.726712  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 256(0x7fa3eba00800), and remaining 0
1884: I0815 04:07:11.726778  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 768(0x7fa3eba00a00), and remaining 0
1884: I0815 04:07:11.726871  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.726893  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.726964  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.726977  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.728425  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19184e00 for it.
1884: I0815 04:07:11.728572  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.728597  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.728652  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 800000(0x7fa3eba00e00), and remaining 0
1884: I0815 04:07:11.728724  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fa3ebac4400), and remaining 0
1884: I0815 04:07:11.850415  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19184e00 for it.
1884: I0815 04:07:11.850692  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.850733  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.851408  6969 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 2400000(0x7fa3ebc00000), and remaining 0
1884: I0815 04:07:11.862628  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x19184e00 for it.
1884: I0815 04:07:11.862728  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:11.862758  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.862795  6969 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:11.862954  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:11.863899  6969 dygraph_functions.cc:33459] Running AD API: full
1884: I0815 04:07:11.863916  6969 dygraph_functions.cc:33480] { Input: []} 
1884: I0815 04:07:11.863965  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:11.864048  6969 dygraph_functions.cc:64553] Running AD API: scale
1884: I0815 04:07:11.864071  6969 dygraph_functions.cc:64610] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.864128  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:11.864202  6969 dygraph_functions.cc:26170] Running AD API: exp
1884: I0815 04:07:11.864220  6969 dygraph_functions.cc:26227] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.864253  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:11.864475  6969 dygraph_functions.cc:72508] Running AD API: sum
1884: I0815 04:07:11.864495  6969 dygraph_functions.cc:72565] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.864657  6969 dygraph_functions.cc:83176] Running AD API: divide
1884: I0815 04:07:11.864682  6969 dygraph_functions.cc:83249] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]),  
1884: ( y , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:11.864751  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=2800, vec_size=4, block_size=64, grid_size=11, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:11.868568  6969 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1884: I0815 04:07:11.868683  6969 dygraph_functions.cc:87338] Running AD API: softmax
1884: I0815 04:07:11.868710  6969 dygraph_functions.cc:87395] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: W0815 04:07:11.868772  6969 gpu_resources.cc:299] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.9, but CUDNN version in your machine is 8.8, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
1884: I0815 04:07:13.323482  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.323534  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.323812  6969 dygraph_functions.cc:75650] Running AD API: transpose
1884: I0815 04:07:13.323830  6969 dygraph_functions.cc:75707] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.328613  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.328647  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.329666  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.329684  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.329696  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.330442  6969 program_interpreter.cc:243] New Executor is Running.
1884: I0815 04:07:13.330456  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.330471  6969 scope.cc:202] Create variable feed
1884: I0815 04:07:13.330478  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.330488  6969 scope.cc:202] Create variable fetch
1884: I0815 04:07:13.330495  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.330507  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.330513  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.330516  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.330520  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.332814  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.333148  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.333161  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.333166  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.334812  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.334858  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.334867  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.334874  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.334882  6969 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:07:13.334889  6969 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x5f918e10 type is 7
1884: I0815 04:07:13.334894  6969 scope.cc:202] Create variable x
1884: I0815 04:07:13.334898  6969 interpreter_util.cc:1206] Create Variable x locally, which pointer is 0x5f918f60 type is 7
1884: I0815 04:07:13.334961  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.334968  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.334972  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.334976  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.335090  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.335112  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.335222  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.335232  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.335248  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.335410  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.335441  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.335461  6969 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.335466  6969 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5f920fd0Variable Type 7
1884: I0815 04:07:13.335485  6969 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.335505  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.335556  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.335573  6969 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.336809  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.336863  6969 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.337235  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.342890  6969 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:07:13.342909  6969 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:07:13.342996  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.343022  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.343488  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: I0815 04:07:13.343559  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.343583  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.344031  6969 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: I0815 04:07:13.344092  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.344115  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.344138  6969 tensor_utils.cc:57] TensorCopy 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.344398  6969 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:07:13.344409  6969 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:07:13.344521  6969 dygraph_functions.cc:87192] Running AD API: set_value_
1884: I0815 04:07:13.344544  6969 dygraph_functions.cc:87236] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.344904  6969 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:07:13.344915  6969 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:07:13.344957  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.344976  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.345157  6969 dygraph_functions.cc:77638] Running AD API: uniform
1884: I0815 04:07:13.345166  6969 dygraph_functions.cc:77659] { Input: []} 
1884: I0815 04:07:13.345202  6969 dygraph_functions.cc:53762] Running AD API: multinomial
1884: I0815 04:07:13.345219  6969 dygraph_functions.cc:53816] { Input: [ 
1884: ( x , [[ Not specified tensor log level ]]), ]} 
1884: I0815 04:07:13.345235  6969 tensor_utils.cc:57] TensorCopy 4 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.347829  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.347851  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.347901  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.347911  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.349771  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.350128  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.350142  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.350147  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.351914  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.351963  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.351974  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.351979  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5f94c500 type is 7
1884: I0815 04:07:13.351987  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.351994  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5f94c870 type is 7
1884: I0815 04:07:13.351999  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.352005  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.352061  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.352068  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.352072  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.352078  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.352123  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.352138  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.352192  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.352201  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.352219  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.352478  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.352494  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.352512  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.352520  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5f952fc0Variable Type 7
1884: I0815 04:07:13.352537  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.352555  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.352578  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.352595  6969 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.353318  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.353343  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.353508  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.365729  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: I0815 04:07:13.365911  6969 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x191a3390 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> builtin.tensor<4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100000xi64>) -> builtin.tensor<100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: I0815 04:07:13.371754  6969 pir_interpreter.cc:161] PirInterpreter(): 0x5fb0f750 on Place(gpu:0)
1884: I0815 04:07:13.371793  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.371820  6969 scope.cc:202] Create variable 0x5fb0f7501723694833371779619_inner_var_1
1884: I0815 04:07:13.371832  6969 scope.cc:202] Create variable 0x5fb0f7501723694833371779619_inner_var_2
1884: I0815 04:07:13.371841  6969 scope.cc:202] Create variable 0x5fb0f7501723694833371779619_inner_var_3
1884: I0815 04:07:13.371851  6969 scope.cc:202] Create variable 0x5fb0f7501723694833371779619_inner_var_4
1884: I0815 04:07:13.371860  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.372267  6969 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:07:13.372282  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.372285  6969 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: I0815 04:07:13.372337  6969 pir_interpreter.cc:1455] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[4],stop_gradient:[true]} : () -> undefined_tensor<4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<4xf16>) -> gpu_tensor<4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100000xi64>) -> cpu_tensor<100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5fb0f6b0
1884: 1 -> 0x5fb0f7501723694833371779619_inner_var_1 -> 0x5fb0f730
1884: 2 -> 0x5fb0f7501723694833371779619_inner_var_2 -> 0x5fb10000
1884: 3 -> 0x5fb0f7501723694833371779619_inner_var_3 -> 0x5fb0e960
1884: 4 -> 0x5fb0f7501723694833371779619_inner_var_4 -> 0x5fb103b0
1884: 5 -> fetch0@fetch -> 0x5fb10bc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:07:13.373056  6969 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1884: I0815 04:07:13.373348  7007 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.373503  7008 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.373559  7009 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.373584  7010 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.373656  7011 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.373703  7010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fb0f7501723694833371779619_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.373728  7012 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.373790  7010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5fb0f7501723694833371779619_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.373783  7012 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.373823  7012 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}.
1884: I0815 04:07:13.373869  7012 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fb0f7501723694833371779619_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fb0f7501723694833371779619_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.374054  7012 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5fb0f7501723694833371779619_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5fb0f7501723694833371779619_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=4;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}.
1884: I0815 04:07:13.374123  7010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fb0f7501723694833371779619_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.374146  7010 tensor_utils.cc:57] TensorCopy 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.375386  7010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5fb0f7501723694833371779619_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100000;lod={};]}, outputs:{0x5fb0f7501723694833371779619_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:07:13.375429  7010 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5fb0f7501723694833371779619_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.375454  7010 tensor_utils.cc:57] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.376044  7010 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5fb0f7501723694833371779619_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100000;lod={};]}.
1884: I0815 04:07:13.376081  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x5fb0f8c0) got event_name: TaskCompletion
1884: I0815 04:07:13.376107  6969 tensor_util.cc:48] TensorCopy 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.450613  7007 thread_data_registry.h:135] Add data {current : -800000, peak : 0} from thread 3754869559457448081 to 8863317776170848761 , after update, data is {current : 0, peak : 800768}.
1884: I0815 04:07:13.450634  7007 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 3754869559457448081 to 17215111427179135468 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:07:13.450640  7007 thread_data_registry.h:135] Add data {current : -800004, peak : 0} from thread 3754869559457448081 to 17215111427179135468 , after update, data is {current : 800000, peak : 1600004}.
1884: I0815 04:07:13.450806  7010 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 17215111427179135468 to 3970409447905172152 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:07:13.450822  7010 thread_data_registry.h:135] Add data {current : 800000, peak : 1600004} from thread 17215111427179135468 to 3970409447905172152 , after update, data is {current : 800000, peak : 2400000}.
1884: I0815 04:07:13.450996  7012 thread_data_registry.h:135] Add data {current : 0, peak : 767} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 3203088, peak : 3203847}.
1884: I0815 04:07:13.451013  7012 thread_data_registry.h:135] Add data {current : 0, peak : 800768} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:07:13.456928  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.456951  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.457005  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.457013  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.458727  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.459066  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.459079  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.459085  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.460608  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.460697  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.460708  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.460714  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x2146fa0 type is 7
1884: I0815 04:07:13.460721  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.460726  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x620e3480 type is 7
1884: I0815 04:07:13.460729  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.460734  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.460789  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.460795  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.460801  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.460804  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.460850  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.460865  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.460922  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.460929  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.460943  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.461083  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.461093  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.461109  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.461115  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x398e2c0Variable Type 7
1884: I0815 04:07:13.461130  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.461148  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.461167  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.461181  6969 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.462868  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.462903  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.463100  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.467465  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x191a3390 for it.
1884: I0815 04:07:13.467644  6969 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf16>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:07:13.471532  6969 pir_interpreter.cc:161] PirInterpreter(): 0x5f8f9b20 on Place(gpu:0)
1884: I0815 04:07:13.471562  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.471580  6969 scope.cc:202] Create variable 0x5f8f9b201723694833471555424_inner_var_1
1884: I0815 04:07:13.471590  6969 scope.cc:202] Create variable 0x5f8f9b201723694833471555424_inner_var_2
1884: I0815 04:07:13.471597  6969 scope.cc:202] Create variable 0x5f8f9b201723694833471555424_inner_var_3
1884: I0815 04:07:13.471606  6969 scope.cc:202] Create variable 0x5f8f9b201723694833471555424_inner_var_4
1884: I0815 04:07:13.471613  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.471957  6969 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:07:13.471971  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.471974  6969 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf16>) -> gpu_tensor<3x4xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf16>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x20c5770
1884: 1 -> 0x5f8f9b201723694833471555424_inner_var_1 -> 0x5fb0da00
1884: 2 -> 0x5f8f9b201723694833471555424_inner_var_2 -> 0x5f923fc0
1884: 3 -> 0x5f8f9b201723694833471555424_inner_var_3 -> 0x5f918df0
1884: 4 -> 0x5f8f9b201723694833471555424_inner_var_4 -> 0x23ec860
1884: 5 -> fetch0@fetch -> 0x3986100
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:07:13.472541  7013 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.472613  7014 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.472623  7015 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.472659  7016 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.472688  7017 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.472723  7018 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.472721  7017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f8f9b201723694833471555424_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.472746  7018 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.472776  7017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f8f9b201723694833471555424_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.472783  7018 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:07:13.472822  7018 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f8f9b201723694833471555424_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f8f9b201723694833471555424_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.472930  7018 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f8f9b201723694833471555424_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f8f9b201723694833471555424_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.472987  7017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f8f9b201723694833471555424_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.473008  7017 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.475628  7017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f8f9b201723694833471555424_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5f8f9b201723694833471555424_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.475672  7017 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5f8f9b201723694833471555424_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.475692  7017 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.477687  7017 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5f8f9b201723694833471555424_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.477730  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x5f8f9c90) got event_name: TaskCompletion
1884: I0815 04:07:13.477751  6969 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.515153  7013 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8863317776170848761 to 8647409854972599470 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:07:13.515167  7013 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8863317776170848761 to 9454530309328821616 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:07:13.515173  7013 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8863317776170848761 to 9454530309328821616 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:07:13.515389  7017 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 9454530309328821616 to 3970409447905172152 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:07:13.515400  7017 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 9454530309328821616 to 3970409447905172152 , after update, data is {current : 3200000, peak : 4800004}.
1884: I0815 04:07:13.515509  7018 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 8647409854972599470 to 3970409447905172152 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:07:13.519352  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.519373  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.519419  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.519428  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.521036  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.521366  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.521379  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.521384  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.522879  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.522960  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.522972  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.522977  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x399e710 type is 7
1884: I0815 04:07:13.522985  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.522990  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x3986be0 type is 7
1884: I0815 04:07:13.522994  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.523000  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.523051  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.523057  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.523061  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.523065  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.523105  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.523118  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.523169  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.523177  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.523192  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.523226  6969 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.523360  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.523425  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.523435  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.523450  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.523458  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x2019ea0Variable Type 7
1884: I0815 04:07:13.523473  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.523489  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.523509  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.523522  6969 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.523800  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.523820  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.523991  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.524775  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: I0815 04:07:13.524942  6969 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x191a3390 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf16>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf16>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:07:13.527957  6969 pir_interpreter.cc:161] PirInterpreter(): 0x5f8fcd10 on Place(gpu:0)
1884: I0815 04:07:13.527988  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.528009  6969 scope.cc:202] Create variable 0x5f8fcd101723694833527980970_inner_var_1
1884: I0815 04:07:13.528019  6969 scope.cc:202] Create variable 0x5f8fcd101723694833527980970_inner_var_2
1884: I0815 04:07:13.528029  6969 scope.cc:202] Create variable 0x5f8fcd101723694833527980970_inner_var_3
1884: I0815 04:07:13.528039  6969 scope.cc:202] Create variable 0x5f8fcd101723694833527980970_inner_var_4
1884: I0815 04:07:13.528048  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.528374  6969 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:07:13.528390  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.528394  6969 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf16>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf16>) -> gpu_tensor<1000xf16>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf16>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x398ec60
1884: 1 -> 0x5f8fcd101723694833527980970_inner_var_1 -> 0x39a01b0
1884: 2 -> 0x5f8fcd101723694833527980970_inner_var_2 -> 0x5f954430
1884: 3 -> 0x5f8fcd101723694833527980970_inner_var_3 -> 0x397e550
1884: 4 -> 0x5f8fcd101723694833527980970_inner_var_4 -> 0x399e880
1884: 5 -> fetch0@fetch -> 0x399e140
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:07:13.529026  7019 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.529101  7020 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.529165  7022 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.529163  7021 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.529202  7023 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.529237  7024 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.529240  7021 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f8fcd101723694833527980970_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.529260  7024 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.529295  7024 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:07:13.529345  7021 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f8fcd101723694833527980970_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.529374  7024 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f8fcd101723694833527980970_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f8fcd101723694833527980970_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.529419  7024 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.529532  7024 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.529558  7024 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f8fcd101723694833527980970_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f8fcd101723694833527980970_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:07:13.529640  7021 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f8fcd101723694833527980970_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.529670  7021 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.529961  7021 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f8fcd101723694833527980970_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x5f8fcd101723694833527980970_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:07:13.529992  7021 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5f8fcd101723694833527980970_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.530014  7021 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.530027  7021 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5f8fcd101723694833527980970_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:07:13.530066  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x5f8fce80) got event_name: TaskCompletion
1884: I0815 04:07:13.530097  6969 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.561797  7019 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : 0, peak : 3328}.
1884: I0815 04:07:13.561815  7019 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:07:13.561820  7019 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : -804, peak : 2000}.
1884: I0815 04:07:13.562027  7021 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 9950764060847217122 to 8863317776170848761 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:07:13.562047  7021 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 9950764060847217122 to 8863317776170848761 , after update, data is {current : 800, peak : 2000}.
1884: I0815 04:07:13.562264  7024 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:07:13.562276  7024 thread_data_registry.h:135] Add data {current : 800, peak : 2000} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 3200800, peak : 4800004}.
1884: I0815 04:07:13.562283  7024 thread_data_registry.h:135] Add data {current : 0, peak : 3328} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:07:13.570240  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.570268  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.570336  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.570345  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.572216  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.572623  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.572638  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.572643  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.574249  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.574357  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.574370  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.574376  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x5f936ce0 type is 7
1884: I0815 04:07:13.574386  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.574390  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5f8fb170 type is 7
1884: I0815 04:07:13.574394  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.574399  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.574457  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.574463  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.574467  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.574471  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.574528  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.574544  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.574609  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.574617  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.574632  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.574893  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.574905  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.574923  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.574929  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5f91d440Variable Type 7
1884: I0815 04:07:13.574944  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.574965  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.574987  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.575002  6969 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.576561  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.576602  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.576834  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.584467  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x191a3390 for it.
1884: I0815 04:07:13.584658  6969 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:true,stop_gradient:[true]} : (builtin.tensor<3x4xf64>, builtin.tensor<1xi32>) -> builtin.tensor<3x100000xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<3x100000xi64>) -> builtin.tensor<3x100000xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: I0815 04:07:13.587836  6969 pir_interpreter.cc:161] PirInterpreter(): 0x5f9330a0 on Place(gpu:0)
1884: I0815 04:07:13.587870  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.587893  6969 scope.cc:202] Create variable 0x5f9330a01723694833587861021_inner_var_1
1884: I0815 04:07:13.587905  6969 scope.cc:202] Create variable 0x5f9330a01723694833587861021_inner_var_2
1884: I0815 04:07:13.587916  6969 scope.cc:202] Create variable 0x5f9330a01723694833587861021_inner_var_3
1884: I0815 04:07:13.587927  6969 scope.cc:202] Create variable 0x5f9330a01723694833587861021_inner_var_4
1884: I0815 04:07:13.587939  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.588291  6969 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:07:13.588317  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.588321  6969 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<3x4xf64>) -> gpu_tensor<3x4xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100000} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:true,stop_gradient:[true]} : (gpu_tensor<3x4xf64>, cpu_tensor<1xi32>) -> gpu_tensor<3x100000xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<3x100000xi64>) -> cpu_tensor<3x100000xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x5f8fb340
1884: 1 -> 0x5f9330a01723694833587861021_inner_var_1 -> 0x5f8faff0
1884: 2 -> 0x5f9330a01723694833587861021_inner_var_2 -> 0x5f8e9920
1884: 3 -> 0x5f9330a01723694833587861021_inner_var_3 -> 0x5f9218e0
1884: 4 -> 0x5f9330a01723694833587861021_inner_var_4 -> 0x5f9219f0
1884: 5 -> fetch0@fetch -> 0x436dab40
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:07:13.589025  7025 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.589118  7026 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.589188  7027 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.589196  7028 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.589232  7029 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.589267  7029 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f9330a01723694833587861021_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.589339  7029 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f9330a01723694833587861021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.589402  7030 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.589424  7030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.589458  7030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:07:13.589491  7030 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f9330a01723694833587861021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f9330a01723694833587861021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.589604  7030 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f9330a01723694833587861021_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x5f9330a01723694833587861021_inner_var_1:[dtype=double;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.589684  7029 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9330a01723694833587861021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.589726  7029 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.592443  7029 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9330a01723694833587861021_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=3, 100000;lod={};]}, outputs:{0x5f9330a01723694833587861021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.592489  7029 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9330a01723694833587861021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.592512  7029 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.594522  7029 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9330a01723694833587861021_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 100000;lod={};]}.
1884: I0815 04:07:13.594566  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x5f933210) got event_name: TaskCompletion
1884: I0815 04:07:13.594586  6969 tensor_util.cc:48] TensorCopy 3, 100000 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.603890  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.603915  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.603967  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.603976  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.605876  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.606309  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.606324  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.606329  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.608254  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.608351  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.608366  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.608372  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x61aa90d0 type is 7
1884: I0815 04:07:13.608381  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.608384  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x61a95810 type is 7
1884: I0815 04:07:13.608389  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.608397  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.608465  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.608474  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.608477  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.608482  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.608533  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.608551  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.608615  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.608629  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.608649  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.608765  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.608777  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.608795  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.608803  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x620de340Variable Type 7
1884: I0815 04:07:13.608821  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.608841  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.608866  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.608882  6969 tensor_utils.cc:57] TensorCopy 3, 100000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.610433  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.610471  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.610679  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.660188  7025 thread_data_registry.h:135] Add data {current : -2400000, peak : 0} from thread 8863317776170848761 to 8647409854972599470 , after update, data is {current : 0, peak : 2400768}.
1884: I0815 04:07:13.660214  7025 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8863317776170848761 to 9454530309328821616 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:07:13.660219  7025 thread_data_registry.h:135] Add data {current : -2400004, peak : 0} from thread 8863317776170848761 to 9454530309328821616 , after update, data is {current : 2400000, peak : 4800004}.
1884: I0815 04:07:13.660392  7029 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 9454530309328821616 to 3970409447905172152 , after update, data is {current : 5600800, peak : 5600800}.
1884: I0815 04:07:13.660406  7029 thread_data_registry.h:135] Add data {current : 2400000, peak : 4800004} from thread 9454530309328821616 to 3970409447905172152 , after update, data is {current : 5600800, peak : 8000800}.
1884: I0815 04:07:13.660594  7030 thread_data_registry.h:135] Add data {current : 0, peak : 2400768} from thread 8647409854972599470 to 3970409447905172152 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:07:13.665594  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.665619  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.665673  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.665681  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.667413  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.667776  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.667790  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.667794  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.669308  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.669404  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.669416  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.669425  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x19137d60 type is 7
1884: I0815 04:07:13.669431  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.669437  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x5f90d390 type is 7
1884: I0815 04:07:13.669441  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.669445  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.669499  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.669507  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.669510  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.669513  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.669564  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.669579  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.669641  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.669649  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.669663  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.669703  6969 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.669874  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.669970  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.669979  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.669996  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.670003  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x5fae9d40Variable Type 7
1884: I0815 04:07:13.670018  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.670038  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.670061  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.670076  6969 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.670185  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.670207  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.670414  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.671231  6969 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1926ce20 for it.
1884: I0815 04:07:13.671419  6969 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x191a3390 for it.
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float64,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> builtin.tensor<1000xf64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)int32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> builtin.tensor<1xi32>
1884:     (%2) = "pd_op.multinomial" (%0, %1) {replacement:false,stop_gradient:[true]} : (builtin.tensor<1000xf64>, builtin.tensor<1xi32>) -> builtin.tensor<100xi64>
1884:     (%3) = "pd_op.fetch" (%2) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<100xi64>) -> builtin.tensor<100xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: I0815 04:07:13.674489  6969 pir_interpreter.cc:161] PirInterpreter(): 0x624ea5e0 on Place(gpu:0)
1884: I0815 04:07:13.674522  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.674546  6969 scope.cc:202] Create variable 0x624ea5e01723694833674513543_inner_var_1
1884: I0815 04:07:13.674556  6969 scope.cc:202] Create variable 0x624ea5e01723694833674513543_inner_var_2
1884: I0815 04:07:13.674568  6969 scope.cc:202] Create variable 0x624ea5e01723694833674513543_inner_var_3
1884: I0815 04:07:13.674578  6969 scope.cc:202] Create variable 0x624ea5e01723694833674513543_inner_var_4
1884: I0815 04:07:13.674589  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.674919  6969 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1884: I0815 04:07:13.674934  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.674938  6969 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float64,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1000],stop_gradient:[true]} : () -> undefined_tensor<1000xf64>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float64>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<1000xf64>) -> gpu_tensor<1000xf64>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)100} : () -> cpu_tensor<1xi32>
1884:     (%3) = "multinomial(phi_kernel)" (%1, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float64>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[true]} : (gpu_tensor<1000xf64>, cpu_tensor<1xi32>) -> gpu_tensor<100xi64>
1884:     (%4) = "memcpy_d2h(phi_kernel)" (%3) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<100xi64>) -> cpu_tensor<100xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 2 )  = pd_op.full
1884: 2: ( 3 )  = pd_op.multinomial ( 2 )  ( 1 ) 
1884: 3: ( 4 )  = pd_op.memcpy_d2h ( 3 ) 
1884: 4: ( 5 )  = pd_op.fetch ( 4 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> X -> 0x61a8caf0
1884: 1 -> 0x624ea5e01723694833674513543_inner_var_1 -> 0x1e72780
1884: 2 -> 0x624ea5e01723694833674513543_inner_var_2 -> 0x5f91a660
1884: 3 -> 0x624ea5e01723694833674513543_inner_var_3 -> 0x3986840
1884: 4 -> 0x624ea5e01723694833674513543_inner_var_4 -> 0x398f780
1884: 5 -> fetch0@fetch -> 0x61fc76b0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 2 
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.multinomial]->
1884: 2 downstreams: 3[pd_op.memcpy_d2h]->
1884: 3 downstreams: 4[pd_op.fetch]->
1884: 4 downstreams: 
1884: I0815 04:07:13.675630  7031 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.675712  7032 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.675740  7033 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.675782  7034 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.675813  7035 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.675863  7036 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.675858  7035 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x624ea5e01723694833674513543_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.675889  7036 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.675916  7035 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x624ea5e01723694833674513543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.675923  7036 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}.
1884: I0815 04:07:13.675954  7036 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x624ea5e01723694833674513543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x624ea5e01723694833674513543_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_3:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.676002  7036 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.676119  7036 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.676148  7036 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x624ea5e01723694833674513543_inner_var_2:[dtype=int;place=Place(cpu);dim=1;lod={};], 0x624ea5e01723694833674513543_inner_var_1:[dtype=double;place=Place(gpu:0);dim=1000;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}.
1884: I0815 04:07:13.676215  7035 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x624ea5e01723694833674513543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.676239  7035 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.676396  7035 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x624ea5e01723694833674513543_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=100;lod={};]}, outputs:{0x624ea5e01723694833674513543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:07:13.676424  7035 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x624ea5e01723694833674513543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.676445  7035 tensor_utils.cc:57] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.676458  7035 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x624ea5e01723694833674513543_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}, outputs:{fetch0@fetch:[dtype=int64_t;place=Place(cpu);dim=100;lod={};]}.
1884: I0815 04:07:13.676493  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x624ea750) got event_name: TaskCompletion
1884: I0815 04:07:13.676518  6969 tensor_util.cc:48] TensorCopy 100 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.677953  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.677976  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.678025  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.678035  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.679865  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.680264  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.680280  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.680284  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.682114  6969 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1884: I0815 04:07:13.682199  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.682210  6969 scope.cc:202] Create variable Out
1884: I0815 04:07:13.682219  6969 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x62260180 type is 7
1884: I0815 04:07:13.682227  6969 scope.cc:202] Create variable X
1884: I0815 04:07:13.682233  6969 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x6225f4c0 type is 7
1884: I0815 04:07:13.682240  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.682245  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.682319  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.682327  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.682332  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.682336  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.682381  6969 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.682396  6969 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.682454  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.682464  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[double]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.682482  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.682518  6969 tensor_utils.cc:57] TensorCopy 1000 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.682619  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1000, vec_size=1, block_size=64, grid_size=16, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.682668  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.682679  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.682698  6969 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.682705  6969 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x623cbc80Variable Type 7
1884: I0815 04:07:13.682722  6969 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.682741  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.682765  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.682781  6969 tensor_utils.cc:57] TensorCopy 100 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.682916  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.682940  6969 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.683151  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.719887  7031 thread_data_registry.h:135] Add data {current : -1024, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : 0, peak : 10240}.
1884: I0815 04:07:13.719899  7031 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:07:13.719905  7031 thread_data_registry.h:135] Add data {current : -804, peak : 0} from thread 8647409854972599470 to 8863317776170848761 , after update, data is {current : -804, peak : 8000}.
1884: I0815 04:07:13.720052  7035 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 14997327967325241272 to 8863317776170848761 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:07:13.720062  7035 thread_data_registry.h:135] Add data {current : 1604, peak : 1604} from thread 14997327967325241272 to 8863317776170848761 , after update, data is {current : 800, peak : 8000}.
1884: I0815 04:07:13.720245  7036 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 5601600, peak : 5608800}.
1884: I0815 04:07:13.720255  7036 thread_data_registry.h:135] Add data {current : 800, peak : 8000} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 5601600, peak : 8000800}.
1884: I0815 04:07:13.720261  7036 thread_data_registry.h:135] Add data {current : 0, peak : 10240} from thread 8863317776170848761 to 3970409447905172152 , after update, data is {current : 0, peak : 2401024}.
1884: I0815 04:07:13.728555  6969 op_desc.cc:1111] CompileTime infer shape on uniform_random
1884: I0815 04:07:13.728601  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:07:13.729640  6969 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:07:13.730491  6969 op_desc.cc:1111] CompileTime infer shape on gaussian_random
1884: I0815 04:07:13.730521  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:07:13.731859  6969 op_desc.cc:1111] CompileTime infer shape on matmul_v2
1884: I0815 04:07:13.731884  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:07:13.732585  6969 op_desc.cc:1111] CompileTime infer shape on elementwise_add
1884: I0815 04:07:13.733572  6969 op_desc.cc:1111] CompileTime infer shape on abs
1884: I0815 04:07:13.733597  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:07:13.734969  6969 op_desc.cc:1111] CompileTime infer shape on assign_value
1884: I0815 04:07:13.734992  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:07:13.735589  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.735616  6969 op_desc.cc:1111] CompileTime infer shape on multinomial
1884: I0815 04:07:13.735623  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.735630  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.737630  6969 op_desc.cc:1111] CompileTime infer shape on cast
1884: I0815 04:07:13.737655  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:07:13.738622  6969 op_desc.cc:1111] CompileTime infer shape on reduce_mean
1884: I0815 04:07:13.738649  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:07:13.739563  6969 pybind.cc:1827] need skip: 0
1884: I0815 04:07:13.739856  6969 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:07:13.741636  6969 op_desc.cc:1111] CompileTime infer shape on fill_constant
1884: I0815 04:07:13.745325  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.745342  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.745347  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.747298  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.747326  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.747334  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.747339  6969 scope.cc:202] Create variable learning_rate_0
1884: I0815 04:07:13.747345  6969 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61f793f0 type is 7
1884: I0815 04:07:13.747352  6969 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:07:13.747356  6969 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x624cf3c0 type is 7
1884: I0815 04:07:13.747360  6969 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:07:13.747364  6969 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x624cf470 type is 7
1884: I0815 04:07:13.747431  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.747437  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.747442  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.747445  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.747493  6969 operator.cc:2295] op type:uniform_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747505  6969 interpreter_util.cc:844] uniform_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747527  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: uniform; inputs: ; attributes: shape, dtype, min, max, seed; outputs: Out
1884: I0815 04:07:13.747629  6969 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747640  6969 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747699  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.747740  6969 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747748  6969 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.747774  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.748812  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.750130  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.750602  6969 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1884: I0815 04:07:13.750833  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.751153  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.751379  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.751397  6969 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:07:13.751461  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.751466  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.751470  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.751569  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.751581  6969 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:07:13.753123  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.754453  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.755550  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.755738  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.755749  6969 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:07:13.755754  6969 interpreter_util.cc:1206] Create Variable abs_0.tmp_0 locally, which pointer is 0x61fd70d0 type is 7
1884: I0815 04:07:13.755762  6969 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:07:13.755766  6969 interpreter_util.cc:1206] Create Variable assign_0.tmp_0 locally, which pointer is 0x61fd6e80 type is 7
1884: I0815 04:07:13.755770  6969 scope.cc:202] Create variable cast_0.tmp_0
1884: I0815 04:07:13.755774  6969 interpreter_util.cc:1206] Create Variable cast_0.tmp_0 locally, which pointer is 0x61fd6f40 type is 7
1884: I0815 04:07:13.755779  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.755784  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.755788  6969 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:07:13.755792  6969 interpreter_util.cc:1206] Create Variable gaussian_0.tmp_0 locally, which pointer is 0x61fd84c0 type is 7
1884: I0815 04:07:13.755796  6969 interpreter_util.cc:1201] Create Variable learning_rate_0 global, which pointer is 0x61f793f0 type is 7
1884: I0815 04:07:13.755801  6969 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x624cf3c0 type is 7
1884: I0815 04:07:13.755806  6969 scope.cc:202] Create variable linear_0.tmp_0
1884: I0815 04:07:13.755810  6969 interpreter_util.cc:1206] Create Variable linear_0.tmp_0 locally, which pointer is 0x61fd84a0 type is 7
1884: I0815 04:07:13.755815  6969 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:07:13.755818  6969 interpreter_util.cc:1206] Create Variable linear_0.tmp_1 locally, which pointer is 0x61fd8a00 type is 7
1884: I0815 04:07:13.755823  6969 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x624cf470 type is 7
1884: I0815 04:07:13.755827  6969 scope.cc:202] Create variable mean_0.tmp_0
1884: I0815 04:07:13.755831  6969 interpreter_util.cc:1206] Create Variable mean_0.tmp_0 locally, which pointer is 0x61fd8c70 type is 7
1884: I0815 04:07:13.755837  6969 scope.cc:202] Create variable mean_0.tmp_0@GRAD
1884: I0815 04:07:13.755841  6969 interpreter_util.cc:1206] Create Variable mean_0.tmp_0@GRAD locally, which pointer is 0x61fd8eb0 type is 7
1884: I0815 04:07:13.755847  6969 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:07:13.755851  6969 interpreter_util.cc:1206] Create Variable multinomial_0.tmp_0 locally, which pointer is 0x61fd9110 type is 7
1884: I0815 04:07:13.755935  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.755950  6969 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:07:13.756008  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.756016  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.756019  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.756023  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.756067  6969 operator.cc:2295] op type:gaussian_random, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.756078  6969 interpreter_util.cc:844] gaussian_random : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.756094  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: gaussian; inputs: ; attributes: shape, mean, std, seed, dtype; outputs: Out
1884: I0815 04:07:13.756196  6969 operator.cc:2295] op type:matmul_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.756207  6969 interpreter_util.cc:844] matmul_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.756225  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:07:13.756290  6969 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:07:13.756480  6969 dynamic_loader.cc:226] Try to find library: libcublas.so from default system path.
1884: I0815 04:07:13.757576  6969 operator.cc:2295] op type:elementwise_add, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757596  6969 interpreter_util.cc:844] elementwise_add : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757653  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.757715  6969 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757725  6969 interpreter_util.cc:844] abs : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757738  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:07:13.757761  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.757802  6969 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757812  6969 interpreter_util.cc:844] assign_value : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757823  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:07:13.757898  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757908  6969 interpreter_util.cc:844] multinomial : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.757921  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.758013  6969 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.758095  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.758152  6969 operator.cc:2295] op type:cast, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758162  6969 interpreter_util.cc:844] cast : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758174  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:07:13.758203  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.758253  6969 operator.cc:2295] op type:reduce_mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758263  6969 interpreter_util.cc:844] reduce_mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758277  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_raw; inputs: X; attributes: dim, keep_dim, reduce_all; outputs: Out
1884: I0815 04:07:13.758388  6969 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758399  6969 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758417  6969 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.758463  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.758472  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.758488  6969 scope.cc:202] Create variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.758495  6969 data_transfer.cc:396] Create Variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x61622040Variable Type 7
1884: I0815 04:07:13.758512  6969 data_transfer.cc:439] Insert memcpy_d2h with linear_0.tmp_1(Place(gpu:0)) -> linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.758527  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.758548  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758559  6969 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.758597  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.758618  6969 fetch_v2_op.cc:138] Fetch variable linear_0.tmp_1_device_Place(gpu:0)_Place(cpu)'s 0 column.
1884: I0815 04:07:13.758643  6969 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.758652  6969 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.758666  6969 scope.cc:202] Create variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1884: I0815 04:07:13.758671  6969 data_transfer.cc:396] Create Variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x620cc640Variable Type 7
1884: I0815 04:07:13.758685  6969 data_transfer.cc:439] Insert memcpy_d2h with multinomial_0.tmp_0(Place(gpu:0)) -> multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1884: I0815 04:07:13.758697  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.758713  6969 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.758723  6969 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.758756  6969 data_transfer.cc:232] Run memcpy_d2h done.
1884: I0815 04:07:13.758769  6969 fetch_v2_op.cc:138] Fetch variable multinomial_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1884: I0815 04:07:13.759204  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: matmul; inputs: X, Y; attributes: trans_x, trans_y; outputs: Out
1884: I0815 04:07:13.759238  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:07:13.759258  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:07:13.759294  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: cast; inputs: X; attributes: out_dtype; outputs: Out
1884: I0815 04:07:13.759341  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.759358  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1884: I0815 04:07:13.765017  6969 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:07:13.765049  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:07:13.765754  6969 op_desc.cc:1111] CompileTime infer shape on scale
1884: I0815 04:07:13.765777  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:07:13.766104  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.767753  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.768620  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.768747  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.769250  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.770141  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.772209  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.773247  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.774994  6969 op_desc.cc:1111] CompileTime infer shape on save_combine
1884: I0815 04:07:13.775764  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.775779  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.775784  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.776933  6969 interpreter_util.cc:1169] Creating Variables
1884: I0815 04:07:13.776950  6969 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x3965b20 type is 9
1884: I0815 04:07:13.776957  6969 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x397efc0 type is 10
1884: I0815 04:07:13.776962  6969 interpreter_util.cc:1201] Create Variable linear_0.b_0 global, which pointer is 0x624cf3c0 type is 7
1884: I0815 04:07:13.776968  6969 interpreter_util.cc:1201] Create Variable linear_0.w_0 global, which pointer is 0x624cf470 type is 7
1884: I0815 04:07:13.776973  6969 scope.cc:202] Create variable saved_params
1884: I0815 04:07:13.776978  6969 interpreter_util.cc:1201] Create Variable saved_params global, which pointer is 0x61688f40 type is 17
1884: I0815 04:07:13.777005  6969 interpreter_util.cc:594] Static build: 0
1884: I0815 04:07:13.777011  6969 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1884: I0815 04:07:13.777015  6969 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1884: I0815 04:07:13.777019  6969 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1884: I0815 04:07:13.777055  6969 operator.cc:2295] op type:save_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.777066  6969 interpreter_util.cc:844] save_combine : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1884: I0815 04:07:13.777746  6969 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:07:13.777786  6969 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:07:13.777832  6969 dynamic_loader.cc:226] Try to find library: libmklml_intel.so from default system path.
1884: I0815 04:07:13.778949  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.779004  6969 scope.cc:202] Create variable feed
1884: I0815 04:07:13.779012  6969 naive_executor.cc:189] 0x61fbda50 Create persistable variable feed, which pointer is 0x6213f3c0
1884: I0815 04:07:13.779017  6969 scope.cc:202] Create variable fetch
1884: I0815 04:07:13.779021  6969 naive_executor.cc:189] 0x61fbda50 Create persistable variable fetch, which pointer is 0x623b2f20
1884: I0815 04:07:13.779024  6969 scope.cc:202] Create variable linear_0.b_0
1884: I0815 04:07:13.779027  6969 naive_executor.cc:189] 0x61fbda50 Create persistable variable linear_0.b_0, which pointer is 0x61fbf5a0
1884: I0815 04:07:13.779032  6969 scope.cc:202] Create variable linear_0.w_0
1884: I0815 04:07:13.779035  6969 naive_executor.cc:189] 0x61fbda50 Create persistable variable linear_0.w_0, which pointer is 0x6199e1b0
1884: I0815 04:07:13.779049  6969 analysis_predictor.cc:2001] AnalysisPredictor::PrepareArgument
1884: [1m[35m--- Running analysis [ir_graph_build_pass][0m
1884: I0815 04:07:13.779399  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.779479  6969 program_converter.cc:296] is_legacy_program : 0
1884: I0815 04:07:13.779520  6969 executor.cc:183] Old Executor is Running.
1884: I0815 04:07:13.779585  6969 executor.cc:92] Creating Variables for block 0
1884: I0815 04:07:13.779592  6969 executor.cc:107] Initialize Variable linear_0.b_0
1884: I0815 04:07:13.779597  6969 executor.cc:109] Create Variable linear_0.b_0 global, which pointer is 0x61fbf5a0 type is 7
1884: I0815 04:07:13.779601  6969 executor.cc:107] Initialize Variable linear_0.w_0
1884: I0815 04:07:13.779605  6969 executor.cc:109] Create Variable linear_0.w_0 global, which pointer is 0x6199e1b0 type is 7
1884: I0815 04:07:13.779635  6969 operator.cc:2295] op type:load_combine, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.779706  6969 operator.cc:834] Place(cpu) Op(load_combine), inputs:{}, outputs:{Out[linear_0.b_0:float[10]({})(Place(cpu)), linear_0.w_0:float[4, 10]({})(Place(cpu))]}.
1884: I0815 04:07:13.779745  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.779750  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:13.779886  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.779995  6969 graph.cc:149] create OpNode by feed
1884: I0815 04:07:13.780032  6969 graph.cc:149] create OpNode by matmul_v2
1884: I0815 04:07:13.780048  6969 graph.cc:149] create OpNode by elementwise_add
1884: I0815 04:07:13.780063  6969 graph.cc:149] create OpNode by abs
1884: I0815 04:07:13.780073  6969 graph.cc:149] create OpNode by assign_value
1884: I0815 04:07:13.780092  6969 graph.cc:149] create OpNode by multinomial
1884: I0815 04:07:13.780102  6969 op_desc.cc:1187] Update AttrVar num_samples with assign_0.tmp_0
1884: I0815 04:07:13.780115  6969 graph.cc:149] create OpNode by scale
1884: I0815 04:07:13.780128  6969 graph.cc:149] create OpNode by scale
1884: I0815 04:07:13.780139  6969 graph.cc:149] create OpNode by fetch
1884: I0815 04:07:13.780155  6969 graph.cc:149] create OpNode by fetch
1884: I0815 04:07:13.780175  6969 graph.cc:224] kStaleProgramOpDescs.size: 10
1884: [1m[35m--- Running analysis [ir_analysis_pass][0m
1884: [32m--- Running IR pass [simplify_with_basic_ops_pass][0m
1884: I0815 04:07:13.781381  6969 simplify_with_basic_ops_pass.cc:57] Running simplify_with_basic_ops_pass.
1884: I0815 04:07:13.781389  6969 simplify_with_basic_ops_pass.cc:59] The ID of block running simplify_with_basic_ops_pass is: 0(main_graph)
1884: I0815 04:07:13.781456  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.781462  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [layer_norm_fuse_pass][0m
1884: I0815 04:07:13.781574  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.781826  6969 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:07:13.781883  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.781888  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [attention_lstm_fuse_pass][0m
1884: I0815 04:07:13.781921  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.781926  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass][0m
1884: I0815 04:07:13.781966  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782027  6969 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:07:13.782059  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782065  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seqpool_cvm_concat_fuse_pass][0m
1884: I0815 04:07:13.782083  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782095  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782119  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782123  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_lstm_fuse_pass][0m
1884: I0815 04:07:13.782163  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782184  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782209  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782214  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_gru_fuse_pass][0m
1884: I0815 04:07:13.782258  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782344  6969 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:07:13.782373  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782378  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [mul_gru_fuse_pass][0m
1884: I0815 04:07:13.782413  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782433  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782454  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782459  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [seq_concat_fc_fuse_pass][0m
1884: I0815 04:07:13.782490  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782637  6969 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:07:13.782665  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782671  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass][0m
1884: I0815 04:07:13.782699  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782716  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782737  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782743  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass][0m
1884: I0815 04:07:13.782765  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782780  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782801  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782806  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass][0m
1884: I0815 04:07:13.782828  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782842  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.782863  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.782868  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_v2_scale_fuse_pass][0m
1884: I0815 04:07:13.782891  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.782958  6969 graph_pattern_detector.cc:126] 5 nodes marked
1884: I0815 04:07:13.782989  6969 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:07:13.783003  6969 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:07:13.783017  6969 graph_pattern_detector.cc:250] step 3 get records: 0
1884: I0815 04:07:13.783041  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.783046  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass][0m
1884: I0815 04:07:13.783069  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.783109  6969 graph_pattern_detector.cc:126] 4 nodes marked
1884: I0815 04:07:13.783129  6969 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:07:13.783140  6969 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:07:13.783154  6969 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:07:13.783182  6969 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:07:13.783193  6969 gpu_cpu_map_matmul_to_mul_pass.cc:337] gpu_cpu map matmul_v2 to mul
1884: I0815 04:07:13.784361  6969 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:07:13.784407  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.784413  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass][0m
1884: I0815 04:07:13.784444  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.784464  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.784490  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.784495  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [matmul_scale_fuse_pass][0m
1884: I0815 04:07:13.784520  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.784569  6969 graph_pattern_detector.cc:126] 2 nodes marked
1884: I0815 04:07:13.784600  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.784605  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass][0m
1884: I0815 04:07:13.784623  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.784639  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.784662  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.784667  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [fc_fuse_pass][0m
1884: I0815 04:07:13.784700  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.784788  6969 graph_pattern_detector.cc:126] 6 nodes marked
1884: I0815 04:07:13.784814  6969 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:07:13.784829  6969 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:07:13.784844  6969 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:07:13.784859  6969 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:07:13.784874  6969 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:07:13.784889  6969 graph_pattern_detector.cc:250] step 6 get records: 0
1884: I0815 04:07:13.784912  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.784984  6969 graph_pattern_detector.cc:126] 7 nodes marked
1884: I0815 04:07:13.785007  6969 graph_pattern_detector.cc:250] step 1 get records: 1
1884: I0815 04:07:13.785020  6969 graph_pattern_detector.cc:250] step 2 get records: 1
1884: I0815 04:07:13.785033  6969 graph_pattern_detector.cc:250] step 3 get records: 1
1884: I0815 04:07:13.785046  6969 graph_pattern_detector.cc:250] step 4 get records: 1
1884: I0815 04:07:13.785063  6969 graph_pattern_detector.cc:250] step 5 get records: 1
1884: I0815 04:07:13.785077  6969 graph_pattern_detector.cc:250] step 6 get records: 1
1884: I0815 04:07:13.785120  6969 graph_pattern_detector.cc:100] optimizing #0 subgraph
1884: I0815 04:07:13.785395  6969 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:07:13.785425  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.785431  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [repeated_fc_relu_fuse_pass][0m
1884: I0815 04:07:13.785478  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785537  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785570  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785617  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785645  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785686  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785709  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785748  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785768  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785801  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785820  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785851  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785866  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785892  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785907  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785929  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785940  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.785959  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.785984  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.785988  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [squared_mat_sub_fuse_pass][0m
1884: I0815 04:07:13.786015  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.786056  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.786080  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786087  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_bn_fuse_pass][0m
1884: I0815 04:07:13.786098  6969 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:07:13.786101  6969 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:07:13.786149  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.786170  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.786195  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786201  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:07:13.786211  6969 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:07:13.786216  6969 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:07:13.786257  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.786278  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.786310  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786316  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_bn_fuse_pass][0m
1884: I0815 04:07:13.786325  6969 conv_bn_fuse_pass.cc:310] Running conv_bn_fuse_pass.
1884: I0815 04:07:13.786329  6969 conv_bn_fuse_pass.cc:312] The ID of block running conv_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:07:13.786361  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.786381  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.786402  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786407  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass][0m
1884: I0815 04:07:13.786417  6969 conv_bn_fuse_pass.cc:618] Running conv_eltwiseadd_bn_fuse_pass.
1884: I0815 04:07:13.786422  6969 conv_bn_fuse_pass.cc:620] The ID of block running conv_eltwiseadd_bn_fuse_pass is: 0(main_graph)
1884: I0815 04:07:13.786459  6969 graph_pattern_detector.cc:106] mark pdnodes in graph
1884: I0815 04:07:13.786479  6969 graph_pattern_detector.cc:126] 0 nodes marked
1884: I0815 04:07:13.786502  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786507  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [is_test_pass][0m
1884: I0815 04:07:13.786521  6969 is_test_pass.cc:24] Sets is_test attribute to true and if it is missing, inserts it for activations and pooling.
1884: I0815 04:07:13.786559  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786564  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [32m--- Running IR pass [constant_folding_pass][0m
1884: I0815 04:07:13.786633  6969 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:07:13.786652  6969 operator.cc:2295] op type:assign_value, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.786670  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: assign_value; inputs: ; attributes: shape, dtype, values; outputs: Out
1884: I0815 04:07:13.786717  6969 operator.cc:834] Place(cpu) Op(assign_value), inputs:{}, outputs:{Out[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}.
1884: I0815 04:07:13.786734  6969 scope.cc:202] Create variable assign_0.tmp_0
1884: I0815 04:07:13.786760  6969 fuse_pass_base.cc:59] ---  detected 1 subgraphs
1884: I0815 04:07:13.786783  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.786788  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: [1m[35m--- Running analysis [ir_params_sync_among_devices_pass][0m
1884: [1m[35m--- Running analysis [adjust_cudnn_workspace_size_pass][0m
1884: [1m[35m--- Running analysis [inference_op_replace_pass][0m
1884: [1m[35m--- Running analysis [save_optimized_model_pass][0m
1884: [1m[35m--- Running analysis [ir_graph_to_program_pass][0m
1884: I0815 04:07:13.787670  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.787685  6969 runtime_context_cache_pass.cc:27] Applies Runtime Context Cache strategy.
1884: I0815 04:07:13.787737  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.787743  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:13.788342  6969 graph_helper.cc:774] Graph to program need convert 1 sub graph
1884: I0815 04:07:13.788552  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.788625  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.788630  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:13.789032  6969 program_desc.cc:166] InitFromProto: SetVarAttr num_samples from assign_0.tmp_0
1884: I0815 04:07:13.789237  6969 graph.h:183] deleting __fuse_statis__
1884: I0815 04:07:13.789244  6969 graph.h:183] deleting pass_recorder
1884: I0815 04:07:13.789250  6969 graph.h:183] deleting stale_program_op_descs
1884: I0815 04:07:13.789338  6969 analysis_predictor.cc:2310] ======= ir optimization completed =======
1884: I0815 04:07:13.789350  6969 scope.cc:202] Create variable abs_0.tmp_0
1884: I0815 04:07:13.789352  6969 naive_executor.cc:195] 0x61fbda50 Create variable abs_0.tmp_0, which pointer is 0x616d3790
1884: I0815 04:07:13.789359  6969 scope.cc:202] Create variable gaussian_0.tmp_0
1884: I0815 04:07:13.789362  6969 naive_executor.cc:195] 0x61fbda50 Create variable gaussian_0.tmp_0, which pointer is 0x62141a80
1884: I0815 04:07:13.789374  6969 scope.cc:202] Create variable linear_0.tmp_1
1884: I0815 04:07:13.789379  6969 naive_executor.cc:195] 0x61fbda50 Create variable linear_0.tmp_1, which pointer is 0x5fb1ddf0
1884: I0815 04:07:13.789383  6969 scope.cc:202] Create variable multinomial_0.tmp_0
1884: I0815 04:07:13.789386  6969 naive_executor.cc:195] 0x61fbda50 Create variable multinomial_0.tmp_0, which pointer is 0x620cbf10
1884: I0815 04:07:13.789391  6969 scope.cc:202] Create variable save_infer_model/scale_0.tmp_0
1884: I0815 04:07:13.789393  6969 naive_executor.cc:195] 0x61fbda50 Create variable save_infer_model/scale_0.tmp_0, which pointer is 0x620cc210
1884: I0815 04:07:13.789397  6969 scope.cc:202] Create variable save_infer_model/scale_1.tmp_0
1884: I0815 04:07:13.789399  6969 naive_executor.cc:195] 0x61fbda50 Create variable save_infer_model/scale_1.tmp_0, which pointer is 0x620ca810
1884: I0815 04:07:13.789405  6969 scope.cc:202] Create variable feed
1884: I0815 04:07:13.789409  6969 scope.cc:202] Create variable fetch
1884: I0815 04:07:13.789429  6969 naive_executor.cc:46] NaiveExecutor init with scope 0x61fbda50
1884: I0815 04:07:13.789435  6969 naive_executor.cc:207] ---  skip [feed], feed -> gaussian_0.tmp_0
1884: I0815 04:07:13.789623  6969 attribute_checker.h:253] Found Attribute num_samples with type(Variable).
1884: I0815 04:07:13.789637  6969 operator.cc:1021] found Attribute with Variable type: num_samples
1884: I0815 04:07:13.789664  6969 naive_executor.cc:207] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch
1884: I0815 04:07:13.789670  6969 naive_executor.cc:207] ---  skip [save_infer_model/scale_1.tmp_0], fetch -> fetch
1884: I0815 04:07:13.789677  6969 helper.h:461] Init predictor : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.789707  6969 helper.h:475] Init predictor : [cpu current allocated memory: 5.3423MB], [cpu current reserved memory: 5.3423MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:07:13.789899  6969 helper.h:461] before run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.789914  6969 helper.h:475] before run : [cpu current allocated memory: 5.34235MB], [cpu current reserved memory: 5.34235MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:07:13.789959  6969 operator.cc:2295] op type:fc, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.789984  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: fc; inputs: Input, W, Bias; attributes: in_num_col_dims, activation_type, padding_weights; outputs: Out
1884: I0815 04:07:13.828123  6969 operator.cc:834] Place(cpu) Op(fc), inputs:{Bias[linear_0.b_0:float[10]({})(Place(cpu))], Input[gaussian_0.tmp_0:float[3, 4]({})(Place(cpu))], W[linear_0.w_0:float[4, 10]({})(Place(cpu))]}, outputs:{Out[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:07:13.828205  6969 operator.cc:2295] op type:abs, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.828230  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: abs; inputs: X; attributes: ; outputs: Out
1884: I0815 04:07:13.828286  6969 operator.cc:834] Place(cpu) Op(abs), inputs:{X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[abs_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:07:13.828326  6969 operator.cc:2295] op type:multinomial, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.828352  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: multinomial; inputs: X; attributes: num_samples, replacement; outputs: Out
1884: I0815 04:07:13.828416  6969 operator.cc:834] Place(cpu) Op(multinomial), inputs:{X[abs_0.tmp_0:float[3, 10]({})(Place(cpu))], num_samples[assign_0.tmp_0:int64_t[1]({})(Place(cpu))]}, outputs:{Out[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:07:13.828460  6969 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.828477  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:07:13.828531  6969 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[linear_0.tmp_1:float[3, 10]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_0.tmp_0:float[3, 10]({})(Place(cpu))]}.
1884: I0815 04:07:13.828562  6969 operator.cc:2295] op type:scale, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1884: I0815 04:07:13.828576  6969 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: scale; inputs: X; attributes: scale, bias, bias_after_scale; outputs: Out
1884: I0815 04:07:13.828610  6969 operator.cc:834] Place(cpu) Op(scale), inputs:{BiasTensor[], ScaleTensor[], X[multinomial_0.tmp_0:int64_t[3, 3]({})(Place(cpu))]}, outputs:{Out[save_infer_model/scale_1.tmp_0:int64_t[3, 3]({})(Place(cpu))]}.
1884: I0815 04:07:13.828629  6969 helper.h:461] after run : [gpu current allocated memory: 0.000732422MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.828658  6969 helper.h:475] after run : [cpu current allocated memory: 5.34283MB], [cpu current reserved memory: 5.34283MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:07:13.828683  6969 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:07:13.829118  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.829128  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> builtin.tensor<2xi64>
1884:     (%1) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> builtin.tensor<1xf32>
1884:     (%2) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> builtin.tensor<1xf32>
1884:     (%3) = "pd_op.uniform" (%0, %1, %2) {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (builtin.tensor<2xi64>, builtin.tensor<1xf32>, builtin.tensor<1xf32>) -> builtin.tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (builtin.tensor<4x10xf32>) -> 
1884:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (builtin.tensor<10xf32>) -> 
1884:     (%5) = "pd_op.full" () {dtype:(pd_op.DataType)float32,persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> builtin.tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (builtin.tensor<f32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: I0815 04:07:13.870122  6969 pir_interpreter.cc:161] PirInterpreter(): 0x615ce020 on Place(gpu:0)
1884: I0815 04:07:13.870152  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_0
1884: I0815 04:07:13.870167  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_1
1884: I0815 04:07:13.870175  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_2
1884: I0815 04:07:13.870184  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_3
1884: I0815 04:07:13.870210  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_4
1884: I0815 04:07:13.870224  6969 scope.cc:202] Create variable 0x615ce0201723694833870143167_inner_var_5
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)4,(Int64)10]} : () -> cpu_tensor<2xi64>
1884:     (%1) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)-0.654654} : () -> cpu_tensor<1xf32>
1884:     (%2) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0.654654} : () -> cpu_tensor<1xf32>
1884:     (%3) = "uniform(phi_kernel)" (%0, %1, %2) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"uniform",op_name:"pd_op.uniform",persistable:[true],place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,stop_gradient:[true]} : (cpu_tensor<2xi64>, cpu_tensor<1xf32>, cpu_tensor<1xf32>) -> gpu_tensor<4x10xf32>
1884:     () = "builtin.set_parameter" (%3) {parameter_name:"linear_1.w_0"} : (gpu_tensor<4x10xf32>) -> 
1884:     (%4) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[10],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<10xf32>
1884:     () = "builtin.set_parameter" (%4) {parameter_name:"linear_1.b_0"} : (gpu_tensor<10xf32>) -> 
1884:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true],value:(Double)0.001} : () -> gpu_tensor<f32>
1884:     () = "builtin.shadow_output" (%5) {output_name:"learning_rate_1"} : (gpu_tensor<f32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: 1: ( 1 )  = pd_op.full
1884: 2: ( 2 )  = pd_op.full
1884: 3: ( 3 )  = pd_op.uniform ( 1 )  ( 2 )  ( 0 ) 
1884: 4: ( 4 )  = pd_op.full
1884: 5: ( 5 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> 0x615ce0201723694833870143167_inner_var_0 -> 0x61a99a60
1884: 1 -> 0x615ce0201723694833870143167_inner_var_1 -> 0x623b24c0
1884: 2 -> 0x615ce0201723694833870143167_inner_var_2 -> 0x61a99080
1884: 3 -> linear_1.w_0 -> 0x61a97770
1884: 4 -> linear_1.b_0 -> 0x615e86d0
1884: 5 -> learning_rate_1 -> 0x5fb2d9a0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 3 
1884: 1 -> 3 
1884: 2 -> 3 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->1[pd_op.full]->2[pd_op.full]->4[pd_op.full]->5[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 
1884: 2 downstreams: 3[pd_op.uniform]->
1884: 3 downstreams: 
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:07:13.870963  7037 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.870986  7038 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.871022  7039 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.871052  7040 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.871104  7041 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.871104  7040 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871109  7038 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871106  7039 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871138  7038 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:07:13.871138  7041 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871145  7040 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full type:kCpuSync runs on HostTasks_thread_3
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.871153  7039 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x615ce0201723694833870143167_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.871183  7041 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.871214  7041 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};]}.
1884: I0815 04:07:13.871237  7041 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871253  7041 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.871263  7041 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:07:13.871277  7041 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.uniform), inputs:{0x615ce0201723694833870143167_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615ce0201723694833870143167_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615ce0201723694833870143167_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.871335  7041 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.uniform type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.uniform), inputs:{0x615ce0201723694833870143167_inner_var_1:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615ce0201723694833870143167_inner_var_2:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x615ce0201723694833870143167_inner_var_0:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};]}.
1884: I0815 04:07:13.871387  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x615ce190) got event_name: TaskCompletion
1884: IR before lowering = {
1884:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"learning_rate_1",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> builtin.tensor<f32>
1884:     (%1) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%3) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     (%4) = "pd_op.gaussian" (%3) {dtype:(pd_op.DataType)float32,mean:(Float)0,place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (builtin.tensor<2xi64>) -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %2) {stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %1) {stop_gradient:[false],struct_name:"/Linear_1/"} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     (%9) = "pd_op.assign_value_" (%8) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     (%10) = "pd_op.multinomial" (%7, %9) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%11) = "pd_op.cast" (%10) {dtype:(pd_op.DataType)float32,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xf32>
1884:     (%12) = "pd_op.mean" (%11) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<3x-1xf32>) -> builtin.tensor<f32>
1884:     (%13) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     (%14) = "pd_op.full_like" (%12, %13) {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f32>, builtin.tensor<1xf32>) -> builtin.tensor<f32>
1884:     (%15) = "pd_op.fetch" (%6) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%16) = "pd_op.fetch" (%10) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[false]} : (builtin.tensor<3x-1xi64>) -> builtin.tensor<3x-1xi64>
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add(phi_kernel)" (%6, %2) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: I0815 04:07:13.873286  6969 pir_interpreter.cc:161] PirInterpreter(): 0x5f9342f0 on Place(gpu:0)
1884: I0815 04:07:13.873327  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_1
1884: I0815 04:07:13.873342  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_4
1884: I0815 04:07:13.873350  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_5
1884: I0815 04:07:13.873358  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_6
1884: I0815 04:07:13.873379  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_7
1884: I0815 04:07:13.873390  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_8
1884: I0815 04:07:13.873399  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_9
1884: I0815 04:07:13.873426  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_10
1884: I0815 04:07:13.873437  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_11
1884: I0815 04:07:13.873445  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_12
1884: I0815 04:07:13.873454  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_13
1884: I0815 04:07:13.873462  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_14
1884: I0815 04:07:13.873471  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_15
1884: I0815 04:07:13.873478  6969 scope.cc:202] Create variable fetch0@fetch
1884: I0815 04:07:13.873489  6969 scope.cc:202] Create variable 0x5f9342f01723694833873314834_inner_var_17
1884: I0815 04:07:13.873497  6969 scope.cc:202] Create variable fetch1@fetch
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"learning_rate_1",op_name:"pd_op.data",persistable:[true],place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[],stop_gradient:[true]} : () -> undefined_tensor<f32>
1884:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<f32>) -> gpu_tensor<f32>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> gpu_tensor<4x10xf32>
1884:     (%4) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     (%5) = "gaussian(phi_kernel)" (%4) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"gaussian",mean:(Float)0,op_name:"pd_op.gaussian",place:(pd_op.Place)Place(undefined:0),seed:(Int32)0,std:(Float)1,stop_gradient:[false]} : (cpu_tensor<2xi64>) -> gpu_tensor<3x4xf32>
1884:     (%6) = "matmul(phi_kernel)" (%5, %3) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],struct_name:"/Linear_1/",transpose_x:false,transpose_y:false} : (gpu_tensor<3x4xf32>, gpu_tensor<4x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%7) = "add_(phi_kernel)" (%6, %2) {is_inplace:true,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false],struct_name:"/Linear_1/"} : (gpu_tensor<3x10xf32>, gpu_tensor<10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%8) = "abs(phi_kernel)" (%7) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (gpu_tensor<3x10xf32>) -> gpu_tensor<3x10xf32>
1884:     (%9) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> gpu_tensor<1xi64>
1884:     (%10) = "assign_value_(phi_kernel)" (%9) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (gpu_tensor<1xi64>) -> gpu_tensor<1xi64>
1884:     (%11) = "multinomial(phi_kernel)" (%8, %10) {kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (gpu_tensor<3x10xf32>, gpu_tensor<1xi64>) -> gpu_tensor<3x-1xi64>
1884:     (%12) = "cast(phi_kernel)" (%11) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:NCHW|dtype:int64>,kernel_name:"cast",op_name:"pd_op.cast",stop_gradient:[false]} : (gpu_tensor<3x-1xi64>) -> gpu_tensor<3x-1xf32>
1884:     (%13) = "mean(phi_kernel)" (%12) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<3x-1xf32>) -> gpu_tensor<f32>
1884:     (%14) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     (%15) = "full_like(phi_kernel)" (%13, %14) {dtype:(pd_op.DataType)float32,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f32>, cpu_tensor<1xf32>) -> gpu_tensor<f32>
1884:     (%16) = "memcpy_d2h(phi_kernel)" (%7) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%18) = "memcpy_d2h(phi_kernel)" (%11) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<3x-1xi64>) -> cpu_tensor<3x-1xi64>
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1884: 1: ( 4 )  = pd_op.full_int_array
1884: 2: ( 5 )  = pd_op.gaussian ( 4 ) 
1884: 3: ( 6 )  = pd_op.matmul ( 3 )  ( 5 ) 
1884: 4: ( 6 ) ( 7 )  = pd_op.add_ ( 2 )  ( 6 ) 
1884: 5: ( 8 )  = pd_op.abs ( 7 ) 
1884: 6: ( 9 )  = pd_op.full
1884: 7: ( 9 )  = pd_op.assign_value_ ( 9 ) 
1884: 8: ( 10 )  = pd_op.multinomial ( 9 )  ( 8 ) 
1884: 9: ( 11 )  = pd_op.cast ( 10 ) 
1884: 10: ( 12 )  = pd_op.mean ( 11 ) 
1884: 11: ( 13 )  = pd_op.full
1884: 12: ( 14 )  = pd_op.full_like ( 13 )  ( 12 ) 
1884: 13: ( 15 )  = pd_op.memcpy_d2h ( 7 ) 
1884: 14: ( 16 )  = pd_op.fetch ( 15 ) 
1884: 15: ( 17 )  = pd_op.memcpy_d2h ( 10 ) 
1884: 16: ( 18 )  = pd_op.fetch ( 17 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> learning_rate_1 -> 0x5fb2d9a0
1884: 1 -> 0x5f9342f01723694833873314834_inner_var_1 -> 0x624e46b0
1884: 2 -> linear_1.b_0 -> 0x615e86d0
1884: 3 -> linear_1.w_0 -> 0x61a97770
1884: 4 -> 0x5f9342f01723694833873314834_inner_var_4 -> 0x61623eb0
1884: 5 -> 0x5f9342f01723694833873314834_inner_var_5 -> 0x61fc4910
1884: 6 -> 0x5f9342f01723694833873314834_inner_var_6 -> 0x61623460
1884: 7 -> 0x5f9342f01723694833873314834_inner_var_7 -> 0x5f941ff0
1884: 8 -> 0x5f9342f01723694833873314834_inner_var_8 -> 0x5ec43770
1884: 9 -> 0x5f9342f01723694833873314834_inner_var_9 -> 0x616da180
1884: 10 -> 0x5f9342f01723694833873314834_inner_var_10 -> 0x5f9069a0
1884: 11 -> 0x5f9342f01723694833873314834_inner_var_11 -> 0x46f0320
1884: 12 -> 0x5f9342f01723694833873314834_inner_var_12 -> 0x61fddd50
1884: 13 -> 0x5f9342f01723694833873314834_inner_var_13 -> 0x61623ba0
1884: 14 -> 0x5f9342f01723694833873314834_inner_var_14 -> 0x6203de20
1884: 15 -> 0x5f9342f01723694833873314834_inner_var_15 -> 0x5fb29630
1884: 16 -> fetch0@fetch -> 0x615c9dd0
1884: 17 -> 0x5f9342f01723694833873314834_inner_var_17 -> 0x61a994c0
1884: 18 -> fetch1@fetch -> 0x615cf180
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 1 -> 2 
1884: 2 -> 3 
1884: 3 -> 4 
1884: 4 -> 5 13 
1884: 5 -> 8 
1884: 6 -> 7 
1884: 7 -> 8 
1884: 8 -> 9 15 
1884: 9 -> 10 
1884: 10 -> 12 
1884: 11 -> 12 
1884: 13 -> 14 
1884: 15 -> 16 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.shadow_feed]->1[pd_op.full_int_array]->6[pd_op.full]->11[pd_op.full]->
1884: 0 downstreams: 
1884: 1 downstreams: 2[pd_op.gaussian]->
1884: 2 downstreams: 3[pd_op.matmul]->
1884: 3 downstreams: 4[pd_op.add_]->
1884: 4 downstreams: 5[pd_op.abs]->13[pd_op.memcpy_d2h]->
1884: 5 downstreams: 
1884: 6 downstreams: 7[pd_op.assign_value_]->
1884: 7 downstreams: 8[pd_op.multinomial]->
1884: 8 downstreams: 9[pd_op.cast]->15[pd_op.memcpy_d2h]->
1884: 9 downstreams: 10[pd_op.mean]->
1884: 10 downstreams: 
1884: 11 downstreams: 12[pd_op.full_like]->
1884: 12 downstreams: 
1884: 13 downstreams: 14[pd_op.fetch]->
1884: 14 downstreams: 
1884: 15 downstreams: 16[pd_op.fetch]->
1884: 16 downstreams: 
1884: I0815 04:07:13.875092  7042 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.875209  7043 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.875227  7044 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.875293  7045 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.875310  7044 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875322  7043 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_13:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875351  7044 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:07:13.875365  7046 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1884: I0815 04:07:13.875380  7043 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:11 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.875401  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875424  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{learning_rate_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_1:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:07:13.875447  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875476  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.875507  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:6 name:pd_op.full type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full), inputs:{}, outputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:07:13.875522  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:07:13.875566  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:7 name:pd_op.assign_value_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.assign_value_), inputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};]}.
1884: I0815 04:07:13.875582  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x5f9342f01723694833873314834_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875617  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.gaussian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.gaussian), inputs:{0x5f9342f01723694833873314834_inner_var_4:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}.
1884: I0815 04:07:13.875639  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x5f9342f01723694833873314834_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875669  7046 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:07:13.875699  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.matmul type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(gpu:0);dim=4, 10;lod={};], 0x5f9342f01723694833873314834_inner_var_5:[dtype=float;place=Place(gpu:0);dim=3, 4;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.875725  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x5f9342f01723694833873314834_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x5f9342f01723694833873314834_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875761  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.875775  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.add_ type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(gpu:0);dim=10;lod={};], 0x5f9342f01723694833873314834_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_6:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};, 0x5f9342f01723694833873314834_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.875804  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.abs), inputs:{0x5f9342f01723694833873314834_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875823  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.875828  7043 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9342f01723694833873314834_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_15:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875833  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.abs type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.abs), inputs:{0x5f9342f01723694833873314834_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.875849  7043 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.875849  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x5f9342f01723694833873314834_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875870  7046 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.875913  7043 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:13 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9342f01723694833873314834_inner_var_7:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.875943  7043 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9342f01723694833873314834_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.875955  7046 tensor_utils.cc:57] TensorCopy 1 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.875962  7043 tensor_utils.cc:57] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.875974  7043 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:14 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9342f01723694833873314834_inner_var_15:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.876003  7046 tensor_utils.cc:57] TensorCopy 3, 10 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.876060  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.876076  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:8 name:pd_op.multinomial type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.multinomial), inputs:{0x5f9342f01723694833873314834_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=1;lod={};], 0x5f9342f01723694833873314834_inner_var_8:[dtype=float;place=Place(gpu:0);dim=3, 10;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.876107  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.cast), inputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_11:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.876116  7043 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_17:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.876130  7043 tensor_utils.cc:57] TensorCopy 3, 3 from Place(gpu:0) to Place(cpu)
1884: I0815 04:07:13.876132  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=9, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.876155  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:9 name:pd_op.cast type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.cast), inputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.876168  7043 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:15 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1884: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x5f9342f01723694833873314834_inner_var_10:[dtype=int64_t;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.876173  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x5f9342f01723694833873314834_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.876185  7043 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9342f01723694833873314834_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.876197  7043 tensor_utils.cc:57] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.876211  7043 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:16 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.fetch), inputs:{0x5f9342f01723694833873314834_inner_var_17:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch1@fetch:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.876233  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:10 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x5f9342f01723694833873314834_inner_var_11:[dtype=float;place=Place(gpu:0);dim=3, 3;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:07:13.876253  7046 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x5f9342f01723694833873314834_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5f9342f01723694833873314834_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.876278  7046 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1884: I0815 04:07:13.876288  7046 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:12 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1884: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x5f9342f01723694833873314834_inner_var_13:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x5f9342f01723694833873314834_inner_var_12:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x5f9342f01723694833873314834_inner_var_14:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1884: I0815 04:07:13.876329  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x5f934460) got event_name: TaskCompletion
1884: I0815 04:07:13.876351  6969 tensor_util.cc:48] TensorCopy 3, 10 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.876377  6969 tensor_util.cc:48] TensorCopy 3, 3 from Place(cpu) to Place(cpu)
1884: I0815 04:07:13.881558  6969 analysis_predictor.cc:2412] create AnalysisPredictor
1884: I0815 04:07:13.881603  6969 analysis_predictor.cc:433] Predictor::init()
1884: I0815 04:07:13.882230  6969 scope.cc:202] Create variable linear_1.b_0
1884: I0815 04:07:13.882277  6969 scope.cc:202] Create variable linear_1.w_0
1884: [1m[35m--- Running PIR pass [delete_quant_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [delete_weight_dequant_linear_op_pass][0m
1884: [1m[35m--- Running PIR pass [common_subexpression_elimination_pass][0m
1884: I0815 04:07:13.882687  6969 print_statistics.cc:50] --- detected [1] subgraphs!
1884: [1m[35m--- Running PIR pass [constant_folding_pass][0m
1884: IR before lowering = {
1884:     (%0) = "pd_op.full_int_array" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> builtin.tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338827379210"} : (builtin.tensor<2xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338827379210"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: I0815 04:07:13.882858  6969 pir_interpreter.cc:161] PirInterpreter(): 0x623cfae0 on Place(cpu)
1884: I0815 04:07:13.882877  6969 scope.cc:202] Create variable 0x623cfae01723694833882871690_inner_var_0
1884: I0815 04:07:13.882900  6969 pir_interpreter.cc:1539] New Executor is Running ...
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full_int_array(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full_int_array",op_name:"pd_op.full_int_array",place:(pd_op.Place)Place(cpu),stop_gradient:[true],value:[(Int64)3,(Int64)4]} : () -> cpu_tensor<2xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338827379210"} : (cpu_tensor<2xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full_int_array
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236948338827379210 -> 0x61fc0c70
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full_int_array]->
1884: 0 downstreams: 
1884: I0815 04:07:13.883029  6969 pir_interpreter.cc:1566] pir interpreter is running by multi-thread mode ...
1884: I0815 04:07:13.883141  7047 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.883284  7048 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.883303  7049 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.883387  7049 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: Before: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236948338827379210:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.883433  7050 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.883452  7049 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full_int_array type:kCpuSync runs on HostTasks_thread_1
1884: After: Place(cpu) Op(pd_op.full_int_array), inputs:{}, outputs:{constant_folding@_17236948338827379210:[dtype=int64_t;place=Place(cpu);dim=2;lod={};]}.
1884: I0815 04:07:13.883466  7051 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.883474  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x623cfc50) got event_name: TaskCompletion
1884: I0815 04:07:13.883726  7049 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16117572429540044651 to 13385905633866668040 , after update, data is {current : 212, peak : 268}.
1884: I0815 04:07:13.883735  7049 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 16117572429540044651 to 13385905633866668040 , after update, data is {current : 212, peak : 268}.
1884: I0815 04:07:13.883790  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.883798  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338838637251"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338838637251"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:07:13.883997  6969 pir_interpreter.cc:161] PirInterpreter(): 0x623cfae0 on Place(cpu)
1884: I0815 04:07:13.884016  6969 scope.cc:202] Create variable 0x623cfae01723694833884010617_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)int64,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)0} : () -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338838637251"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236948338838637251 -> 0x61960630
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:07:13.884230  7052 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.884289  7053 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.884325  7054 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.884351  7055 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.884375  7056 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.884371  7055 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236948338838637251:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.884419  7055 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236948338838637251:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.884442  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x623cfc50) got event_name: TaskCompletion
1884: I0815 04:07:13.884609  7055 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 6870240717934555681 to 13385905633866668040 , after update, data is {current : 220, peak : 268}.
1884: I0815 04:07:13.884617  7055 thread_data_registry.h:135] Add data {current : 8, peak : 8} from thread 6870240717934555681 to 13385905633866668040 , after update, data is {current : 220, peak : 268}.
1884: I0815 04:07:13.884717  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.884724  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236948338838637251",stop_gradient:[false]} : () -> builtin.tensor<1xi64>
1884:     (%1) = "pd_op.assign_value_" (%0) {dtype:(pd_op.DataType)int64,place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (builtin.tensor<1xi64>) -> builtin.tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236948338847978502"} : (builtin.tensor<1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236948338838637251",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236948338847978502"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: I0815 04:07:13.884944  6969 pir_interpreter.cc:161] PirInterpreter(): 0x623cfae0 on Place(cpu)
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.parameter" () {parameter_name:"constant_folding@_17236948338838637251",stop_gradient:[false]} : () -> cpu_tensor<1xi64>
1884:     (%1) = "assign_value_(phi_kernel)" (%0) {dtype:(pd_op.DataType)int64,is_inplace:true,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:int64>,kernel_name:"assign_value",op_name:"pd_op.assign_value_",place:(pd_op.Place)Place(undefined:0),shape:[(Int32)1],stop_gradient:[true],values:[(Double)3]} : (cpu_tensor<1xi64>) -> cpu_tensor<1xi64>
1884:     () = "builtin.shadow_output" (%1) {output_name:"constant_folding@_17236948338847978502"} : (cpu_tensor<1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.assign_value_ ( 0 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236948338847978502 -> 0x61960630
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.assign_value_]->
1884: 0 downstreams: 
1884: I0815 04:07:13.885196  7057 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.885263  7058 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.885285  7059 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.885321  7060 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.885345  7061 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.885342  7060 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.885366  7060 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.assign_value_ type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.assign_value_), inputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}, outputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.885388  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x623cfc50) got event_name: TaskCompletion
1884: I0815 04:07:13.885651  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.885658  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: IR before lowering = {
1884:     (%0) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338857343593"} : (builtin.tensor<1xf32>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338857343593"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: I0815 04:07:13.885860  6969 pir_interpreter.cc:161] PirInterpreter(): 0x623cfae0 on Place(cpu)
1884: I0815 04:07:13.885879  6969 scope.cc:202] Create variable 0x623cfae01723694833885873549_inner_var_0
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1884:     () = "builtin.shadow_output" (%0) {output_name:"constant_folding@_17236948338857343593"} : (cpu_tensor<1xf32>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 0 )  = pd_op.full
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236948338857343593 -> 0x5f901cc0
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.full]->
1884: 0 downstreams: 
1884: I0815 04:07:13.886067  7062 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.886121  7063 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1884: I0815 04:07:13.886138  7064 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1884: I0815 04:07:13.886173  7065 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1884: I0815 04:07:13.886193  7066 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1884: I0815 04:07:13.886191  7065 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236948338857343593:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.886224  7065 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.full type:kCpuSync runs on HostTasks_thread_2
1884: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{constant_folding@_17236948338857343593:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1884: I0815 04:07:13.886247  6969 pir_interpreter.cc:1766] main_thread_blocker_(0x623cfc50) got event_name: TaskCompletion
1884: I0815 04:07:13.886415  7065 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6870240717934555681 to 13385905633866668040 , after update, data is {current : 224, peak : 268}.
1884: I0815 04:07:13.886425  7065 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 6870240717934555681 to 13385905633866668040 , after update, data is {current : 224, peak : 268}.
1884: I0815 04:07:13.886515  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.886523  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:13.886584  6969 print_statistics.cc:44] --- detected [4, 15] subgraphs!
1884: [1m[35m--- Running PIR pass [dead_code_elimination_pass][0m
1884: I0815 04:07:13.886642  6969 print_statistics.cc:50] --- detected [2] subgraphs!
1884: [1m[35m--- Running PIR pass [replace_fetch_with_shadow_output_pass][0m
1884: I0815 04:07:13.886679  6969 print_statistics.cc:50] --- detected [2] subgraphs!
1884: IR before lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338857343593"} : () -> builtin.tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338847978502"} : () -> builtin.tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> builtin.tensor<4x10xf32>
1884:     (%4) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"feed_name_0",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> builtin.tensor<3x4xf32>
1884:     (%5) = "pd_op.matmul" (%4, %3) {stop_gradient:[false],transpose_x:false,transpose_y:false} : (builtin.tensor<3x4xf32>, builtin.tensor<4x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%6) = "pd_op.add" (%5, %2) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%7) = "pd_op.abs" (%6) {stop_gradient:[false]} : (builtin.tensor<3x10xf32>) -> builtin.tensor<3x10xf32>
1884:     (%8) = "pd_op.multinomial" (%7, %1) {replacement:false,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xi64>) -> builtin.tensor<3x-1xi64>
1884:     (%9) = "pd_op.scale" (%6, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x10xf32>, builtin.tensor<1xf32>) -> builtin.tensor<3x10xf32>
1884:     (%10) = "pd_op.scale" (%8, %0) {bias:(Float)0,bias_after_scale:true,stop_gradient:[false]} : (builtin.tensor<3x-1xi64>, builtin.tensor<1xf32>) -> builtin.tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (builtin.tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (builtin.tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: IR after lowering = {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338857343593"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338847978502"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add(phi_kernel)" (%5, %2) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: [1m[35m--- Running PIR pass [remove_shadow_feed_pass][0m
1884: [1m[35m--- Running PIR pass [inplace_pass][0m
1884: I0815 04:07:13.887317  6969 print_statistics.cc:50] --- detected [2] subgraphs!
1884: I0815 04:07:13.887336  6969 analysis_predictor.cc:1019] ======= pir optimization completed =======
1884: I0815 04:07:13.887367  6969 pir_interpreter.cc:161] PirInterpreter(): 0x623cfae0 on Place(cpu)
1884: I0815 04:07:13.887398  6969 scope.cc:202] Create variable feed_name_0
1884: I0815 04:07:13.887411  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_5
1884: I0815 04:07:13.887432  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_6
1884: I0815 04:07:13.887444  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_7
1884: I0815 04:07:13.887452  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_8
1884: I0815 04:07:13.887471  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_9
1884: I0815 04:07:13.887483  6969 scope.cc:202] Create variable 0x623cfae01723694833887381751_inner_var_10
1884: I0815 04:07:13.887506  6969 helper.h:461] Init predictor : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.887524  6969 helper.h:475] Init predictor : [cpu current allocated memory: 5.34267MB], [cpu current reserved memory: 5.34267MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:07:13.887627  6969 helper.h:461] before run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.887641  6969 helper.h:475] before run : [cpu current allocated memory: 5.34272MB], [cpu current reserved memory: 5.34272MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: ======================== The network executed by pir interpreter ========================
1884: {
1884:     (%0) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338857343593"} : () -> cpu_tensor<1xf32>
1884:     (%1) = "builtin.constant" () {persistable:[true],value:"constant_folding@_17236948338847978502"} : () -> cpu_tensor<1xi64>
1884:     (%2) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.b_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<10xf32>
1884:     (%3) = "builtin.parameter" () {is_distributed:[false],is_parameter:[true],need_clip:[true],parameter_name:"linear_1.w_0",persistable:[true],stop_gradient:[false],trainable:[true]} : () -> cpu_tensor<4x10xf32>
1884:     (%4) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"feed_name_0",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[3,4],stop_gradient:[true]} : () -> undefined_tensor<3x4xf32>
1884:     (%5) = "matmul(phi_kernel)" (%4, %3) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"matmul",op_name:"pd_op.matmul",stop_gradient:[false],transpose_x:false,transpose_y:false} : (undefined_tensor<3x4xf32>, cpu_tensor<4x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%6) = "add_(phi_kernel)" (%5, %2) {is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"add",op_name:"pd_op.add_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%7) = "abs(phi_kernel)" (%6) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"abs",op_name:"pd_op.abs",stop_gradient:[false]} : (cpu_tensor<3x10xf32>) -> cpu_tensor<3x10xf32>
1884:     (%8) = "multinomial(phi_kernel)" (%7, %1) {kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"multinomial",op_name:"pd_op.multinomial",replacement:false,stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xi64>) -> cpu_tensor<3x-1xi64>
1884:     (%9) = "scale_(phi_kernel)" (%6, %0) {bias:(Float)0,bias_after_scale:true,is_inplace:true,kernel_key:<backend:CPU|layout:NCHW|dtype:float32>,kernel_name:"scale",op_name:"pd_op.scale_",stop_gradient:[false]} : (cpu_tensor<3x10xf32>, cpu_tensor<1xf32>) -> cpu_tensor<3x10xf32>
1884:     (%10) = "scale(phi_kernel)" (%8, %0) {bias:(Float)0,bias_after_scale:true,kernel_key:<backend:CPU|layout:NCHW|dtype:int64>,kernel_name:"scale",op_name:"pd_op.scale",stop_gradient:[false]} : (cpu_tensor<3x-1xi64>, cpu_tensor<1xf32>) -> cpu_tensor<3x-1xi64>
1884:     () = "builtin.shadow_output" (%9) {output_name:"fetch_name_0"} : (cpu_tensor<3x10xf32>) -> 
1884:     () = "builtin.shadow_output" (%10) {output_name:"fetch_name_1"} : (cpu_tensor<3x-1xi64>) -> 
1884: }
1884: 
1884: ======================== The instruction executed by pir interpreter ========================
1884: {outputs} =  instruction_name[idx] ({inputs})
1884: 0: ( 5 )  = pd_op.matmul ( 3 )  ( 4 ) 
1884: 1: ( 5 ) ( 6 )  = pd_op.add_ ( 2 )  ( 5 ) 
1884: 2: ( 7 )  = pd_op.abs ( 6 ) 
1884: 3: ( 8 )  = pd_op.multinomial ( 1 )  ( 7 ) 
1884: 4: ( 6 ) ( 9 )  = pd_op.scale_ ( 0 )  ( 6 ) 
1884: 5: ( 10 )  = pd_op.scale ( 0 )  ( 8 ) 
1884: ---------------------------var_id -> var_name -> variable*---------------------------
1884: 0 -> constant_folding@_17236948338857343593 -> 0x5f901cc0
1884: 1 -> constant_folding@_17236948338847978502 -> 0x61960630
1884: 2 -> linear_1.b_0 -> 0x623ced70
1884: 3 -> linear_1.w_0 -> 0x620be8c0
1884: 4 -> feed_name_0 -> 0x5f901bc0
1884: 5 -> 0x623cfae01723694833887381751_inner_var_5 -> 0x62127280
1884: 6 -> 0x623cfae01723694833887381751_inner_var_6 -> 0x615e09d0
1884: 7 -> 0x623cfae01723694833887381751_inner_var_7 -> 0x619855b0
1884: 8 -> 0x623cfae01723694833887381751_inner_var_8 -> 0x5fb3e850
1884: 9 -> fetch_name_0 -> 0x191d9590
1884: 10 -> fetch_name_1 -> 0x616fb960
1884: 
1884: 
1884: ======================= The dependency of all instruction ========================
1884: id -> down_stream_id
1884: 0 -> 1 
1884: 1 -> 2 
1884: 2 -> 3 4 
1884: 3 -> 5 
1884: 
1884: 
1884: ======================== pir interpreter trace order ========================
1884: 
1884: Leaf nodes: 0[pd_op.matmul]->
1884: 0 downstreams: 1[pd_op.add_]->
1884: 1 downstreams: 2[pd_op.abs]->
1884: 2 downstreams: 3[pd_op.multinomial]->4[pd_op.scale_]->
1884: 3 downstreams: 5[pd_op.scale]->
1884: 4 downstreams: 
1884: 5 downstreams: 
1884: I0815 04:07:13.888180  6969 pir_interpreter.cc:1563] pir interpreter is running by trace mode ...
1884: I0815 04:07:13.888236  7067 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1884: I0815 04:07:13.888234  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888283  6969 matmul_kernel_impl.h:374] MatMul's case 8
1884: I0815 04:07:13.888312  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:0 name:pd_op.matmul type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.matmul), inputs:{linear_1.w_0:[dtype=float;place=Place(cpu);dim=4, 10;lod={};], feed_name_0:[dtype=float;place=Place(cpu);dim=3, 4;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.888345  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x623cfae01723694833887381751_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x623cfae01723694833887381751_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888382  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:1 name:pd_op.add_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.add_), inputs:{linear_1.b_0:[dtype=float;place=Place(cpu);dim=10;lod={};], 0x623cfae01723694833887381751_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_5:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, 0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.888414  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.abs), inputs:{0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888434  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:2 name:pd_op.abs type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.abs), inputs:{0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.888450  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888479  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:3 name:pd_op.multinomial type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.multinomial), inputs:{constant_folding@_17236948338847978502:[dtype=int64_t;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_7:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.888504  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236948338857343593:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888532  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:4 name:pd_op.scale_ type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale_), inputs:{constant_folding@_17236948338857343593:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}, outputs:{0x623cfae01723694833887381751_inner_var_6:[dtype=float;place=Place(cpu);dim=3, 10;lod={};, fetch_name_0:[dtype=float;place=Place(cpu);dim=3, 10;lod={};]}.
1884: I0815 04:07:13.888559  6969 pir_interpreter.cc:1876] 
1884: begin: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: Before: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236948338857343593:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=;place=;dim=;lod={};]}.
1884: I0815 04:07:13.888587  6969 pir_interpreter.cc:1916] 
1884: done: RunInstructionBase OP id:5 name:pd_op.scale type:kCpuSync runs on MainThread
1884: After: Place(cpu) Op(pd_op.scale), inputs:{constant_folding@_17236948338857343593:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x623cfae01723694833887381751_inner_var_8:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}, outputs:{fetch_name_1:[dtype=int64_t;place=Place(cpu);dim=3, 3;lod={};]}.
1884: I0815 04:07:13.888615  6969 helper.h:461] after run : [gpu current allocated memory: 0.00146484MB], [gpu current reserved memory: 3.07198MB], [gpu peak allocated memory: 2.28979MB], [gpu peak reserved memory: 3.07961MB]
1884: I0815 04:07:13.888635  6969 helper.h:475] after run : [cpu current allocated memory: 5.3429MB], [cpu current reserved memory: 5.3429MB], [cpu peak allocated memory: 7.63016MB], [cpu peak reserved memory: 9.91898MB]
1884: I0815 04:07:13.888656  6969 reset_tensor_array.cc:45] Collect 0 arrays
1884: I0815 04:07:13.888774  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.888782  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:13.888830  7067 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2406058261455497675 to 13385905633866668040 , after update, data is {current : 32, peak : 268}.
1884: I0815 04:07:13.888839  7067 thread_data_registry.h:135] Add data {current : -192, peak : 0} from thread 2406058261455497675 to 13385905633866668040 , after update, data is {current : 32, peak : 268}.
1884: I0815 04:07:13.888871  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:13.888877  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: test_multinomial_op failed
1884:  ......sss......EEFF..
1884: ======================================================================
1884: ERROR: test_check_output (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 56, in setUp
1884:     self.init_data()
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 62, in init_data
1884:     self.outputs = {"Out": paddle.multinomial(paddle.vision.transforms.ToTensor(self.input_np))}
1884:   File "/home/code/Paddle/build/python/paddle/tensor/random.py", line 496, in multinomial
1884:     check_variable_and_dtype(
1884:   File "/home/code/Paddle/build/python/paddle/base/data_feeder.py", line 172, in check_variable_and_dtype
1884:     check_type(input, input_name, (Variable, Value), op_name, extra_message)
1884:   File "/home/code/Paddle/build/python/paddle/base/data_feeder.py", line 203, in check_type
1884:     raise TypeError(
1884: TypeError: The type of 'x' in multinomial must be (<class 'paddle.base.framework.Variable'>, <class 'paddle.base.libpaddle.pir.Value'>), but received <class 'paddle.vision.transforms.transforms.ToTensor'>. 
1884: 
1884: ======================================================================
1884: ERROR: tearDownClass (test_multinomial_op.TestMultinomialOp)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 486, in tearDownClass
1884:     raise AssertionError(
1884: AssertionError: This test do not have op_type in class attrs, please set self.__class__.op_type=the_real_op_type manually.
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp2)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 210492 / 300000 (70.2%)
1884: Max absolute difference: 3
1884: Max relative difference: inf
1884:  x: array([[2, 3, 1, ..., 3, 2, 2],
1884:        [2, 3, 2, ..., 2, 1, 2],
1884:        [2, 0, 3, ..., 1, 3, 0]], dtype=int64)
1884:  y: array([[0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0],
1884:        [0, 0, 0, ..., 0, 0, 0]])
1884: 
1884: ======================================================================
1884: FAIL: test_check_output (test_multinomial_op.TestMultinomialOp3)
1884: ----------------------------------------------------------------------
1884: Traceback (most recent call last):
1884:   File "/home/code/Paddle/build/test/legacy_test/test_multinomial_op.py", line 67, in test_check_output
1884:     self.check_output(check_pir=True)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1884:     res = self.check_output_with_place(
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2755, in check_output_with_place
1884:     static_checker.check()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2315, in check
1884:     self.compare_outputs_with_expects()
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2305, in compare_outputs_with_expects
1884:     self.compare_single_output_with_expect(out_name, expect)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2283, in compare_single_output_with_expect
1884:     self._compare_numpy(name, actual_np, expect_np)
1884:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2246, in _compare_numpy
1884:     np.testing.assert_allclose(
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 1504, in assert_allclose
1884:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
1884:   File "/usr/lib/python3.9/contextlib.py", line 79, in inner
1884:     return func(*args, **kwds)
1884:   File "/usr/local/lib/python3.9/dist-packages/numpy/testing/_private/utils.py", line 797, in assert_array_compare
1884:     raise AssertionError(msg)
1884: AssertionError: 
1884: Not equal to tolerance rtol=1e-05, atol=0
1884: Operator (multinomial) Output (Out) has diff at Place(gpu:0) in static checker
1884: Mismatched elements: 100 / 100 (100%)
1884: Max absolute difference: 991
1884: Max relative difference: inf
1884:  x: array([445, 610, 838, 525, 770, 862, 286, 564, 744, 991, 734, 763,  58,
1884:        515, 257, 731, 242, 210, 627,  13, 665, 937, 371, 909, 405, 252,
1884:        615, 863, 760, 446, 413,  60, 146, 617, 337, 574, 195, 857, 780,...
1884:  y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
1884:        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
1884: 
1884: ----------------------------------------------------------------------
1884: Ran 20 tests in 4.632s
1884: 
1884: FAILED (failures=2, errors=2, skipped=3)
1884: 
1884: I0815 04:07:13.890758  6969 mmap_allocator.cc:348] PID: 6969, MemoryMapFdSet: set size - 0
1884: I0815 04:07:13.903326  6969 mmap_allocator.cc:348] PID: 6969, MemoryMapFdSet: set size - 0
1884: I0815 04:07:13.973505  7040 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 9917409711833413600 to 13385905633866668040 , after update, data is {current : 36, peak : 268}.
1884: I0815 04:07:13.973546  7040 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 9917409711833413600 to 13385905633866668040 , after update, data is {current : 36, peak : 268}.
1884: I0815 04:07:13.973556  7039 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17215111427179135468 to 13385905633866668040 , after update, data is {current : 40, peak : 268}.
1884: I0815 04:07:13.973572  7039 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 17215111427179135468 to 13385905633866668040 , after update, data is {current : 40, peak : 268}.
1884: I0815 04:07:13.973592  7038 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14997327967325241272 to 13385905633866668040 , after update, data is {current : 56, peak : 268}.
1884: I0815 04:07:13.973605  7038 thread_data_registry.h:135] Add data {current : 16, peak : 16} from thread 14997327967325241272 to 13385905633866668040 , after update, data is {current : 56, peak : 268}.
1884: I0815 04:07:13.973888  7041 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 8647409854972599470 to 13385905633866668040 , after update, data is {current : 32, peak : 268}.
1884: I0815 04:07:13.973901  7041 thread_data_registry.h:135] Add data {current : -24, peak : 0} from thread 8647409854972599470 to 13385905633866668040 , after update, data is {current : 32, peak : 268}.
1884: I0815 04:07:13.973907  7041 thread_data_registry.h:135] Add data {current : 768, peak : 768} from thread 8647409854972599470 to 13385905633866668040 , after update, data is {current : 256, peak : 768}.
1884: I0815 04:07:13.974200  7043 thread_data_registry.h:135] Add data {current : 256, peak : 768} from thread 13385905633866668040 to 12604922123687041224 , after update, data is {current : 768, peak : 1536}.
1884: I0815 04:07:13.974215  7043 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 13385905633866668040 to 9686218908415379917 , after update, data is {current : 48, peak : 268}.
1884: I0815 04:07:13.974220  7043 thread_data_registry.h:135] Add data {current : 32, peak : 268} from thread 13385905633866668040 to 9686218908415379917 , after update, data is {current : 48, peak : 268}.
1884: I0815 04:07:13.974264  7044 thread_data_registry.h:135] Add data {current : 48, peak : 268} from thread 9686218908415379917 to 12604922123687041224 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:07:13.974274  7044 thread_data_registry.h:135] Add data {current : 48, peak : 268} from thread 9686218908415379917 to 12604922123687041224 , after update, data is {current : 28, peak : 268}.
1884: I0815 04:07:13.974418  7046 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 12604922123687041224 to 3970409447905172152 , after update, data is {current : 5601792, peak : 8000800}.
1884: I0815 04:07:13.974428  7046 thread_data_registry.h:135] Add data {current : 28, peak : 268} from thread 12604922123687041224 to 3970409447905172152 , after update, data is {current : 5601792, peak : 5608800}.
1884: I0815 04:07:13.974433  7046 thread_data_registry.h:135] Add data {current : 768, peak : 1536} from thread 12604922123687041224 to 3970409447905172152 , after update, data is {current : 1536, peak : 2401024}.
1884: I0815 04:07:14.104764  6969 onednn_context.cc:104] Clearing DNNL cache.
1884: I0815 04:07:14.104794  6969 onednn_context.cc:122] Resetting Paddle data layout to NCHW.
1884: I0815 04:07:14.104837  6969 mmap_allocator.cc:348] PID: 6969, MemoryMapFdSet: set size - 0
1/1 Test #1884: test_multinomial_op ..............***Failed   13.54 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  13.72 sec

The following tests FAILED:
	1884 - test_multinomial_op (Failed)
