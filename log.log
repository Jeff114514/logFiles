UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/code/Paddle/build/DartConfiguration.tcl
Test project /home/code/Paddle/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 1901
    Start 1901: test_nanmedian

1901: Test command: /home/cmake-3.18.0-Linux-x86_64/bin/cmake "-E" "env" "PYTHONPATH=/home/code/Paddle/build/python" "/usr/bin/python" "/home/code/Paddle/tools/test_runner.py" "test_nanmedian"
1901: Environment variables: 
1901:  FLAGS_PIR_OPTEST_WHITE_LIST=True
1901: Test timeout computed to be: 10000000
1901: grep: warning: GREP_OPTIONS is deprecated; please use an alias or script
1901: WARNING: Logging before InitGoogleLogging() is written to STDERR
1901: I0815 06:23:55.386636 29935 dynamic_loader.cc:176] Set paddle lib path : /usr/local/lib/python3.9/dist-packages/paddle/libs
1901: I0815 06:23:56.171044 29935 init.cc:100] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=new_executor_use_local_scope,cudnn_exhaustive_search_times,eager_delete_scope,gpugraph_offload_param_stat,check_nan_inf_level,enable_neighbor_list_use_uva,reader_queue_speed_test_mode,graph_embedding_split_infer_mode,paddle_num_threads,gpugraph_sparse_table_storage_mode,cublaslt_exhaustive_search_times,cusolver_dir,log_memory_stats,benchmark,use_cuda_malloc_async_allocator,dygraph_debug,memory_fraction_of_eager_deletion,gpugraph_dedup_pull_push_mode,multiple_of_cupti_buffer_size,prim_backward,use_fast_math,einsum_opt,cusparse_dir,inner_op_parallelism,cudnn_dir,logging_trunc_pir_py_code,host_trace_level,eager_delete_tensor_gb,pir_apply_inplace_pass,fraction_of_cpu_memory_to_use,jit_engine_type,async_trace_count,dataloader_use_file_descriptor,use_xqa_optim,tracer_profile_fname,logging_pir_py_code_dir,gpugraph_enable_print_op_debug,prim_check_ops,static_runtime_data_save_path,cudnn_deterministic,new_executor_sequential_run,prim_skip_dynamic,mkl_dir,gpugraph_load_node_list_into_hbm,gemm_use_half_precision_compute_type,sync_after_alloc,use_autotune,tensor_operants_mode,use_auto_growth_v2,mklml_dir,gpugraph_enable_hbm_table_collision_stat,enable_cse_in_dy2st,enable_all2all_use_fp16,enable_gpu_memory_usage_log,lapack_dir,print_sub_graph_dir,pir_subgraph_saving_dir,gpugraph_enable_segment_merge_grads,executor_log_deps_every_microseconds,deny_cinn_ops,cudnn_batchnorm_spatial_persistent,save_static_runtime_data,prim_enabled,enable_gpu_memory_usage_log_mb,alloc_fill_value,enable_exit_when_partial_worker,conv2d_disable_cudnn,gpugraph_merge_grads_segment_size,enable_api_kernel_fallback,cuda_memory_async_pool_realease_threshold,custom_device_mem_record,graph_metapath_split_opt,allreduce_record_one_event,accuracy_check_rtol_fp16,use_auto_growth_pinned_allocator,enable_auto_detect_gpu_topo,embedding_deterministic,use_stride_kernel,ir_inplace_kernel_blacklist,cinn_compile_thread_num,gpugraph_force_device_batch_num_equal,local_exe_sub_scope_limit,fuse_parameter_memory_size,accuracy_check_atol_fp32,selected_gpus,sync_nccl_allreduce,gpugraph_debug_gpu_memory,enable_fuse_parallel_matmul_pass,query_dest_rank_by_multi_node,reallocate_gpu_memory_in_mb,max_inplace_grad_add,multi_node_sample_use_gpu_table,gpugraph_parallel_stream_num,gpugraph_hbm_table_load_factor,cupti_dir,enable_unused_var_check,tracer_onednn_ops_off,enable_opt_get_features,prim_all,pir_apply_shape_optimization_pass,enable_cinn_auto_tune,gpugraph_offload_gather_copy_maxsize,check_kernel_launch,use_cinn,enable_dump_main_program,enable_pir_in_executor_trace_run,cublas_dir,new_executor_use_cuda_graph,cudnn_exhaustive_search,curand_dir,initial_gpu_memory_in_mb,tracer_onednn_ops_on,accuracy_check_atol_bf16,search_cache_max_number,gpugraph_storage_mode,enable_sparse_inner_gather,benchmark_nccl,enable_auto_rdma_trans,gpu_memory_limit_mb,enable_fusion_fallback,graph_get_neighbor_id,prim_enable_dynamic,enable_record_memory,cse_max_count,enable_interpretercore_launch_cinn,init_allocated_mem,use_shm_cache,new_executor_serial_run,pir_broadcast_tree_limit,cuda_dir,low_precision_op_list,print_ir,check_infer_symbolic,disable_dyshape_in_train,trt_ibuilder_cache,get_host_by_name_time,free_when_no_cache_hit,prim_forward_blacklist,new_executor_use_inplace,allow_cinn_ops,gpugraph_parallel_copyer_split_maxsize,fast_eager_deletion_mode,manually_trans_conv_filter,use_stream_safe_cuda_allocator,sort_sum_gradient,initial_cpu_memory_in_mb,gpugraph_slot_feasign_max_num,graph_load_in_parallel,fleet_executor_with_standalone,cublaslt_device_best_config,apply_pass_to_program,nvidia_package_dir,cusparselt_dir,fraction_of_gpu_memory_to_use,enable_cinn_compile_cache,set_to_1d,enable_collect_shape,enable_cublas_tensor_op_math,logging_pir_py_code_int_tensor_element_limit,enable_adjust_op_order,nccl_dir,fraction_of_cuda_pinned_memory_to_use,conv_workspace_size_limit,enable_graph_multi_node_sampling,use_mkldnn,check_nan_inf,static_executor_perfstat_filepath,enable_pir_api,dist_threadpool_size,dynamic_static_unified_comm,auto_free_cudagraph_allocations_on_launch,cinn_subgraph_graphviz_dir,enable_pir_in_executor,use_system_allocator,use_virtual_memory_auto_growth,print_allocator_trace_info,tensorrt_dir,pir_debug,npu_storage_format,enable_cinn_accuracy_check,enable_dependency_builder_debug_info,graph_neighbor_size_percent,cuda_malloc_async_pool_memory_throttle_ratio,add_dependency_for_communication_op,enable_blaslt_global_search,pinned_memory_as_cpu_backend,accuracy_check_rtol_fp32,enable_async_trace,fuse_parameter_groups_size,dump_chunk_info,run_kp_kernel,free_idle_chunk,auto_growth_chunk_size_in_mb,enable_tracker_all2all,gpugraph_enable_gpu_direct_access,win_cuda_bin_dir,prim_forward,convert_all_blocks,enable_pir_with_pt_in_dy2st,accuracy_check_atol_fp16,cache_inference_while_scope,call_stack_level,nccl_blocking_wait,all_blocks_convert_trt,accuracy_check_rtol_bf16,logging_pir_py_code_dump_symbolic_dims,use_pinned_memory,op_dir,gpugraph_offload_param_extends,new_executor_static_build,gpu_allocator_retry_time,allocator_strategy,use_cuda_managed_memory 
1901: I0815 06:23:56.171154 29935 init.cc:108] After Parse: argc is 2
1901: I0815 06:24:02.028232 29935 scope.cc:202] Create variable X
1901: I0815 06:24:02.028313 29935 scope.cc:202] Create variable Out
1901: I0815 06:24:02.028330 29935 scope.cc:202] Create variable MedianIndex
1901: I0815 06:24:02.028474 29935 op_registry.cc:112] CreateOp directly from OpDesc is deprecated. It should only beused in unit tests. Use CreateOp(const OpDesc& op_desc) instead.
1901: I0815 06:24:02.029086 29935 allocator_facade.cc:212] selected allocator strategy:1
1901: I0815 06:24:02.029371 29935 dynamic_loader.cc:226] Try to find library: libcuda.so from default system path.
1901: I0815 06:24:04.497053 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.497110 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.497228 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.497239 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.498261 29935 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 06:24:04.498287 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.498337 29935 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 06:24:04.498343 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.498742 29935 pybind.cc:1827] need skip: 0
1901: I0815 06:24:04.498824 29935 pybind.cc:1827] need skip: 0
1901: I0815 06:24:04.499228 29935 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0815 06:24:04.499552 29935 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0815 06:24:04.499568 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 06:24:04.499653 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0815 06:24:04.499667 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 06:24:04.503090 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.503696 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.503713 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.503726 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.506685 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.506702 29935 scope.cc:202] Create variable feed
1901: I0815 06:24:04.506767 29935 program_interpreter.cc:243] New Executor is Running.
1901: I0815 06:24:04.506775 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.506783 29935 scope.cc:202] Create variable MedianIndex
1901: I0815 06:24:04.506793 29935 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x435332f0 type is 7
1901: I0815 06:24:04.506803 29935 scope.cc:202] Create variable Out
1901: I0815 06:24:04.506809 29935 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x435326f0 type is 7
1901: I0815 06:24:04.506814 29935 scope.cc:202] Create variable Out@GRAD
1901: I0815 06:24:04.506817 29935 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x43532ba0 type is 7
1901: I0815 06:24:04.506821 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.506824 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x43533770 type is 7
1901: I0815 06:24:04.506829 29935 scope.cc:202] Create variable X@GRAD
1901: I0815 06:24:04.506831 29935 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x43533940 type is 7
1901: I0815 06:24:04.506836 29935 scope.cc:202] Create variable _generated_var_0
1901: I0815 06:24:04.506839 29935 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x43533b30 type is 7
1901: I0815 06:24:04.506843 29935 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0815 06:24:04.506846 29935 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x43533d40 type is 7
1901: I0815 06:24:04.506850 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x435332d0 type is 9
1901: I0815 06:24:04.506855 29935 scope.cc:202] Create variable fetch
1901: I0815 06:24:04.506858 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43533b10 type is 10
1901: I0815 06:24:04.506958 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.506964 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.506968 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.506971 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: W0815 06:24:04.507633 29935 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 6.1, Driver API Version: 12.0, Runtime API Version: 12.0
1901: I0815 06:24:04.507925 29935 dynamic_loader.cc:226] Try to find library: libcudnn.so from default system path.
1901: W0815 06:24:04.508738 29935 gpu_resources.cc:164] device: 0, cuDNN Version: 8.8.
1901: I0815 06:24:04.508955 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.508980 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.509117 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.509127 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.509146 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.513818 29935 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.513839 29935 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.513859 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.513933 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.514030 29935 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514041 29935 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514094 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.514115 29935 interpreter_util.cc:647] Standalone Executor is Used.
1901: I0815 06:24:04.514148 29935 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514156 29935 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514173 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 06:24:04.514264 29935 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514276 29935 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514293 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 06:24:04.514425 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.514451 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.514473 29935 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.514480 29935 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x44a7efc0Variable Type 7
1901: I0815 06:24:04.514503 29935 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.514526 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.514572 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.514595 29935 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.514681 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.514711 29935 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.515130 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.515177 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.516196 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.516217 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.516260 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.516269 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.516887 29935 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 06:24:04.516904 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.516937 29935 op_desc.cc:1111] CompileTime infer shape on mean
1901: I0815 06:24:04.516943 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.517212 29935 pybind.cc:1827] need skip: 0
1901: I0815 06:24:04.517266 29935 pybind.cc:1827] need skip: 0
1901: I0815 06:24:04.517581 29935 op_desc.cc:1111] CompileTime infer shape on fill_constant
1901: I0815 06:24:04.517657 29935 op_desc.cc:1111] CompileTime infer shape on mean_grad
1901: I0815 06:24:04.517665 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 06:24:04.517733 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian_grad
1901: I0815 06:24:04.517743 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 06:24:04.519809 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.520330 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.520344 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.520349 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.523061 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.523078 29935 scope.cc:202] Create variable feed
1901: I0815 06:24:04.523104 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.523113 29935 scope.cc:202] Create variable MedianIndex
1901: I0815 06:24:04.523116 29935 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x4410dcd0 type is 7
1901: I0815 06:24:04.523123 29935 scope.cc:202] Create variable Out
1901: I0815 06:24:04.523128 29935 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x4410e0f0 type is 7
1901: I0815 06:24:04.523133 29935 scope.cc:202] Create variable Out@GRAD
1901: I0815 06:24:04.523135 29935 interpreter_util.cc:1206] Create Variable Out@GRAD locally, which pointer is 0x442ca110 type is 7
1901: I0815 06:24:04.523139 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.523142 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x4410ee50 type is 7
1901: I0815 06:24:04.523146 29935 scope.cc:202] Create variable X@GRAD
1901: I0815 06:24:04.523149 29935 interpreter_util.cc:1206] Create Variable X@GRAD locally, which pointer is 0x4410f0c0 type is 7
1901: I0815 06:24:04.523154 29935 scope.cc:202] Create variable _generated_var_0
1901: I0815 06:24:04.523156 29935 interpreter_util.cc:1206] Create Variable _generated_var_0 locally, which pointer is 0x4410f300 type is 7
1901: I0815 06:24:04.523160 29935 scope.cc:202] Create variable _generated_var_0@GRAD
1901: I0815 06:24:04.523164 29935 interpreter_util.cc:1206] Create Variable _generated_var_0@GRAD locally, which pointer is 0x4410f560 type is 7
1901: I0815 06:24:04.523167 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x442ca130 type is 9
1901: I0815 06:24:04.523172 29935 scope.cc:202] Create variable fetch
1901: I0815 06:24:04.523175 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x4410f2e0 type is 10
1901: I0815 06:24:04.523263 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.523270 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.523274 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.523278 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.523327 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523342 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523389 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523398 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523414 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.523886 29935 operator.cc:2295] op type:mean, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523902 29935 interpreter_util.cc:844] mean : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.523916 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.523964 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.524024 29935 operator.cc:2295] op type:fill_constant, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524034 29935 interpreter_util.cc:844] fill_constant : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524071 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.524103 29935 operator.cc:2295] op type:mean_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524111 29935 interpreter_util.cc:844] mean_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524127 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all_grad; inputs: X, Out@GRAD; attributes: ; outputs: X@GRAD
1901: I0815 06:24:04.524201 29935 operator.cc:2295] op type:nanmedian_grad, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524211 29935 interpreter_util.cc:844] nanmedian_grad : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524226 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian_grad; inputs: X, MedianIndex, Out@GRAD; attributes: axis, keepdim, mode; outputs: X@GRAD
1901: I0815 06:24:04.524331 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.524344 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.524359 29935 scope.cc:202] Create variable X@GRAD_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.524366 29935 data_transfer.cc:396] Create Variable X@GRAD_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x43d42b80Variable Type 7
1901: I0815 06:24:04.524381 29935 data_transfer.cc:439] Insert memcpy_d2h with X@GRAD(Place(gpu:0)) -> X@GRAD_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.524396 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.524415 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.524428 29935 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.524485 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.524505 29935 fetch_v2_op.cc:138] Fetch variable X@GRAD_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.524783 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: mean_all; inputs: X; attributes: ; outputs: Out
1901: I0815 06:24:04.524817 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.526366 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad4b910 for it.
1901: I0815 06:24:04.526525 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.526566 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.527741 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.527791 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.528434 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x43532bc0)  to GradNodeAccumulation (addr: 0x1ad4b910)
1901: I0815 06:24:04.528554 29935 dygraph_functions.cc:51757] Running AD API: mean
1901: I0815 06:24:04.528577 29935 dygraph_functions.cc:51814] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.528652 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.528671 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from MeanGradNode (addr: 0x2e7ac070)  to NanmedianGradNode (addr: 0x43532bc0)
1901: I0815 06:24:04.528743 29935 backward.cc:442] Run in Backward
1901: I0815 06:24:04.528752 29935 backward.cc:113] Start Backward
1901: I0815 06:24:04.528770 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:04.528812 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.528838 29935 backward.cc:255] Preparing GradNode:MeanGradNode addr:0x2e7ac070
1901: I0815 06:24:04.528849 29935 nodes.cc:25338] Running AD API GRAD: mean_grad
1901: I0815 06:24:04.528884 29935 nodes.cc:25394] { Input: [ 
1901: ( grad_out , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.528947 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.528967 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.528976 29935 backward.cc:335] Node: MeanGradNode addr:0x2e7ac070, Found pending node: NanmedianGradNode addr: 0x43532bc0
1901: I0815 06:24:04.528985 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.529011 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x43532bc0
1901: I0815 06:24:04.529024 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:04.529047 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.529090 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.529110 29935 backward.cc:335] Node: NanmedianGradNode addr:0x43532bc0, Found pending node: GradNodeAccumulation addr: 0x1ad4b910
1901: I0815 06:24:04.529117 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.529131 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ad4b910
1901: I0815 06:24:04.529140 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.529148 29935 accumulation_node.cc:40] Move Tensor ptr: 0x442c9fe0
1901: I0815 06:24:04.529151 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.529155 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: /home/code/Paddle/build/python/paddle/base/framework.py:667: VisibleDeprecationWarning: [93m
1901: Warning:
1901: API "paddle.base.dygraph.tensor_patch_methods.gradient" is deprecated since 2.1.0, and will be removed in future versions.
1901:     Reason: Please use tensor.grad, which returns the tensor value of the gradient. [0m
1901:   return func(*args, **kwargs)
1901: I0815 06:24:04.536182 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad4b910 for it.
1901: I0815 06:24:04.536324 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.536366 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.mean" (%1) {axis:(pd_op.IntArray)[],keepdim:false,stop_gradient:[false]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%4) = "pd_op.full" () {dtype:(pd_op.DataType)float32,place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> builtin.tensor<1xf32>
1901:     (%5) = "pd_op.full_like" (%3, %4) {dtype:(pd_op.DataType)float16,place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<1xf32>) -> builtin.tensor<f16>
1901:     (%6) = "pd_op.mean_grad" (%1, %5) {axis:(pd_op.IntArray)[],keepdim:false,reduce_all:false,stop_gradient:[false]} : (builtin.tensor<f16>, builtin.tensor<f16>) -> builtin.tensor<f16>
1901:     (%7) = "pd_op.nanmedian_grad" (%0, %2, %6) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[false]} : (builtin.tensor<100x100xf16>, builtin.tensor<2xi64>, builtin.tensor<f16>) -> builtin.tensor<100x100xf16>
1901:     (%8) = "pd_op.fetch" (%7) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[false]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<100x100xf16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: I0815 06:24:04.611313 29935 pir_interpreter.cc:161] PirInterpreter(): 0x43aa7560 on Place(gpu:0)
1901: I0815 06:24:04.611357 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.611383 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_1
1901: I0815 06:24:04.611393 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_2
1901: I0815 06:24:04.611402 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_3
1901: I0815 06:24:04.611409 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_4
1901: I0815 06:24:04.611418 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_5
1901: I0815 06:24:04.611424 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_6
1901: I0815 06:24:04.611433 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_7
1901: I0815 06:24:04.611440 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_8
1901: I0815 06:24:04.611449 29935 scope.cc:202] Create variable 0x43aa75601723703044611343390_inner_var_9
1901: I0815 06:24:04.611454 29935 scope.cc:202] Create variable fetch0@fetch
1901: I0815 06:24:04.611840 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 06:24:04.611855 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.611858 29935 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: I0815 06:24:04.611901 29935 pir_interpreter.cc:1455] New Executor is Running ...
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[false]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[false,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "mean(phi_kernel)" (%2) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean",op_name:"pd_op.mean",stop_gradient:[false]} : (gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%5) = "full(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"full",op_name:"pd_op.full",place:(pd_op.Place)Place(cpu),shape:(pd_op.IntArray)[1],stop_gradient:[true],value:(Double)1} : () -> cpu_tensor<1xf32>
1901:     (%6) = "full_like(phi_kernel)" (%4, %5) {dtype:(pd_op.DataType)float16,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"full_like",op_name:"pd_op.full_like",place:(pd_op.Place)Place(undefined:0),stop_gradient:[false]} : (gpu_tensor<f16>, cpu_tensor<1xf32>) -> gpu_tensor<f16>
1901:     (%7) = "mean_grad(phi_kernel)" (%2, %6) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"mean_grad",op_name:"pd_op.mean_grad",reduce_all:false,stop_gradient:[false]} : (gpu_tensor<f16>, gpu_tensor<f16>) -> gpu_tensor<f16>
1901:     (%8) = "nanmedian_grad(phi_kernel)" (%1, %3, %7) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian_grad",mode:"avg",op_name:"pd_op.nanmedian_grad",stop_gradient:[false]} : (gpu_tensor<100x100xf16>, gpu_tensor<2xi64>, gpu_tensor<f16>) -> gpu_tensor<100x100xf16>
1901:     (%9) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901:     (%10) = "fetch(phi_kernel)" (%9) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[false]} : (cpu_tensor<100x100xf16>) -> cpu_tensor<100x100xf16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.mean ( 2 ) 
1901: 3: ( 5 )  = pd_op.full
1901: 4: ( 6 )  = pd_op.full_like ( 5 )  ( 4 ) 
1901: 5: ( 7 )  = pd_op.mean_grad ( 6 )  ( 2 ) 
1901: 6: ( 8 )  = pd_op.nanmedian_grad ( 7 )  ( 3 )  ( 1 ) 
1901: 7: ( 9 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 8: ( 10 )  = pd_op.fetch ( 9 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x44130fa0
1901: 1 -> 0x43aa75601723703044611343390_inner_var_1 -> 0x44132940
1901: 2 -> 0x43aa75601723703044611343390_inner_var_2 -> 0x4412f160
1901: 3 -> 0x43aa75601723703044611343390_inner_var_3 -> 0x44132050
1901: 4 -> 0x43aa75601723703044611343390_inner_var_4 -> 0x44130eb0
1901: 5 -> 0x43aa75601723703044611343390_inner_var_5 -> 0x44132a70
1901: 6 -> 0x43aa75601723703044611343390_inner_var_6 -> 0x43aa7c70
1901: 7 -> 0x43aa75601723703044611343390_inner_var_7 -> 0x43aa8090
1901: 8 -> 0x43aa75601723703044611343390_inner_var_8 -> 0x4412fdf0
1901: 9 -> 0x43aa75601723703044611343390_inner_var_9 -> 0x43aa84b0
1901: 10 -> fetch0@fetch -> 0x43aa8cc0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 4 
1901: 3 -> 4 
1901: 4 -> 5 
1901: 5 -> 6 
1901: 6 -> 7 
1901: 7 -> 8 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->3[pd_op.full]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.mean]->
1901: 2 downstreams: 
1901: 3 downstreams: 4[pd_op.full_like]->
1901: 4 downstreams: 5[pd_op.mean_grad]->
1901: 5 downstreams: 6[pd_op.nanmedian_grad]->
1901: 6 downstreams: 7[pd_op.memcpy_d2h]->
1901: 7 downstreams: 8[pd_op.fetch]->
1901: 8 downstreams: 
1901: I0815 06:24:04.612825 29935 pir_interpreter.cc:1481] pir interpreter is running by multi-thread mode ...
1901: I0815 06:24:04.613039 29972 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 06:24:04.613245 29973 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:04.613252 29974 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:04.613369 29975 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:04.613369 29976 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:04.613405 29974 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x43aa75601723703044611343390_inner_var_5:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.613435 29977 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:04.613518 29974 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.full type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.full), inputs:{}, outputs:{0x43aa75601723703044611343390_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};]}.
1901: I0815 06:24:04.613504 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.613564 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.613622 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43aa75601723703044611343390_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_3:[dtype=;place=;dim=;lod={};, 0x43aa75601723703044611343390_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614126 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43aa75601723703044611343390_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x43aa75601723703044611343390_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.614157 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean), inputs:{0x43aa75601723703044611343390_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614209 29977 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.614224 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.mean type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean), inputs:{0x43aa75601723703044611343390_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.614251 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.full_like), inputs:{0x43aa75601723703044611343390_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x43aa75601723703044611343390_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614310 29977 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.614369 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.full_like type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.full_like), inputs:{0x43aa75601723703044611343390_inner_var_5:[dtype=float;place=Place(cpu);dim=1;lod={};], 0x43aa75601723703044611343390_inner_var_4:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.614398 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x43aa75601723703044611343390_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x43aa75601723703044611343390_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_7:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614466 29977 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.614483 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.mean_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.mean_grad), inputs:{0x43aa75601723703044611343390_inner_var_6:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x43aa75601723703044611343390_inner_var_2:[dtype=unknown_dtype;place=unknown_place;dim=;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.614506 29977 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x43aa75601723703044611343390_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x43aa75601723703044611343390_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x43aa75601723703044611343390_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614589 29977 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.nanmedian_grad type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian_grad), inputs:{0x43aa75601723703044611343390_inner_var_7:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};], 0x43aa75601723703044611343390_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};], 0x43aa75601723703044611343390_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.614660 29974 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43aa75601723703044611343390_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_9:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614687 29974 tensor_utils.cc:57] TensorCopy 100, 100 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.614784 29974 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43aa75601723703044611343390_inner_var_8:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43aa75601723703044611343390_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.614813 29974 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x43aa75601723703044611343390_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.614838 29974 tensor_utils.cc:57] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.614868 29974 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x43aa75601723703044611343390_inner_var_9:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.614899 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x43aa76d0) got event_name: TaskCompletion
1901: I0815 06:24:04.614925 29935 tensor_util.cc:48] TensorCopy 100, 100 from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.618077 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.618105 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.618165 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.618176 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.620151 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.620723 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.621186 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.621201 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.621206 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.623524 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.623544 29935 scope.cc:202] Create variable feed
1901: I0815 06:24:04.623576 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.623585 29935 scope.cc:202] Create variable MedianIndex
1901: I0815 06:24:04.623590 29935 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x43c84da0 type is 7
1901: I0815 06:24:04.623597 29935 scope.cc:202] Create variable Out
1901: I0815 06:24:04.623605 29935 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x43c84140 type is 7
1901: I0815 06:24:04.623610 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.623613 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x43c84b30 type is 7
1901: I0815 06:24:04.623618 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x43c84d80 type is 9
1901: I0815 06:24:04.623623 29935 scope.cc:202] Create variable fetch
1901: I0815 06:24:04.623627 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43c852b0 type is 10
1901: I0815 06:24:04.623704 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.623711 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.623716 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.623720 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.623771 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.623786 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.623852 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.623863 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.623883 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.624392 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.624410 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.624430 29935 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.624439 29935 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x43c892d0Variable Type 7
1901: I0815 06:24:04.624456 29935 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.624476 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.624500 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[::phi::dtype::float16]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.624517 29935 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.624560 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.624585 29935 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.624635 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.624647 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.624663 29935 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.624671 29935 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x44474c80Variable Type 7
1901: I0815 06:24:04.624686 29935 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.624699 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.624718 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.624733 29935 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.624769 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.624801 29935 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 06:24:04.625131 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.625161 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.626493 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.626518 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.626570 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.626581 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.628505 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.629063 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.629521 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.629537 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.629541 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.631795 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.631870 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.631881 29935 scope.cc:202] Create variable MedianIndex
1901: I0815 06:24:04.631887 29935 interpreter_util.cc:1206] Create Variable MedianIndex locally, which pointer is 0x44a37990 type is 7
1901: I0815 06:24:04.631897 29935 scope.cc:202] Create variable Out
1901: I0815 06:24:04.631901 29935 interpreter_util.cc:1206] Create Variable Out locally, which pointer is 0x44a36cc0 type is 7
1901: I0815 06:24:04.631906 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.631909 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x44a376c0 type is 7
1901: I0815 06:24:04.631914 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x43c84d80 type is 9
1901: I0815 06:24:04.631920 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43c852b0 type is 10
1901: I0815 06:24:04.631996 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.632004 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.632009 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.632012 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.632052 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.632066 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.632119 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.632129 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.632148 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.632678 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.632696 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.632715 29935 scope.cc:202] Create variable Out_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.632722 29935 data_transfer.cc:396] Create Variable Out_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x42a2f0a0Variable Type 7
1901: I0815 06:24:04.632740 29935 data_transfer.cc:439] Insert memcpy_d2h with Out(Place(gpu:0)) -> Out_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.632756 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.632779 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.632795 29935 tensor_utils.cc:57] TensorCopy 1, 1 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.632836 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.632859 29935 fetch_v2_op.cc:138] Fetch variable Out_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.632905 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.632916 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.632932 29935 scope.cc:202] Create variable MedianIndex_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.632939 29935 data_transfer.cc:396] Create Variable MedianIndex_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x42a2f0c0Variable Type 7
1901: I0815 06:24:04.632952 29935 data_transfer.cc:439] Insert memcpy_d2h with MedianIndex(Place(gpu:0)) -> MedianIndex_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.632967 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.632987 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[int64_t]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.633001 29935 tensor_utils.cc:57] TensorCopy 1, 1, 2 from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.633037 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.633059 29935 fetch_v2_op.cc:138] Fetch variable MedianIndex_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 06:24:04.633333 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.633363 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.633926 29972 thread_data_registry.h:135] Add data {current : -20004, peak : 0} from thread 14783351677738605560 to 7110887826760878444 , after update, data is {current : -20004, peak : 16}.
1901: I0815 06:24:04.633946 29972 thread_data_registry.h:135] Add data {current : -20024, peak : 0} from thread 14783351677738605560 to 7110887826760878444 , after update, data is {current : 0, peak : 330659}.
1901: I0815 06:24:04.634126 29974 thread_data_registry.h:135] Add data {current : 40004, peak : 40004} from thread 4053306302942050912 to 7110887826760878444 , after update, data is {current : 20000, peak : 40004}.
1901: I0815 06:24:04.634313 29977 thread_data_registry.h:135] Add data {current : 20000, peak : 40004} from thread 7110887826760878444 to 12945557604293332363 , after update, data is {current : 20038, peak : 40004}.
1901: I0815 06:24:04.634323 29977 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 7110887826760878444 to 12945557604293332363 , after update, data is {current : 140000, peak : 520631}.
1901: I0815 06:24:04.764954 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44a455a0 for it.
1901: I0815 06:24:04.765340 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.765420 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.766022 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.766072 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.767375 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44a455a0 for it.
1901: I0815 06:24:04.767541 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.767607 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.767952 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.767987 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.770316 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44a455a0 for it.
1901: I0815 06:24:04.770486 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.770542 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 06:24:04.774047 29935 pir_interpreter.cc:161] PirInterpreter(): 0x43d62020 on Place(gpu:0)
1901: I0815 06:24:04.774083 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.774108 29935 scope.cc:202] Create variable 0x43d620201723703044774073389_inner_var_1
1901: I0815 06:24:04.774123 29935 scope.cc:202] Create variable 0x43d620201723703044774073389_inner_var_2
1901: I0815 06:24:04.774137 29935 scope.cc:202] Create variable 0x43d620201723703044774073389_inner_var_3
1901: I0815 06:24:04.774152 29935 scope.cc:202] Create variable 0x43d620201723703044774073389_inner_var_4
1901: I0815 06:24:04.774164 29935 scope.cc:202] Create variable fetch0@fetch
1901: I0815 06:24:04.774561 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 06:24:04.774578 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.774583 29935 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x4412ecb0
1901: 1 -> 0x43d620201723703044774073389_inner_var_1 -> 0x44ed3230
1901: 2 -> 0x43d620201723703044774073389_inner_var_2 -> 0x44ed8040
1901: 3 -> 0x43d620201723703044774073389_inner_var_3 -> 0x44ab1c10
1901: 4 -> 0x43d620201723703044774073389_inner_var_4 -> 0x44131810
1901: 5 -> fetch0@fetch -> 0x43aa88d0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 06:24:04.775308 29979 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 06:24:04.775463 29980 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:04.775522 29981 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:04.775578 29982 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:04.775592 29983 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:04.775679 29984 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:04.775725 29984 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.775763 29984 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.775799 29984 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43d620201723703044774073389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_3:[dtype=;place=;dim=;lod={};, 0x43d620201723703044774073389_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.776229 29984 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43d620201723703044774073389_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x43d620201723703044774073389_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.776304 29983 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43d620201723703044774073389_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.776340 29983 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.776407 29983 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43d620201723703044774073389_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43d620201723703044774073389_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.776440 29983 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x43d620201723703044774073389_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.776459 29983 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.776471 29983 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x43d620201723703044774073389_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.776501 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x43d62190) got event_name: TaskCompletion
1901: I0815 06:24:04.776528 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.777150 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.777339 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.777403 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x44a455a0 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 06:24:04.780090 29935 pir_interpreter.cc:161] PirInterpreter(): 0x43b18020 on Place(gpu:0)
1901: I0815 06:24:04.780123 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.780145 29935 scope.cc:202] Create variable 0x43b180201723703044780115892_inner_var_1
1901: I0815 06:24:04.780160 29935 scope.cc:202] Create variable 0x43b180201723703044780115892_inner_var_2
1901: I0815 06:24:04.780174 29935 scope.cc:202] Create variable 0x43b180201723703044780115892_inner_var_3
1901: I0815 06:24:04.780189 29935 scope.cc:202] Create variable 0x43b180201723703044780115892_inner_var_4
1901: I0815 06:24:04.780202 29935 scope.cc:202] Create variable fetch0@fetch
1901: I0815 06:24:04.780555 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 06:24:04.780572 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.780576 29935 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x435c3960
1901: 1 -> 0x43b180201723703044780115892_inner_var_1 -> 0x44132b80
1901: 2 -> 0x43b180201723703044780115892_inner_var_2 -> 0x42a35a70
1901: 3 -> 0x43b180201723703044780115892_inner_var_3 -> 0x44861c10
1901: 4 -> 0x43b180201723703044780115892_inner_var_4 -> 0x42a34bf0
1901: 5 -> fetch0@fetch -> 0x4412f6c0
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 06:24:04.781226 29985 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 06:24:04.781503 29986 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:04.781520 29987 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:04.781648 29988 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:04.781651 29989 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:04.781769 29990 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:04.781805 29990 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.781824 29990 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.781852 29990 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43b180201723703044780115892_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_3:[dtype=;place=;dim=;lod={};, 0x43b180201723703044780115892_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.782267 29990 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43b180201723703044780115892_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x43b180201723703044780115892_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.782351 29988 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43b180201723703044780115892_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.782397 29988 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.782474 29988 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_2
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43b180201723703044780115892_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43b180201723703044780115892_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.782517 29988 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x43b180201723703044780115892_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.782543 29988 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.782562 29988 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_2
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x43b180201723703044780115892_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.782603 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x43b18190) got event_name: TaskCompletion
1901: I0815 06:24:04.782625 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.783849 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x44a455a0 for it.
1901: I0815 06:24:04.784010 29935 eager.cc:133] Tensor(Out_out_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.784060 29935 eager.cc:133] Tensor(MedianIndex_out_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float16,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> builtin.tensor<100x100xf16>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<100x100xf16>) -> builtin.tensor<f16>, builtin.tensor<2xi64>
1901:     (%3) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f16>) -> builtin.tensor<f16>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: I0815 06:24:04.786568 29935 pir_interpreter.cc:161] PirInterpreter(): 0x43d01f70 on Place(gpu:0)
1901: I0815 06:24:04.786597 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.786615 29935 scope.cc:202] Create variable 0x43d01f701723703044786591527_inner_var_1
1901: I0815 06:24:04.786626 29935 scope.cc:202] Create variable 0x43d01f701723703044786591527_inner_var_2
1901: I0815 06:24:04.786638 29935 scope.cc:202] Create variable 0x43d01f701723703044786591527_inner_var_3
1901: I0815 06:24:04.786648 29935 scope.cc:202] Create variable 0x43d01f701723703044786591527_inner_var_4
1901: I0815 06:24:04.786659 29935 scope.cc:202] Create variable fetch0@fetch
1901: I0815 06:24:04.786976 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 06:24:04.786991 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.786995 29935 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float16,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[100,100],stop_gradient:[true]} : () -> undefined_tensor<100x100xf16>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<100x100xf16>) -> gpu_tensor<100x100xf16>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float16>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<100x100xf16>) -> gpu_tensor<f16>, gpu_tensor<2xi64>
1901:     (%4) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f16>) -> cpu_tensor<f16>
1901:     (%5) = "fetch(phi_kernel)" (%4) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float16>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f16>) -> cpu_tensor<f16>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 4 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 3: ( 5 )  = pd_op.fetch ( 4 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x44471fc0
1901: 1 -> 0x43d01f701723703044786591527_inner_var_1 -> 0x44472040
1901: 2 -> 0x43d01f701723703044786591527_inner_var_2 -> 0x458497b0
1901: 3 -> 0x43d01f701723703044786591527_inner_var_3 -> 0x44a36db0
1901: 4 -> 0x43d01f701723703044786591527_inner_var_4 -> 0x43cfed80
1901: 5 -> fetch0@fetch -> 0x44282500
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 
1901: 1 -> 2 
1901: 2 -> 3 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->
1901: 1 downstreams: 2[pd_op.memcpy_d2h]->
1901: 2 downstreams: 3[pd_op.fetch]->
1901: 3 downstreams: 
1901: I0815 06:24:04.787632 29991 nonblocking_threadpool.h:251] GarbageCollector_thread_0 started 
1901: I0815 06:24:04.787904 29992 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:04.787992 29993 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:04.788003 29994 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:04.788096 29995 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:04.788151 29996 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:04.788190 29996 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.788215 29996 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}.
1901: I0815 06:24:04.788244 29996 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43d01f701723703044786591527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_3:[dtype=;place=;dim=;lod={};, 0x43d01f701723703044786591527_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.788673 29996 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x43d01f701723703044786591527_inner_var_1:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=100, 100;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x43d01f701723703044786591527_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.788740 29995 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43d01f701723703044786591527_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.788772 29995 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.788830 29995 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x43d01f701723703044786591527_inner_var_2:[dtype=::phi::dtype::float16;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x43d01f701723703044786591527_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.788859 29995 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x43d01f701723703044786591527_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.788877 29995 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.788889 29995 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x43d01f701723703044786591527_inner_var_4:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=::phi::dtype::float16;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.788915 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x43d020e0) got event_name: TaskCompletion
1901: I0815 06:24:04.788933 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.789072 29935 shape_analysis.cc:506] InferShapeOrDataForValue,  defining_op: pd_op.fetch id:48
1901: I0815 06:24:04.841037 29985 thread_data_registry.h:135] Add data {current : -2, peak : 0} from thread 17443096225622184462 to 15837561344705494719 , after update, data is {current : -4, peak : 0}.
1901: I0815 06:24:04.841058 29985 thread_data_registry.h:135] Add data {current : -18, peak : 0} from thread 17443096225622184462 to 15837561344705494719 , after update, data is {current : -36, peak : 0}.
1901: I0815 06:24:04.841234 29988 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 5579797919543060382 to 15837561344705494719 , after update, data is {current : 0, peak : 4}.
1901: I0815 06:24:04.841418 29990 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 1844197911914963963 to 15837561344705494719 , after update, data is {current : 0, peak : 16}.
1901: I0815 06:24:04.841434 29990 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 1844197911914963963 to 15837561344705494719 , after update, data is {current : -18, peak : 330659}.
1901: I0815 06:24:04.841600 29991 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 15837561344705494719 to 7110887826760878444 , after update, data is {current : -2, peak : 16}.
1901: I0815 06:24:04.841609 29991 thread_data_registry.h:135] Add data {current : -18, peak : 330659} from thread 15837561344705494719 to 7110887826760878444 , after update, data is {current : -36, peak : 330659}.
1901: I0815 06:24:04.841759 29995 thread_data_registry.h:135] Add data {current : 4, peak : 4} from thread 13101486947023862968 to 7110887826760878444 , after update, data is {current : 2, peak : 16}.
1901: I0815 06:24:04.841935 29996 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 13832365527669712378 to 7110887826760878444 , after update, data is {current : 2, peak : 16}.
1901: I0815 06:24:04.841948 29996 thread_data_registry.h:135] Add data {current : 18, peak : 330659} from thread 13832365527669712378 to 7110887826760878444 , after update, data is {current : -18, peak : 330659}.
1901: I0815 06:24:04.846004 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.846133 29935 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 512(0x7fdec0027c00), and remaining 0
1901: I0815 06:24:04.846247 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.846284 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.846570 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.847393 29935 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.847477 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.847505 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.847672 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.848352 29935 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.848424 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.848450 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.848598 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.849215 29935 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.849287 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.849320 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.849475 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:464: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data)
1901: I0815 06:24:04.850152 29935 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.850205 29935 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fdec0018e00), and remaining 0
1901: I0815 06:24:04.850257 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.850281 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.850754 29935 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.850821 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.850845 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.851290 29935 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.851373 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.851397 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.851805 29935 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.851869 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.851892 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.852453 29935 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.852523 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.852547 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.852727 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.853366 29935 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.853439 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.853464 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.853618 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.854290 29935 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.854369 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.854395 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.854573 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.855172 29935 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.855244 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.855268 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.855432 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.856081 29935 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.856153 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.856177 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.856380 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.857019 29935 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.857090 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.857115 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.857268 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.857959 29935 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.858031 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.858057 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.858213 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.858879 29935 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.858953 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.858978 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.859126 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.859825 29935 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.859896 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.859921 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.860090 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.860719 29935 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.860792 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.860817 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.860972 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.861502 29935 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.861568 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.861593 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.861735 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.862418 29935 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.862484 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.862509 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.862653 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.863296 29935 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.863374 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.863399 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.863638 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: /home/code/Paddle/build/test/legacy_test/test_nanmedian.py:478: RuntimeWarning: All-NaN slice encountered
1901:   np_res = np.nanmedian(data, axis)
1901: I0815 06:24:04.865471 29935 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.865545 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.865571 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.865747 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.867092 29935 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.867166 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.867192 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.867398 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.868700 29935 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.868774 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.868800 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.868988 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.870199 29935 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.870275 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.870308 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.870549 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.871768 29935 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.871840 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.871866 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.872045 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.873322 29935 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.873395 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.873420 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.873597 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.874878 29935 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.874953 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.874979 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.875152 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.876363 29935 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.876437 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.876463 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.876698 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.878002 29935 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.878077 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.878103 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.878281 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.879488 29935 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.879563 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.879588 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.879789 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.880997 29935 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.881072 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.881098 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.881273 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.882548 29935 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.882623 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.882649 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.882825 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.884037 29935 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.884112 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.884137 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.884316 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.885586 29935 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.885659 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.885685 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.885859 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.887063 29935 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.887138 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.887164 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.887344 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.888536 29935 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.888610 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.888636 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.888808 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.889536 29935 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.889607 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.889633 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.889799 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.892575 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.892602 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.893467 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.893491 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.894253 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.894274 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.895058 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.895081 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.895844 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.895866 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.898434 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.898975 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.899505 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.900034 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.900563 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.901404 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.901422 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.901428 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.906371 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.906451 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.906466 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.906473 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47bfe640 type is 7
1901: I0815 06:24:04.906482 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x43c84d80 type is 9
1901: I0815 06:24:04.906489 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43c852b0 type is 10
1901: I0815 06:24:04.906495 29935 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0815 06:24:04.906499 29935 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x47bfd900 type is 7
1901: I0815 06:24:04.906504 29935 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0815 06:24:04.906508 29935 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x47bfe4f0 type is 7
1901: I0815 06:24:04.906513 29935 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0815 06:24:04.906519 29935 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x43d649a0 type is 7
1901: I0815 06:24:04.906524 29935 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0815 06:24:04.906528 29935 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x47bfd320 type is 7
1901: I0815 06:24:04.906533 29935 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0815 06:24:04.906538 29935 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x47bfe750 type is 7
1901: I0815 06:24:04.906543 29935 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0815 06:24:04.906548 29935 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x47bfe880 type is 7
1901: I0815 06:24:04.906551 29935 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0815 06:24:04.906555 29935 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x47bfea90 type is 7
1901: I0815 06:24:04.906559 29935 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0815 06:24:04.906563 29935 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x47bfecf0 type is 7
1901: I0815 06:24:04.906567 29935 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0815 06:24:04.906571 29935 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x47bfef50 type is 7
1901: I0815 06:24:04.906576 29935 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0815 06:24:04.906579 29935 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x47bff1b0 type is 7
1901: I0815 06:24:04.906716 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.906723 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.906728 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.906733 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.906795 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.906809 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.906878 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.906888 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.906904 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.907080 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.907285 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.907312 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.907333 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.907467 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.907658 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.907671 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.907687 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.907811 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.907994 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.908006 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.908022 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.908169 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.908380 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.908394 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.908411 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.908565 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.908762 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.908775 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.908793 29935 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.908802 29935 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47c1b8f0Variable Type 7
1901: I0815 06:24:04.908820 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.908838 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.908862 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.908880 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.908926 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.908946 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.908990 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909000 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909014 29935 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.909022 29935 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47cd6ff0Variable Type 7
1901: I0815 06:24:04.909036 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.909050 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.909070 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.909085 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.909118 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.909145 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 06:24:04.909193 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909204 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909219 29935 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.909225 29935 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47b95b30Variable Type 7
1901: I0815 06:24:04.909240 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.909253 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.909271 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.909286 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.909327 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.909343 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0815 06:24:04.909389 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909399 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909412 29935 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.909420 29935 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47cd70b0Variable Type 7
1901: I0815 06:24:04.909433 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.909446 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.909461 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.909473 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.909503 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.909515 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0815 06:24:04.909552 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909564 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.909576 29935 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.909583 29935 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47c13000Variable Type 7
1901: I0815 06:24:04.909596 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.909610 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.909626 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.909641 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.909672 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.909685 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0815 06:24:04.910310 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.910346 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.910367 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.910387 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.910406 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"avg",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<2xi64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0815 06:24:04.916621 29935 pir_interpreter.cc:161] PirInterpreter(): 0x47cd8830 on Place(gpu:0)
1901: I0815 06:24:04.916654 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.916675 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_1
1901: I0815 06:24:04.916687 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_2
1901: I0815 06:24:04.916697 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_3
1901: I0815 06:24:04.916708 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_4
1901: I0815 06:24:04.916715 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_5
1901: I0815 06:24:04.916724 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_6
1901: I0815 06:24:04.916731 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_7
1901: I0815 06:24:04.916741 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_8
1901: I0815 06:24:04.916749 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_9
1901: I0815 06:24:04.916759 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_10
1901: I0815 06:24:04.916766 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_11
1901: I0815 06:24:04.916776 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_12
1901: I0815 06:24:04.916785 29935 scope.cc:202] Create variable fetch0@fetch
1901: I0815 06:24:04.916800 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_14
1901: I0815 06:24:04.916810 29935 scope.cc:202] Create variable fetch1@fetch
1901: I0815 06:24:04.916818 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_16
1901: I0815 06:24:04.916827 29935 scope.cc:202] Create variable fetch2@fetch
1901: I0815 06:24:04.916836 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_18
1901: I0815 06:24:04.916844 29935 scope.cc:202] Create variable fetch3@fetch
1901: I0815 06:24:04.916852 29935 scope.cc:202] Create variable 0x47cd88301723703044916645429_inner_var_20
1901: I0815 06:24:04.916862 29935 scope.cc:202] Create variable fetch4@fetch
1901: I0815 06:24:04.917145 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: I0815 06:24:04.917160 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.917165 29935 feed_fetch_method.cc:58] Reset X to phi::DenseTensor
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"avg",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<2xi64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47cf8060
1901: 1 -> 0x47cd88301723703044916645429_inner_var_1 -> 0x44284c50
1901: 2 -> 0x47cd88301723703044916645429_inner_var_2 -> 0x47cb8b60
1901: 3 -> 0x47cd88301723703044916645429_inner_var_3 -> 0x47bf2440
1901: 4 -> 0x47cd88301723703044916645429_inner_var_4 -> 0x47bff550
1901: 5 -> 0x47cd88301723703044916645429_inner_var_5 -> 0x47c10840
1901: 6 -> 0x47cd88301723703044916645429_inner_var_6 -> 0x47bfdee0
1901: 7 -> 0x47cd88301723703044916645429_inner_var_7 -> 0x47c11c80
1901: 8 -> 0x47cd88301723703044916645429_inner_var_8 -> 0x43b19220
1901: 9 -> 0x47cd88301723703044916645429_inner_var_9 -> 0x44064ba0
1901: 10 -> 0x47cd88301723703044916645429_inner_var_10 -> 0x44064f70
1901: 11 -> 0x47cd88301723703044916645429_inner_var_11 -> 0x44b0f480
1901: 12 -> 0x47cd88301723703044916645429_inner_var_12 -> 0x47cd74d0
1901: 13 -> fetch0@fetch -> 0x47bf1e20
1901: 14 -> 0x47cd88301723703044916645429_inner_var_14 -> 0x47b90dc0
1901: 15 -> fetch1@fetch -> 0x47cb7da0
1901: 16 -> 0x47cd88301723703044916645429_inner_var_16 -> 0x47bf1e00
1901: 17 -> fetch2@fetch -> 0x4351f460
1901: 18 -> 0x47cd88301723703044916645429_inner_var_18 -> 0x47cb7d80
1901: 19 -> fetch3@fetch -> 0x47c656b0
1901: 20 -> 0x47cd88301723703044916645429_inner_var_20 -> 0x4351f440
1901: 21 -> fetch4@fetch -> 0x47d08780
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0815 06:24:04.918655 29997 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:04.918694 29999 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:04.918725 30000 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:04.918747 29998 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:04.918776 30001 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:04.918820 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.918884 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0815 06:24:04.918946 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_3:[dtype=;place=;dim=;lod={};, 0x47cd88301723703044916645429_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919214 30001 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.919373 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47cd88301723703044916645429_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.919430 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_5:[dtype=;place=;dim=;lod={};, 0x47cd88301723703044916645429_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919457 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919502 29998 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.919551 30001 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.919595 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.919716 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919740 29998 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.919739 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47cd88301723703044916645429_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.919754 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.919781 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919781 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_7:[dtype=;place=;dim=;lod={};, 0x47cd88301723703044916645429_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919802 29998 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.919878 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.919917 30001 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.919945 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.919965 29998 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.919975 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920083 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47cd88301723703044916645429_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.920125 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_9:[dtype=;place=;dim=;lod={};, 0x47cd88301723703044916645429_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920131 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920152 29998 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.920185 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920224 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920241 29998 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.920250 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920295 30001 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.920452 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47cd88301723703044916645429_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.920493 30001 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_11:[dtype=;place=;dim=;lod={};, 0x47cd88301723703044916645429_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920500 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920514 29998 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.920547 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920583 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920599 29998 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.920609 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920660 30001 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.920809 30001 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x47cd88301723703044916645429_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=2;lod={};, 0x47cd88301723703044916645429_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:04.920855 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920872 29998 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.920899 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_1
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x47cd88301723703044916645429_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x47cd88301723703044916645429_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920933 29998 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:04.920948 29998 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.920957 29998 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_1
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x47cd88301723703044916645429_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:04.920984 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x47cd89a0) got event_name: TaskCompletion
1901: I0815 06:24:04.921008 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.921034 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.921046 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.921056 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.921067 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:04.922691 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.922782 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.922811 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.922987 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.923087 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x44ab2c20)  to GradNodeAccumulation (addr: 0x1ad9fd50)
1901: I0815 06:24:04.923213 29935 backward.cc:459] Run in Grad
1901: I0815 06:24:04.923230 29935 backward.cc:113] Start Backward
1901: I0815 06:24:04.923282 29935 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x44ab2c20 to ptr: 0x47c65be0
1901: I0815 06:24:04.923295 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:04.923352 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.923386 29935 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ad9fd50 to ptr: 0x42a33bd0
1901: I0815 06:24:04.923415 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47c65be0
1901: I0815 06:24:04.923424 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:04.923457 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.923516 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.923527 29935 backward.cc:335] Node: NanmedianGradNode addr:0x47c65be0, Found pending node: GradNodeAccumulation addr: 0x42a33bd0
1901: I0815 06:24:04.923532 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.923556 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x42a33bd0
1901: I0815 06:24:04.923564 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.923568 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.923573 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.923578 29935 backward.cc:435] Finish Backward
1901: I0815 06:24:04.924270 29935 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0815 06:24:04.924288 29935 dygraph_functions.cc:77659] { Input: []} 
1901: I0815 06:24:04.924401 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.924425 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.924566 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.924638 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x44ab2c20)  to GradNodeAccumulation (addr: 0x1ad9fd50)
1901: I0815 06:24:04.924731 29935 backward.cc:442] Run in Backward
1901: I0815 06:24:04.924739 29935 backward.cc:113] Start Backward
1901: I0815 06:24:04.924746 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:04.924777 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.924803 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x44ab2c20
1901: I0815 06:24:04.924810 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:04.924839 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.924893 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.924919 29935 backward.cc:335] Node: NanmedianGradNode addr:0x44ab2c20, Found pending node: GradNodeAccumulation addr: 0x1ad9fd50
1901: I0815 06:24:04.924927 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.924945 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ad9fd50
1901: I0815 06:24:04.924952 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.924957 29935 accumulation_node.cc:40] Move Tensor ptr: 0x43aac5a0
1901: I0815 06:24:04.924960 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.924964 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.927925 29935 layout_autotune.cc:83] The number of layout agnostic OPs: 523, heavily layout sensitive OPs: 39, lightly layout sensitive OPs: 135
1901: I0815 06:24:04.928438 29935 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.928552 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.928579 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.928745 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x44067370)  to GradNodeAccumulation (addr: 0x1ad62430)
1901: I0815 06:24:04.928844 29935 backward.cc:442] Run in Backward
1901: I0815 06:24:04.928853 29935 backward.cc:113] Start Backward
1901: I0815 06:24:04.928859 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:04.928890 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.928913 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x44067370
1901: I0815 06:24:04.928921 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:04.928951 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.928997 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.929023 29935 backward.cc:335] Node: NanmedianGradNode addr:0x44067370, Found pending node: GradNodeAccumulation addr: 0x1ad62430
1901: I0815 06:24:04.929030 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.929046 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ad62430
1901: I0815 06:24:04.929054 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.929057 29935 accumulation_node.cc:40] Move Tensor ptr: 0x44a7e9a0
1901: I0815 06:24:04.929061 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.929065 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.929806 29935 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.929888 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.929912 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.930114 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.930212 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x44ab2c20)  to GradNodeAccumulation (addr: 0x1ad62430)
1901: I0815 06:24:04.930341 29935 backward.cc:459] Run in Grad
1901: I0815 06:24:04.930351 29935 backward.cc:113] Start Backward
1901: I0815 06:24:04.930364 29935 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x44ab2c20 to ptr: 0x440672f0
1901: I0815 06:24:04.930373 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:04.930404 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.930428 29935 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ad62430 to ptr: 0x42a33bd0
1901: I0815 06:24:04.930449 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x440672f0
1901: I0815 06:24:04.930455 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:04.930485 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.930613 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.930622 29935 backward.cc:335] Node: NanmedianGradNode addr:0x440672f0, Found pending node: GradNodeAccumulation addr: 0x42a33bd0
1901: I0815 06:24:04.930627 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:04.930644 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x42a33bd0
1901: I0815 06:24:04.930650 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.930653 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:04.930658 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:04.930662 29935 backward.cc:435] Finish Backward
1901: I0815 06:24:04.931567 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.931645 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.931670 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.931830 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.933279 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.933343 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.933367 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.937623 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.937650 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.938690 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.938715 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.944327 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.944429 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.944458 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.944695 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.945315 29935 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.945397 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.945425 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.945595 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.946193 29935 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.946274 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.946311 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.946475 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.947042 29935 eager.cc:133] Tensor(generated_tensor_3) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.947118 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.947145 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.947321 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.947919 29935 eager.cc:133] Tensor(generated_tensor_4) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.947970 29935 auto_growth_best_fit_allocator.cc:159] Not found and reallocate 1024(0x7fdec0019200), and remaining 0
1901: I0815 06:24:04.948026 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.948052 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.948539 29935 eager.cc:133] Tensor(generated_tensor_5) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.948613 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.948639 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.949149 29935 eager.cc:133] Tensor(generated_tensor_6) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.949222 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.949249 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.949733 29935 eager.cc:133] Tensor(generated_tensor_7) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.949805 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.949831 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.950347 29935 eager.cc:133] Tensor(generated_tensor_8) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.950419 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.950446 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.950639 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.951200 29935 eager.cc:133] Tensor(generated_tensor_9) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.951275 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.951313 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.951480 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.952080 29935 eager.cc:133] Tensor(generated_tensor_10) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.952155 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.952183 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.952347 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.952924 29935 eager.cc:133] Tensor(generated_tensor_11) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.953001 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.953028 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.953193 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.953796 29935 eager.cc:133] Tensor(generated_tensor_12) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.953871 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.953899 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.954080 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.954663 29935 eager.cc:133] Tensor(generated_tensor_13) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.954739 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.954767 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.954933 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.955549 29935 eager.cc:133] Tensor(generated_tensor_14) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.955626 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.955654 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.955817 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.956426 29935 eager.cc:133] Tensor(generated_tensor_15) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.956503 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.956532 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.956692 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.957321 29935 eager.cc:133] Tensor(generated_tensor_16) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.957397 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.957425 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.957602 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.958163 29935 eager.cc:133] Tensor(generated_tensor_17) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.958240 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.958267 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.958443 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.958956 29935 eager.cc:133] Tensor(generated_tensor_18) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.959010 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.959031 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.959148 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.959746 29935 eager.cc:133] Tensor(generated_tensor_19) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.959816 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.959841 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.959983 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.960628 29935 eager.cc:133] Tensor(generated_tensor_20) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.960704 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.960731 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.960937 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.961618 29935 eager.cc:133] Tensor(generated_tensor_21) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.961689 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.961714 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.961879 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=2, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.962546 29935 eager.cc:133] Tensor(generated_tensor_22) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.962616 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.962641 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.962822 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.963464 29935 eager.cc:133] Tensor(generated_tensor_23) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.963534 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.963559 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.963724 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.964363 29935 eager.cc:133] Tensor(generated_tensor_24) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.964433 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.964458 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.964660 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.965263 29935 eager.cc:133] Tensor(generated_tensor_25) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.965341 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.965368 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.965538 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.966145 29935 eager.cc:133] Tensor(generated_tensor_26) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.966214 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.966239 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.966416 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.967051 29935 eager.cc:133] Tensor(generated_tensor_27) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:04.967119 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.967144 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.967321 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.967957 29935 eager.cc:133] Tensor(generated_tensor_28) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.968026 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.968051 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.968246 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.968870 29935 eager.cc:133] Tensor(generated_tensor_29) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.968940 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.968966 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.969134 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=12, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.969743 29935 eager.cc:133] Tensor(generated_tensor_30) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.969813 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.969838 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.970016 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.970620 29935 eager.cc:133] Tensor(generated_tensor_31) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.970690 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.970714 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.970880 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=10, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.971479 29935 eager.cc:133] Tensor(generated_tensor_32) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.971549 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.971575 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.971742 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.972352 29935 eager.cc:133] Tensor(generated_tensor_33) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.972422 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.972447 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.972611 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=30, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.973219 29935 eager.cc:133] Tensor(generated_tensor_34) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.973290 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.973323 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.973489 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.974105 29935 eager.cc:133] Tensor(generated_tensor_35) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.974175 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.974200 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.974376 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=60, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.974978 29935 eager.cc:133] Tensor(generated_tensor_36) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.975047 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.975072 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.975240 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.975957 29935 eager.cc:133] Tensor(generated_tensor_37) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:04.976037 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:04.976064 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:04.976251 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.978148 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.978170 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.978792 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.978811 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.979414 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.979434 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.980046 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.980063 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.980912 29998 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 13101486947023862968 to 13702201230157956386 , after update, data is {current : 0, peak : 1260}.
1901: I0815 06:24:04.980924 29998 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 13101486947023862968 to 13702201230157956386 , after update, data is {current : 20, peak : 24}.
1901: I0815 06:24:04.981256 30001 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 13702201230157956386 to 12945557604293332363 , after update, data is {current : 0, peak : 260}.
1901: I0815 06:24:04.981266 30001 thread_data_registry.h:135] Add data {current : 20, peak : 24} from thread 13702201230157956386 to 7110887826760878444 , after update, data is {current : 22, peak : 24}.
1901: I0815 06:24:04.981271 30001 thread_data_registry.h:135] Add data {current : 0, peak : 1260} from thread 13702201230157956386 to 7110887826760878444 , after update, data is {current : -18, peak : 330659}.
1901: I0815 06:24:04.981822 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:04.981845 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.983824 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.984254 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.984683 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.985098 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.985561 29935 op_desc.cc:1111] CompileTime infer shape on fetch_v2
1901: I0815 06:24:04.986238 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.986253 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.986258 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.990213 29935 feed_fetch_method.cc:53] SetFeedVariable name=feed index=0
1901: I0815 06:24:04.990258 29935 interpreter_util.cc:1169] Creating Variables
1901: I0815 06:24:04.990268 29935 scope.cc:202] Create variable X
1901: I0815 06:24:04.990272 29935 interpreter_util.cc:1206] Create Variable X locally, which pointer is 0x47bbccb0 type is 7
1901: I0815 06:24:04.990278 29935 interpreter_util.cc:1201] Create Variable feed global, which pointer is 0x43c84d80 type is 9
1901: I0815 06:24:04.990286 29935 interpreter_util.cc:1201] Create Variable fetch global, which pointer is 0x43c852b0 type is 10
1901: I0815 06:24:04.990290 29935 scope.cc:202] Create variable nanmedian_0.tmp_0
1901: I0815 06:24:04.990293 29935 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_0 locally, which pointer is 0x47bbc2e0 type is 7
1901: I0815 06:24:04.990305 29935 scope.cc:202] Create variable nanmedian_0.tmp_1
1901: I0815 06:24:04.990309 29935 interpreter_util.cc:1206] Create Variable nanmedian_0.tmp_1 locally, which pointer is 0x47bb8de0 type is 7
1901: I0815 06:24:04.990314 29935 scope.cc:202] Create variable nanmedian_1.tmp_0
1901: I0815 06:24:04.990316 29935 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_0 locally, which pointer is 0x47bbce30 type is 7
1901: I0815 06:24:04.990320 29935 scope.cc:202] Create variable nanmedian_1.tmp_1
1901: I0815 06:24:04.990324 29935 interpreter_util.cc:1206] Create Variable nanmedian_1.tmp_1 locally, which pointer is 0x47bbcf10 type is 7
1901: I0815 06:24:04.990327 29935 scope.cc:202] Create variable nanmedian_2.tmp_0
1901: I0815 06:24:04.990330 29935 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_0 locally, which pointer is 0x47bbcff0 type is 7
1901: I0815 06:24:04.990334 29935 scope.cc:202] Create variable nanmedian_2.tmp_1
1901: I0815 06:24:04.990337 29935 interpreter_util.cc:1206] Create Variable nanmedian_2.tmp_1 locally, which pointer is 0x47bbd2e0 type is 7
1901: I0815 06:24:04.990342 29935 scope.cc:202] Create variable nanmedian_3.tmp_0
1901: I0815 06:24:04.990345 29935 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_0 locally, which pointer is 0x47bbd4f0 type is 7
1901: I0815 06:24:04.990350 29935 scope.cc:202] Create variable nanmedian_3.tmp_1
1901: I0815 06:24:04.990353 29935 interpreter_util.cc:1206] Create Variable nanmedian_3.tmp_1 locally, which pointer is 0x47bbd750 type is 7
1901: I0815 06:24:04.990357 29935 scope.cc:202] Create variable nanmedian_4.tmp_0
1901: I0815 06:24:04.990360 29935 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_0 locally, which pointer is 0x47bbd9b0 type is 7
1901: I0815 06:24:04.990363 29935 scope.cc:202] Create variable nanmedian_4.tmp_1
1901: I0815 06:24:04.990366 29935 interpreter_util.cc:1206] Create Variable nanmedian_4.tmp_1 locally, which pointer is 0x47bbdc10 type is 7
1901: I0815 06:24:04.990478 29935 interpreter_util.cc:594] Static build: 0
1901: I0815 06:24:04.990484 29935 conditional_block_op_helper.cc:105] Found conditional_block op num: 0, conditional_block_grad op num: 0
1901: I0815 06:24:04.990489 29935 pylayer_op_helper.cc:103] Found pylayer op num: 0, pylayer_grad op num: 0
1901: I0815 06:24:04.990492 29935 while_op_helper.cc:154] Found while op num: 0, while grad op num: 0
1901: I0815 06:24:04.990540 29935 operator.cc:2295] op type:feed, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990553 29935 interpreter_util.cc:844] feed : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990600 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990608 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990622 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.990765 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.990947 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990958 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.990972 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.991080 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.991253 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991263 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991276 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.991386 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.991554 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991564 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991577 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.991698 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.991879 29935 operator.cc:2295] op type:nanmedian, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991889 29935 interpreter_util.cc:844] nanmedian : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.991904 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:04.992022 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:04.992198 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992208 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992223 29935 scope.cc:202] Create variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.992229 29935 data_transfer.cc:396] Create Variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4421ef50Variable Type 7
1901: I0815 06:24:04.992245 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_0.tmp_0(Place(gpu:0)) -> nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.992259 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.992278 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.992293 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.992347 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.992363 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_0.tmp_0_device_Place(gpu:0)_Place(cpu)'s 0 column.
1901: I0815 06:24:04.992403 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992412 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992424 29935 scope.cc:202] Create variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.992431 29935 data_transfer.cc:396] Create Variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4421dd90Variable Type 7
1901: I0815 06:24:04.992442 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_1.tmp_0(Place(gpu:0)) -> nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.992455 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.992471 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.992483 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.992512 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.992534 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_1.tmp_0_device_Place(gpu:0)_Place(cpu)'s 1 column.
1901: I0815 06:24:04.992573 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992581 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992594 29935 scope.cc:202] Create variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.992600 29935 data_transfer.cc:396] Create Variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x47bce360Variable Type 7
1901: I0815 06:24:04.992612 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_2.tmp_0(Place(gpu:0)) -> nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.992623 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.992638 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.992650 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.992678 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.992689 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_2.tmp_0_device_Place(gpu:0)_Place(cpu)'s 2 column.
1901: I0815 06:24:04.992725 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992733 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992744 29935 scope.cc:202] Create variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.992750 29935 data_transfer.cc:396] Create Variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x4421efb0Variable Type 7
1901: I0815 06:24:04.992760 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_3.tmp_0(Place(gpu:0)) -> nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.992772 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.992786 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.992797 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.992825 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.992835 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_3.tmp_0_device_Place(gpu:0)_Place(cpu)'s 3 column.
1901: I0815 06:24:04.992867 29935 operator.cc:2295] op type:fetch_v2, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992875 29935 interpreter_util.cc:844] fetch_v2 : finally selected kernel_key: {data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(cpu)]; library_type[PLAIN]}
1901: I0815 06:24:04.992887 29935 scope.cc:202] Create variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)
1901: I0815 06:24:04.992893 29935 data_transfer.cc:396] Create Variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu) locally, which pointer is 0x43523d30Variable Type 7
1901: I0815 06:24:04.992903 29935 data_transfer.cc:439] Insert memcpy_d2h with nanmedian_4.tmp_0(Place(gpu:0)) -> nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)(Place(cpu)).
1901: I0815 06:24:04.992914 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.992928 29935 operator.cc:2295] op type:memcpy_d2h, expected_kernel_key:{data_type[float]; data_layout[Undefined(AnyLayout)]; place[Place(gpu:0)]; library_type[PLAIN]}
1901: I0815 06:24:04.992939 29935 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:04.992966 29935 data_transfer.cc:232] Run memcpy_d2h done.
1901: I0815 06:24:04.992976 29935 fetch_v2_op.cc:138] Fetch variable nanmedian_4.tmp_0_device_Place(gpu:0)_Place(cpu)'s 4 column.
1901: I0815 06:24:04.993474 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.993503 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.993520 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.993537 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: I0815 06:24:04.993553 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: memcpy_d2h; inputs: X; attributes: dst_place_type; outputs: Out
1901: IR before lowering = {
1901:     (%0) = "pd_op.data" () {dtype:(pd_op.DataType)float32,name:"X",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> builtin.tensor<2x3x4x5xf32>
1901:     (%1, %2) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%3, %4) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%5, %6) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%7, %8) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%9, %10) = "pd_op.nanmedian" (%0) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,mode:"min",stop_gradient:[true,true]} : (builtin.tensor<2x3x4x5xf32>) -> builtin.tensor<f32>, builtin.tensor<i64>
1901:     (%11) = "pd_op.fetch" (%1) {col:(Int32)0,name:"fetch0",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%12) = "pd_op.fetch" (%3) {col:(Int32)1,name:"fetch1",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%13) = "pd_op.fetch" (%5) {col:(Int32)2,name:"fetch2",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%14) = "pd_op.fetch" (%7) {col:(Int32)3,name:"fetch3",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901:     (%15) = "pd_op.fetch" (%9) {col:(Int32)4,name:"fetch4",persistable:[true],stop_gradient:[true]} : (builtin.tensor<f32>) -> builtin.tensor<f32>
1901: }
1901: 
1901: IR after lowering = {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: I0815 06:24:04.998806 29935 pir_interpreter.cc:161] PirInterpreter(): 0x44a33530 on Place(gpu:0)
1901: I0815 06:24:04.998838 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_1
1901: I0815 06:24:04.998848 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_2
1901: I0815 06:24:04.998857 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_3
1901: I0815 06:24:04.998864 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_4
1901: I0815 06:24:04.998872 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_5
1901: I0815 06:24:04.998879 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_6
1901: I0815 06:24:04.998886 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_7
1901: I0815 06:24:04.998893 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_8
1901: I0815 06:24:04.998901 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_9
1901: I0815 06:24:04.998907 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_10
1901: I0815 06:24:04.998915 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_11
1901: I0815 06:24:04.998922 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_12
1901: I0815 06:24:04.998935 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_14
1901: I0815 06:24:04.998948 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_16
1901: I0815 06:24:04.998960 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_18
1901: I0815 06:24:04.998967 29935 scope.cc:202] Create variable 0x44a335301723703044998825372_inner_var_20
1901: I0815 06:24:04.999203 29935 feed_fetch_method.cc:53] SetFeedVariable name=X index=0
1901: ======================== The network executed by pir interpreter ========================
1901: {
1901:     (%0) = "data(phi_kernel)" () {dtype:(pd_op.DataType)float32,kernel_key:<backend:Undefined|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"data",name:"X",op_name:"pd_op.data",place:(pd_op.Place)Place(undefined:0),shape:(pd_op.IntArray)[2,3,4,5],stop_gradient:[true]} : () -> undefined_tensor<2x3x4x5xf32>
1901:     (%1) = "shadow_feed(phi_kernel)" (%0) {kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"shadow_feed",op_name:"pd_op.shadow_feed"} : (undefined_tensor<2x3x4x5xf32>) -> gpu_tensor<2x3x4x5xf32>
1901:     (%2, %3) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%4, %5) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%6, %7) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%8, %9) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%10, %11) = "nanmedian(phi_kernel)" (%1) {axis:(pd_op.IntArray)[0,1,2,3],keepdim:false,kernel_key:<backend:GPU|layout:NCHW|dtype:float32>,kernel_name:"nanmedian",mode:"min",op_name:"pd_op.nanmedian",stop_gradient:[true,true]} : (gpu_tensor<2x3x4x5xf32>) -> gpu_tensor<f32>, gpu_tensor<i64>
1901:     (%12) = "memcpy_d2h(phi_kernel)" (%2) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%13) = "fetch(phi_kernel)" (%12) {col:(Int32)0,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch0",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%14) = "memcpy_d2h(phi_kernel)" (%4) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%15) = "fetch(phi_kernel)" (%14) {col:(Int32)1,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch1",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%16) = "memcpy_d2h(phi_kernel)" (%6) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%17) = "fetch(phi_kernel)" (%16) {col:(Int32)2,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch2",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%18) = "memcpy_d2h(phi_kernel)" (%8) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%19) = "fetch(phi_kernel)" (%18) {col:(Int32)3,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch3",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%20) = "memcpy_d2h(phi_kernel)" (%10) {dst_place_type:(Int32)0,kernel_key:<backend:GPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"memcpy_d2h",op_name:"pd_op.memcpy_d2h"} : (gpu_tensor<f32>) -> cpu_tensor<f32>
1901:     (%21) = "fetch(phi_kernel)" (%20) {col:(Int32)4,kernel_key:<backend:CPU|layout:Undefined(AnyLayout)|dtype:float32>,kernel_name:"fetch",name:"fetch4",op_name:"pd_op.fetch",persistable:[true],stop_gradient:[true]} : (cpu_tensor<f32>) -> cpu_tensor<f32>
1901: }
1901: 
1901: ======================== The instruction executed by pir interpreter ========================
1901: {outputs} =  instruction_name[idx] ({inputs})
1901: 0: ( 1 )  = pd_op.shadow_feed ( 0 ) 
1901: 1: ( 3 ) ( 2 )  = pd_op.nanmedian ( 1 ) 
1901: 2: ( 5 ) ( 4 )  = pd_op.nanmedian ( 1 ) 
1901: 3: ( 7 ) ( 6 )  = pd_op.nanmedian ( 1 ) 
1901: 4: ( 9 ) ( 8 )  = pd_op.nanmedian ( 1 ) 
1901: 5: ( 11 ) ( 10 )  = pd_op.nanmedian ( 1 ) 
1901: 6: ( 12 )  = pd_op.memcpy_d2h ( 2 ) 
1901: 7: ( 13 )  = pd_op.fetch ( 12 ) 
1901: 8: ( 14 )  = pd_op.memcpy_d2h ( 4 ) 
1901: 9: ( 15 )  = pd_op.fetch ( 14 ) 
1901: 10: ( 16 )  = pd_op.memcpy_d2h ( 6 ) 
1901: 11: ( 17 )  = pd_op.fetch ( 16 ) 
1901: 12: ( 18 )  = pd_op.memcpy_d2h ( 8 ) 
1901: 13: ( 19 )  = pd_op.fetch ( 18 ) 
1901: 14: ( 20 )  = pd_op.memcpy_d2h ( 10 ) 
1901: 15: ( 21 )  = pd_op.fetch ( 20 ) 
1901: ---------------------------var_id -> var_name -> variable*---------------------------
1901: 0 -> X -> 0x47cb86e0
1901: 1 -> 0x44a335301723703044998825372_inner_var_1 -> 0x47bf5480
1901: 2 -> 0x44a335301723703044998825372_inner_var_2 -> 0x4421b400
1901: 3 -> 0x44a335301723703044998825372_inner_var_3 -> 0x47bba420
1901: 4 -> 0x44a335301723703044998825372_inner_var_4 -> 0x4421b3c0
1901: 5 -> 0x44a335301723703044998825372_inner_var_5 -> 0x47c6b450
1901: 6 -> 0x44a335301723703044998825372_inner_var_6 -> 0x47be8b60
1901: 7 -> 0x44a335301723703044998825372_inner_var_7 -> 0x44b0eb20
1901: 8 -> 0x44a335301723703044998825372_inner_var_8 -> 0x47be8520
1901: 9 -> 0x44a335301723703044998825372_inner_var_9 -> 0x47bf0380
1901: 10 -> 0x44a335301723703044998825372_inner_var_10 -> 0x47c1a670
1901: 11 -> 0x44a335301723703044998825372_inner_var_11 -> 0x443eed20
1901: 12 -> 0x44a335301723703044998825372_inner_var_12 -> 0x47be94f0
1901: 13 -> fetch0@fetch -> 0x47bf1e20
1901: 14 -> 0x44a335301723703044998825372_inner_var_14 -> 0x47bd5b30
1901: 15 -> fetch1@fetch -> 0x47cb7da0
1901: 16 -> 0x44a335301723703044998825372_inner_var_16 -> 0x47c6ae80
1901: 17 -> fetch2@fetch -> 0x4351f460
1901: 18 -> 0x44a335301723703044998825372_inner_var_18 -> 0x42a31320
1901: 19 -> fetch3@fetch -> 0x47c656b0
1901: 20 -> 0x44a335301723703044998825372_inner_var_20 -> 0x47ce9100
1901: 21 -> fetch4@fetch -> 0x47d08780
1901: 
1901: 
1901: ======================= The dependency of all instruction ========================
1901: id -> down_stream_id
1901: 0 -> 1 2 3 4 5 
1901: 1 -> 6 
1901: 2 -> 8 
1901: 3 -> 10 
1901: 4 -> 12 
1901: 5 -> 14 
1901: 6 -> 7 
1901: 8 -> 9 
1901: 10 -> 11 
1901: 12 -> 13 
1901: 14 -> 15 
1901: 
1901: 
1901: ======================== pir interpreter trace order ========================
1901: 
1901: Leaf nodes: 0[pd_op.shadow_feed]->
1901: 0 downstreams: 1[pd_op.nanmedian]->2[pd_op.nanmedian]->3[pd_op.nanmedian]->4[pd_op.nanmedian]->5[pd_op.nanmedian]->
1901: 1 downstreams: 6[pd_op.memcpy_d2h]->
1901: 2 downstreams: 8[pd_op.memcpy_d2h]->
1901: 3 downstreams: 10[pd_op.memcpy_d2h]->
1901: 4 downstreams: 12[pd_op.memcpy_d2h]->
1901: 5 downstreams: 14[pd_op.memcpy_d2h]->
1901: 6 downstreams: 7[pd_op.fetch]->
1901: 7 downstreams: 
1901: 8 downstreams: 9[pd_op.fetch]->
1901: 9 downstreams: 
1901: 10 downstreams: 11[pd_op.fetch]->
1901: 11 downstreams: 
1901: 12 downstreams: 13[pd_op.fetch]->
1901: 13 downstreams: 
1901: 14 downstreams: 15[pd_op.fetch]->
1901: 15 downstreams: 
1901: I0815 06:24:05.000416 30002 nonblocking_threadpool.h:251] HostTasks_thread_0 started 
1901: I0815 06:24:05.000434 30003 nonblocking_threadpool.h:251] HostTasks_thread_1 started 
1901: I0815 06:24:05.000453 30004 nonblocking_threadpool.h:251] HostTasks_thread_2 started 
1901: I0815 06:24:05.000500 30005 nonblocking_threadpool.h:251] HostTasks_thread_3 started 
1901: I0815 06:24:05.000530 30006 nonblocking_threadpool.h:251] DeviceKernelLaunch_thread_0 started 
1901: I0815 06:24:05.000558 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_1:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.000594 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:0 name:pd_op.shadow_feed type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.shadow_feed), inputs:{X:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}.
1901: I0815 06:24:05.000634 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_3:[dtype=;place=;dim=;lod={};, 0x44a335301723703044998825372_inner_var_2:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.000852 30006 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.001009 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:1 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_3:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x44a335301723703044998825372_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:05.001060 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_5:[dtype=;place=;dim=;lod={};, 0x44a335301723703044998825372_inner_var_4:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001080 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_12:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001116 30005 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:05.001184 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:6 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_2:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001204 30006 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.001243 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001266 30005 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.001273 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:7 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_12:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch0@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001380 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:2 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_5:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x44a335301723703044998825372_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:05.001427 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_7:[dtype=;place=;dim=;lod={};, 0x44a335301723703044998825372_inner_var_6:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001435 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_14:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001453 30005 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:05.001515 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:8 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_4:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001559 30006 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.001582 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001601 30005 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.001607 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:9 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_14:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch1@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001719 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:3 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_7:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x44a335301723703044998825372_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:05.001763 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_9:[dtype=;place=;dim=;lod={};, 0x44a335301723703044998825372_inner_var_8:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001770 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_16:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.001793 30005 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:05.001829 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:10 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_6:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001868 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001886 30005 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.001891 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:11 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_16:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch2@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.001945 30006 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.002095 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:4 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_9:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x44a335301723703044998825372_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:05.002137 30006 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: Before: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_11:[dtype=;place=;dim=;lod={};, 0x44a335301723703044998825372_inner_var_10:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.002143 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_18:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.002159 30005 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:05.002190 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:12 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_8:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002265 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002283 30005 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002290 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:13 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_18:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch3@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002318 30006 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.002472 30006 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:5 name:pd_op.nanmedian type:kGpuAsync runs on DeviceKernelLaunch_thread_0
1901: After: Place(gpu:0) Op(pd_op.nanmedian), inputs:{0x44a335301723703044998825372_inner_var_1:[dtype=float;place=Place(gpu:0);dim=2, 3, 4, 5;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_11:[dtype=int64_t;place=Place(gpu:0);dim=;lod={};, 0x44a335301723703044998825372_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}.
1901: I0815 06:24:05.002521 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: Before: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_20:[dtype=;place=;dim=;lod={};]}.
1901: I0815 06:24:05.002537 30005 tensor_utils.cc:57] TensorCopy  from Place(gpu:0) to Place(cpu)
1901: I0815 06:24:05.002561 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:14 name:pd_op.memcpy_d2h type:kGpuSync runs on HostTasks_thread_3
1901: After: Place(gpu:0) Op(pd_op.memcpy_d2h), inputs:{0x44a335301723703044998825372_inner_var_10:[dtype=float;place=Place(gpu:0);dim=;lod={};]}, outputs:{0x44a335301723703044998825372_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002595 30005 pir_interpreter.cc:1876] 
1901: begin: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: Before: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002614 30005 tensor_utils.cc:57] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002619 30005 pir_interpreter.cc:1916] 
1901: done: RunInstructionBase OP id:15 name:pd_op.fetch type:kCpuSync runs on HostTasks_thread_3
1901: After: Place(cpu) Op(pd_op.fetch), inputs:{0x44a335301723703044998825372_inner_var_20:[dtype=float;place=Place(cpu);dim=;lod={};]}, outputs:{fetch4@fetch:[dtype=float;place=Place(cpu);dim=;lod={};]}.
1901: I0815 06:24:05.002648 29935 pir_interpreter.cc:1766] main_thread_blocker_(0x44a336a0) got event_name: TaskCompletion
1901: I0815 06:24:05.002676 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002701 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002712 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002722 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.002733 29935 tensor_util.cc:48] TensorCopy  from Place(cpu) to Place(cpu)
1901: I0815 06:24:05.004334 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad62430 for it.
1901: I0815 06:24:05.004422 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.004449 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.004629 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=20, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.004735 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x444d60c0)  to GradNodeAccumulation (addr: 0x1ad62430)
1901: I0815 06:24:05.004849 29935 backward.cc:459] Run in Grad
1901: I0815 06:24:05.004859 29935 backward.cc:113] Start Backward
1901: I0815 06:24:05.004873 29935 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x444d60c0 to ptr: 0x47ccf9d0
1901: I0815 06:24:05.004884 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:05.004915 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.004940 29935 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ad62430 to ptr: 0x42a33bd0
1901: I0815 06:24:05.004961 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47ccf9d0
1901: I0815 06:24:05.004968 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:05.004999 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.005074 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.005082 29935 backward.cc:335] Node: NanmedianGradNode addr:0x47ccf9d0, Found pending node: GradNodeAccumulation addr: 0x42a33bd0
1901: I0815 06:24:05.005088 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:05.005115 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x42a33bd0
1901: I0815 06:24:05.005121 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.005125 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.005131 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.005136 29935 backward.cc:435] Finish Backward
1901: I0815 06:24:05.005774 29935 dygraph_functions.cc:77638] Running AD API: uniform
1901: I0815 06:24:05.005791 29935 dygraph_functions.cc:77659] { Input: []} 
1901: I0815 06:24:05.005875 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.005899 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.006036 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.006109 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x47ccf9d0)  to GradNodeAccumulation (addr: 0x1ad62430)
1901: I0815 06:24:05.006191 29935 backward.cc:442] Run in Backward
1901: I0815 06:24:05.006198 29935 backward.cc:113] Start Backward
1901: I0815 06:24:05.006206 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:05.006237 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.006260 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47ccf9d0
1901: I0815 06:24:05.006269 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:05.006297 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.006366 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.006393 29935 backward.cc:335] Node: NanmedianGradNode addr:0x47ccf9d0, Found pending node: GradNodeAccumulation addr: 0x1ad62430
1901: I0815 06:24:05.006402 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:05.006419 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ad62430
1901: I0815 06:24:05.006426 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.006430 29935 accumulation_node.cc:40] Move Tensor ptr: 0x44b10090
1901: I0815 06:24:05.006434 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.006438 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.006873 29935 eager.cc:133] Tensor(generated_tensor_1) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:05.006983 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.007009 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.007172 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x444d6090)  to GradNodeAccumulation (addr: 0x1ad9fd50)
1901: I0815 06:24:05.007269 29935 backward.cc:442] Run in Backward
1901: I0815 06:24:05.007278 29935 backward.cc:113] Start Backward
1901: I0815 06:24:05.007285 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:05.007329 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=1, vec_size=4, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.007354 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x444d6090
1901: I0815 06:24:05.007364 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:05.007391 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.007438 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.007463 29935 backward.cc:335] Node: NanmedianGradNode addr:0x444d6090, Found pending node: GradNodeAccumulation addr: 0x1ad9fd50
1901: I0815 06:24:05.007472 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:05.007488 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x1ad9fd50
1901: I0815 06:24:05.007495 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.007500 29935 accumulation_node.cc:40] Move Tensor ptr: 0x47c15c90
1901: I0815 06:24:05.007503 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.007508 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.008205 29935 eager.cc:133] Tensor(generated_tensor_2) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:05.008286 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.008322 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.008512 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=5, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.008606 29935 grad_node_info.cc:394] Add Edges for slot: 0, the Edge is from NanmedianGradNode (addr: 0x444d6090)  to GradNodeAccumulation (addr: 0x1ad9fd50)
1901: I0815 06:24:05.008724 29935 backward.cc:459] Run in Grad
1901: I0815 06:24:05.008735 29935 backward.cc:113] Start Backward
1901: I0815 06:24:05.008747 29935 general_grad.h:515] Copied Node: NanmedianGradNode ptr: 0x444d6090 to ptr: 0x47ccf7f0
1901: I0815 06:24:05.008756 29935 backward.cc:197] Fill grad input tensor 0 with 1.0
1901: I0815 06:24:05.008787 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=4, vec_size=1, block_size=64, grid_size=1, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.008812 29935 general_grad.h:562] Copied Node: GradNodeAccumulation ptr: 0x1ad9fd50 to ptr: 0x42a33bd0
1901: I0815 06:24:05.008831 29935 backward.cc:255] Preparing GradNode:NanmedianGradNode addr:0x47ccf7f0
1901: I0815 06:24:05.008838 29935 nodes.cc:26680] Running AD API GRAD: nanmedian_grad
1901: I0815 06:24:05.008867 29935 nodes.cc:26740] { Input: [ 
1901: ( out_grad , [[ Not specified tensor log level ]]),  
1901: ( x , [[ Not specified tensor log level ]]),  
1901: ( medians , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.008989 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.008999 29935 backward.cc:335] Node: NanmedianGradNode addr:0x47ccf7f0, Found pending node: GradNodeAccumulation addr: 0x42a33bd0
1901: I0815 06:24:05.009006 29935 backward.cc:376] Sum or Move grad inputs for edge slot: 0, rank: 0
1901: I0815 06:24:05.009020 29935 backward.cc:255] Preparing GradNode:GradNodeAccumulation addr:0x42a33bd0
1901: I0815 06:24:05.009028 29935 accumulation_node.cc:157] Running AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.009032 29935 accumulation_node.cc:194] Finish AD API Grad: GradNodeAccumulation
1901: I0815 06:24:05.009037 29935 backward.cc:306] retain_graph is false, need to clear the TensorWrapper of nodes.
1901: I0815 06:24:05.009042 29935 backward.cc:435] Finish Backward
1901: I0815 06:24:05.009953 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:05.010031 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.010057 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.010215 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=120, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.011564 29935 eager.cc:133] Tensor(generated_tensor_0) have not GradNode, add GradNodeAccumulation0x1ad9fd50 for it.
1901: I0815 06:24:05.011608 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.011627 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.013695 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:05.013721 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:05.014626 29935 op_desc.cc:1111] CompileTime infer shape on nanmedian
1901: I0815 06:24:05.014649 29935 infershape_utils.cc:546] BuildInferMetaContext: op kernel signature - Kernel Signature - name: nanmedian; inputs: X; attributes: axis, keepdim, mode; outputs: Out, MedianIndex
1901: I0815 06:24:05.015411 29935 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 06:24:05.015429 29935 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 06:24:05.015528 29935 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 06:24:05.015537 29935 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 06:24:05.015596 29935 dygraph_functions.cc:33459] Running AD API: full
1901: I0815 06:24:05.015604 29935 dygraph_functions.cc:33480] { Input: []} 
1901: I0815 06:24:05.015648 29935 dygraph_functions.cc:82227] Running AD API: arange
1901: I0815 06:24:05.015682 29935 dygraph_functions.cc:82291] { Input: [ 
1901: ( start , [[ Not specified tensor log level ]]),  
1901: ( end , [[ Not specified tensor log level ]]),  
1901: ( step , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.015826 29935 dygraph_functions.cc:62112] Running AD API: reshape
1901: I0815 06:24:05.015847 29935 dygraph_functions.cc:62169] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.015867 29935 api.cc:40807] Perform View between Output and Input Tensor, share allocation and inplace version.
1901: I0815 06:24:05.015928 29935 dygraph_functions.cc:14224] Running AD API: cast
1901: I0815 06:24:05.015944 29935 dygraph_functions.cc:14268] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.016013 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=200, vec_size=1, block_size=64, grid_size=4, limit_blocks=2147483647, limit_threads=512
1901: I0815 06:24:05.016074 29935 dygraph_functions.cc:54355] Running AD API: nanmedian
1901: I0815 06:24:05.016090 29935 dygraph_functions.cc:54415] { Input: [ 
1901: ( x , [[ Not specified tensor log level ]]), ]} 
1901: I0815 06:24:05.016271 29935 gpu_launch_config.h:156] Get 1-D launch config: numel=100, vec_size=1, block_size=64, grid_size=2, limit_blocks=2147483647, limit_threads=512
1901: test_nanmedian failed
1901:  ss.E.................
1901: ======================================================================
1901: ERROR: test_check_output (test_nanmedian.TestNanmedianFP16Op)
1901: ----------------------------------------------------------------------
1901: Traceback (most recent call last):
1901:   File "/home/code/Paddle/build/test/legacy_test/test_nanmedian.py", line 617, in test_check_output
1901:     self.check_output(check_pir=True)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2934, in check_output
1901:     res = self.check_output_with_place(
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2786, in check_output_with_place
1901:     symbol_checker.check()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2637, in check
1901:     self.infer_and_compare_symbol()
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 2645, in infer_and_compare_symbol
1901:     self.op_test._infer_and_compare_symbol(place)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1712, in _infer_and_compare_symbol
1901:     self._compare_symbol(program, outs)
1901:   File "/home/code/Paddle/build/test/legacy_test/op_test.py", line 1633, in _compare_symbol
1901:     shape_analysis.get_shape_or_data_for_var(
1901: RuntimeError: (PreconditionNotMet) The op operand index [1] must less than operands size[1].
1901:   [Hint: Expected index < num_operands_, but received index:1 >= num_operands_:1.] (at /home/code/Paddle/paddle/pir/src/core/operation.cc:420)
1901: 
1901: 
1901: ----------------------------------------------------------------------
1901: Ran 21 tests in 2.990s
1901: 
1901: FAILED (errors=1, skipped=2)
1901: 
1901: I0815 06:24:05.017532 29935 mmap_allocator.cc:348] PID: 29935, MemoryMapFdSet: set size - 0
1901: I0815 06:24:05.029287 29935 mmap_allocator.cc:348] PID: 29935, MemoryMapFdSet: set size - 0
1901: I0815 06:24:05.088068 30005 thread_data_registry.h:135] Add data {current : -20, peak : 0} from thread 8217336541708056440 to 5579797919543060382 , after update, data is {current : 0, peak : 1252}.
1901: I0815 06:24:05.088086 30005 thread_data_registry.h:135] Add data {current : 0, peak : 4} from thread 8217336541708056440 to 5579797919543060382 , after update, data is {current : 0, peak : 16}.
1901: I0815 06:24:05.088390 30006 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 5579797919543060382 to 12945557604293332363 , after update, data is {current : 0, peak : 260}.
1901: I0815 06:24:05.088408 30006 thread_data_registry.h:135] Add data {current : 0, peak : 16} from thread 5579797919543060382 to 7110887826760878444 , after update, data is {current : 22, peak : 24}.
1901: I0815 06:24:05.088414 30006 thread_data_registry.h:135] Add data {current : 0, peak : 1252} from thread 5579797919543060382 to 7110887826760878444 , after update, data is {current : -18, peak : 330659}.
1901: I0815 06:24:05.089030 29979 thread_data_registry.h:135] Add data {current : 22, peak : 24} from thread 7110887826760878444 to 5242912019445735189 , after update, data is {current : 26, peak : 26}.
1901: I0815 06:24:05.089054 29979 thread_data_registry.h:135] Add data {current : -18, peak : 330659} from thread 7110887826760878444 to 4053306302942050912 , after update, data is {current : 0, peak : 330659}.
1901: I0815 06:24:05.089346 29983 thread_data_registry.h:135] Add data {current : 26, peak : 26} from thread 5242912019445735189 to 4053306302942050912 , after update, data is {current : 26, peak : 26}.
1901: I0815 06:24:05.089540 29984 thread_data_registry.h:135] Add data {current : 26, peak : 26} from thread 4053306302942050912 to 12945557604293332363 , after update, data is {current : 20026, peak : 40004}.
1901: I0815 06:24:05.089558 29984 thread_data_registry.h:135] Add data {current : 0, peak : 330659} from thread 4053306302942050912 to 12945557604293332363 , after update, data is {current : 162560, peak : 560633}.
1901: I0815 06:24:05.221133 29935 mmap_allocator.cc:348] PID: 29935, MemoryMapFdSet: set size - 0
1/1 Test #1901: test_nanmedian ...................***Failed   10.76 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  10.93 sec

The following tests FAILED:
	1901 - test_nanmedian (Failed)
